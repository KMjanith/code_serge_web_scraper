[
    {
        "title": "What is AWS Lambda?",
        "href": "https://docs.aws.amazon.com/lambda/latest/dg/welcome.html",
        "sections": [
            "You can use AWS Lambda to run code without provisioning or managing servers.",
            " Lambda runs your code    on a high-availability compute infrastructure and performs all of the administration of the compute resources,    including server and operating system maintenance, capacity provisioning and automatic scaling, and    logging. With Lambda, all you need to do is supply your code in one of the language runtimes that Lambda supports.",
            "You organize your code into Lambda functions. The Lambda service runs your function only when needed and scales automatically. You only pay for the compute time that you consume—there is no charge when your code is not running. For more information, see AWS Lambda Pricing.",
            "To learn how to build serverless solutions, check out the Serverless Developer Guide.",
            "Tip",
            {
                "sub_header": "When to use Lambda",
                "content": [
                    "Lambda is an ideal compute service for application scenarios that need to scale up rapidly, and scale down to      zero when not in demand. For example, you can use Lambda for:",
                    "  1.File processing: :  Use Amazon Simple Storage Service (Amazon S3) to trigger Lambda data processing in real time after an upload.",
                    "  2.Stream processing: :  Use Lambda and Amazon Kinesis to process real-time streaming data for application activity tracking, transaction order processing, clickstream analysis, data cleansing, log filtering, indexing, social media analysis, Internet of Things (IoT) device data telemetry, and metering.",
                    "  3.Web applications: :  Combine Lambda with other AWS services to build powerful web applications that automatically scale up and down and run in a highly available configuration across multiple data centers.",
                    "  4.IoT backends: :  Build serverless backends using Lambda to handle web, mobile, IoT, and third-party API requests.",
                    "  5.Mobile backends: :  Build backends using Lambda and Amazon API Gateway  to authenticate and process API requests. Use AWS Amplify to easily integrate with your iOS, Android, Web, and React Native frontends.",
                    "When using Lambda, you are responsible only for your code. Lambda manages the compute fleet that offers a      balance of memory, CPU, network, and other resources to run your code. Because Lambda manages these resources, you      cannot log in to compute instances or customize the operating system on provided        runtimes. Lambda performs operational and administrative activities on your behalf, including managing      capacity, monitoring, and logging your Lambda functions."
                ]
            },
            {
                "sub_header": "Key features",
                "content": [
                    "The following key features help you develop Lambda applications that are scalable, secure, and easily      extensible:",
                    "  1. Environment variables : \nUse environment variables to adjust your function's behavior without updating code.\n",
                    "  2.Versions : \nManage the deployment of your functions with versions, so that, for example, a new function can be used for beta testing without affecting users of the stable production version.\n",
                    "  3.Container images : \nCreate a container image for a Lambda function by using an AWS provided base image or an alternative base\n            image so that you can reuse your existing container tooling or deploy larger workloads that rely on sizable dependencies, such as machine learning.\n",
                    "  4.Layers : \nPackage libraries and other dependencies to reduce the size of deployment archives and makes it faster to deploy your code.\n",
                    "  5.Lambda extensions : \nAugment your Lambda functions with tools for monitoring, observability, security, and governance.\n",
                    "  6.Function URLs : \nAdd a dedicated HTTP(S) endpoint to your Lambda function.\n",
                    "  7.Response streaming : \nConfigure your Lambda function URLs to stream response payloads back to clients from Node.js functions, to improve time to first byte (TTFB) performance or to return larger payloads.\n",
                    "  8.Concurrency and scaling controls : \nApply fine-grained control over the scaling and responsiveness of your production applications.\n",
                    "  9.Code signing : \nVerify that only approved developers publish unaltered, trusted code in your Lambda functions \n",
                    "  10.Private networking : \nCreate a private network for resources such as databases, cache instances, or internal services.\n",
                    "  11.File system access : \nConfigure a function to mount an Amazon Elastic File System (Amazon EFS) to a local directory, so that your function code can access and modify shared resources safely and at high concurrency.\n",
                    "  12.Lambda SnapStart for Java : \nImprove startup performance for Java runtimes by up to 10x at no extra cost, typically with no changes to your function code.\n"
                ]
            }
        ]
    },
    {
        "title": "Example apps",
        "href": "https://docs.aws.amazon.com/lambda/latest/dg/example-apps.html",
        "contents": [
            {
                "title": "File-processing app",
                "href": "https://docs.aws.amazon.com/lambda/latest/dg/file-processing-app.html",
                "sections": [
                    "One of the most common use cases for Lambda is to perform file processing tasks. For example, you might use a Lambda function     to automatically create PDF files from HTML files or images, or to create thumbnails when a user uploads an image.",
                    "In this example, you create an app which automatically encrypts PDF files when they are uploaded to an Amazon Simple Storage Service (Amazon S3) bucket.     To implement this app, you create the following resources:",
                    "  1.An S3 bucket for users to upload PDF files to",
                    "  2.A Lambda function in Python which reads the uploaded file and creates an encrypted, password-protected version of it",
                    "  3.A second S3 bucket for Lambda to save the encrypted file in",
                    "You also create an AWS Identity and Access Management (IAM) policy to give your Lambda function permission to perform read and write operations     on your S3 buckets.",
                    "If you’re brand new to Lambda, we recommend that you carry out the tutorial Create your first Lambda function before      creating this example app.",
                    "Tip",
                    "You can deploy your app manually by creating and configuring resources with the AWS Management Console or the AWS Command Line Interface (AWS CLI). You can     also deploy the app by using the AWS Serverless Application Model (AWS SAM). AWS SAM is an infrastructure as code (IaC) tool. With IaC, you don’t create     resources manually, but define them in code and then deploy them automatically.",
                    "If you want to learn more about using Lambda with IaC before deploying this example app, see Using Lambda with infrastructure as code (IaC).",
                    {
                        "sub_header": "Prerequisites",
                        "content": [
                            "Before you can create the example app, make sure you have the required command line tools installed.",
                            "  1.AWS CLI : You can manually deploy the resources for your app using either the AWS Management Console or the . To use the CLI, install it by following               the installation                 instructions in the AWS Command Line Interface User Guide.",
                            "  2.AWS SAM CLI : If you want to deploy the example app using AWS SAM, you need to install both the AWS CLI and the . To install the ,               follow the installation instructions               in the AWS SAM User Guide.",
                            "  3.pytest module : After you’ve deployed your app, you can test it using an automated Python test script that we provide. To use this script, install the               pytest package in you local development environment by running the following command:pip install pytest",
                            {
                                "code_example": "pip install pytest"
                            },
                            "To deploy the app using AWS SAM, Docker must also be installed on your build machine."
                        ]
                    },
                    {
                        "sub_header": "Downloading the example app files",
                        "content": [
                            "To create and test the example app, you create the following files in your project directory:",
                            "  1.lambda_function.py - the Python function code for the Lambda function that performs the file encryption",
                            "  2.requirements.txt - a manifest file defining the dependencies that your Python function code requires",
                            "  3.template.yaml - an AWS SAM template you can use to deploy the app",
                            "  4.test_pdf_encrypt.py - a test script you can use to automatically test your application",
                            "  5.pytest.ini - a configuration file for the the test script",
                            "Expand the following sections to view the code and to learn more about the role of each file in creating and testing your app. To create the       files on your local machine, either copy and paste the code below, or download the files from the aws-lambda-developer-guide GitHub repo.",
                            "This is needed to specify the order in which the tests in the test_pdf_encrypt.py script run.",
                            {
                                "code_example": "[pytest]\nmarkers =\n    order: specify test execution order"
                            },
                            "Copy and paste the following code into a file named pytest.ini.",
                            "Test script configuration file",
                            "This is needed to specify the order in which the tests in the test_pdf_encrypt.py script run.",
                            {
                                "code_example": "[pytest]\nmarkers =\n    order: specify test execution order"
                            },
                            "Copy and paste the following code into a file named pytest.ini.",
                            "As with the AWS SAM template, the bucket names specified in this file are placeholders. Before running the test, you need to edit this file           with your app's real bucket names. This step is explained further in Testing the app with the automated script",
                            "After all these tests have run, the script runs an additional cleanup step to delete the test.pdf and test_encrypted.pdf files from         both your source and destination buckets.",
                            "  1.The test test_source_bucket_available confirms that your source bucket has been successfully created             by uploading a test PDF file to the bucket.",
                            "  2.The test test_lambda_invoked interrogates the latest CloudWatch Logs log stream for your function to confirm that             when you uploaded the test file, your Lambda function ran and reported success.",
                            "  3.The test test_encrypted_file_in_bucket confirms that your destination bucket contains the encrypted test_encrypted.pdf             file.",
                            "The automated test script executes three test functions to confirm correct operation of your app:",
                            {
                                "code_example": "import boto3\nimport json\nimport pytest\nimport time\nimport os\n\n@pytest.fixture\ndef lambda_client():\n    return boto3.client('lambda')\n    \n@pytest.fixture\ndef s3_client():\n    return boto3.client('s3')\n\n@pytest.fixture\ndef logs_client():\n    return boto3.client('logs')\n\n@pytest.fixture(scope='session')\ndef cleanup():\n    # Create a new S3 client for cleanup\n    s3_client = boto3.client('s3')\n\n    yield\n    # Cleanup code will be executed after all tests have finished\n\n    # Delete test.pdf from the source bucket\n    source_bucket = 'EXAMPLE-BUCKET'\n    source_file_key = 'test.pdf'\n    s3_client.delete_object(Bucket=source_bucket, Key=source_file_key)\n    print(f\"\\nDeleted {source_file_key} from {source_bucket}\")\n\n    # Delete test_encrypted.pdf from the destination bucket\n    destination_bucket = 'EXAMPLE-BUCKET-encrypted'\n    destination_file_key = 'test_encrypted.pdf'\n    s3_client.delete_object(Bucket=destination_bucket, Key=destination_file_key)\n    print(f\"Deleted {destination_file_key} from {destination_bucket}\")\n        \n\n@pytest.mark.order(1)\ndef test_source_bucket_available(s3_client):\n    s3_bucket_name = 'EXAMPLE-BUCKET'\n    file_name = 'test.pdf'\n    file_path = os.path.join(os.path.dirname(__file__), file_name)\n\n    file_uploaded = False\n    try:\n        s3_client.upload_file(file_path, s3_bucket_name, file_name)\n        file_uploaded = True\n    except:\n        print(\"Error: couldn't upload file\")\n\n    assert file_uploaded, \"Could not upload file to S3 bucket\"\n\n    \n\n@pytest.mark.order(2)\ndef test_lambda_invoked(logs_client):\n\n    # Wait for a few seconds to make sure the logs are available\n    time.sleep(5)\n\n    # Get the latest log stream for the specified log group\n    log_streams = logs_client.describe_log_streams(\n        logGroupName='/aws/lambda/EncryptPDF',\n        orderBy='LastEventTime',\n        descending=True,\n        limit=1\n    )\n\n    latest_log_stream_name = log_streams['logStreams'][0]['logStreamName']\n\n    # Retrieve the log events from the latest log stream\n    log_events = logs_client.get_log_events(\n        logGroupName='/aws/lambda/EncryptPDF',\n        logStreamName=latest_log_stream_name\n    )\n\n    success_found = False\n    for event in log_events['events']:\n        message = json.loads(event['message'])\n        status = message.get('record', {}).get('status')\n        if status == 'success':\n            success_found = True\n            break\n\n    assert success_found, \"Lambda function execution did not report 'success' status in logs.\"\n\n@pytest.mark.order(3)\ndef test_encrypted_file_in_bucket(s3_client):\n    # Specify the destination S3 bucket and the expected converted file key\n    destination_bucket = 'EXAMPLE-BUCKET-encrypted'\n    converted_file_key = 'test_encrypted.pdf'\n\n    try:\n        # Attempt to retrieve the metadata of the converted file from the destination S3 bucket\n        s3_client.head_object(Bucket=destination_bucket, Key=converted_file_key)\n    except s3_client.exceptions.ClientError as e:\n        # If the file is not found, the test will fail\n        pytest.fail(f\"Converted file '{converted_file_key}' not found in the destination bucket: {str(e)}\")\n\ndef test_cleanup(cleanup):\n    # This test uses the cleanup fixture and will be executed last\n    pass"
                            },
                            "Copy and paste the following code into a file named test_pdf_encrypt.py.",
                            "Automated test script",
                            "As with the AWS SAM template, the bucket names specified in this file are placeholders. Before running the test, you need to edit this file           with your app's real bucket names. This step is explained further in Testing the app with the automated script",
                            "After all these tests have run, the script runs an additional cleanup step to delete the test.pdf and test_encrypted.pdf files from         both your source and destination buckets.",
                            "  1.The test test_source_bucket_available confirms that your source bucket has been successfully created             by uploading a test PDF file to the bucket.",
                            "  2.The test test_lambda_invoked interrogates the latest CloudWatch Logs log stream for your function to confirm that             when you uploaded the test file, your Lambda function ran and reported success.",
                            "  3.The test test_encrypted_file_in_bucket confirms that your destination bucket contains the encrypted test_encrypted.pdf             file.",
                            "The automated test script executes three test functions to confirm correct operation of your app:",
                            {
                                "code_example": "import boto3\nimport json\nimport pytest\nimport time\nimport os\n\n@pytest.fixture\ndef lambda_client():\n    return boto3.client('lambda')\n    \n@pytest.fixture\ndef s3_client():\n    return boto3.client('s3')\n\n@pytest.fixture\ndef logs_client():\n    return boto3.client('logs')\n\n@pytest.fixture(scope='session')\ndef cleanup():\n    # Create a new S3 client for cleanup\n    s3_client = boto3.client('s3')\n\n    yield\n    # Cleanup code will be executed after all tests have finished\n\n    # Delete test.pdf from the source bucket\n    source_bucket = 'EXAMPLE-BUCKET'\n    source_file_key = 'test.pdf'\n    s3_client.delete_object(Bucket=source_bucket, Key=source_file_key)\n    print(f\"\\nDeleted {source_file_key} from {source_bucket}\")\n\n    # Delete test_encrypted.pdf from the destination bucket\n    destination_bucket = 'EXAMPLE-BUCKET-encrypted'\n    destination_file_key = 'test_encrypted.pdf'\n    s3_client.delete_object(Bucket=destination_bucket, Key=destination_file_key)\n    print(f\"Deleted {destination_file_key} from {destination_bucket}\")\n        \n\n@pytest.mark.order(1)\ndef test_source_bucket_available(s3_client):\n    s3_bucket_name = 'EXAMPLE-BUCKET'\n    file_name = 'test.pdf'\n    file_path = os.path.join(os.path.dirname(__file__), file_name)\n\n    file_uploaded = False\n    try:\n        s3_client.upload_file(file_path, s3_bucket_name, file_name)\n        file_uploaded = True\n    except:\n        print(\"Error: couldn't upload file\")\n\n    assert file_uploaded, \"Could not upload file to S3 bucket\"\n\n    \n\n@pytest.mark.order(2)\ndef test_lambda_invoked(logs_client):\n\n    # Wait for a few seconds to make sure the logs are available\n    time.sleep(5)\n\n    # Get the latest log stream for the specified log group\n    log_streams = logs_client.describe_log_streams(\n        logGroupName='/aws/lambda/EncryptPDF',\n        orderBy='LastEventTime',\n        descending=True,\n        limit=1\n    )\n\n    latest_log_stream_name = log_streams['logStreams'][0]['logStreamName']\n\n    # Retrieve the log events from the latest log stream\n    log_events = logs_client.get_log_events(\n        logGroupName='/aws/lambda/EncryptPDF',\n        logStreamName=latest_log_stream_name\n    )\n\n    success_found = False\n    for event in log_events['events']:\n        message = json.loads(event['message'])\n        status = message.get('record', {}).get('status')\n        if status == 'success':\n            success_found = True\n            break\n\n    assert success_found, \"Lambda function execution did not report 'success' status in logs.\"\n\n@pytest.mark.order(3)\ndef test_encrypted_file_in_bucket(s3_client):\n    # Specify the destination S3 bucket and the expected converted file key\n    destination_bucket = 'EXAMPLE-BUCKET-encrypted'\n    converted_file_key = 'test_encrypted.pdf'\n\n    try:\n        # Attempt to retrieve the metadata of the converted file from the destination S3 bucket\n        s3_client.head_object(Bucket=destination_bucket, Key=converted_file_key)\n    except s3_client.exceptions.ClientError as e:\n        # If the file is not found, the test will fail\n        pytest.fail(f\"Converted file '{converted_file_key}' not found in the destination bucket: {str(e)}\")\n\ndef test_cleanup(cleanup):\n    # This test uses the cleanup fixture and will be executed last\n    pass"
                            },
                            "Copy and paste the following code into a file named test_pdf_encrypt.py.",
                            "The function definition also specifies an AWS Identity and Access Management (IAM) policy to be attached to the function's execution role.           The AWS managed policyAmazonS3FullAccess gives your function the permissions it needs to read and write objects to Amazon S3.",
                            "The definition of the Lambda function resource configures a trigger for the function using the S3Event event property. This         trigger causes your function to be invoked whenever an object is created in your source bucket.",
                            "The AWS SAM template defines the resources you create for your app. In this example, the template defines a Lambda function using the         AWS::Serverless::Function type and two S3 buckets using the AWS::S3::Bucket type. The bucket names specified in the         template are placeholders. Before you deploy the app using AWS SAM, you need to edit the template to rename the buckets with globally unique names that          meet the S3 bucket naming rules. This step is           explained further in Deploy the resources using AWS SAM.",
                            {
                                "code_example": "AWSTemplateFormatVersion: '2010-09-09'\nTransform: AWS::Serverless-2016-10-31\n\nResources:\n  EncryptPDFFunction:\n    Type: AWS::Serverless::Function\n    Properties:\n      FunctionName: EncryptPDF\n      Architectures: [x86_64]\n      CodeUri: ./\n      Handler: lambda_function.lambda_handler\n      Runtime: python3.12\n      Timeout: 15\n      MemorySize: 256\n      LoggingConfig:\n        LogFormat: JSON\n      Policies:\n        - AmazonS3FullAccess\n      Events:\n        S3Event:\n          Type: S3\n          Properties:\n            Bucket: !Ref PDFSourceBucket\n            Events: s3:ObjectCreated:*\n\n  PDFSourceBucket:\n    Type: AWS::S3::Bucket\n    Properties:\n      BucketName: EXAMPLE-BUCKET\n\n  EncryptedPDFBucket:\n    Type: AWS::S3::Bucket\n    Properties:\n      BucketName: EXAMPLE-BUCKET-encrypted"
                            },
                            "Copy and paste the following code into a file named template.yaml.",
                            "AWS SAM template",
                            "The function definition also specifies an AWS Identity and Access Management (IAM) policy to be attached to the function's execution role.           The AWS managed policyAmazonS3FullAccess gives your function the permissions it needs to read and write objects to Amazon S3.",
                            "The definition of the Lambda function resource configures a trigger for the function using the S3Event event property. This         trigger causes your function to be invoked whenever an object is created in your source bucket.",
                            "The AWS SAM template defines the resources you create for your app. In this example, the template defines a Lambda function using the         AWS::Serverless::Function type and two S3 buckets using the AWS::S3::Bucket type. The bucket names specified in the         template are placeholders. Before you deploy the app using AWS SAM, you need to edit the template to rename the buckets with globally unique names that          meet the S3 bucket naming rules. This step is           explained further in Deploy the resources using AWS SAM.",
                            {
                                "code_example": "AWSTemplateFormatVersion: '2010-09-09'\nTransform: AWS::Serverless-2016-10-31\n\nResources:\n  EncryptPDFFunction:\n    Type: AWS::Serverless::Function\n    Properties:\n      FunctionName: EncryptPDF\n      Architectures: [x86_64]\n      CodeUri: ./\n      Handler: lambda_function.lambda_handler\n      Runtime: python3.12\n      Timeout: 15\n      MemorySize: 256\n      LoggingConfig:\n        LogFormat: JSON\n      Policies:\n        - AmazonS3FullAccess\n      Events:\n        S3Event:\n          Type: S3\n          Properties:\n            Bucket: !Ref PDFSourceBucket\n            Events: s3:ObjectCreated:*\n\n  PDFSourceBucket:\n    Type: AWS::S3::Bucket\n    Properties:\n      BucketName: EXAMPLE-BUCKET\n\n  EncryptedPDFBucket:\n    Type: AWS::S3::Bucket\n    Properties:\n      BucketName: EXAMPLE-BUCKET-encrypted"
                            },
                            "Copy and paste the following code into a file named template.yaml.",
                            "A version of the SDK for Python (Boto3) is included as part of the Lambda runtime, so your code would run without adding Boto3 to your             function's deployment package. However, to maintain full control of your function's dependencies and avoid possible issues with             version misalignment, best practice for Python is to include all function dependencies in your function's deployment package.           See Runtime dependencies in Python to learn more.",
                            "Note",
                            "For this example, your function code has only two dependencies that aren't part of the standard Python library -           the SDK for Python (Boto3) and the pypdf package the function uses to perform the PDF encryption.",
                            {
                                "code_example": "boto3\npypdf"
                            },
                            "Copy and paste the following code into a file named requirements.txt.",
                            "requirements.txt manifest file",
                            "A version of the SDK for Python (Boto3) is included as part of the Lambda runtime, so your code would run without adding Boto3 to your             function's deployment package. However, to maintain full control of your function's dependencies and avoid possible issues with             version misalignment, best practice for Python is to include all function dependencies in your function's deployment package.           See Runtime dependencies in Python to learn more.",
                            "Note",
                            "For this example, your function code has only two dependencies that aren't part of the standard Python library -           the SDK for Python (Boto3) and the pypdf package the function uses to perform the PDF encryption.",
                            {
                                "code_example": "boto3\npypdf"
                            },
                            "Copy and paste the following code into a file named requirements.txt.",
                            "Finally, the function uses the Boto3 SDK to store the encrypted file in your S3 destination bucket.",
                            "Your function then uses the AWS SDK for Python (Boto3) to download the PDF files specified in the event object to its local temporary storage directory, before           encrypting them using the pypdf library.",
                            "When your function is invoked by Amazon S3, Lambda passes a JSON formatted event argument to the function that contains details about the           event that caused the invocation. In this case, the information includes name of the S3 bucket and the object keys for the uploaded files.           To learn more about the format of event object for Amazon S3, see Process Amazon S3 event notifications with Lambda.",
                            "The python function code contains three functions - the handler function that Lambda runs           when your function is invoked, and two separate function named add_encrypted_suffix and encrypt_pdf that the handler calls to perform the PDF encryption.",
                            "In this example code, a password for the encrypted file (my-secret-password) is hardcoded into the           function code. In a production application, don't include sensitive information like passwords in your function code. Use           AWS Secrets Manager to securely store sensitive parameters.",
                            "Note",
                            {
                                "code_example": "from pypdf import PdfReader, PdfWriter\nimport uuid\nimport os\nfrom urllib.parse import unquote_plus\nimport boto3\n\n# Create the S3 client to download and upload objects from S3\ns3_client = boto3.client('s3')\n\ndef lambda_handler(event, context):\n    # Iterate over the S3 event object and get the key for all uploaded files\n    for record in event['Records']:\n        bucket = record['s3']['bucket']['name']\n        key = unquote_plus(record['s3']['object']['key']) # Decode the S3 object key to remove any URL-encoded characters\n        download_path = f'/tmp/{uuid.uuid4()}.pdf' # Create a path in the Lambda tmp directory to save the file to \n        upload_path = f'/tmp/converted-{uuid.uuid4()}.pdf' # Create another path to save the encrypted file to\n        \n        # If the file is a PDF, encrypt it and upload it to the destination S3 bucket\n        if key.lower().endswith('.pdf'):\n            s3_client.download_file(bucket, key, download_path)\n            encrypt_pdf(download_path, upload_path)\n            encrypted_key = add_encrypted_suffix(key)\n            s3_client.upload_file(upload_path, f'{bucket}-encrypted', encrypted_key)\n\n# Define the function to encrypt the PDF file with a password\ndef encrypt_pdf(file_path, encrypted_file_path):\n    reader = PdfReader(file_path)\n    writer = PdfWriter()\n    \n    for page in reader.pages:\n        writer.add_page(page)\n\n    # Add a password to the new PDF\n    writer.encrypt(\"my-secret-password\")\n\n    # Save the new PDF to a file\n    with open(encrypted_file_path, \"wb\") as file:\n        writer.write(file)\n\n# Define a function to add a suffix to the original filename after encryption\ndef add_encrypted_suffix(original_key):\n    filename, extension = original_key.rsplit('.', 1)\n    return f'{filename}_encrypted.{extension}'"
                            },
                            "Copy and paste the following code into a file named lambda_function.py.",
                            "Python function code",
                            "Finally, the function uses the Boto3 SDK to store the encrypted file in your S3 destination bucket.",
                            "Your function then uses the AWS SDK for Python (Boto3) to download the PDF files specified in the event object to its local temporary storage directory, before           encrypting them using the pypdf library.",
                            "When your function is invoked by Amazon S3, Lambda passes a JSON formatted event argument to the function that contains details about the           event that caused the invocation. In this case, the information includes name of the S3 bucket and the object keys for the uploaded files.           To learn more about the format of event object for Amazon S3, see Process Amazon S3 event notifications with Lambda.",
                            "The python function code contains three functions - the handler function that Lambda runs           when your function is invoked, and two separate function named add_encrypted_suffix and encrypt_pdf that the handler calls to perform the PDF encryption.",
                            "In this example code, a password for the encrypted file (my-secret-password) is hardcoded into the           function code. In a production application, don't include sensitive information like passwords in your function code. Use           AWS Secrets Manager to securely store sensitive parameters.",
                            "Note",
                            {
                                "code_example": "from pypdf import PdfReader, PdfWriter\nimport uuid\nimport os\nfrom urllib.parse import unquote_plus\nimport boto3\n\n# Create the S3 client to download and upload objects from S3\ns3_client = boto3.client('s3')\n\ndef lambda_handler(event, context):\n    # Iterate over the S3 event object and get the key for all uploaded files\n    for record in event['Records']:\n        bucket = record['s3']['bucket']['name']\n        key = unquote_plus(record['s3']['object']['key']) # Decode the S3 object key to remove any URL-encoded characters\n        download_path = f'/tmp/{uuid.uuid4()}.pdf' # Create a path in the Lambda tmp directory to save the file to \n        upload_path = f'/tmp/converted-{uuid.uuid4()}.pdf' # Create another path to save the encrypted file to\n        \n        # If the file is a PDF, encrypt it and upload it to the destination S3 bucket\n        if key.lower().endswith('.pdf'):\n            s3_client.download_file(bucket, key, download_path)\n            encrypt_pdf(download_path, upload_path)\n            encrypted_key = add_encrypted_suffix(key)\n            s3_client.upload_file(upload_path, f'{bucket}-encrypted', encrypted_key)\n\n# Define the function to encrypt the PDF file with a password\ndef encrypt_pdf(file_path, encrypted_file_path):\n    reader = PdfReader(file_path)\n    writer = PdfWriter()\n    \n    for page in reader.pages:\n        writer.add_page(page)\n\n    # Add a password to the new PDF\n    writer.encrypt(\"my-secret-password\")\n\n    # Save the new PDF to a file\n    with open(encrypted_file_path, \"wb\") as file:\n        writer.write(file)\n\n# Define a function to add a suffix to the original filename after encryption\ndef add_encrypted_suffix(original_key):\n    filename, extension = original_key.rsplit('.', 1)\n    return f'{filename}_encrypted.{extension}'"
                            },
                            "Copy and paste the following code into a file named lambda_function.py."
                        ]
                    },
                    {
                        "sub_header": "Deploying the app",
                        "content": [
                            "You can create and deploy the resources for this example app either manually or by using AWS SAM. In a production environment, we       recommend that you use an IaC tool like AWS SAM to quickly and repeatably deploy whole serverless applications without using manual processes.",
                            "For this example, follow the console or AWS CLI instructions to learn how to configure each AWS resource separately, or skip ahead to        Deploy the resources using AWS SAM to quickly deploy the app using a few CLI commands.",
                            {
                                "sub_header": "Deploy the resources manually",
                                "content": [
                                    "To deploy your app manually, you carry out the following steps:",
                                    "  1.Create source and destination Amazon S3 buckets",
                                    "  2.Create a Lambda function that encrypts a PDF file and saves the encrypted version to an S3 bucket",
                                    "  3.Configure a Lambda trigger that invokes your function when objects are uploaded to your source bucket",
                                    "Follow the instructions in the following paragraphs to create and configure your resources.",
                                    {
                                        "sub_header": "Create two S3 buckets",
                                        "content": [
                                            "First create two S3 buckets. The first bucket is the source bucket you will upload your PDF files to. The second bucket is used by     Lambda to save the encrypted file when you invoke your function.",
                                            "  1 : Open the Buckets page of the Amazon S3 console.",
                                            "  2 : Choose Create bucket.",
                                            "  3 : Under General configuration, do the following:For Bucket name, enter a globally unique name that meets the Amazon S3 Bucket naming rules.                     Bucket names can contain only lower case letters, numbers, dots (.), and hyphens (-). For AWS Region, choose the AWS Region                     closest to your geographical location. Later in the deployment process, you must create your Lambda function in the same AWS Region, so                     make a note of the region you chose.",
                                            "  4 : Leave all other options set to their default values and choose Create bucket.",
                                            "  5 : Repeat steps 1 to 4 to create your destination bucket. For Bucket name, enter SOURCEBUCKET-encrypted,                 where SOURCEBUCKET is the name of the source bucket you just created.",
                                            "To create the S3 buckets (console)",
                                            "anchor",
                                            "anchor",
                                            "  1.Console : SOURCEBUCKET-encrypted",
                                            "  2.AWS CLI : region"
                                        ]
                                    },
                                    {
                                        "sub_header": "Create an execution role (AWS CLI only)",
                                        "content": [
                                            "An execution role is an IAM role that grants a Lambda function permission to access AWS services and resources. When you create a function       using the Lambda console, Lambda automatically creates an execution role. You only need to create a role manually if you choose to deploy the app       using the AWS CLI. To give your function read and write access to Amazon S3, you attach the       AWS managed policyAmazonS3FullAccess.",
                                            "This step is only required if you choose to deploy your app using the AWS CLI.",
                                            "anchor",
                                            "anchor",
                                            "  1.Console : \nThis step is only required if you choose to deploy your app using the AWS CLI.\n",
                                            "  2.AWS CLI : AmazonS3FullAccess"
                                        ]
                                    },
                                    {
                                        "sub_header": "Create the function deployment package",
                                        "content": [
                                            "To create your function, you create a deployment package containing your function code and its dependencies. For this       application, your function code uses a separate library for the PDF encryption.",
                                            "  1 : Navigate to the project directory containing the lambda_function.py and requirements.txt           files you created or downloaded from GitHub earlier and create a new directory named package.",
                                            "  \nInstall the dependencies specified in the requirements.txt file in your package directory by running the following command.\n \n",
                                            {
                                                "code_example": "pip install -r requirements.txt --target ./package/"
                                            },
                                            "  \nCreate a .zip file containing your application code and its dependencies. In Linux or MacOS, run the following commands from your \n          command line interface.\n \n In Windows, use your preferred zip tool to create the lambda_function.zip file. Make sure that your \n        lambda_function.py file and the folders containing your dependencies are all at the root of the .zip file.\n",
                                            {
                                                "code_example": "cd package\nzip -r ../lambda_function.zip .\ncd ..\nzip lambda_function.zip lambda_function.py"
                                            },
                                            "To create the deployment package",
                                            "You can also create your deployment package using a Python virtual environment. See Working with .zip file archives for Python Lambda functions"
                                        ]
                                    },
                                    {
                                        "sub_header": "Create the Lambda function",
                                        "content": [
                                            "You now use the deployment package you created in the previous step to deploy your Lambda function.",
                                            "  1 : Select the Configuration tab for your function.",
                                            "  2 : Select Monitoring and operations tools.",
                                            "  3 : In the Logging configuration pane, choose Edit.",
                                            "  4 : For Logging configuration, select JSON.",
                                            "  5 : Choose Save.",
                                            "To configure the log format (console)",
                                            "  1 : Select the Configuration tab for your function.",
                                            "  2 : In the General configuration pane, choose Edit.",
                                            "  3 : Set Memory to 256 MB and Timeout to 15 seconds.",
                                            "  4 : Choose Save.",
                                            "To configure the function memory and timeout (console)",
                                            "  1 : In the Code source pane, choose Upload from.",
                                            "  2 : Choose .zip file.",
                                            "  3 : Choose Upload.",
                                            "  4 : In the file selector, select your .zip file and choose Open.",
                                            "  5 : Choose Save.",
                                            "To upload the function code (console)",
                                            "  1 : Open the Functions page of the Lambda console.",
                                            "  2 : Make sure you're working in the same AWS Region you created your S3 bucket in. You can change your region using the drop-down             list at the top of the screen.",
                                            "  3 : Choose Create function.",
                                            "  4 : Choose Author from scratch.",
                                            "  5 : Under Basic information, do the following:For Function name, enter EncryptPDF.For Runtime choose Python 3.12.For Architecture, choose x86_64.",
                                            "  6 : Choose Create function.",
                                            "To ensure that your function doesn't time out when encrypting large PDF files, you configure the function's memory and timeout settings.           You also set the function's log format to JSON. Configuring JSON formatted logs is necessary when using the provided test script so it can read the           function's invocation status from CloudWatch Logs to confirm successful invocation.",
                                            "To create your Lambda function using the console, you first create a basic function containing some ‘Hello world’ code. You then           replace this code with your own function code by uploading the.zip file you created in the previous step.",
                                            "To create the function (console)",
                                            "anchor",
                                            "anchor",
                                            "  1.Console : EncryptPDF",
                                            "  2.AWS CLI : lambda_function.zip"
                                        ]
                                    },
                                    {
                                        "sub_header": "Configure an Amazon S3 trigger to invoke the function",
                                        "content": [
                                            "For your Lambda function to run when you upload a file to your source bucket, you need to configure a trigger for your function. You can     configure the Amazon S3 trigger using either the console or the AWS CLI.",
                                            "This procedure configures the S3 bucket to invoke your function every time that an object is created in the bucket. Be sure to       configure this only on the source bucket. If your Lambda function creates objects in the same bucket that invokes it, your function can be         invoked continuously in a loop. This can result         in un expected charges being billed to your AWS account.",
                                            "Important",
                                            "  1 : Open the Functions page of the Lambda console and choose your function (EncryptPDF).",
                                            "  2 : Choose Add trigger.",
                                            "  3 : Select S3.",
                                            "  4 : Under Bucket, select your source bucket.",
                                            "  5 : Under Event types, select All object create events.",
                                            "  6 : Under Recursive invocation, select the check box to acknowledge that using the same S3 bucket for input                 and output is not recommended. You can learn more about recursive invocation patterns in Lambda by reading                Recursive patterns that cause run-away Lambda functions                in Serverless Land.",
                                            "  7 : Choose Add.When you create a trigger using the Lambda console, Lambda automatically creates a resource based policy                 to give the service you select permission to invoke your function. ",
                                            "To configure the Amazon S3 trigger (console)",
                                            "anchor",
                                            "anchor",
                                            "  1.Console : EncryptPDF",
                                            "  2.AWS CLI : source-account"
                                        ]
                                    }
                                ]
                            },
                            {
                                "sub_header": "Deploy the resources using AWS SAM",
                                "content": [
                                    "To deploy the example app using the AWS SAM CLI, carry out the following steps.",
                                    "Make sure that you have         installed the latest version of the           CLI and that Docker is installed on your build machine.",
                                    "  1 : Edit the template.yaml file to specify the name of your             S3 buckets. S3 buckets must have globally unique names that meet the S3 bucket naming rules.Replace the bucket name EXAMPLE-BUCKET with a name of your choice consisting of lowercase letters, numbers, dots (.), and hyphens (-). For the destination           bucket, replace EXAMPLE-BUCKET-encrypted with <source-bucket-name>-encrypted, where <source-bucket> is the name           you chose for your source bucket.",
                                    "  \nRun the following command from the directory in which you saved your template.yaml, lambda_function.py, \n            and requirements.txtfiles.\n \nThis command gathers the build artifacts for your application and places them in the proper format and location to deploy them. Specifying \n          the --use-container option builds your function inside a Lambda-like Docker container. We use it here so you don't need to have Python 3.12\n          installed on your local machine for the build to work.\nDuring the build process, AWS SAM looks for the Lambda function code in the location you specified with the CodeUri \n          property in the template. In this case, we specified the current directory as the location (./).\nIf a requirements.txt file is present, AWS SAM uses it to gather the specified dependencies. By default, AWS SAM creates a .zip \n            deployment package with your function code and dependencies. You can also choose to deploy your function as a container image using the \n            PackageType \n          property.\n",
                                    {
                                        "code_example": "sam build --use-container"
                                    },
                                    "  \nTo deploy your application and create the Lambda and Amazon S3 resources specified in your AWS SAM template, run the following \n            command.\n \nUsing the --guided flag means that AWS SAM will show you prompts to guide you through the deployment process. For this \n            deployment, accept the default options by pressing Enter.\n",
                                    {
                                        "code_example": "sam deploy --guided"
                                    },
                                    "During the deployment process, AWS SAM creates the following resources in your AWS account:",
                                    "  1.An AWS CloudFormation stack             named sam-app",
                                    "  2.A Lambda function with the name EncryptPDF",
                                    "  3.Two S3 buckets with the names you chose when you edited the template.yaml AWS SAM template file",
                                    "  4.An IAM execution role for your function with the name format sam-app-EncryptPDFFunctionRole-2qGaapHFWOQ8",
                                    "When AWS SAM finishes creating your resources, you should see the following message:",
                                    "Successfully created/updated stack - sam-app in us-west-2"
                                ]
                            }
                        ]
                    },
                    {
                        "sub_header": "Testing the app",
                        "content": [
                            "To test your app, you upload a PDF file to your source bucket, and confirm that Lambda creates an encrypted version of the file in your       destination bucket. In this example, you can either test this manually using the console or the AWS CLI, or by using the provided test script.",
                            "For production applications, you can use traditional test methods and techniques, such as unit testing, to confirm the       correct functioning of your Lambda function code. Best practice is also to conduct tests like those in the provided test script which perform integration       testing with real, cloud-based resources. Integration testing in the cloud confirms that your infrastructure has been correctly deployed and that events flow       between different services as expected. To learn more, see How to test serverless functions and applications.",
                            {
                                "sub_header": "Testing the app manually",
                                "content": [
                                    "You can test your function manually by adding a PDF file to           your Amazon S3 source bucket. When you add your file to the source bucket, your Lambda function should be automatically invoked and should store an encrypted           version of the file in your target bucket.",
                                    "  1 : To upload a PDF file to your S3 bucket, do the following:Open the Buckets page of the Amazon S3 console and choose your source bucket.Choose Upload.Choose Add files and use the file selector to choose the PDF file you want to upload.Choose Open, then choose Upload.",
                                    "  2 : Verify that Lambda has saved an encrypted version of your PDF file in your target bucket by doing the following:Navigate back to the Buckets page of the Amazon S3 console and choose your destination bucket.In the Objects pane, you should now see a file with name format filename_encrypted.pdf (where                         filename.pdf was the name of the file you uploaded to your source bucket).                        To download your encrypted PDF, select the file, then choose Download.Confirm that you can open the downloaded file with the password your Lambda function protected it with (my-secret-password).",
                                    "To test your app by uploading a file (console)",
                                    "anchor",
                                    "anchor",
                                    "  1.Console : filename_encrypted.pdf",
                                    "  2.AWS CLI : --bucket"
                                ]
                            },
                            {
                                "sub_header": "Testing the app with the automated script",
                                "content": [
                                    "To test your app using the provided test script, first ensure that the pytest module is installed in your local environment. You can install       pytest by running the following command:",
                                    "pip install pytest",
                                    "You also need to edit the code in the test_pdf_encrypt.py file to replace the placeholder bucket names with the names of           your Amazon S3 source and destination buckets. Make the following changes to test_pdf_encrypt.py:",
                                    "  1.In the test_source_bucket_available function, replace EXAMPLE-BUCKET with the name of your source bucket.",
                                    "  2.In the test_encrypted_file_in_bucket function, replace EXAMPLE-BUCKET-encrypted with <source-bucket>-encrypted,            where <source-bucket> is the name of your source bucket.",
                                    "  3.In the cleanup function, replace EXAMPLE-BUCKET with the name of your source bucket, and replace               EXAMPLE-BUCKET-encrypted with ≪source-bucket>-encrypted, where <source-bucket> is the name of your source bucket.",
                                    "To run the tests do the following:",
                                    "  1.Save a PDF file named test.pdfin the directory containing the test_pdf_encrypt.py and pytest.ini             files.",
                                    "  2.Open a terminal or shell program and run the following command from the directory containing the test files.pytest -s -v",
                                    {
                                        "code_example": "pytest -s -v"
                                    },
                                    "When the test completes, you should see output like the following:",
                                    "============================================================== test session starts =========================================================platform linux -- Python 3.12.2, pytest-7.2.2, pluggy-1.0.0 -- /usr/bin/python3cachedir: .pytest_cachehypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/pdf_encrypt_app/.hypothesis/examples')Test order randomisation NOT enabled. Enable with --random-order or --random-order-bucket=<bucket_type>rootdir: /home/pdf_encrypt_app, configfile: pytest.iniplugins: anyio-3.7.1, hypothesis-6.70.0, localserver-0.7.1, random-order-1.1.0collected 4 itemstest_pdf_encrypt.py::test_source_bucket_available PASSEDtest_pdf_encrypt.py::test_lambda_invoked PASSEDtest_pdf_encrypt.py::test_encrypted_file_in_bucket PASSEDtest_pdf_encrypt.py::test_cleanup PASSEDDeleted test.pdf from EXAMPLE-BUCKETDeleted test_encrypted.pdf from EXAMPLE-BUCKET-encrypted=============================================================== 4 passed in 7.32s =========================================================="
                                ]
                            }
                        ]
                    },
                    {
                        "sub_header": "Next steps",
                        "content": [
                            "Now you've created this example app, you can use the provided code as a basis to create other types of file-processing application. Modify the     code in the lambda_function.py file to implement the file-processing logic for your use case.",
                            "Many typical file-processing use cases involve image processing. When using Python, the most popular image-processing libraries like       pillow typically contain C or C++ components. In order to ensure that your function's deployment package is     compatible with the Lambda execution environment, it's important to use the correct source distribution binary.",
                            "When deploying your resources with AWS SAM, you need to take some extra steps to include the right source distribution in your deployment package. Because AWS SAM won't install dependencies     for a different platform than your build machine, specifying the correct source distribution (.whl file) in your requirements.txt       file won't work if your build machine uses an operating system or architecture that's different from the Lambda execution environment. Instead, you should do one of the following:",
                            "  1.Use the --use-container option when running sam build. When you specify this option, AWS SAM downloads a container base image that's         compatible with the Lambda execution environment and builds your function's deployment package in a Docker container using that image. To learn more, see           Building a Lambda           function inside of a provided container.",
                            "  2.Build your function's .zip deployment package yourself using the correct source distribution binary and save the .zip file in the directory you specify as the           CodeUri in the AWS SAM template. To learn more about building .zip deployment packages for Python using binary distributions, see           Creating a .zip deployment package with dependencies and Creating .zip deployment packages with native libraries."
                        ]
                    }
                ]
            },
            {
                "title": "Scheduled-maintenance app",
                "href": "https://docs.aws.amazon.com/lambda/latest/dg/scheduled-task-app.html",
                "sections": [
                    "You can use AWS Lambda to replace scheduled processes such as automated system backups, file conversions, and maintenance tasks.   In this example, you create a serverless application that performs regular scheduled maintenance on a DynamoDB table by deleting old entries. The app uses EventBridge Scheduler to invoke a   Lambda function on a cron schedule. When invoked, the function queries the table for items older than one year, and deletes them. The function logs each deleted item  in CloudWatch Logs.",
                    "To implement this example, first create a DynamoDB table and populate it with some test data for your function to query. Then, create a Python Lambda function with   an EventBridge Scheduler trigger and an IAM execution role that gives the function permission to read, and delete, items from your table.",
                    "If you’re new to Lambda, we recommend that you complete the tutorial Create your first Lambda function before      creating this example app.",
                    "Tip",
                    "You can deploy your app manually by creating and configuring resources with the AWS Management Console. You can     also deploy the app by using the AWS Serverless Application Model (AWS SAM). AWS SAM is an infrastructure as code (IaC) tool. With IaC, you don’t create     resources manually, but define them in code and then deploy them automatically.",
                    "If you want to learn more about using Lambda with IaC before deploying this example app, see Using Lambda with infrastructure as code (IaC).",
                    {
                        "sub_header": "Prerequisites",
                        "content": [
                            "Before you can create the example app, make sure you have the required command line tools and programs installed.",
                            "  1.Python : To populate the DynamoDB table you create to test your app, this example uses a  script and a CSV file to write data into the table. Make sure you have          version 3.8 or later installed on your machine.",
                            "  2.AWS SAM CLI : If you want to create the DynamoDB table and deploy the example app using AWS SAM, you need to install the .           Follow the installation instructions           in the AWS SAM User Guide.",
                            "  3.AWS CLI : To use the provided Python script to populate your test table, you need to have installed and configured the . This is because the script uses           the AWS SDK for Python (Boto3), which needs access to your AWS Identity and Access Management (IAM) credentials. You also need the  installed to deploy resources using AWS SAM. Install the CLI by following           the installation instructions in the AWS Command Line Interface User Guide.",
                            "  4.Docker : To deploy the app using AWS SAM,  must also be installed on your build machine. Follow the instructions in Install  Engine         on the  documentation website."
                        ]
                    },
                    {
                        "sub_header": "Downloading the example app files",
                        "content": [
                            "To create the example database and the scheduled-maintenance app, you need to create the following files in your project directory:",
                            "Example database files",
                            "  1.template.yaml - an AWS SAM template you can use to create the DynamoDB table",
                            "  2.sample_data.csv - a CSV file containing sample data to load into your table",
                            "  3.load_sample_data.py - a Python script that writes the data in the CSV file into the table",
                            "Scheduled-maintenance app files",
                            "  1.lambda_function.py - the Python function code for the Lambda function that performs the database maintenance",
                            "  2.requirements.txt - a manifest file defining the dependencies that your Python function code requires",
                            "  3.template.yaml - an AWS SAM template you can use to deploy the app",
                            "Test file",
                            "  1.test_app.py - a Python script that scans the table and confirms successful operation of your function by outputting all records older than one year",
                            "Expand the following sections to view the code and to learn more about the role of each file in creating and testing your app. To create the       files on your local machine, copy and paste the code below.",
                            "This test script uses the AWS SDK for Python (Boto3) to create a connection to your DynamoDB table and scan for items older than one year. To confirm if the Lambda function           has run successfully, at the end of the test, the function prints the number of records older than one year still in the table. If the Lambda function was successful,           the number of old records in the table should be zero.        ",
                            {
                                "code_example": "import boto3\nfrom datetime import datetime, timedelta\nimport json\n\n# Initialize the DynamoDB client\ndynamodb = boto3.resource('dynamodb')\n\n# Specify your table name\ntable_name = 'YourTableName'\ntable = dynamodb.Table(table_name)\n\n# Get the current date\ncurrent_date = datetime.now()\n\n# Calculate the date one year ago\none_year_ago = current_date - timedelta(days=365)\n\n# Convert the date to string format (assuming the date in DynamoDB is stored as a string)\none_year_ago_str = one_year_ago.strftime('%Y-%m-%d')\n\n# Scan the table\nresponse = table.scan(\n    FilterExpression='#date < :one_year_ago',\n    ExpressionAttributeNames={\n        '#date': 'Date'\n    },\n    ExpressionAttributeValues={\n        ':one_year_ago': one_year_ago_str\n    }\n)\n\n# Process the results\nold_records = response['Items']\n\n# Continue scanning if we have more items (pagination)\nwhile 'LastEvaluatedKey' in response:\n    response = table.scan(\n        FilterExpression='#date < :one_year_ago',\n        ExpressionAttributeNames={\n            '#date': 'Date'\n        },\n        ExpressionAttributeValues={\n            ':one_year_ago': one_year_ago_str\n        },\n        ExclusiveStartKey=response['LastEvaluatedKey']\n    )\n    old_records.extend(response['Items'])\n\nfor record in old_records:\n    print(json.dumps(record))\n\n# The total number of old records should be zero.\nprint(f\"Total number of old records: {len(old_records)}\")\n"
                            },
                            "Copy and paste the following code into a file named test_app.py.",
                            "Test script",
                            "This test script uses the AWS SDK for Python (Boto3) to create a connection to your DynamoDB table and scan for items older than one year. To confirm if the Lambda function           has run successfully, at the end of the test, the function prints the number of records older than one year still in the table. If the Lambda function was successful,           the number of old records in the table should be zero.        ",
                            {
                                "code_example": "import boto3\nfrom datetime import datetime, timedelta\nimport json\n\n# Initialize the DynamoDB client\ndynamodb = boto3.resource('dynamodb')\n\n# Specify your table name\ntable_name = 'YourTableName'\ntable = dynamodb.Table(table_name)\n\n# Get the current date\ncurrent_date = datetime.now()\n\n# Calculate the date one year ago\none_year_ago = current_date - timedelta(days=365)\n\n# Convert the date to string format (assuming the date in DynamoDB is stored as a string)\none_year_ago_str = one_year_ago.strftime('%Y-%m-%d')\n\n# Scan the table\nresponse = table.scan(\n    FilterExpression='#date < :one_year_ago',\n    ExpressionAttributeNames={\n        '#date': 'Date'\n    },\n    ExpressionAttributeValues={\n        ':one_year_ago': one_year_ago_str\n    }\n)\n\n# Process the results\nold_records = response['Items']\n\n# Continue scanning if we have more items (pagination)\nwhile 'LastEvaluatedKey' in response:\n    response = table.scan(\n        FilterExpression='#date < :one_year_ago',\n        ExpressionAttributeNames={\n            '#date': 'Date'\n        },\n        ExpressionAttributeValues={\n            ':one_year_ago': one_year_ago_str\n        },\n        ExclusiveStartKey=response['LastEvaluatedKey']\n    )\n    old_records.extend(response['Items'])\n\nfor record in old_records:\n    print(json.dumps(record))\n\n# The total number of old records should be zero.\nprint(f\"Total number of old records: {len(old_records)}\")\n"
                            },
                            "Copy and paste the following code into a file named test_app.py.",
                            "In addition to the Lambda function and the EventBridge Scheduler schedule, we also define a CloudWatch log group for your function to send records of deleted items to.",
                            "This AWS SAM template defines the resources for your app. We define the Lambda function using the AWS::Serverless::Function resource. The EventBridge Scheduler schedule and the           trigger to invoke the Lambda function are created by using the Events property of this resource using a type of ScheduleV2. To learn more about defining EventBridge Scheduler schedules in AWS SAM templates,           see ScheduleV2 in the           AWS Serverless Application Model Developer Guide.",
                            "AWS SAM templates use a standard naming convention of template.yaml. In this example, you have two template files - one to create the             example database and another to create the app itself. Save them in separate sub-directories in your project folder.",
                            "Note",
                            {
                                "code_example": "AWSTemplateFormatVersion: '2010-09-09'\nTransform: AWS::Serverless-2016-10-31\nDescription: SAM Template for Lambda function and EventBridge Scheduler rule\n\nResources:\n  MyLambdaFunction:\n    Type: AWS::Serverless::Function\n    Properties:\n      FunctionName: ScheduledDBMaintenance\n      CodeUri: ./\n      Handler: lambda_function.lambda_handler\n      Runtime: python3.11\n      Architectures:\n        - x86_64\n      Events:\n        ScheduleEvent:\n          Type: ScheduleV2\n          Properties:\n            ScheduleExpression: cron(0 3 1 * ? *)\n            Description: Run on the first day of every month at 03:00 AM\n      Policies:\n        - CloudWatchLogsFullAccess\n        - Statement:\n            - Effect: Allow\n              Action:\n                - dynamodb:Scan\n                - dynamodb:BatchWriteItem\n              Resource: !Sub 'arn:aws:dynamodb:${AWS::Region}:${AWS::AccountId}:table/MyOrderTable'\n\n  LambdaLogGroup:\n    Type: AWS::Logs::LogGroup\n    Properties:\n      LogGroupName: !Sub /aws/lambda/${MyLambdaFunction}\n      RetentionInDays: 30\n\nOutputs:\n  LambdaFunctionName:\n    Description: Lambda Function Name\n    Value: !Ref MyLambdaFunction\n  LambdaFunctionArn:\n    Description: Lambda Function ARN\n    Value: !GetAtt MyLambdaFunction.Arn"
                            },
                            "Copy and paste the following code into a file named template.yaml.",
                            "AWS SAM template (scheduled-maintenance app)",
                            "In addition to the Lambda function and the EventBridge Scheduler schedule, we also define a CloudWatch log group for your function to send records of deleted items to.",
                            "This AWS SAM template defines the resources for your app. We define the Lambda function using the AWS::Serverless::Function resource. The EventBridge Scheduler schedule and the           trigger to invoke the Lambda function are created by using the Events property of this resource using a type of ScheduleV2. To learn more about defining EventBridge Scheduler schedules in AWS SAM templates,           see ScheduleV2 in the           AWS Serverless Application Model Developer Guide.",
                            "AWS SAM templates use a standard naming convention of template.yaml. In this example, you have two template files - one to create the             example database and another to create the app itself. Save them in separate sub-directories in your project folder.",
                            "Note",
                            {
                                "code_example": "AWSTemplateFormatVersion: '2010-09-09'\nTransform: AWS::Serverless-2016-10-31\nDescription: SAM Template for Lambda function and EventBridge Scheduler rule\n\nResources:\n  MyLambdaFunction:\n    Type: AWS::Serverless::Function\n    Properties:\n      FunctionName: ScheduledDBMaintenance\n      CodeUri: ./\n      Handler: lambda_function.lambda_handler\n      Runtime: python3.11\n      Architectures:\n        - x86_64\n      Events:\n        ScheduleEvent:\n          Type: ScheduleV2\n          Properties:\n            ScheduleExpression: cron(0 3 1 * ? *)\n            Description: Run on the first day of every month at 03:00 AM\n      Policies:\n        - CloudWatchLogsFullAccess\n        - Statement:\n            - Effect: Allow\n              Action:\n                - dynamodb:Scan\n                - dynamodb:BatchWriteItem\n              Resource: !Sub 'arn:aws:dynamodb:${AWS::Region}:${AWS::AccountId}:table/MyOrderTable'\n\n  LambdaLogGroup:\n    Type: AWS::Logs::LogGroup\n    Properties:\n      LogGroupName: !Sub /aws/lambda/${MyLambdaFunction}\n      RetentionInDays: 30\n\nOutputs:\n  LambdaFunctionName:\n    Description: Lambda Function Name\n    Value: !Ref MyLambdaFunction\n  LambdaFunctionArn:\n    Description: Lambda Function ARN\n    Value: !GetAtt MyLambdaFunction.Arn"
                            },
                            "Copy and paste the following code into a file named template.yaml.",
                            "A version of the SDK for Python (Boto3) is included as part of the Lambda runtime, so your code would run without adding Boto3 to your             function's deployment package. However, to maintain full control of your function's dependencies and avoid possible issues with             version misalignment, best practice for Python is to include all function dependencies in your function's deployment package.             See Runtime dependencies in Python to learn more.",
                            "Note",
                            "For this example, your function code has only one dependency that isn't part of the standard Python library -           the SDK for Python (Boto3) that the function uses to scan and delete items from the DynamoDB table.",
                            {
                                "code_example": "boto3"
                            },
                            "Copy and paste the following code into a file named requirements.txt.",
                            "requirements.txt manifest file",
                            "A version of the SDK for Python (Boto3) is included as part of the Lambda runtime, so your code would run without adding Boto3 to your             function's deployment package. However, to maintain full control of your function's dependencies and avoid possible issues with             version misalignment, best practice for Python is to include all function dependencies in your function's deployment package.             See Runtime dependencies in Python to learn more.",
                            "Note",
                            "For this example, your function code has only one dependency that isn't part of the standard Python library -           the SDK for Python (Boto3) that the function uses to scan and delete items from the DynamoDB table.",
                            {
                                "code_example": "boto3"
                            },
                            "Copy and paste the following code into a file named requirements.txt.",
                            "Note that responses from DynamoDB query and scan operations are limited to a maximum of 1 MB in size. If the response is larger than 1 MB, DynamoDB paginates the       data and returns a LastEvaluatedKey element in the response. To ensure that our function processes all the records in the table, we check for the presence of this key       and continue performing table scans from the last evaluated position until the whole table has been scanned.",
                            "When the function is invoked by EventBridge Scheduler, it uses the AWS SDK for Python (Boto3) to create a connection to the DynamoDB table on which the scheduled maintenance task is to be performed.     It then uses the Python datetime library to calculate the date one year ago, before scanning the table for items older than this and deleting them.",
                            "The Python function code contains the handler function (lambda_handler) that Lambda runs when your function is     invoked.",
                            {
                                "code_example": "import boto3\nfrom datetime import datetime, timedelta\nfrom boto3.dynamodb.conditions import Key, Attr\nimport logging\n\nlogger = logging.getLogger()\nlogger.setLevel(\"INFO\")\n\ndef lambda_handler(event, context):\n    # Initialize the DynamoDB client\n    dynamodb = boto3.resource('dynamodb')\n    \n    # Specify the table name\n    table_name = 'MyOrderTable'\n    table = dynamodb.Table(table_name)\n    \n    # Get today's date\n    today = datetime.now()\n    \n    # Calculate the date one year ago\n    one_year_ago = (today - timedelta(days=365)).strftime('%Y-%m-%d')\n    \n    # Scan the table using a global secondary index\n    response = table.scan(\n        IndexName='Date-index',\n        FilterExpression='#date < :one_year_ago',\n        ExpressionAttributeNames={\n            '#date': 'Date'\n        },\n        ExpressionAttributeValues={\n            ':one_year_ago': one_year_ago\n        }\n    )\n    \n     # Delete old items\n    with table.batch_writer() as batch:\n        for item in response['Items']:\n            Order_number = item['Order_number']\n            batch.delete_item(\n                Key={\n                    'Order_number': Order_number,\n                    'Date': item['Date']\n                }\n            )\n            logger.info(f'deleted order number {Order_number}')\n    \n    # Check if there are more items to scan\n    while 'LastEvaluatedKey' in response:\n        response = table.scan(\n            IndexName='DateIndex',\n            FilterExpression='#date < :one_year_ago',\n            ExpressionAttributeNames={\n                '#date': 'Date'\n            },\n            ExpressionAttributeValues={\n                ':one_year_ago': one_year_ago\n            },\n            ExclusiveStartKey=response['LastEvaluatedKey']\n        )\n        \n        # Delete old items\n        with table.batch_writer() as batch:\n            for item in response['Items']:\n                batch.delete_item(\n                    Key={\n                        'Order_number': item['Order_number'],\n                        'Date': item['Date']\n                    }\n                )\n    \n    return {\n        'statusCode': 200,\n        'body': 'Cleanup completed successfully'\n    }"
                            },
                            "Copy and paste the following code into a file named lambda_function.py.",
                            "Python function code",
                            "Note that responses from DynamoDB query and scan operations are limited to a maximum of 1 MB in size. If the response is larger than 1 MB, DynamoDB paginates the       data and returns a LastEvaluatedKey element in the response. To ensure that our function processes all the records in the table, we check for the presence of this key       and continue performing table scans from the last evaluated position until the whole table has been scanned.",
                            "When the function is invoked by EventBridge Scheduler, it uses the AWS SDK for Python (Boto3) to create a connection to the DynamoDB table on which the scheduled maintenance task is to be performed.     It then uses the Python datetime library to calculate the date one year ago, before scanning the table for items older than this and deleting them.",
                            "The Python function code contains the handler function (lambda_handler) that Lambda runs when your function is     invoked.",
                            {
                                "code_example": "import boto3\nfrom datetime import datetime, timedelta\nfrom boto3.dynamodb.conditions import Key, Attr\nimport logging\n\nlogger = logging.getLogger()\nlogger.setLevel(\"INFO\")\n\ndef lambda_handler(event, context):\n    # Initialize the DynamoDB client\n    dynamodb = boto3.resource('dynamodb')\n    \n    # Specify the table name\n    table_name = 'MyOrderTable'\n    table = dynamodb.Table(table_name)\n    \n    # Get today's date\n    today = datetime.now()\n    \n    # Calculate the date one year ago\n    one_year_ago = (today - timedelta(days=365)).strftime('%Y-%m-%d')\n    \n    # Scan the table using a global secondary index\n    response = table.scan(\n        IndexName='Date-index',\n        FilterExpression='#date < :one_year_ago',\n        ExpressionAttributeNames={\n            '#date': 'Date'\n        },\n        ExpressionAttributeValues={\n            ':one_year_ago': one_year_ago\n        }\n    )\n    \n     # Delete old items\n    with table.batch_writer() as batch:\n        for item in response['Items']:\n            Order_number = item['Order_number']\n            batch.delete_item(\n                Key={\n                    'Order_number': Order_number,\n                    'Date': item['Date']\n                }\n            )\n            logger.info(f'deleted order number {Order_number}')\n    \n    # Check if there are more items to scan\n    while 'LastEvaluatedKey' in response:\n        response = table.scan(\n            IndexName='DateIndex',\n            FilterExpression='#date < :one_year_ago',\n            ExpressionAttributeNames={\n                '#date': 'Date'\n            },\n            ExpressionAttributeValues={\n                ':one_year_ago': one_year_ago\n            },\n            ExclusiveStartKey=response['LastEvaluatedKey']\n        )\n        \n        # Delete old items\n        with table.batch_writer() as batch:\n            for item in response['Items']:\n                batch.delete_item(\n                    Key={\n                        'Order_number': item['Order_number'],\n                        'Date': item['Date']\n                    }\n                )\n    \n    return {\n        'statusCode': 200,\n        'body': 'Cleanup completed successfully'\n    }"
                            },
                            "Copy and paste the following code into a file named lambda_function.py.",
                            "This Python script first uses the AWS SDK for Python (Boto3) to create a connection to your DynamoDB table. It then iterates over each row in the example-data CSV file, creates an item from that row, and            writes the item to the DynamoDB table using the boto3 SDK.",
                            {
                                "code_example": "import boto3\nimport csv\nfrom decimal import Decimal\n\n# Initialize the DynamoDB client\ndynamodb = boto3.resource('dynamodb')\ntable = dynamodb.Table('MyOrderTable') \nprint(\"DDB client initialized.\")\n\ndef load_data_from_csv(filename):\n    with open(filename, 'r') as file:\n        csv_reader = csv.DictReader(file)\n        for row in csv_reader:\n            item = {\n                'Order_number': row['Order_number'],\n                'Date': row['Date'],\n                'CustomerName': row['CustomerName'],\n                'ProductID': row['ProductID'],\n                'Quantity': int(row['Quantity']),\n                'TotalAmount': Decimal(str(row['TotalAmount']))\n            }\n            table.put_item(Item=item)\n            print(f\"Added item: {item['Order_number']} - {item['Date']}\")\n\nif __name__ == \"__main__\":\n    load_data_from_csv('sample_data.csv')\n    print(\"Data loading completed.\")"
                            },
                            "Copy and paste the following code into a file named load_sample_data.py.",
                            "Python script to load sample data",
                            "This Python script first uses the AWS SDK for Python (Boto3) to create a connection to your DynamoDB table. It then iterates over each row in the example-data CSV file, creates an item from that row, and            writes the item to the DynamoDB table using the boto3 SDK.",
                            {
                                "code_example": "import boto3\nimport csv\nfrom decimal import Decimal\n\n# Initialize the DynamoDB client\ndynamodb = boto3.resource('dynamodb')\ntable = dynamodb.Table('MyOrderTable') \nprint(\"DDB client initialized.\")\n\ndef load_data_from_csv(filename):\n    with open(filename, 'r') as file:\n        csv_reader = csv.DictReader(file)\n        for row in csv_reader:\n            item = {\n                'Order_number': row['Order_number'],\n                'Date': row['Date'],\n                'CustomerName': row['CustomerName'],\n                'ProductID': row['ProductID'],\n                'Quantity': int(row['Quantity']),\n                'TotalAmount': Decimal(str(row['TotalAmount']))\n            }\n            table.put_item(Item=item)\n            print(f\"Added item: {item['Order_number']} - {item['Date']}\")\n\nif __name__ == \"__main__\":\n    load_data_from_csv('sample_data.csv')\n    print(\"Data loading completed.\")"
                            },
                            "Copy and paste the following code into a file named load_sample_data.py.",
                            "This file contains some example test data to populate your DynamoDB table with in a standard comma-separated values (CSV) format.",
                            {
                                "code_example": "Date,Order_number,CustomerName,ProductID,Quantity,TotalAmount\n2023-09-01,ORD001,Alejandro Rosalez,PROD123,2,199.98\n2023-09-01,ORD002,Akua Mansa,PROD456,1,49.99\n2023-09-02,ORD003,Ana Carolina Silva,PROD789,3,149.97\n2023-09-03,ORD004,Arnav Desai,PROD123,1,99.99\n2023-10-01,ORD005,Carlos Salazar,PROD456,2,99.98\n2023-10-02,ORD006,Diego Ramirez,PROD789,1,49.99\n2023-10-03,ORD007,Efua Owusu,PROD123,4,399.96\n2023-10-04,ORD008,John Stiles,PROD456,2,99.98\n2023-10-05,ORD009,Jorge Souza,PROD789,3,149.97\n2023-10-06,ORD010,Kwaku Mensah,PROD123,1,99.99\n2023-11-01,ORD011,Li Juan,PROD456,5,249.95\n2023-11-02,ORD012,Marcia Oliveria,PROD789,2,99.98\n2023-11-03,ORD013,Maria Garcia,PROD123,3,299.97\n2023-11-04,ORD014,Martha Rivera,PROD456,1,49.99\n2023-11-05,ORD015,Mary Major,PROD789,4,199.96\n2023-12-01,ORD016,Mateo Jackson,PROD123,2,199.99\n2023-12-02,ORD017,Nikki Wolf,PROD456,3,149.97\n2023-12-03,ORD018,Pat Candella,PROD789,1,49.99\n2023-12-04,ORD019,Paulo Santos,PROD123,5,499.95\n2023-12-05,ORD020,Richard Roe,PROD456,2,99.98\n2024-01-01,ORD021,Saanvi Sarkar,PROD789,3,149.97\n2024-01-02,ORD022,Shirley Rodriguez,PROD123,1,99.99\n2024-01-03,ORD023,Sofia Martinez,PROD456,4,199.96\n2024-01-04,ORD024,Terry Whitlock,PROD789,2,99.98\n2024-01-05,ORD025,Wang Xiulan,PROD123,3,299.97"
                            },
                            "Copy and paste the following code into a file named sample_data.csv.",
                            "Sample database data file",
                            "This file contains some example test data to populate your DynamoDB table with in a standard comma-separated values (CSV) format.",
                            {
                                "code_example": "Date,Order_number,CustomerName,ProductID,Quantity,TotalAmount\n2023-09-01,ORD001,Alejandro Rosalez,PROD123,2,199.98\n2023-09-01,ORD002,Akua Mansa,PROD456,1,49.99\n2023-09-02,ORD003,Ana Carolina Silva,PROD789,3,149.97\n2023-09-03,ORD004,Arnav Desai,PROD123,1,99.99\n2023-10-01,ORD005,Carlos Salazar,PROD456,2,99.98\n2023-10-02,ORD006,Diego Ramirez,PROD789,1,49.99\n2023-10-03,ORD007,Efua Owusu,PROD123,4,399.96\n2023-10-04,ORD008,John Stiles,PROD456,2,99.98\n2023-10-05,ORD009,Jorge Souza,PROD789,3,149.97\n2023-10-06,ORD010,Kwaku Mensah,PROD123,1,99.99\n2023-11-01,ORD011,Li Juan,PROD456,5,249.95\n2023-11-02,ORD012,Marcia Oliveria,PROD789,2,99.98\n2023-11-03,ORD013,Maria Garcia,PROD123,3,299.97\n2023-11-04,ORD014,Martha Rivera,PROD456,1,49.99\n2023-11-05,ORD015,Mary Major,PROD789,4,199.96\n2023-12-01,ORD016,Mateo Jackson,PROD123,2,199.99\n2023-12-02,ORD017,Nikki Wolf,PROD456,3,149.97\n2023-12-03,ORD018,Pat Candella,PROD789,1,49.99\n2023-12-04,ORD019,Paulo Santos,PROD123,5,499.95\n2023-12-05,ORD020,Richard Roe,PROD456,2,99.98\n2024-01-01,ORD021,Saanvi Sarkar,PROD789,3,149.97\n2024-01-02,ORD022,Shirley Rodriguez,PROD123,1,99.99\n2024-01-03,ORD023,Sofia Martinez,PROD456,4,199.96\n2024-01-04,ORD024,Terry Whitlock,PROD789,2,99.98\n2024-01-05,ORD025,Wang Xiulan,PROD123,3,299.97"
                            },
                            "Copy and paste the following code into a file named sample_data.csv.",
                            "To learn more about creating and configuring a DynamoDB table using the AWS::DynamoDB::Table resource, see AWS::DynamoDB::Table in           the AWS CloudFormation User Guide.",
                            "This AWS SAM template defines the DynamoDB table resource you create to test your app. The table uses a primary key of Order_number with a sort           key of Date. In order for your Lambda function to find items directly by date, we also define a Global Secondary Index         named Date-index.",
                            "AWS SAM templates use a standard naming convention of template.yaml. In this example, you have two template files - one to create the           example database and another to create the app itself. Save them in separate sub-directories in your project folder.",
                            "Note",
                            {
                                "code_example": "AWSTemplateFormatVersion: '2010-09-09'\nTransform: AWS::Serverless-2016-10-31\nDescription: SAM Template for DynamoDB Table with Order_number as Partition Key and Date as Sort Key\n\nResources:\n  MyDynamoDBTable:\n    Type: AWS::DynamoDB::Table\n    DeletionPolicy: Retain\n    UpdateReplacePolicy: Retain\n    Properties:\n      TableName: MyOrderTable\n      BillingMode: PAY_PER_REQUEST\n      AttributeDefinitions:\n        - AttributeName: Order_number\n          AttributeType: S\n        - AttributeName: Date\n          AttributeType: S\n      KeySchema:\n        - AttributeName: Order_number\n          KeyType: HASH\n        - AttributeName: Date\n          KeyType: RANGE\n      SSESpecification:\n        SSEEnabled: true\n      GlobalSecondaryIndexes:\n        - IndexName: Date-index\n          KeySchema:\n            - AttributeName: Date\n              KeyType: HASH\n          Projection:\n            ProjectionType: ALL\n      PointInTimeRecoverySpecification:\n        PointInTimeRecoveryEnabled: true\n\nOutputs:\n  TableName:\n    Description: DynamoDB Table Name\n    Value: !Ref MyDynamoDBTable\n  TableArn:\n    Description: DynamoDB Table ARN\n    Value: !GetAtt MyDynamoDBTable.Arn"
                            },
                            "Copy and paste the following code into a file named template.yaml.",
                            "AWS SAM template (example DynamoDB table)",
                            "To learn more about creating and configuring a DynamoDB table using the AWS::DynamoDB::Table resource, see AWS::DynamoDB::Table in           the AWS CloudFormation User Guide.",
                            "This AWS SAM template defines the DynamoDB table resource you create to test your app. The table uses a primary key of Order_number with a sort           key of Date. In order for your Lambda function to find items directly by date, we also define a Global Secondary Index         named Date-index.",
                            "AWS SAM templates use a standard naming convention of template.yaml. In this example, you have two template files - one to create the           example database and another to create the app itself. Save them in separate sub-directories in your project folder.",
                            "Note",
                            {
                                "code_example": "AWSTemplateFormatVersion: '2010-09-09'\nTransform: AWS::Serverless-2016-10-31\nDescription: SAM Template for DynamoDB Table with Order_number as Partition Key and Date as Sort Key\n\nResources:\n  MyDynamoDBTable:\n    Type: AWS::DynamoDB::Table\n    DeletionPolicy: Retain\n    UpdateReplacePolicy: Retain\n    Properties:\n      TableName: MyOrderTable\n      BillingMode: PAY_PER_REQUEST\n      AttributeDefinitions:\n        - AttributeName: Order_number\n          AttributeType: S\n        - AttributeName: Date\n          AttributeType: S\n      KeySchema:\n        - AttributeName: Order_number\n          KeyType: HASH\n        - AttributeName: Date\n          KeyType: RANGE\n      SSESpecification:\n        SSEEnabled: true\n      GlobalSecondaryIndexes:\n        - IndexName: Date-index\n          KeySchema:\n            - AttributeName: Date\n              KeyType: HASH\n          Projection:\n            ProjectionType: ALL\n      PointInTimeRecoverySpecification:\n        PointInTimeRecoveryEnabled: true\n\nOutputs:\n  TableName:\n    Description: DynamoDB Table Name\n    Value: !Ref MyDynamoDBTable\n  TableArn:\n    Description: DynamoDB Table ARN\n    Value: !GetAtt MyDynamoDBTable.Arn"
                            },
                            "Copy and paste the following code into a file named template.yaml."
                        ]
                    },
                    {
                        "sub_header": "Creating and populating the example DynamoDB table",
                        "content": [
                            "To test your scheduled-maintenance app, you first create a DynamoDB table and populate it with some sample data. You can create the table either manually using the       AWS Management Console or by using AWS SAM. We recommend that you use AWS SAM to quickly create and configure the table using a few AWS CLI commands.",
                            "  1 : Open the Tables page of the DynamoDB console.",
                            "  2 : Choose Create table.",
                            "  3 : Create the table by doing the following:Under Table details, for Table name, enter MyOrderTable.For Partition key, enter Order_number and leave the type as String.For Sort key, enter Date and leave the type as String.Leave Table settings set to Default settings and choose Create table.",
                            "  4 : When your table has finished creating and its Status shows as Active, create a global secondary index (GSI) by doing the               following. Your app will use this GSI to search for items directly by date to determine what to delete.Choose MyOrderTable from the list of tables.Choose the Indexes tab.Under Global secondary indexes, choose Create index.Under Index details, enter Date for the Partition key and leave the                   Data type set to String.For Index name, enter Date-index.Leave all other parameters set to their default values, scroll to the bottom of the page, and choose Create index.",
                            "To create the DynamoDB table",
                            "anchor",
                            "anchor",
                            "  1.Console : MyOrderTable",
                            "  2.AWS SAM : template.yaml",
                            "After you've created your table, you next add some sample data to test your app. The CSV file sample_data.csv you downloaded     earlier contains a number of example entries comprised of order numbers, dates, and customer and order information. Use the provided python script     load_sample_data.py to add this data to your table.",
                            "  1 : Navigate to the directory containing the sample_data.csv and load_sample_data.py files. If these files are in         separate directories, move them so they're saved in the same location.",
                            "  \nCreate a Python virtual environment to run the script in by running the following command. We recommend that you use a virtual environment because in a following \n        step you'll need to install the AWS SDK for Python (Boto3).\n \n",
                            {
                                "code_example": "python -m venv venv"
                            },
                            "  \nActivate the virtual environment by running the following command.\n \n",
                            {
                                "code_example": "source venv/bin/activate"
                            },
                            "  \nInstall the SDK for Python (Boto3) in your virtual environment by running the following command. The script uses this library to connect to your DynamoDB table and add the items.\n \n",
                            {
                                "code_example": "pip install boto3"
                            },
                            "  \nRun the script to populate the table by running the following command.\n \nIf the script runs successfully, it should print each item to the console as it loads it and report Data loading completed.\n",
                            {
                                "code_example": "python load_sample_data.py"
                            },
                            "  \nDeactivate the virtual environment by running the following command.\n \n",
                            {
                                "code_example": "deactivate"
                            },
                            "  7 : You can verify that the data has been loaded to your DynamoDB table by doing the following:Open the Explore items page of the DynamoDB console and select your table (MyOrderTable).In the Items returned pane, you should see the 25 items from the CSV file that the script added to the table.",
                            "To add the sample data to the table"
                        ]
                    },
                    {
                        "sub_header": "Creating the scheduled-maintenance app",
                        "content": [
                            "You can create and deploy the resources for this example app step by step using the AWS Management Console or by using AWS SAM. In a production environment, we       recommend that you use an Infrustracture-as-Code (IaC) tool like AWS SAM to repeatably deploy serverless applications without using manual processes.",
                            "For this example, follow the console instructions to learn how to configure each AWS resource separately, or follow the AWS SAM instructions     to quickly deploy the app using AWS CLI commands.",
                            "  1 : Open the EventBridge console.",
                            "  2 : In the left navigation pane, choose Schedulers under the                        Scheduler section.",
                            "  3 : Choose Create schedule.",
                            "  4 : Configure the schedule by doing the following:Under Schedule name, enter a name for your schedule (for example, DynamoDBCleanupSchedule).Under Schedule pattern, choose Recurring schedule.For Schedule type leave the default as Cron-based schedule,                          then enter the following schedule details:Minutes: 0Hours: 3Day of month: 1Month: *Day of the week: ?Year: *When evaluated, this cron expression runs on the first day of every month at 03:00 AM.For Flexible time window, select Off.",
                            "  5 : Choose Next.",
                            "  6 : Configure the trigger for your Lambda function by doing the following:In the Target detail pane, leave Target API set to Templated targets,                         then select AWS Lambda Invoke.Under Invoke, select your Lambda function (ScheduledDBMaintenance) from the dropdown list.Leave the Payload empty and choose Next.Scroll down to Permissions and select Create a new role for this schedule.                            When you create a new EventBridge Scheduler schedule using the console, EventBridge Scheduler creates a new policy with the required                          permissions the schedule needs to invoke your function. For more information about managing your schedule permissions, see                          Cron-based schedules.                          in the EventBridge Scheduler User Guide.Choose Next.",
                            "  7 : Review your settings and choose Create schedule to complete creation of the schedule and Lambda trigger.",
                            "To set up EventBridge Scheduler as a trigger (console)",
                            "  1 : Open the Configuration tab, then choose                        Permissions from the left navigation bar.",
                            "  2 : Choose the role name under Execution role.",
                            "  3 : In the IAM console, choose Add permissions, then                        Create inline policy.",
                            "  \nUse the JSON editor and enter the following policy:\n \n",
                            {
                                "code_example": "{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"dynamodb:Scan\",\n                \"dynamodb:DeleteItem\",\n                \"dynamodb:BatchWriteItem\"\n            ],\n            \"Resource\": \"arn:aws:dynamodb:*:*:table/MyOrderTable\"\n        }\n    ]\n}"
                            },
                            "  5 : Name the policy DynamoDBCleanupPolicy, then create it.",
                            "To give your function the permissions it needs to read and delete DynamoDB items, you need to add a policy to your function's                     execution role defining the necessary permissions.",
                            "To set Up IAM permissions",
                            "  1 : Select the Configuration tab for your function.",
                            "  2 : Select Monitoring and operations tools.",
                            "  3 : In the Logging configuration pane, choose Edit.",
                            "  4 : For Logging configuration, select JSON.",
                            "  5 : Choose Save.",
                            "You can configure Lambda functions to output logs in either unstructured text or JSON format. We recommend that you use JSON format for logs to                   make it easier to search and filter log data. To learn more about Lambda log configuration options, see Configuring advanced logging controls for Lambda functions.",
                            "To configure the log format (console)",
                            "  1 : Select the Configuration tab for your function.",
                            "  2 : In the General configuration pane, choose Edit.",
                            "  3 : Set Memory to 256 MB and Timeout to 15 seconds.                  If you are processing a large table with many records, for example in the case of a production environment,                  you might consider setting Timeout to a larger number. This gives your function                  more time to scan, and clean the database.",
                            "  4 : Choose Save.",
                            "To configure the function memory and timeout (console)",
                            "  1 : Open the Functions page of the Lambda console.",
                            "  2 : Choose Create function.",
                            "  3 : Choose Author from scratch.",
                            "  4 : Under Basic information, do the following:For Function name, enter ScheduledDBMaintenance.For Runtime choose the latest Python version.For Architecture, choose x86_64.",
                            "  5 : Choose Create function.",
                            "  6 : After your function is created, you can configure your function with the provided function code.In the Code source pane, replace the Hello world code that Lambda created with the Python function code from                        the lambda_function.py file that you saved earlier.In the DEPLOY section, choose Deploy to update your function's code:",
                            "First, create a function containing basic starter code. You then                 replace this code with your own function code by either copying and pasting the code directly in the Lambda code editor, or by uploading your code                as a .zip package. For this task, we recommend simply copying and pasting the code.",
                            "To create the function using the AWS Management Console",
                            "anchor",
                            "anchor",
                            "  1.Console : .zip",
                            "  2.AWS SAM : template.yaml"
                        ]
                    },
                    {
                        "sub_header": "Testing the app",
                        "content": [
                            "      To test that your schedule correctly triggers your function, and that your function correctly cleans records      from the database, you can temporarily modify your schedule to run once at a specific time. You can then run sam deploy again to      reset your recurrence schedule to run once a month.    ",
                            "  1 : Navigate back to the EventBridge Scheduler console page.",
                            "  2 : Choose your schedule, then choose Edit.",
                            "  3 : In the Schedule pattern section, under Recurrence, choose One-time schedule.",
                            "  4 :           Set your invocation time to a few minutes from now, review your settings, then choose Save.        ",
                            "To run the application using the AWS Management Console",
                            "      After the schedule runs and invokes its target, you run the test_app.py script to verify that your function successfully removed all old records      from the DynamoDB table.    ",
                            "  1 :           In your command line windown, navigate to the folder where you saved test_app.py.        ",
                            "  \n\n          Run the script.\n        \n \n\n          If successful, you will see the following output.\n        \nTotal number of old records: 0\n",
                            {
                                "code_example": "python test_app.py"
                            },
                            "To verify that old records are deleted using a Python script"
                        ]
                    },
                    {
                        "sub_header": "Next steps",
                        "content": [
                            "      You can now modify the EventBridge Scheduler schedule to meet your partifuclar application requirements. EventBridge Scheduler supports the following schedule expressions: cron, rate, and one-time schedules.    ",
                            "      For more information about EventBridge Scheduler schedule expresssions, see Schedule types in the      EventBridge Scheduler User Guide.    "
                        ]
                    }
                ]
            }
        ],
        "sections": [
            "The following examples provide function code and  infrastructure as code (IaC) templates to quickly create and deploy serverless apps that implement some common Lambda uses cases. The  examples also include code examples and instructions to test the apps after you deploy them.",
            "For each of the example apps, we provide instructions to either create and configure resources manually using the AWS Management Console, or to   use the AWS Serverless Application Model to deploy the resources using IaC. Follow the console intructions to learn more about configuring the individual AWS   resources for each app, or use to AWS SAM to quickly deploy resources as you would in a production environment.",
            "You can use the provided examples as a basis for your own serverless applications by modifying the provided function code and templates   for your own use case.",
            "We're continuing to create new examples, so check back again to find more severless apps for common Lambda use cases.",
            {
                "sub_header": "Example apps",
                "content": [
                    "  1.Example serverless file-processing appCreate a serverless app to automatically perform a file-processing task when an object is uploaded to an Amazon S3 bucket. In this         example, when a PDF file is uploaded, the app encrypts the file and saves it to another S3 bucket.",
                    "  2.Example scheduled cron task appCreate an app to perform a scheduled task using a cron schedule. In this example, the app performs maintenance on a         Amazon DynamoDB table by deleting entries more than 12 months old."
                ]
            }
        ]
    },
    {
        "title": "Building with TypeScript",
        "href": "https://docs.aws.amazon.com/lambda/latest/dg/lambda-typescript.html",
        "contents": [
            {
                "title": "Handler",
                "href": "https://docs.aws.amazon.com/lambda/latest/dg/typescript-handler.html",
                "sections": []
            },
            {
                "title": "Deploy .zip file archives",
                "href": "https://docs.aws.amazon.com/lambda/latest/dg/typescript-package.html",
                "sections": []
            },
            {
                "title": "Deploy container images",
                "href": "https://docs.aws.amazon.com/lambda/latest/dg/typescript-image.html",
                "sections": []
            },
            {
                "title": "Layers",
                "href": "https://docs.aws.amazon.com/lambda/latest/dg/typescript-layers.html",
                "sections": []
            },
            {
                "title": "Context",
                "href": "https://docs.aws.amazon.com/lambda/latest/dg/typescript-context.html",
                "sections": []
            },
            {
                "title": "Logging",
                "href": "https://docs.aws.amazon.com/lambda/latest/dg/typescript-logging.html",
                "sections": []
            },
            {
                "title": "Tracing",
                "href": "https://docs.aws.amazon.com/lambda/latest/dg/typescript-tracing.html",
                "sections": []
            }
        ],
        "sections": []
    },
    {
        "title": "Integrating other services",
        "href": "https://docs.aws.amazon.com/lambda/latest/dg/lambda-services.html",
        "contents": [
            {
                "title": "Apache Kafka",
                "href": "https://docs.aws.amazon.com/lambda/latest/dg/with-kafka.html",
                "contents": [
                    {
                        "title": "Configure event source",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/with-kafka-configure.html",
                        "sections": []
                    },
                    {
                        "title": "Process messages",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/with-kafka-process.html",
                        "sections": []
                    },
                    {
                        "title": "Event filtering",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/with-kafka-filtering.html",
                        "sections": []
                    },
                    {
                        "title": "On-failure destinations",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/with-kafka-on-failure.html",
                        "sections": []
                    },
                    {
                        "title": "Troubleshooting",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/with-kafka-troubleshoot.html",
                        "sections": []
                    }
                ],
                "source": "Aws",
                "sections": []
            },
            {
                "title": "API Gateway",
                "href": "https://docs.aws.amazon.com/lambda/latest/dg/services-apigateway.html",
                "contents": [
                    {
                        "title": "Tutorial",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/services-apigateway-tutorial.html",
                        "sections": []
                    },
                    {
                        "title": "Errors",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/services-apigateway-errors.html",
                        "sections": []
                    },
                    {
                        "title": "Select an HTTP invoke method for Lambda",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/apig-http-invoke-decision.html",
                        "sections": []
                    }
                ],
                "source": "Aws",
                "sections": []
            },
            {
                "title": "Infrastructure Composer",
                "href": "https://docs.aws.amazon.com/lambda/latest/dg/services-appcomposer.html",
                "sections": []
            },
            {
                "title": "CloudFormation",
                "href": "https://docs.aws.amazon.com/lambda/latest/dg/services-cloudformation.html",
                "sections": []
            },
            {
                "title": "Amazon DocumentDB",
                "href": "https://docs.aws.amazon.com/lambda/latest/dg/with-documentdb.html",
                "contents": [
                    {
                        "title": "Tutorial",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/with-documentdb-tutorial.html",
                        "sections": []
                    }
                ],
                "source": "Aws",
                "sections": []
            },
            {
                "title": "DynamoDB",
                "href": "https://docs.aws.amazon.com/lambda/latest/dg/with-ddb.html",
                "contents": [
                    {
                        "title": "Create mapping",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/services-dynamodb-eventsourcemapping.html",
                        "sections": []
                    },
                    {
                        "title": "Batch item failures",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/services-ddb-batchfailurereporting.html",
                        "sections": []
                    },
                    {
                        "title": "Error handling",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/services-dynamodb-errors.html",
                        "sections": []
                    },
                    {
                        "title": "Stateful processing",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/services-ddb-windows.html",
                        "sections": []
                    },
                    {
                        "title": "Parameters",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/services-ddb-params.html",
                        "sections": []
                    },
                    {
                        "title": "Event filtering",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/with-ddb-filtering.html",
                        "sections": []
                    },
                    {
                        "title": "Tutorial",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/with-ddb-example.html",
                        "sections": []
                    }
                ],
                "source": "Aws",
                "sections": []
            },
            {
                "title": "EC2",
                "href": "https://docs.aws.amazon.com/lambda/latest/dg/services-ec2.html",
                "sections": []
            },
            {
                "title": "Elastic Load Balancing (Application Load Balancer)",
                "href": "https://docs.aws.amazon.com/lambda/latest/dg/services-alb.html",
                "sections": []
            },
            {
                "title": "Invoke using an EventBridge Scheduler",
                "href": "https://docs.aws.amazon.com/lambda/latest/dg/with-eventbridge-scheduler.html",
                "sections": []
            },
            {
                "title": "IoT",
                "href": "https://docs.aws.amazon.com/lambda/latest/dg/services-iot.html",
                "sections": []
            },
            {
                "title": "Kinesis Data Streams",
                "href": "https://docs.aws.amazon.com/lambda/latest/dg/with-kinesis.html",
                "contents": [
                    {
                        "title": "Create mapping",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/services-kinesis-create.html",
                        "sections": []
                    },
                    {
                        "title": "Batch item failures",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/services-kinesis-batchfailurereporting.html",
                        "sections": []
                    },
                    {
                        "title": "Error handling",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/kinesis-on-failure-destination.html",
                        "sections": []
                    },
                    {
                        "title": "Stateful processing",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/services-kinesis-windows.html",
                        "sections": []
                    },
                    {
                        "title": "Parameters",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/services-kinesis-parameters.html",
                        "sections": []
                    },
                    {
                        "title": "Event filtering",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/with-kinesis-filtering.html",
                        "sections": []
                    },
                    {
                        "title": "Tutorial",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/with-kinesis-example.html",
                        "sections": []
                    }
                ],
                "source": "Aws",
                "sections": []
            },
            {
                "title": "MQ",
                "href": "https://docs.aws.amazon.com/lambda/latest/dg/with-mq.html",
                "contents": [
                    {
                        "title": "Configure event source",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/process-mq-messages-with-lambda.html",
                        "sections": []
                    },
                    {
                        "title": "Parameters",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/services-mq-params.html",
                        "sections": []
                    },
                    {
                        "title": "Event filtering",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/with-mq-filtering.html",
                        "sections": []
                    },
                    {
                        "title": "Troubleshoot",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/services-mq-errors.html",
                        "sections": []
                    }
                ],
                "source": "Aws",
                "sections": []
            },
            {
                "title": "MSK",
                "href": "https://docs.aws.amazon.com/lambda/latest/dg/with-msk.html",
                "contents": [
                    {
                        "title": "Configure event source",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/with-msk-configure.html",
                        "sections": []
                    },
                    {
                        "title": "Process messages",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/with-msk-process.html",
                        "sections": []
                    },
                    {
                        "title": "Event filtering",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/with-msk-filtering.html",
                        "sections": []
                    },
                    {
                        "title": "On-failure destinations",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/with-msk-on-failure.html",
                        "sections": []
                    },
                    {
                        "title": "Tutorial",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/services-msk-tutorial.html",
                        "sections": []
                    }
                ],
                "source": "Aws",
                "sections": []
            },
            {
                "title": "RDS",
                "href": "https://docs.aws.amazon.com/lambda/latest/dg/services-rds.html",
                "sections": []
            },
            {
                "title": "S3",
                "href": "https://docs.aws.amazon.com/lambda/latest/dg/with-s3.html",
                "contents": [
                    {
                        "title": "Tutorial: Use an S3 trigger",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/with-s3-example.html",
                        "sections": []
                    },
                    {
                        "title": "Tutorial: Use an Amazon S3 trigger to create thumbnails",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/with-s3-tutorial.html",
                        "sections": []
                    }
                ],
                "source": "Aws",
                "sections": []
            },
            {
                "title": "SQS",
                "href": "https://docs.aws.amazon.com/lambda/latest/dg/with-sqs.html",
                "contents": [
                    {
                        "title": "Create mapping",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/services-sqs-configure.html",
                        "sections": []
                    },
                    {
                        "title": "Scaling behavior",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/services-sqs-scaling.html",
                        "sections": []
                    },
                    {
                        "title": "Error handling",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/services-sqs-errorhandling.html",
                        "sections": []
                    },
                    {
                        "title": "Parameters",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/services-sqs-parameters.html",
                        "sections": []
                    },
                    {
                        "title": "Event filtering",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/with-sqs-filtering.html",
                        "sections": []
                    },
                    {
                        "title": "Tutorial",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/with-sqs-example.html",
                        "sections": []
                    },
                    {
                        "title": "SQS cross-account tutorial",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/with-sqs-cross-account-example.html",
                        "sections": []
                    }
                ],
                "source": "Aws",
                "sections": []
            },
            {
                "title": "S3 Batch",
                "href": "https://docs.aws.amazon.com/lambda/latest/dg/services-s3-batch.html",
                "sections": []
            },
            {
                "title": "SNS",
                "href": "https://docs.aws.amazon.com/lambda/latest/dg/with-sns.html",
                "contents": [
                    {
                        "title": "Tutorial",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/with-sns-example.html",
                        "sections": []
                    }
                ],
                "source": "Aws",
                "sections": []
            }
        ],
        "sections": []
    },
    {
        "title": "Code examples",
        "href": "https://docs.aws.amazon.com/lambda/latest/dg/service_code_examples.html",
        "contents": [
            {
                "title": "Basics",
                "href": "https://docs.aws.amazon.com/lambda/latest/dg/service_code_examples_basics.html",
                "contents": [
                    {
                        "title": "Hello Lambda",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_lambda_Hello_section.html",
                        "sections": []
                    },
                    {
                        "title": "Learn the basics",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_lambda_Scenario_GettingStartedFunctions_section.html",
                        "sections": []
                    },
                    {
                        "title": "Actions",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/service_code_examples_actions.html",
                        "contents": [
                            {
                                "title": "CreateAlias",
                                "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_lambda_CreateAlias_section.html",
                                "sections": []
                            },
                            {
                                "title": "CreateFunction",
                                "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_lambda_CreateFunction_section.html",
                                "sections": []
                            },
                            {
                                "title": "DeleteAlias",
                                "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_lambda_DeleteAlias_section.html",
                                "sections": []
                            },
                            {
                                "title": "DeleteFunction",
                                "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_lambda_DeleteFunction_section.html",
                                "sections": []
                            },
                            {
                                "title": "DeleteFunctionConcurrency",
                                "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_lambda_DeleteFunctionConcurrency_section.html",
                                "sections": []
                            },
                            {
                                "title": "DeleteProvisionedConcurrencyConfig",
                                "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_lambda_DeleteProvisionedConcurrencyConfig_section.html",
                                "sections": []
                            },
                            {
                                "title": "GetAccountSettings",
                                "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_lambda_GetAccountSettings_section.html",
                                "sections": []
                            },
                            {
                                "title": "GetAlias",
                                "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_lambda_GetAlias_section.html",
                                "sections": []
                            },
                            {
                                "title": "GetFunction",
                                "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_lambda_GetFunction_section.html",
                                "sections": []
                            },
                            {
                                "title": "GetFunctionConcurrency",
                                "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_lambda_GetFunctionConcurrency_section.html",
                                "sections": []
                            },
                            {
                                "title": "GetFunctionConfiguration",
                                "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_lambda_GetFunctionConfiguration_section.html",
                                "sections": []
                            },
                            {
                                "title": "GetPolicy",
                                "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_lambda_GetPolicy_section.html",
                                "sections": []
                            },
                            {
                                "title": "GetProvisionedConcurrencyConfig",
                                "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_lambda_GetProvisionedConcurrencyConfig_section.html",
                                "sections": []
                            },
                            {
                                "title": "Invoke",
                                "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_lambda_Invoke_section.html",
                                "sections": []
                            },
                            {
                                "title": "ListFunctions",
                                "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_lambda_ListFunctions_section.html",
                                "sections": []
                            },
                            {
                                "title": "ListProvisionedConcurrencyConfigs",
                                "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_lambda_ListProvisionedConcurrencyConfigs_section.html",
                                "sections": []
                            },
                            {
                                "title": "ListTags",
                                "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_lambda_ListTags_section.html",
                                "sections": []
                            },
                            {
                                "title": "ListVersionsByFunction",
                                "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_lambda_ListVersionsByFunction_section.html",
                                "sections": []
                            },
                            {
                                "title": "PublishVersion",
                                "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_lambda_PublishVersion_section.html",
                                "sections": []
                            },
                            {
                                "title": "PutFunctionConcurrency",
                                "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_lambda_PutFunctionConcurrency_section.html",
                                "sections": []
                            },
                            {
                                "title": "PutProvisionedConcurrencyConfig",
                                "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_lambda_PutProvisionedConcurrencyConfig_section.html",
                                "sections": []
                            },
                            {
                                "title": "RemovePermission",
                                "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_lambda_RemovePermission_section.html",
                                "sections": []
                            },
                            {
                                "title": "TagResource",
                                "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_lambda_TagResource_section.html",
                                "sections": []
                            },
                            {
                                "title": "UntagResource",
                                "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_lambda_UntagResource_section.html",
                                "sections": []
                            },
                            {
                                "title": "UpdateAlias",
                                "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_lambda_UpdateAlias_section.html",
                                "sections": []
                            },
                            {
                                "title": "UpdateFunctionCode",
                                "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_lambda_UpdateFunctionCode_section.html",
                                "sections": []
                            },
                            {
                                "title": "UpdateFunctionConfiguration",
                                "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_lambda_UpdateFunctionConfiguration_section.html",
                                "sections": []
                            }
                        ],
                        "source": "Aws",
                        "sections": []
                    }
                ],
                "source": "Aws",
                "sections": []
            },
            {
                "title": "Scenarios",
                "href": "https://docs.aws.amazon.com/lambda/latest/dg/service_code_examples_scenarios.html",
                "contents": [
                    {
                        "title": "Automatically confirm known users with a Lambda function",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_cross_CognitoAutoConfirmUser_section.html",
                        "sections": []
                    },
                    {
                        "title": "Automatically migrate known users with a Lambda function",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_cross_CognitoAutoMigrateUser_section.html",
                        "sections": []
                    },
                    {
                        "title": "Create a REST API to track COVID-19 data",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_cross_ApiGatewayDataTracker_section.html",
                        "sections": []
                    },
                    {
                        "title": "Create a lending library REST API",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_cross_AuroraRestLendingLibrary_section.html",
                        "sections": []
                    },
                    {
                        "title": "Create a messenger application",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_cross_StepFunctionsMessenger_section.html",
                        "sections": []
                    },
                    {
                        "title": "Create a serverless application to manage photos",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_cross_PAM_section.html",
                        "sections": []
                    },
                    {
                        "title": "Create a websocket chat application",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_cross_ApiGatewayWebsocketChat_section.html",
                        "sections": []
                    },
                    {
                        "title": "Create an application to analyze customer feedback",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_cross_FSA_section.html",
                        "sections": []
                    },
                    {
                        "title": "Invoke a Lambda function from a browser",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_cross_LambdaForBrowser_section.html",
                        "sections": []
                    },
                    {
                        "title": "Transform data with S3 Object Lambda",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_cross_ServerlessS3DataTransformation_section.html",
                        "sections": []
                    },
                    {
                        "title": "Use API Gateway to invoke a Lambda function",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_cross_LambdaAPIGateway_section.html",
                        "sections": []
                    },
                    {
                        "title": "Use Step Functions to invoke Lambda functions",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_cross_ServerlessWorkflows_section.html",
                        "sections": []
                    },
                    {
                        "title": "Use scheduled events to invoke a Lambda function",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_cross_LambdaScheduledEvents_section.html",
                        "sections": []
                    },
                    {
                        "title": "Write custom activity data with a Lambda function after Amazon Cognito user authentication",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_cross_CognitoCustomActivityLog_section.html",
                        "sections": []
                    }
                ],
                "source": "Aws",
                "sections": []
            },
            {
                "title": "Serverless examples",
                "href": "https://docs.aws.amazon.com/lambda/latest/dg/service_code_examples_serverless_examples.html",
                "contents": [
                    {
                        "title": "Connecting to an Amazon RDS database in a Lambda function",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_serverless_connect_RDS_Lambda_section.html",
                        "sections": []
                    },
                    {
                        "title": "Invoke a Lambda function from a Kinesis trigger",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_serverless_Kinesis_Lambda_section.html",
                        "sections": []
                    },
                    {
                        "title": "Invoke a Lambda function from a DynamoDB trigger",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_serverless_DynamoDB_Lambda_section.html",
                        "sections": []
                    },
                    {
                        "title": "Invoke a Lambda function from a Amazon DocumentDB trigger",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_serverless_DocumentDB_Lambda_section.html",
                        "sections": []
                    },
                    {
                        "title": "Invoke a Lambda function from an Amazon MSK trigger",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_serverless_MSK_Lambda_section.html",
                        "sections": []
                    },
                    {
                        "title": "Invoke a Lambda function from an Amazon S3 trigger",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_serverless_S3_Lambda_section.html",
                        "sections": []
                    },
                    {
                        "title": "Invoke a Lambda function from an Amazon SNS trigger",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_serverless_SNS_Lambda_section.html",
                        "sections": []
                    },
                    {
                        "title": "Invoke a Lambda function from an Amazon SQS trigger",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_serverless_SQS_Lambda_section.html",
                        "sections": []
                    },
                    {
                        "title": "Reporting batch item failures for Lambda functions with a Kinesis trigger",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_serverless_Kinesis_Lambda_batch_item_failures_section.html",
                        "sections": []
                    },
                    {
                        "title": "Reporting batch item failures for Lambda functions with a DynamoDB trigger",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_serverless_DynamoDB_Lambda_batch_item_failures_section.html",
                        "sections": []
                    },
                    {
                        "title": "Reporting batch item failures for Lambda functions with an Amazon SQS trigger",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_serverless_SQS_Lambda_batch_item_failures_section.html",
                        "sections": []
                    }
                ],
                "source": "Aws",
                "sections": []
            }
        ],
        "sections": []
    }
]