[
    {
        "title": "What is AWS Lambda?",
        "href": "https://docs.aws.amazon.com/lambda/latest/dg/welcome.html",
        "sections": [
            "You can use AWS Lambda to run code without provisioning or managing servers.",
            " Lambda runs your code    on a high-availability compute infrastructure and performs all of the administration of the compute resources,    including server and operating system maintenance, capacity provisioning and automatic scaling, and    logging. With Lambda, all you need to do is supply your code in one of the language runtimes that Lambda supports.",
            "You organize your code into Lambda functions. The Lambda service runs your function only when needed and scales automatically. You only pay for the compute time that you consume—there is no charge when your code is not running. For more information, see AWS Lambda Pricing.",
            "Tip",
            "To learn how to build serverless solutions, check out the Serverless Developer Guide.",
            {
                "sub_header": "When to use Lambda",
                "content": [
                    "Lambda is an ideal compute service for application scenarios that need to scale up rapidly, and scale down to      zero when not in demand. For example, you can use Lambda for:",
                    "  1.File processing: :  Use Amazon Simple Storage Service (Amazon S3) to trigger Lambda data processing in real time after an upload.",
                    "  2.Stream processing: :  Use Lambda and Amazon Kinesis to process real-time streaming data for application activity tracking, transaction order processing, clickstream analysis, data cleansing, log filtering, indexing, social media analysis, Internet of Things (IoT) device data telemetry, and metering.",
                    "  3.Web applications: :  Combine Lambda with other AWS services to build powerful web applications that automatically scale up and down and run in a highly available configuration across multiple data centers.",
                    "  4.IoT backends: :  Build serverless backends using Lambda to handle web, mobile, IoT, and third-party API requests.",
                    "  5.Mobile backends: :  Build backends using Lambda and Amazon API Gateway  to authenticate and process API requests. Use AWS Amplify to easily integrate with your iOS, Android, Web, and React Native frontends.",
                    "When using Lambda, you are responsible only for your code. Lambda manages the compute fleet that offers a      balance of memory, CPU, network, and other resources to run your code. Because Lambda manages these resources, you      cannot log in to compute instances or customize the operating system on provided        runtimes. Lambda performs operational and administrative activities on your behalf, including managing      capacity, monitoring, and logging your Lambda functions."
                ]
            },
            {
                "sub_header": "Key features",
                "content": [
                    "The following key features help you develop Lambda applications that are scalable, secure, and easily      extensible:",
                    "  1. Environment variables : \nUse environment variables to adjust your function's behavior without updating code.\n",
                    "  2.Versions : \nManage the deployment of your functions with versions, so that, for example, a new function can be used for beta testing without affecting users of the stable production version.\n",
                    "  3.Container images : \nCreate a container image for a Lambda function by using an AWS provided base image or an alternative base\n            image so that you can reuse your existing container tooling or deploy larger workloads that rely on sizable dependencies, such as machine learning.\n",
                    "  4.Layers : \nPackage libraries and other dependencies to reduce the size of deployment archives and makes it faster to deploy your code.\n",
                    "  5.Lambda extensions : \nAugment your Lambda functions with tools for monitoring, observability, security, and governance.\n",
                    "  6.Function URLs : \nAdd a dedicated HTTP(S) endpoint to your Lambda function.\n",
                    "  7.Response streaming : \nConfigure your Lambda function URLs to stream response payloads back to clients from Node.js functions, to improve time to first byte (TTFB) performance or to return larger payloads.\n",
                    "  8.Concurrency and scaling controls : \nApply fine-grained control over the scaling and responsiveness of your production applications.\n",
                    "  9.Code signing : \nVerify that only approved developers publish unaltered, trusted code in your Lambda functions \n",
                    "  10.Private networking : \nCreate a private network for resources such as databases, cache instances, or internal services.\n",
                    "  11.File system access : \nConfigure a function to mount an Amazon Elastic File System (Amazon EFS) to a local directory, so that your function code can access and modify shared resources safely and at high concurrency.\n",
                    "  12.Lambda SnapStart for Java : \nImprove startup performance for Java runtimes by up to 10x at no extra cost, typically with no changes to your function code.\n",
                    " Environment variablesUse environment variables to adjust your function's behavior without updating code.VersionsManage the deployment of your functions with versions, so that, for example, a new function can be used for beta testing without affecting users of the stable production version.Container imagesCreate a container image for a Lambda function by using an AWS provided base image or an alternative base            image so that you can reuse your existing container tooling or deploy larger workloads that rely on sizable dependencies, such as machine learning.LayersPackage libraries and other dependencies to reduce the size of deployment archives and makes it faster to deploy your code.Lambda extensionsAugment your Lambda functions with tools for monitoring, observability, security, and governance.Function URLsAdd a dedicated HTTP(S) endpoint to your Lambda function.Response streamingConfigure your Lambda function URLs to stream response payloads back to clients from Node.js functions, to improve time to first byte (TTFB) performance or to return larger payloads.Concurrency and scaling controlsApply fine-grained control over the scaling and responsiveness of your production applications.Code signingVerify that only approved developers publish unaltered, trusted code in your Lambda functions Private networkingCreate a private network for resources such as databases, cache instances, or internal services.File system accessConfigure a function to mount an Amazon Elastic File System (Amazon EFS) to a local directory, so that your function code can access and modify shared resources safely and at high concurrency.Lambda SnapStart for JavaImprove startup performance for Java runtimes by up to 10x at no extra cost, typically with no changes to your function code."
                ]
            }
        ]
    },
    {
        "title": "Example apps",
        "href": "https://docs.aws.amazon.com/lambda/latest/dg/example-apps.html",
        "contents": [
            {
                "title": "File-processing app",
                "href": "https://docs.aws.amazon.com/lambda/latest/dg/file-processing-app.html",
                "sections": [
                    "One of the most common use cases for Lambda is to perform file processing tasks. For example, you might use a Lambda function     to automatically create PDF files from HTML files or images, or to create thumbnails when a user uploads an image.",
                    "In this example, you create an app which automatically encrypts PDF files when they are uploaded to an Amazon Simple Storage Service (Amazon S3) bucket.     To implement this app, you create the following resources:",
                    "  1.An S3 bucket for users to upload PDF files to",
                    "  2.A Lambda function in Python which reads the uploaded file and creates an encrypted, password-protected version of it",
                    "  3.A second S3 bucket for Lambda to save the encrypted file in",
                    "You also create an AWS Identity and Access Management (IAM) policy to give your Lambda function permission to perform read and write operations     on your S3 buckets.",
                    "Tip",
                    "If you’re brand new to Lambda, we recommend that you carry out the tutorial Create your first Lambda function before      creating this example app.",
                    "You can deploy your app manually by creating and configuring resources with the AWS Management Console or the AWS Command Line Interface (AWS CLI). You can     also deploy the app by using the AWS Serverless Application Model (AWS SAM). AWS SAM is an infrastructure as code (IaC) tool. With IaC, you don’t create     resources manually, but define them in code and then deploy them automatically.",
                    "If you want to learn more about using Lambda with IaC before deploying this example app, see Using Lambda with infrastructure as code (IaC).",
                    {
                        "sub_header": "Prerequisites",
                        "content": [
                            "Before you can create the example app, make sure you have the required command line tools installed.",
                            "  1.AWS CLI : You can manually deploy the resources for your app using either the AWS Management Console or the . To use the CLI, install it by following               the installation                 instructions in the AWS Command Line Interface User Guide.",
                            "  2.AWS SAM CLI : If you want to deploy the example app using AWS SAM, you need to install both the AWS CLI and the . To install the ,               follow the installation instructions               in the AWS SAM User Guide.",
                            "  3.pytest module : After you’ve deployed your app, you can test it using an automated Python test script that we provide. To use this script, install the               pytest package in you local development environment by running the following command:pip install pytest",
                            {
                                "code_example": "pip install pytest"
                            },
                            "To deploy the app using AWS SAM, Docker must also be installed on your build machine."
                        ]
                    },
                    {
                        "sub_header": "Downloading the example app files",
                        "content": [
                            "To create and test the example app, you create the following files in your project directory:",
                            "  1.lambda_function.py - the Python function code for the Lambda function that performs the file encryption",
                            "  2.requirements.txt - a manifest file defining the dependencies that your Python function code requires",
                            "  3.template.yaml - an AWS SAM template you can use to deploy the app",
                            "  4.test_pdf_encrypt.py - a test script you can use to automatically test your application",
                            "  5.pytest.ini - a configuration file for the the test script",
                            "Expand the following sections to view the code and to learn more about the role of each file in creating and testing your app. To create the       files on your local machine, either copy and paste the code below, or download the files from the aws-lambda-developer-guide GitHub repo.",
                            "Copy and paste the following code into a file named lambda_function.py.",
                            {
                                "code_example": "from pypdf import PdfReader, PdfWriter\nimport uuid\nimport os\nfrom urllib.parse import unquote_plus\nimport boto3\n\n# Create the S3 client to download and upload objects from S3\ns3_client = boto3.client('s3')\n\ndef lambda_handler(event, context):\n    # Iterate over the S3 event object and get the key for all uploaded files\n    for record in event['Records']:\n        bucket = record['s3']['bucket']['name']\n        key = unquote_plus(record['s3']['object']['key']) # Decode the S3 object key to remove any URL-encoded characters\n        download_path = f'/tmp/{uuid.uuid4()}.pdf' # Create a path in the Lambda tmp directory to save the file to \n        upload_path = f'/tmp/converted-{uuid.uuid4()}.pdf' # Create another path to save the encrypted file to\n        \n        # If the file is a PDF, encrypt it and upload it to the destination S3 bucket\n        if key.lower().endswith('.pdf'):\n            s3_client.download_file(bucket, key, download_path)\n            encrypt_pdf(download_path, upload_path)\n            encrypted_key = add_encrypted_suffix(key)\n            s3_client.upload_file(upload_path, f'{bucket}-encrypted', encrypted_key)\n\n# Define the function to encrypt the PDF file with a password\ndef encrypt_pdf(file_path, encrypted_file_path):\n    reader = PdfReader(file_path)\n    writer = PdfWriter()\n    \n    for page in reader.pages:\n        writer.add_page(page)\n\n    # Add a password to the new PDF\n    writer.encrypt(\"my-secret-password\")\n\n    # Save the new PDF to a file\n    with open(encrypted_file_path, \"wb\") as file:\n        writer.write(file)\n\n# Define a function to add a suffix to the original filename after encryption\ndef add_encrypted_suffix(original_key):\n    filename, extension = original_key.rsplit('.', 1)\n    return f'{filename}_encrypted.{extension}'"
                            },
                            "Note",
                            "In this example code, a password for the encrypted file (my-secret-password) is hardcoded into the           function code. In a production application, don't include sensitive information like passwords in your function code. Use           AWS Secrets Manager to securely store sensitive parameters.",
                            "The python function code contains three functions - the handler function that Lambda runs           when your function is invoked, and two separate function named add_encrypted_suffix and encrypt_pdf that the handler calls to perform the PDF encryption.",
                            "When your function is invoked by Amazon S3, Lambda passes a JSON formatted event argument to the function that contains details about the           event that caused the invocation. In this case, the information includes name of the S3 bucket and the object keys for the uploaded files.           To learn more about the format of event object for Amazon S3, see Process Amazon S3 event notifications with Lambda.",
                            "Your function then uses the AWS SDK for Python (Boto3) to download the PDF files specified in the event object to its local temporary storage directory, before           encrypting them using the pypdf library.",
                            "Finally, the function uses the Boto3 SDK to store the encrypted file in your S3 destination bucket.",
                            "Python function code",
                            "Copy and paste the following code into a file named lambda_function.py.",
                            {
                                "code_example": "from pypdf import PdfReader, PdfWriter\nimport uuid\nimport os\nfrom urllib.parse import unquote_plus\nimport boto3\n\n# Create the S3 client to download and upload objects from S3\ns3_client = boto3.client('s3')\n\ndef lambda_handler(event, context):\n    # Iterate over the S3 event object and get the key for all uploaded files\n    for record in event['Records']:\n        bucket = record['s3']['bucket']['name']\n        key = unquote_plus(record['s3']['object']['key']) # Decode the S3 object key to remove any URL-encoded characters\n        download_path = f'/tmp/{uuid.uuid4()}.pdf' # Create a path in the Lambda tmp directory to save the file to \n        upload_path = f'/tmp/converted-{uuid.uuid4()}.pdf' # Create another path to save the encrypted file to\n        \n        # If the file is a PDF, encrypt it and upload it to the destination S3 bucket\n        if key.lower().endswith('.pdf'):\n            s3_client.download_file(bucket, key, download_path)\n            encrypt_pdf(download_path, upload_path)\n            encrypted_key = add_encrypted_suffix(key)\n            s3_client.upload_file(upload_path, f'{bucket}-encrypted', encrypted_key)\n\n# Define the function to encrypt the PDF file with a password\ndef encrypt_pdf(file_path, encrypted_file_path):\n    reader = PdfReader(file_path)\n    writer = PdfWriter()\n    \n    for page in reader.pages:\n        writer.add_page(page)\n\n    # Add a password to the new PDF\n    writer.encrypt(\"my-secret-password\")\n\n    # Save the new PDF to a file\n    with open(encrypted_file_path, \"wb\") as file:\n        writer.write(file)\n\n# Define a function to add a suffix to the original filename after encryption\ndef add_encrypted_suffix(original_key):\n    filename, extension = original_key.rsplit('.', 1)\n    return f'{filename}_encrypted.{extension}'"
                            },
                            "Note",
                            "In this example code, a password for the encrypted file (my-secret-password) is hardcoded into the           function code. In a production application, don't include sensitive information like passwords in your function code. Use           AWS Secrets Manager to securely store sensitive parameters.",
                            "The python function code contains three functions - the handler function that Lambda runs           when your function is invoked, and two separate function named add_encrypted_suffix and encrypt_pdf that the handler calls to perform the PDF encryption.",
                            "When your function is invoked by Amazon S3, Lambda passes a JSON formatted event argument to the function that contains details about the           event that caused the invocation. In this case, the information includes name of the S3 bucket and the object keys for the uploaded files.           To learn more about the format of event object for Amazon S3, see Process Amazon S3 event notifications with Lambda.",
                            "Your function then uses the AWS SDK for Python (Boto3) to download the PDF files specified in the event object to its local temporary storage directory, before           encrypting them using the pypdf library.",
                            "Finally, the function uses the Boto3 SDK to store the encrypted file in your S3 destination bucket.",
                            "Copy and paste the following code into a file named lambda_function.py.from pypdf import PdfReader, PdfWriterimport uuidimport osfrom urllib.parse import unquote_plusimport boto3# Create the S3 client to download and upload objects from S3s3_client = boto3.client('s3')def lambda_handler(event, context):    # Iterate over the S3 event object and get the key for all uploaded files    for record in event['Records']:        bucket = record['s3']['bucket']['name']        key = unquote_plus(record['s3']['object']['key']) # Decode the S3 object key to remove any URL-encoded characters        download_path = f'/tmp/{uuid.uuid4()}.pdf' # Create a path in the Lambda tmp directory to save the file to         upload_path = f'/tmp/converted-{uuid.uuid4()}.pdf' # Create another path to save the encrypted file to                # If the file is a PDF, encrypt it and upload it to the destination S3 bucket        if key.lower().endswith('.pdf'):            s3_client.download_file(bucket, key, download_path)            encrypt_pdf(download_path, upload_path)            encrypted_key = add_encrypted_suffix(key)            s3_client.upload_file(upload_path, f'{bucket}-encrypted', encrypted_key)# Define the function to encrypt the PDF file with a passworddef encrypt_pdf(file_path, encrypted_file_path):    reader = PdfReader(file_path)    writer = PdfWriter()        for page in reader.pages:        writer.add_page(page)    # Add a password to the new PDF    writer.encrypt(\"my-secret-password\")    # Save the new PDF to a file    with open(encrypted_file_path, \"wb\") as file:        writer.write(file)# Define a function to add a suffix to the original filename after encryptiondef add_encrypted_suffix(original_key):    filename, extension = original_key.rsplit('.', 1)    return f'{filename}_encrypted.{extension}'NoteIn this example code, a password for the encrypted file (my-secret-password) is hardcoded into the           function code. In a production application, don't include sensitive information like passwords in your function code. Use           AWS Secrets Manager to securely store sensitive parameters.The python function code contains three functions - the handler function that Lambda runs           when your function is invoked, and two separate function named add_encrypted_suffix and encrypt_pdf that the handler calls to perform the PDF encryption.When your function is invoked by Amazon S3, Lambda passes a JSON formatted event argument to the function that contains details about the           event that caused the invocation. In this case, the information includes name of the S3 bucket and the object keys for the uploaded files.           To learn more about the format of event object for Amazon S3, see Process Amazon S3 event notifications with Lambda.Your function then uses the AWS SDK for Python (Boto3) to download the PDF files specified in the event object to its local temporary storage directory, before           encrypting them using the pypdf library.Finally, the function uses the Boto3 SDK to store the encrypted file in your S3 destination bucket.Python function codeCopy and paste the following code into a file named lambda_function.py.from pypdf import PdfReader, PdfWriterimport uuidimport osfrom urllib.parse import unquote_plusimport boto3# Create the S3 client to download and upload objects from S3s3_client = boto3.client('s3')def lambda_handler(event, context):    # Iterate over the S3 event object and get the key for all uploaded files    for record in event['Records']:        bucket = record['s3']['bucket']['name']        key = unquote_plus(record['s3']['object']['key']) # Decode the S3 object key to remove any URL-encoded characters        download_path = f'/tmp/{uuid.uuid4()}.pdf' # Create a path in the Lambda tmp directory to save the file to         upload_path = f'/tmp/converted-{uuid.uuid4()}.pdf' # Create another path to save the encrypted file to                # If the file is a PDF, encrypt it and upload it to the destination S3 bucket        if key.lower().endswith('.pdf'):            s3_client.download_file(bucket, key, download_path)            encrypt_pdf(download_path, upload_path)            encrypted_key = add_encrypted_suffix(key)            s3_client.upload_file(upload_path, f'{bucket}-encrypted', encrypted_key)# Define the function to encrypt the PDF file with a passworddef encrypt_pdf(file_path, encrypted_file_path):    reader = PdfReader(file_path)    writer = PdfWriter()        for page in reader.pages:        writer.add_page(page)    # Add a password to the new PDF    writer.encrypt(\"my-secret-password\")    # Save the new PDF to a file    with open(encrypted_file_path, \"wb\") as file:        writer.write(file)# Define a function to add a suffix to the original filename after encryptiondef add_encrypted_suffix(original_key):    filename, extension = original_key.rsplit('.', 1)    return f'{filename}_encrypted.{extension}'NoteIn this example code, a password for the encrypted file (my-secret-password) is hardcoded into the           function code. In a production application, don't include sensitive information like passwords in your function code. Use           AWS Secrets Manager to securely store sensitive parameters.The python function code contains three functions - the handler function that Lambda runs           when your function is invoked, and two separate function named add_encrypted_suffix and encrypt_pdf that the handler calls to perform the PDF encryption.When your function is invoked by Amazon S3, Lambda passes a JSON formatted event argument to the function that contains details about the           event that caused the invocation. In this case, the information includes name of the S3 bucket and the object keys for the uploaded files.           To learn more about the format of event object for Amazon S3, see Process Amazon S3 event notifications with Lambda.Your function then uses the AWS SDK for Python (Boto3) to download the PDF files specified in the event object to its local temporary storage directory, before           encrypting them using the pypdf library.Finally, the function uses the Boto3 SDK to store the encrypted file in your S3 destination bucket.",
                            "Copy and paste the following code into a file named requirements.txt.",
                            {
                                "code_example": "boto3\npypdf"
                            },
                            "For this example, your function code has only two dependencies that aren't part of the standard Python library -           the SDK for Python (Boto3) and the pypdf package the function uses to perform the PDF encryption.",
                            "Note",
                            "A version of the SDK for Python (Boto3) is included as part of the Lambda runtime, so your code would run without adding Boto3 to your             function's deployment package. However, to maintain full control of your function's dependencies and avoid possible issues with             version misalignment, best practice for Python is to include all function dependencies in your function's deployment package.           See Runtime dependencies in Python to learn more.",
                            "requirements.txt manifest file",
                            "Copy and paste the following code into a file named requirements.txt.",
                            {
                                "code_example": "boto3\npypdf"
                            },
                            "For this example, your function code has only two dependencies that aren't part of the standard Python library -           the SDK for Python (Boto3) and the pypdf package the function uses to perform the PDF encryption.",
                            "Note",
                            "A version of the SDK for Python (Boto3) is included as part of the Lambda runtime, so your code would run without adding Boto3 to your             function's deployment package. However, to maintain full control of your function's dependencies and avoid possible issues with             version misalignment, best practice for Python is to include all function dependencies in your function's deployment package.           See Runtime dependencies in Python to learn more.",
                            "Copy and paste the following code into a file named requirements.txt.boto3pypdfFor this example, your function code has only two dependencies that aren't part of the standard Python library -           the SDK for Python (Boto3) and the pypdf package the function uses to perform the PDF encryption.NoteA version of the SDK for Python (Boto3) is included as part of the Lambda runtime, so your code would run without adding Boto3 to your             function's deployment package. However, to maintain full control of your function's dependencies and avoid possible issues with             version misalignment, best practice for Python is to include all function dependencies in your function's deployment package.           See Runtime dependencies in Python to learn more.requirements.txt manifest fileCopy and paste the following code into a file named requirements.txt.boto3pypdfFor this example, your function code has only two dependencies that aren't part of the standard Python library -           the SDK for Python (Boto3) and the pypdf package the function uses to perform the PDF encryption.NoteA version of the SDK for Python (Boto3) is included as part of the Lambda runtime, so your code would run without adding Boto3 to your             function's deployment package. However, to maintain full control of your function's dependencies and avoid possible issues with             version misalignment, best practice for Python is to include all function dependencies in your function's deployment package.           See Runtime dependencies in Python to learn more.",
                            "Copy and paste the following code into a file named template.yaml.",
                            {
                                "code_example": "AWSTemplateFormatVersion: '2010-09-09'\nTransform: AWS::Serverless-2016-10-31\n\nResources:\n  EncryptPDFFunction:\n    Type: AWS::Serverless::Function\n    Properties:\n      FunctionName: EncryptPDF\n      Architectures: [x86_64]\n      CodeUri: ./\n      Handler: lambda_function.lambda_handler\n      Runtime: python3.12\n      Timeout: 15\n      MemorySize: 256\n      LoggingConfig:\n        LogFormat: JSON\n      Policies:\n        - AmazonS3FullAccess\n      Events:\n        S3Event:\n          Type: S3\n          Properties:\n            Bucket: !Ref PDFSourceBucket\n            Events: s3:ObjectCreated:*\n\n  PDFSourceBucket:\n    Type: AWS::S3::Bucket\n    Properties:\n      BucketName: EXAMPLE-BUCKET\n\n  EncryptedPDFBucket:\n    Type: AWS::S3::Bucket\n    Properties:\n      BucketName: EXAMPLE-BUCKET-encrypted"
                            },
                            "The AWS SAM template defines the resources you create for your app. In this example, the template defines a Lambda function using the         AWS::Serverless::Function type and two S3 buckets using the AWS::S3::Bucket type. The bucket names specified in the         template are placeholders. Before you deploy the app using AWS SAM, you need to edit the template to rename the buckets with globally unique names that          meet the S3 bucket naming rules. This step is           explained further in Deploy the resources using AWS SAM.",
                            "The definition of the Lambda function resource configures a trigger for the function using the S3Event event property. This         trigger causes your function to be invoked whenever an object is created in your source bucket.",
                            "The function definition also specifies an AWS Identity and Access Management (IAM) policy to be attached to the function's execution role.           The AWS managed policyAmazonS3FullAccess gives your function the permissions it needs to read and write objects to Amazon S3.",
                            "AWS SAM template",
                            "Copy and paste the following code into a file named template.yaml.",
                            {
                                "code_example": "AWSTemplateFormatVersion: '2010-09-09'\nTransform: AWS::Serverless-2016-10-31\n\nResources:\n  EncryptPDFFunction:\n    Type: AWS::Serverless::Function\n    Properties:\n      FunctionName: EncryptPDF\n      Architectures: [x86_64]\n      CodeUri: ./\n      Handler: lambda_function.lambda_handler\n      Runtime: python3.12\n      Timeout: 15\n      MemorySize: 256\n      LoggingConfig:\n        LogFormat: JSON\n      Policies:\n        - AmazonS3FullAccess\n      Events:\n        S3Event:\n          Type: S3\n          Properties:\n            Bucket: !Ref PDFSourceBucket\n            Events: s3:ObjectCreated:*\n\n  PDFSourceBucket:\n    Type: AWS::S3::Bucket\n    Properties:\n      BucketName: EXAMPLE-BUCKET\n\n  EncryptedPDFBucket:\n    Type: AWS::S3::Bucket\n    Properties:\n      BucketName: EXAMPLE-BUCKET-encrypted"
                            },
                            "The AWS SAM template defines the resources you create for your app. In this example, the template defines a Lambda function using the         AWS::Serverless::Function type and two S3 buckets using the AWS::S3::Bucket type. The bucket names specified in the         template are placeholders. Before you deploy the app using AWS SAM, you need to edit the template to rename the buckets with globally unique names that          meet the S3 bucket naming rules. This step is           explained further in Deploy the resources using AWS SAM.",
                            "The definition of the Lambda function resource configures a trigger for the function using the S3Event event property. This         trigger causes your function to be invoked whenever an object is created in your source bucket.",
                            "The function definition also specifies an AWS Identity and Access Management (IAM) policy to be attached to the function's execution role.           The AWS managed policyAmazonS3FullAccess gives your function the permissions it needs to read and write objects to Amazon S3.",
                            "Copy and paste the following code into a file named template.yaml.AWSTemplateFormatVersion: '2010-09-09'Transform: AWS::Serverless-2016-10-31Resources:  EncryptPDFFunction:    Type: AWS::Serverless::Function    Properties:      FunctionName: EncryptPDF      Architectures: [x86_64]      CodeUri: ./      Handler: lambda_function.lambda_handler      Runtime: python3.12      Timeout: 15      MemorySize: 256      LoggingConfig:        LogFormat: JSON      Policies:        - AmazonS3FullAccess      Events:        S3Event:          Type: S3          Properties:            Bucket: !Ref PDFSourceBucket            Events: s3:ObjectCreated:*  PDFSourceBucket:    Type: AWS::S3::Bucket    Properties:      BucketName: EXAMPLE-BUCKET  EncryptedPDFBucket:    Type: AWS::S3::Bucket    Properties:      BucketName: EXAMPLE-BUCKET-encryptedThe AWS SAM template defines the resources you create for your app. In this example, the template defines a Lambda function using the         AWS::Serverless::Function type and two S3 buckets using the AWS::S3::Bucket type. The bucket names specified in the         template are placeholders. Before you deploy the app using AWS SAM, you need to edit the template to rename the buckets with globally unique names that          meet the S3 bucket naming rules. This step is           explained further in Deploy the resources using AWS SAM.The definition of the Lambda function resource configures a trigger for the function using the S3Event event property. This         trigger causes your function to be invoked whenever an object is created in your source bucket.The function definition also specifies an AWS Identity and Access Management (IAM) policy to be attached to the function's execution role.           The AWS managed policyAmazonS3FullAccess gives your function the permissions it needs to read and write objects to Amazon S3.AWS SAM templateCopy and paste the following code into a file named template.yaml.AWSTemplateFormatVersion: '2010-09-09'Transform: AWS::Serverless-2016-10-31Resources:  EncryptPDFFunction:    Type: AWS::Serverless::Function    Properties:      FunctionName: EncryptPDF      Architectures: [x86_64]      CodeUri: ./      Handler: lambda_function.lambda_handler      Runtime: python3.12      Timeout: 15      MemorySize: 256      LoggingConfig:        LogFormat: JSON      Policies:        - AmazonS3FullAccess      Events:        S3Event:          Type: S3          Properties:            Bucket: !Ref PDFSourceBucket            Events: s3:ObjectCreated:*  PDFSourceBucket:    Type: AWS::S3::Bucket    Properties:      BucketName: EXAMPLE-BUCKET  EncryptedPDFBucket:    Type: AWS::S3::Bucket    Properties:      BucketName: EXAMPLE-BUCKET-encryptedThe AWS SAM template defines the resources you create for your app. In this example, the template defines a Lambda function using the         AWS::Serverless::Function type and two S3 buckets using the AWS::S3::Bucket type. The bucket names specified in the         template are placeholders. Before you deploy the app using AWS SAM, you need to edit the template to rename the buckets with globally unique names that          meet the S3 bucket naming rules. This step is           explained further in Deploy the resources using AWS SAM.The definition of the Lambda function resource configures a trigger for the function using the S3Event event property. This         trigger causes your function to be invoked whenever an object is created in your source bucket.The function definition also specifies an AWS Identity and Access Management (IAM) policy to be attached to the function's execution role.           The AWS managed policyAmazonS3FullAccess gives your function the permissions it needs to read and write objects to Amazon S3.",
                            "Copy and paste the following code into a file named test_pdf_encrypt.py.",
                            {
                                "code_example": "import boto3\nimport json\nimport pytest\nimport time\nimport os\n\n@pytest.fixture\ndef lambda_client():\n    return boto3.client('lambda')\n    \n@pytest.fixture\ndef s3_client():\n    return boto3.client('s3')\n\n@pytest.fixture\ndef logs_client():\n    return boto3.client('logs')\n\n@pytest.fixture(scope='session')\ndef cleanup():\n    # Create a new S3 client for cleanup\n    s3_client = boto3.client('s3')\n\n    yield\n    # Cleanup code will be executed after all tests have finished\n\n    # Delete test.pdf from the source bucket\n    source_bucket = 'EXAMPLE-BUCKET'\n    source_file_key = 'test.pdf'\n    s3_client.delete_object(Bucket=source_bucket, Key=source_file_key)\n    print(f\"\\nDeleted {source_file_key} from {source_bucket}\")\n\n    # Delete test_encrypted.pdf from the destination bucket\n    destination_bucket = 'EXAMPLE-BUCKET-encrypted'\n    destination_file_key = 'test_encrypted.pdf'\n    s3_client.delete_object(Bucket=destination_bucket, Key=destination_file_key)\n    print(f\"Deleted {destination_file_key} from {destination_bucket}\")\n        \n\n@pytest.mark.order(1)\ndef test_source_bucket_available(s3_client):\n    s3_bucket_name = 'EXAMPLE-BUCKET'\n    file_name = 'test.pdf'\n    file_path = os.path.join(os.path.dirname(__file__), file_name)\n\n    file_uploaded = False\n    try:\n        s3_client.upload_file(file_path, s3_bucket_name, file_name)\n        file_uploaded = True\n    except:\n        print(\"Error: couldn't upload file\")\n\n    assert file_uploaded, \"Could not upload file to S3 bucket\"\n\n    \n\n@pytest.mark.order(2)\ndef test_lambda_invoked(logs_client):\n\n    # Wait for a few seconds to make sure the logs are available\n    time.sleep(5)\n\n    # Get the latest log stream for the specified log group\n    log_streams = logs_client.describe_log_streams(\n        logGroupName='/aws/lambda/EncryptPDF',\n        orderBy='LastEventTime',\n        descending=True,\n        limit=1\n    )\n\n    latest_log_stream_name = log_streams['logStreams'][0]['logStreamName']\n\n    # Retrieve the log events from the latest log stream\n    log_events = logs_client.get_log_events(\n        logGroupName='/aws/lambda/EncryptPDF',\n        logStreamName=latest_log_stream_name\n    )\n\n    success_found = False\n    for event in log_events['events']:\n        message = json.loads(event['message'])\n        status = message.get('record', {}).get('status')\n        if status == 'success':\n            success_found = True\n            break\n\n    assert success_found, \"Lambda function execution did not report 'success' status in logs.\"\n\n@pytest.mark.order(3)\ndef test_encrypted_file_in_bucket(s3_client):\n    # Specify the destination S3 bucket and the expected converted file key\n    destination_bucket = 'EXAMPLE-BUCKET-encrypted'\n    converted_file_key = 'test_encrypted.pdf'\n\n    try:\n        # Attempt to retrieve the metadata of the converted file from the destination S3 bucket\n        s3_client.head_object(Bucket=destination_bucket, Key=converted_file_key)\n    except s3_client.exceptions.ClientError as e:\n        # If the file is not found, the test will fail\n        pytest.fail(f\"Converted file '{converted_file_key}' not found in the destination bucket: {str(e)}\")\n\ndef test_cleanup(cleanup):\n    # This test uses the cleanup fixture and will be executed last\n    pass"
                            },
                            "The automated test script executes three test functions to confirm correct operation of your app:",
                            "  1.The test test_source_bucket_available confirms that your source bucket has been successfully created             by uploading a test PDF file to the bucket.",
                            "  2.The test test_lambda_invoked interrogates the latest CloudWatch Logs log stream for your function to confirm that             when you uploaded the test file, your Lambda function ran and reported success.",
                            "  3.The test test_encrypted_file_in_bucket confirms that your destination bucket contains the encrypted test_encrypted.pdf             file.",
                            "After all these tests have run, the script runs an additional cleanup step to delete the test.pdf and test_encrypted.pdf files from         both your source and destination buckets.",
                            "As with the AWS SAM template, the bucket names specified in this file are placeholders. Before running the test, you need to edit this file           with your app's real bucket names. This step is explained further in Testing the app with the automated script",
                            "Automated test script",
                            "Copy and paste the following code into a file named test_pdf_encrypt.py.",
                            {
                                "code_example": "import boto3\nimport json\nimport pytest\nimport time\nimport os\n\n@pytest.fixture\ndef lambda_client():\n    return boto3.client('lambda')\n    \n@pytest.fixture\ndef s3_client():\n    return boto3.client('s3')\n\n@pytest.fixture\ndef logs_client():\n    return boto3.client('logs')\n\n@pytest.fixture(scope='session')\ndef cleanup():\n    # Create a new S3 client for cleanup\n    s3_client = boto3.client('s3')\n\n    yield\n    # Cleanup code will be executed after all tests have finished\n\n    # Delete test.pdf from the source bucket\n    source_bucket = 'EXAMPLE-BUCKET'\n    source_file_key = 'test.pdf'\n    s3_client.delete_object(Bucket=source_bucket, Key=source_file_key)\n    print(f\"\\nDeleted {source_file_key} from {source_bucket}\")\n\n    # Delete test_encrypted.pdf from the destination bucket\n    destination_bucket = 'EXAMPLE-BUCKET-encrypted'\n    destination_file_key = 'test_encrypted.pdf'\n    s3_client.delete_object(Bucket=destination_bucket, Key=destination_file_key)\n    print(f\"Deleted {destination_file_key} from {destination_bucket}\")\n        \n\n@pytest.mark.order(1)\ndef test_source_bucket_available(s3_client):\n    s3_bucket_name = 'EXAMPLE-BUCKET'\n    file_name = 'test.pdf'\n    file_path = os.path.join(os.path.dirname(__file__), file_name)\n\n    file_uploaded = False\n    try:\n        s3_client.upload_file(file_path, s3_bucket_name, file_name)\n        file_uploaded = True\n    except:\n        print(\"Error: couldn't upload file\")\n\n    assert file_uploaded, \"Could not upload file to S3 bucket\"\n\n    \n\n@pytest.mark.order(2)\ndef test_lambda_invoked(logs_client):\n\n    # Wait for a few seconds to make sure the logs are available\n    time.sleep(5)\n\n    # Get the latest log stream for the specified log group\n    log_streams = logs_client.describe_log_streams(\n        logGroupName='/aws/lambda/EncryptPDF',\n        orderBy='LastEventTime',\n        descending=True,\n        limit=1\n    )\n\n    latest_log_stream_name = log_streams['logStreams'][0]['logStreamName']\n\n    # Retrieve the log events from the latest log stream\n    log_events = logs_client.get_log_events(\n        logGroupName='/aws/lambda/EncryptPDF',\n        logStreamName=latest_log_stream_name\n    )\n\n    success_found = False\n    for event in log_events['events']:\n        message = json.loads(event['message'])\n        status = message.get('record', {}).get('status')\n        if status == 'success':\n            success_found = True\n            break\n\n    assert success_found, \"Lambda function execution did not report 'success' status in logs.\"\n\n@pytest.mark.order(3)\ndef test_encrypted_file_in_bucket(s3_client):\n    # Specify the destination S3 bucket and the expected converted file key\n    destination_bucket = 'EXAMPLE-BUCKET-encrypted'\n    converted_file_key = 'test_encrypted.pdf'\n\n    try:\n        # Attempt to retrieve the metadata of the converted file from the destination S3 bucket\n        s3_client.head_object(Bucket=destination_bucket, Key=converted_file_key)\n    except s3_client.exceptions.ClientError as e:\n        # If the file is not found, the test will fail\n        pytest.fail(f\"Converted file '{converted_file_key}' not found in the destination bucket: {str(e)}\")\n\ndef test_cleanup(cleanup):\n    # This test uses the cleanup fixture and will be executed last\n    pass"
                            },
                            "The automated test script executes three test functions to confirm correct operation of your app:",
                            "  1.The test test_source_bucket_available confirms that your source bucket has been successfully created             by uploading a test PDF file to the bucket.",
                            "  2.The test test_lambda_invoked interrogates the latest CloudWatch Logs log stream for your function to confirm that             when you uploaded the test file, your Lambda function ran and reported success.",
                            "  3.The test test_encrypted_file_in_bucket confirms that your destination bucket contains the encrypted test_encrypted.pdf             file.",
                            "After all these tests have run, the script runs an additional cleanup step to delete the test.pdf and test_encrypted.pdf files from         both your source and destination buckets.",
                            "As with the AWS SAM template, the bucket names specified in this file are placeholders. Before running the test, you need to edit this file           with your app's real bucket names. This step is explained further in Testing the app with the automated script",
                            "Copy and paste the following code into a file named test_pdf_encrypt.py.import boto3import jsonimport pytestimport timeimport os@pytest.fixturedef lambda_client():    return boto3.client('lambda')    @pytest.fixturedef s3_client():    return boto3.client('s3')@pytest.fixturedef logs_client():    return boto3.client('logs')@pytest.fixture(scope='session')def cleanup():    # Create a new S3 client for cleanup    s3_client = boto3.client('s3')    yield    # Cleanup code will be executed after all tests have finished    # Delete test.pdf from the source bucket    source_bucket = 'EXAMPLE-BUCKET'    source_file_key = 'test.pdf'    s3_client.delete_object(Bucket=source_bucket, Key=source_file_key)    print(f\"\\nDeleted {source_file_key} from {source_bucket}\")    # Delete test_encrypted.pdf from the destination bucket    destination_bucket = 'EXAMPLE-BUCKET-encrypted'    destination_file_key = 'test_encrypted.pdf'    s3_client.delete_object(Bucket=destination_bucket, Key=destination_file_key)    print(f\"Deleted {destination_file_key} from {destination_bucket}\")        @pytest.mark.order(1)def test_source_bucket_available(s3_client):    s3_bucket_name = 'EXAMPLE-BUCKET'    file_name = 'test.pdf'    file_path = os.path.join(os.path.dirname(__file__), file_name)    file_uploaded = False    try:        s3_client.upload_file(file_path, s3_bucket_name, file_name)        file_uploaded = True    except:        print(\"Error: couldn't upload file\")    assert file_uploaded, \"Could not upload file to S3 bucket\"    @pytest.mark.order(2)def test_lambda_invoked(logs_client):    # Wait for a few seconds to make sure the logs are available    time.sleep(5)    # Get the latest log stream for the specified log group    log_streams = logs_client.describe_log_streams(        logGroupName='/aws/lambda/EncryptPDF',        orderBy='LastEventTime',        descending=True,        limit=1    )    latest_log_stream_name = log_streams['logStreams'][0]['logStreamName']    # Retrieve the log events from the latest log stream    log_events = logs_client.get_log_events(        logGroupName='/aws/lambda/EncryptPDF',        logStreamName=latest_log_stream_name    )    success_found = False    for event in log_events['events']:        message = json.loads(event['message'])        status = message.get('record', {}).get('status')        if status == 'success':            success_found = True            break    assert success_found, \"Lambda function execution did not report 'success' status in logs.\"@pytest.mark.order(3)def test_encrypted_file_in_bucket(s3_client):    # Specify the destination S3 bucket and the expected converted file key    destination_bucket = 'EXAMPLE-BUCKET-encrypted'    converted_file_key = 'test_encrypted.pdf'    try:        # Attempt to retrieve the metadata of the converted file from the destination S3 bucket        s3_client.head_object(Bucket=destination_bucket, Key=converted_file_key)    except s3_client.exceptions.ClientError as e:        # If the file is not found, the test will fail        pytest.fail(f\"Converted file '{converted_file_key}' not found in the destination bucket: {str(e)}\")def test_cleanup(cleanup):    # This test uses the cleanup fixture and will be executed last    passThe automated test script executes three test functions to confirm correct operation of your app:The test test_source_bucket_available confirms that your source bucket has been successfully created             by uploading a test PDF file to the bucket.The test test_lambda_invoked interrogates the latest CloudWatch Logs log stream for your function to confirm that             when you uploaded the test file, your Lambda function ran and reported success.The test test_encrypted_file_in_bucket confirms that your destination bucket contains the encrypted test_encrypted.pdf             file.After all these tests have run, the script runs an additional cleanup step to delete the test.pdf and test_encrypted.pdf files from         both your source and destination buckets.As with the AWS SAM template, the bucket names specified in this file are placeholders. Before running the test, you need to edit this file           with your app's real bucket names. This step is explained further in Testing the app with the automated scriptAutomated test scriptCopy and paste the following code into a file named test_pdf_encrypt.py.import boto3import jsonimport pytestimport timeimport os@pytest.fixturedef lambda_client():    return boto3.client('lambda')    @pytest.fixturedef s3_client():    return boto3.client('s3')@pytest.fixturedef logs_client():    return boto3.client('logs')@pytest.fixture(scope='session')def cleanup():    # Create a new S3 client for cleanup    s3_client = boto3.client('s3')    yield    # Cleanup code will be executed after all tests have finished    # Delete test.pdf from the source bucket    source_bucket = 'EXAMPLE-BUCKET'    source_file_key = 'test.pdf'    s3_client.delete_object(Bucket=source_bucket, Key=source_file_key)    print(f\"\\nDeleted {source_file_key} from {source_bucket}\")    # Delete test_encrypted.pdf from the destination bucket    destination_bucket = 'EXAMPLE-BUCKET-encrypted'    destination_file_key = 'test_encrypted.pdf'    s3_client.delete_object(Bucket=destination_bucket, Key=destination_file_key)    print(f\"Deleted {destination_file_key} from {destination_bucket}\")        @pytest.mark.order(1)def test_source_bucket_available(s3_client):    s3_bucket_name = 'EXAMPLE-BUCKET'    file_name = 'test.pdf'    file_path = os.path.join(os.path.dirname(__file__), file_name)    file_uploaded = False    try:        s3_client.upload_file(file_path, s3_bucket_name, file_name)        file_uploaded = True    except:        print(\"Error: couldn't upload file\")    assert file_uploaded, \"Could not upload file to S3 bucket\"    @pytest.mark.order(2)def test_lambda_invoked(logs_client):    # Wait for a few seconds to make sure the logs are available    time.sleep(5)    # Get the latest log stream for the specified log group    log_streams = logs_client.describe_log_streams(        logGroupName='/aws/lambda/EncryptPDF',        orderBy='LastEventTime',        descending=True,        limit=1    )    latest_log_stream_name = log_streams['logStreams'][0]['logStreamName']    # Retrieve the log events from the latest log stream    log_events = logs_client.get_log_events(        logGroupName='/aws/lambda/EncryptPDF',        logStreamName=latest_log_stream_name    )    success_found = False    for event in log_events['events']:        message = json.loads(event['message'])        status = message.get('record', {}).get('status')        if status == 'success':            success_found = True            break    assert success_found, \"Lambda function execution did not report 'success' status in logs.\"@pytest.mark.order(3)def test_encrypted_file_in_bucket(s3_client):    # Specify the destination S3 bucket and the expected converted file key    destination_bucket = 'EXAMPLE-BUCKET-encrypted'    converted_file_key = 'test_encrypted.pdf'    try:        # Attempt to retrieve the metadata of the converted file from the destination S3 bucket        s3_client.head_object(Bucket=destination_bucket, Key=converted_file_key)    except s3_client.exceptions.ClientError as e:        # If the file is not found, the test will fail        pytest.fail(f\"Converted file '{converted_file_key}' not found in the destination bucket: {str(e)}\")def test_cleanup(cleanup):    # This test uses the cleanup fixture and will be executed last    passThe automated test script executes three test functions to confirm correct operation of your app:The test test_source_bucket_available confirms that your source bucket has been successfully created             by uploading a test PDF file to the bucket.The test test_lambda_invoked interrogates the latest CloudWatch Logs log stream for your function to confirm that             when you uploaded the test file, your Lambda function ran and reported success.The test test_encrypted_file_in_bucket confirms that your destination bucket contains the encrypted test_encrypted.pdf             file.After all these tests have run, the script runs an additional cleanup step to delete the test.pdf and test_encrypted.pdf files from         both your source and destination buckets.As with the AWS SAM template, the bucket names specified in this file are placeholders. Before running the test, you need to edit this file           with your app's real bucket names. This step is explained further in Testing the app with the automated script",
                            "Copy and paste the following code into a file named pytest.ini.",
                            {
                                "code_example": "[pytest]\nmarkers =\n    order: specify test execution order"
                            },
                            "This is needed to specify the order in which the tests in the test_pdf_encrypt.py script run.",
                            "Test script configuration file",
                            "Copy and paste the following code into a file named pytest.ini.",
                            {
                                "code_example": "[pytest]\nmarkers =\n    order: specify test execution order"
                            },
                            "This is needed to specify the order in which the tests in the test_pdf_encrypt.py script run.",
                            "Copy and paste the following code into a file named pytest.ini.[pytest]markers =    order: specify test execution orderThis is needed to specify the order in which the tests in the test_pdf_encrypt.py script run.Test script configuration fileCopy and paste the following code into a file named pytest.ini.[pytest]markers =    order: specify test execution orderThis is needed to specify the order in which the tests in the test_pdf_encrypt.py script run."
                        ]
                    },
                    {
                        "sub_header": "Deploying the app",
                        "content": [
                            "You can create and deploy the resources for this example app either manually or by using AWS SAM. In a production environment, we       recommend that you use an IaC tool like AWS SAM to quickly and repeatably deploy whole serverless applications without using manual processes.",
                            "For this example, follow the console or AWS CLI instructions to learn how to configure each AWS resource separately, or skip ahead to        Deploy the resources using AWS SAM to quickly deploy the app using a few CLI commands.",
                            {
                                "sub_header": "Deploy the resources manually",
                                "content": [
                                    "To deploy your app manually, you carry out the following steps:",
                                    "  1.Create source and destination Amazon S3 buckets",
                                    "  2.Create a Lambda function that encrypts a PDF file and saves the encrypted version to an S3 bucket",
                                    "  3.Configure a Lambda trigger that invokes your function when objects are uploaded to your source bucket",
                                    "Follow the instructions in the following paragraphs to create and configure your resources.",
                                    {
                                        "sub_header": "Create two S3 buckets",
                                        "content": [
                                            "First create two S3 buckets. The first bucket is the source bucket you will upload your PDF files to. The second bucket is used by     Lambda to save the encrypted file when you invoke your function.",
                                            "  1.Console : SOURCEBUCKET-encrypted",
                                            "  2.AWS CLI : region",
                                            "ConsoleTo create the S3 buckets (console)Open the Buckets page of the Amazon S3 console.Choose Create bucket.Under General configuration, do the following:For Bucket name, enter a globally unique name that meets the Amazon S3 Bucket naming rules.                     Bucket names can contain only lower case letters, numbers, dots (.), and hyphens (-). For AWS Region, choose the AWS Region                     closest to your geographical location. Later in the deployment process, you must create your Lambda function in the same AWS Region, so                     make a note of the region you chose.Leave all other options set to their default values and choose Create bucket.Repeat steps 1 to 4 to create your destination bucket. For Bucket name, enter SOURCEBUCKET-encrypted,                 where SOURCEBUCKET is the name of the source bucket you just created.AWS CLITo create the Amazon S3 buckets (AWS CLI)Run the following CLI command to create your source bucket. The name you choose for your bucket must be globally unique and                 follow the Amazon S3 Bucket naming rules.                 Names can only contain lower case letters, numbers, dots (.), and hyphens (-). For region and LocationConstraint,                 choose the AWS Region closest to your geographical                 location.aws s3api create-bucket --bucket SOURCEBUCKET --region us-west-2 \\--create-bucket-configuration LocationConstraint=us-west-2Later in the tutorial, you must create your Lambda function in the same AWS Region as your source bucket, so make a note of the                 region you chose.Run the following command to create your destination bucket. For the bucket name, you must use SOURCEBUCKET-encrypted,                 where SOURCEBUCKET is the name of the source bucket you created in step 1. For region                 and LocationConstraint, choose the same AWS Region you used to create your source bucket.aws s3api create-bucket --bucket SOURCEBUCKET-encrypted --region us-west-2 \\--create-bucket-configuration LocationConstraint=us-west-2",
                                            "anchor",
                                            "anchor",
                                            "To create the S3 buckets (console)",
                                            "  1 : Open the Buckets page of the Amazon S3 console.",
                                            "  1 : Open the Buckets page of the Amazon S3 console.",
                                            "  1 : Open the Buckets page of the Amazon S3 console.",
                                            "  2 : Choose Create bucket.",
                                            "  2 : Choose Create bucket.",
                                            "  2 : Choose Create bucket.",
                                            "  3 : Under General configuration, do the following:For Bucket name, enter a globally unique name that meets the Amazon S3 Bucket naming rules.                     Bucket names can contain only lower case letters, numbers, dots (.), and hyphens (-). For AWS Region, choose the AWS Region                     closest to your geographical location. Later in the deployment process, you must create your Lambda function in the same AWS Region, so                     make a note of the region you chose.",
                                            "  3 : Under General configuration, do the following:For Bucket name, enter a globally unique name that meets the Amazon S3 Bucket naming rules.                     Bucket names can contain only lower case letters, numbers, dots (.), and hyphens (-). For AWS Region, choose the AWS Region                     closest to your geographical location. Later in the deployment process, you must create your Lambda function in the same AWS Region, so                     make a note of the region you chose.",
                                            "  3 : Under General configuration, do the following:For Bucket name, enter a globally unique name that meets the Amazon S3 Bucket naming rules.                     Bucket names can contain only lower case letters, numbers, dots (.), and hyphens (-). For AWS Region, choose the AWS Region                     closest to your geographical location. Later in the deployment process, you must create your Lambda function in the same AWS Region, so                     make a note of the region you chose.",
                                            "  1 : For Bucket name, enter a globally unique name that meets the Amazon S3 Bucket naming rules.                     Bucket names can contain only lower case letters, numbers, dots (.), and hyphens (-). ",
                                            "  1 : For Bucket name, enter a globally unique name that meets the Amazon S3 Bucket naming rules.                     Bucket names can contain only lower case letters, numbers, dots (.), and hyphens (-). ",
                                            "  1 : For Bucket name, enter a globally unique name that meets the Amazon S3 Bucket naming rules.                     Bucket names can contain only lower case letters, numbers, dots (.), and hyphens (-). ",
                                            "  2 : For AWS Region, choose the AWS Region                     closest to your geographical location. Later in the deployment process, you must create your Lambda function in the same AWS Region, so                     make a note of the region you chose.",
                                            "  2 : For AWS Region, choose the AWS Region                     closest to your geographical location. Later in the deployment process, you must create your Lambda function in the same AWS Region, so                     make a note of the region you chose.",
                                            "  2 : For AWS Region, choose the AWS Region                     closest to your geographical location. Later in the deployment process, you must create your Lambda function in the same AWS Region, so                     make a note of the region you chose.",
                                            "  3 : Under General configuration, do the following:For Bucket name, enter a globally unique name that meets the Amazon S3 Bucket naming rules.                     Bucket names can contain only lower case letters, numbers, dots (.), and hyphens (-). For AWS Region, choose the AWS Region                     closest to your geographical location. Later in the deployment process, you must create your Lambda function in the same AWS Region, so                     make a note of the region you chose.",
                                            "  4 : Leave all other options set to their default values and choose Create bucket.",
                                            "  4 : Leave all other options set to their default values and choose Create bucket.",
                                            "  4 : Leave all other options set to their default values and choose Create bucket.",
                                            "  5 : Repeat steps 1 to 4 to create your destination bucket. For Bucket name, enter SOURCEBUCKET-encrypted,                 where SOURCEBUCKET is the name of the source bucket you just created.",
                                            "  5 : Repeat steps 1 to 4 to create your destination bucket. For Bucket name, enter SOURCEBUCKET-encrypted,                 where SOURCEBUCKET is the name of the source bucket you just created.",
                                            "  5 : Repeat steps 1 to 4 to create your destination bucket. For Bucket name, enter SOURCEBUCKET-encrypted,                 where SOURCEBUCKET is the name of the source bucket you just created."
                                        ]
                                    },
                                    {
                                        "sub_header": "Create an execution role (AWS CLI only)",
                                        "content": [
                                            "An execution role is an IAM role that grants a Lambda function permission to access AWS services and resources. When you create a function       using the Lambda console, Lambda automatically creates an execution role. You only need to create a role manually if you choose to deploy the app       using the AWS CLI. To give your function read and write access to Amazon S3, you attach the       AWS managed policyAmazonS3FullAccess.",
                                            "  1.Console : \nThis step is only required if you choose to deploy your app using the AWS CLI.\n",
                                            "  2.AWS CLI : AmazonS3FullAccess",
                                            "ConsoleThis step is only required if you choose to deploy your app using the AWS CLI.AWS CLITo create an execution role and attach the AmazonS3FullAccess managed policy (AWS CLI)Save the following JSON in a file named trust-policy.json. This trust policy allows Lambda to use the role’s                permissions by giving the service principal lambda.amazonaws.com permission to call the AWS Security Token Service (AWS STS) AssumeRole                action.{  \"Version\": \"2012-10-17\",  \"Statement\": [    {      \"Effect\": \"Allow\",      \"Principal\": {        \"Service\": \"lambda.amazonaws.com\"      },      \"Action\": \"sts:AssumeRole\"    }  ]}From the directory you saved the JSON trust policy document in, run the following CLI command to create the execution role.aws iam create-role --role-name LambdaS3Role --assume-role-policy-document file://trust-policy.jsonTo attach the AmazonS3FullAccess managed policy, run the following CLI command.aws iam attach-role-policy --role-name LambdaS3Role --policy-arn arn:aws:iam::aws:policy/AmazonS3FullAccess",
                                            "anchor",
                                            "anchor",
                                            "This step is only required if you choose to deploy your app using the AWS CLI."
                                        ]
                                    },
                                    {
                                        "sub_header": "Create the function deployment package",
                                        "content": [
                                            "To create your function, you create a deployment package containing your function code and its dependencies. For this       application, your function code uses a separate library for the PDF encryption.",
                                            "To create the deployment package",
                                            "  1 : Navigate to the project directory containing the lambda_function.py and requirements.txt           files you created or downloaded from GitHub earlier and create a new directory named package.",
                                            "  1 : Navigate to the project directory containing the lambda_function.py and requirements.txt           files you created or downloaded from GitHub earlier and create a new directory named package.",
                                            "  1 : Navigate to the project directory containing the lambda_function.py and requirements.txt           files you created or downloaded from GitHub earlier and create a new directory named package.",
                                            "  \nInstall the dependencies specified in the requirements.txt file in your package directory by running the following command.\n \n",
                                            {
                                                "code_example": "pip install -r requirements.txt --target ./package/"
                                            },
                                            "  \nInstall the dependencies specified in the requirements.txt file in your package directory by running the following command.\n \n",
                                            {
                                                "code_example": "pip install -r requirements.txt --target ./package/"
                                            },
                                            "  \nInstall the dependencies specified in the requirements.txt file in your package directory by running the following command.\n \n",
                                            {
                                                "code_example": "pip install -r requirements.txt --target ./package/"
                                            },
                                            "  \nInstall the dependencies specified in the requirements.txt file in your package directory by running the following command.\n \n",
                                            {
                                                "code_example": "pip install -r requirements.txt --target ./package/"
                                            },
                                            "  \nInstall the dependencies specified in the requirements.txt file in your package directory by running the following command.\n \n",
                                            {
                                                "code_example": "pip install -r requirements.txt --target ./package/"
                                            },
                                            "  \nCreate a .zip file containing your application code and its dependencies. In Linux or MacOS, run the following commands from your \n          command line interface.\n \n In Windows, use your preferred zip tool to create the lambda_function.zip file. Make sure that your \n        lambda_function.py file and the folders containing your dependencies are all at the root of the .zip file.\n",
                                            {
                                                "code_example": "cd package\nzip -r ../lambda_function.zip .\ncd ..\nzip lambda_function.zip lambda_function.py"
                                            },
                                            "  \nCreate a .zip file containing your application code and its dependencies. In Linux or MacOS, run the following commands from your \n          command line interface.\n \n In Windows, use your preferred zip tool to create the lambda_function.zip file. Make sure that your \n        lambda_function.py file and the folders containing your dependencies are all at the root of the .zip file.\n",
                                            {
                                                "code_example": "cd package\nzip -r ../lambda_function.zip .\ncd ..\nzip lambda_function.zip lambda_function.py"
                                            },
                                            "  \nCreate a .zip file containing your application code and its dependencies. In Linux or MacOS, run the following commands from your \n          command line interface.\n \n In Windows, use your preferred zip tool to create the lambda_function.zip file. Make sure that your \n        lambda_function.py file and the folders containing your dependencies are all at the root of the .zip file.\n",
                                            {
                                                "code_example": "cd package\nzip -r ../lambda_function.zip .\ncd ..\nzip lambda_function.zip lambda_function.py"
                                            },
                                            "  \nCreate a .zip file containing your application code and its dependencies. In Linux or MacOS, run the following commands from your \n          command line interface.\n \n In Windows, use your preferred zip tool to create the lambda_function.zip file. Make sure that your \n        lambda_function.py file and the folders containing your dependencies are all at the root of the .zip file.\n",
                                            {
                                                "code_example": "cd package\nzip -r ../lambda_function.zip .\ncd ..\nzip lambda_function.zip lambda_function.py"
                                            },
                                            "  \nCreate a .zip file containing your application code and its dependencies. In Linux or MacOS, run the following commands from your \n          command line interface.\n \n In Windows, use your preferred zip tool to create the lambda_function.zip file. Make sure that your \n        lambda_function.py file and the folders containing your dependencies are all at the root of the .zip file.\n",
                                            {
                                                "code_example": "cd package\nzip -r ../lambda_function.zip .\ncd ..\nzip lambda_function.zip lambda_function.py"
                                            },
                                            "  \nCreate a .zip file containing your application code and its dependencies. In Linux or MacOS, run the following commands from your \n          command line interface.\n \n In Windows, use your preferred zip tool to create the lambda_function.zip file. Make sure that your \n        lambda_function.py file and the folders containing your dependencies are all at the root of the .zip file.\n",
                                            {
                                                "code_example": "cd package\nzip -r ../lambda_function.zip .\ncd ..\nzip lambda_function.zip lambda_function.py"
                                            },
                                            "  \nCreate a .zip file containing your application code and its dependencies. In Linux or MacOS, run the following commands from your \n          command line interface.\n \n In Windows, use your preferred zip tool to create the lambda_function.zip file. Make sure that your \n        lambda_function.py file and the folders containing your dependencies are all at the root of the .zip file.\n",
                                            {
                                                "code_example": "cd package\nzip -r ../lambda_function.zip .\ncd ..\nzip lambda_function.zip lambda_function.py"
                                            },
                                            "You can also create your deployment package using a Python virtual environment. See Working with .zip file archives for Python Lambda functions"
                                        ]
                                    },
                                    {
                                        "sub_header": "Create the Lambda function",
                                        "content": [
                                            "You now use the deployment package you created in the previous step to deploy your Lambda function.",
                                            "  1.Console : EncryptPDF",
                                            "  2.AWS CLI : lambda_function.zip",
                                            "ConsoleTo create the function (console)To create your Lambda function using the console, you first create a basic function containing some ‘Hello world’ code. You then           replace this code with your own function code by uploading the.zip file you created in the previous step.To ensure that your function doesn't time out when encrypting large PDF files, you configure the function's memory and timeout settings.           You also set the function's log format to JSON. Configuring JSON formatted logs is necessary when using the provided test script so it can read the           function's invocation status from CloudWatch Logs to confirm successful invocation.Open the Functions page of the Lambda console.Make sure you're working in the same AWS Region you created your S3 bucket in. You can change your region using the drop-down             list at the top of the screen.Choose Create function.Choose Author from scratch.Under Basic information, do the following:For Function name, enter EncryptPDF.For Runtime choose Python 3.12.For Architecture, choose x86_64.Choose Create function.To upload the function code (console)In the Code source pane, choose Upload from.Choose .zip file.Choose Upload.In the file selector, select your .zip file and choose Open.Choose Save.To configure the function memory and timeout (console)Select the Configuration tab for your function.In the General configuration pane, choose Edit.Set Memory to 256 MB and Timeout to 15 seconds.Choose Save.To configure the log format (console)Select the Configuration tab for your function.Select Monitoring and operations tools.In the Logging configuration pane, choose Edit.For Logging configuration, select JSON.Choose Save.AWS CLITo create the function (AWS CLI)Run the following command from the directory containing your lambda_function.zip             file.For the region parameter, replace us-west-2 with the region you created your             S3 buckets in.aws lambda create-function --function-name EncryptPDF \\--zip-file fileb://lambda_function.zip --handler lambda_function.lambda_handler \\--runtime python3.12 --timeout 15 --memory-size 256 \\--role arn:aws:iam::123456789012:role/LambdaS3Role --region us-west-2 \\--logging-config LogFormat=JSON",
                                            "anchor",
                                            "anchor",
                                            "To create the function (console)",
                                            "To create your Lambda function using the console, you first create a basic function containing some ‘Hello world’ code. You then           replace this code with your own function code by uploading the.zip file you created in the previous step.",
                                            "To ensure that your function doesn't time out when encrypting large PDF files, you configure the function's memory and timeout settings.           You also set the function's log format to JSON. Configuring JSON formatted logs is necessary when using the provided test script so it can read the           function's invocation status from CloudWatch Logs to confirm successful invocation.",
                                            "  1 : Open the Functions page of the Lambda console.",
                                            "  1 : Open the Functions page of the Lambda console.",
                                            "  1 : Open the Functions page of the Lambda console.",
                                            "  2 : Make sure you're working in the same AWS Region you created your S3 bucket in. You can change your region using the drop-down             list at the top of the screen.",
                                            "  2 : Make sure you're working in the same AWS Region you created your S3 bucket in. You can change your region using the drop-down             list at the top of the screen.",
                                            "  2 : Make sure you're working in the same AWS Region you created your S3 bucket in. You can change your region using the drop-down             list at the top of the screen.",
                                            "  2 : Make sure you're working in the same AWS Region you created your S3 bucket in. You can change your region using the drop-down             list at the top of the screen.",
                                            "  2 : Make sure you're working in the same AWS Region you created your S3 bucket in. You can change your region using the drop-down             list at the top of the screen.",
                                            "  3 : Choose Create function.",
                                            "  3 : Choose Create function.",
                                            "  3 : Choose Create function.",
                                            "  4 : Choose Author from scratch.",
                                            "  4 : Choose Author from scratch.",
                                            "  4 : Choose Author from scratch.",
                                            "  5 : Under Basic information, do the following:For Function name, enter EncryptPDF.For Runtime choose Python 3.12.For Architecture, choose x86_64.",
                                            "  5 : Under Basic information, do the following:For Function name, enter EncryptPDF.For Runtime choose Python 3.12.For Architecture, choose x86_64.",
                                            "  5 : Under Basic information, do the following:For Function name, enter EncryptPDF.For Runtime choose Python 3.12.For Architecture, choose x86_64.",
                                            "  1 : For Function name, enter EncryptPDF.",
                                            "  1 : For Function name, enter EncryptPDF.",
                                            "  1 : For Function name, enter EncryptPDF.",
                                            "  2 : For Runtime choose Python 3.12.",
                                            "  2 : For Runtime choose Python 3.12.",
                                            "  2 : For Runtime choose Python 3.12.",
                                            "  3 : For Architecture, choose x86_64.",
                                            "  3 : For Architecture, choose x86_64.",
                                            "  3 : For Architecture, choose x86_64.",
                                            "  5 : Under Basic information, do the following:For Function name, enter EncryptPDF.For Runtime choose Python 3.12.For Architecture, choose x86_64.",
                                            "  6 : Choose Create function.",
                                            "  6 : Choose Create function.",
                                            "  6 : Choose Create function.",
                                            "To upload the function code (console)",
                                            "  1 : In the Code source pane, choose Upload from.",
                                            "  1 : In the Code source pane, choose Upload from.",
                                            "  1 : In the Code source pane, choose Upload from.",
                                            "  2 : Choose .zip file.",
                                            "  2 : Choose .zip file.",
                                            "  2 : Choose .zip file.",
                                            "  3 : Choose Upload.",
                                            "  3 : Choose Upload.",
                                            "  3 : Choose Upload.",
                                            "  4 : In the file selector, select your .zip file and choose Open.",
                                            "  4 : In the file selector, select your .zip file and choose Open.",
                                            "  4 : In the file selector, select your .zip file and choose Open.",
                                            "  5 : Choose Save.",
                                            "  5 : Choose Save.",
                                            "  5 : Choose Save.",
                                            "To configure the function memory and timeout (console)",
                                            "  1 : Select the Configuration tab for your function.",
                                            "  1 : Select the Configuration tab for your function.",
                                            "  1 : Select the Configuration tab for your function.",
                                            "  2 : In the General configuration pane, choose Edit.",
                                            "  2 : In the General configuration pane, choose Edit.",
                                            "  2 : In the General configuration pane, choose Edit.",
                                            "  3 : Set Memory to 256 MB and Timeout to 15 seconds.",
                                            "  3 : Set Memory to 256 MB and Timeout to 15 seconds.",
                                            "  3 : Set Memory to 256 MB and Timeout to 15 seconds.",
                                            "  4 : Choose Save.",
                                            "  4 : Choose Save.",
                                            "  4 : Choose Save.",
                                            "To configure the log format (console)",
                                            "  1 : Select the Configuration tab for your function.",
                                            "  1 : Select the Configuration tab for your function.",
                                            "  1 : Select the Configuration tab for your function.",
                                            "  2 : Select Monitoring and operations tools.",
                                            "  2 : Select Monitoring and operations tools.",
                                            "  2 : Select Monitoring and operations tools.",
                                            "  3 : In the Logging configuration pane, choose Edit.",
                                            "  3 : In the Logging configuration pane, choose Edit.",
                                            "  3 : In the Logging configuration pane, choose Edit.",
                                            "  4 : For Logging configuration, select JSON.",
                                            "  4 : For Logging configuration, select JSON.",
                                            "  4 : For Logging configuration, select JSON.",
                                            "  5 : Choose Save.",
                                            "  5 : Choose Save.",
                                            "  5 : Choose Save."
                                        ]
                                    },
                                    {
                                        "sub_header": "Configure an Amazon S3 trigger to invoke the function",
                                        "content": [
                                            "For your Lambda function to run when you upload a file to your source bucket, you need to configure a trigger for your function. You can     configure the Amazon S3 trigger using either the console or the AWS CLI.",
                                            "Important",
                                            "This procedure configures the S3 bucket to invoke your function every time that an object is created in the bucket. Be sure to       configure this only on the source bucket. If your Lambda function creates objects in the same bucket that invokes it, your function can be         invoked continuously in a loop. This can result         in un expected charges being billed to your AWS account.",
                                            "  1.Console : EncryptPDF",
                                            "  2.AWS CLI : source-account",
                                            "ConsoleTo configure the Amazon S3 trigger (console)Open the Functions page of the Lambda console and choose your function (EncryptPDF).Choose Add trigger.Select S3.Under Bucket, select your source bucket.Under Event types, select All object create events.Under Recursive invocation, select the check box to acknowledge that using the same S3 bucket for input                 and output is not recommended. You can learn more about recursive invocation patterns in Lambda by reading                Recursive patterns that cause run-away Lambda functions                in Serverless Land.Choose Add.When you create a trigger using the Lambda console, Lambda automatically creates a resource based policy                 to give the service you select permission to invoke your function. AWS CLITo configure the Amazon S3 trigger (AWS CLI)For your Amazon S3 source bucket to invoke your function when you add a file, you first need to configure permissions for your                 function using a resource based policy.                 A resource-based policy statement gives other AWS services permission to invoke your function. To give Amazon S3 permission to invoke                 your function, run the following CLI command. Be sure to replace the source-account parameter with your own AWS account ID and to                 use your own source bucket name.aws lambda add-permission --function-name EncryptPDF \\--principal s3.amazonaws.com --statement-id s3invoke --action \"lambda:InvokeFunction\" \\--source-arn arn:aws:s3:::SOURCEBUCKET \\--source-account 123456789012The policy you define with this command allows Amazon S3 to invoke your function only when an action takes place on your source bucket.NoteAlthough S3 bucket names are globally unique, when using resource-based policies it is best practice to specify that the                 bucket must belong to your account.  This is because if you delete a bucket, it is possible for another AWS account to create a                 bucket with the same Amazon Resource Name (ARN).Save the following JSON in a file named notification.json. When applied to your source bucket, this JSON                 configures the bucket to send a notification to your Lambda function every time a new object is added. Replace the AWS account               number and AWS Region in the Lambda function ARN with your own account number and region.{\"LambdaFunctionConfigurations\": [    {      \"Id\": \"EncryptPDFEventConfiguration\",      \"LambdaFunctionArn\": \"arn:aws:lambda:us-west-2:123456789012:function:EncryptPDF\",      \"Events\": [ \"s3:ObjectCreated:Put\" ]    }  ]}Run the following CLI command to apply the notification settings in the JSON file you created to your source bucket. Replace                 SOURCEBUCKET with the name of your own source bucket.aws s3api put-bucket-notification-configuration --bucket SOURCEBUCKET \\--notification-configuration file://notification.jsonTo learn more about the put-bucket-notification-configuration command and the                 notification-configuration option, see put-bucket-notification-configuration                 in the AWS CLI Command Reference.",
                                            "anchor",
                                            "anchor",
                                            "To configure the Amazon S3 trigger (console)",
                                            "  1 : Open the Functions page of the Lambda console and choose your function (EncryptPDF).",
                                            "  1 : Open the Functions page of the Lambda console and choose your function (EncryptPDF).",
                                            "  1 : Open the Functions page of the Lambda console and choose your function (EncryptPDF).",
                                            "  2 : Choose Add trigger.",
                                            "  2 : Choose Add trigger.",
                                            "  2 : Choose Add trigger.",
                                            "  3 : Select S3.",
                                            "  3 : Select S3.",
                                            "  3 : Select S3.",
                                            "  4 : Under Bucket, select your source bucket.",
                                            "  4 : Under Bucket, select your source bucket.",
                                            "  4 : Under Bucket, select your source bucket.",
                                            "  5 : Under Event types, select All object create events.",
                                            "  5 : Under Event types, select All object create events.",
                                            "  5 : Under Event types, select All object create events.",
                                            "  6 : Under Recursive invocation, select the check box to acknowledge that using the same S3 bucket for input                 and output is not recommended. You can learn more about recursive invocation patterns in Lambda by reading                Recursive patterns that cause run-away Lambda functions                in Serverless Land.",
                                            "  6 : Under Recursive invocation, select the check box to acknowledge that using the same S3 bucket for input                 and output is not recommended. You can learn more about recursive invocation patterns in Lambda by reading                Recursive patterns that cause run-away Lambda functions                in Serverless Land.",
                                            "  6 : Under Recursive invocation, select the check box to acknowledge that using the same S3 bucket for input                 and output is not recommended. You can learn more about recursive invocation patterns in Lambda by reading                Recursive patterns that cause run-away Lambda functions                in Serverless Land.",
                                            "  7 : Choose Add.When you create a trigger using the Lambda console, Lambda automatically creates a resource based policy                 to give the service you select permission to invoke your function. ",
                                            "  7 : Choose Add.When you create a trigger using the Lambda console, Lambda automatically creates a resource based policy                 to give the service you select permission to invoke your function. ",
                                            "  7 : Choose Add.When you create a trigger using the Lambda console, Lambda automatically creates a resource based policy                 to give the service you select permission to invoke your function. ",
                                            "  7 : Choose Add.When you create a trigger using the Lambda console, Lambda automatically creates a resource based policy                 to give the service you select permission to invoke your function. ",
                                            "  7 : Choose Add.When you create a trigger using the Lambda console, Lambda automatically creates a resource based policy                 to give the service you select permission to invoke your function. "
                                        ]
                                    }
                                ]
                            },
                            {
                                "sub_header": "Deploy the resources using AWS SAM",
                                "content": [
                                    "To deploy the example app using the AWS SAM CLI, carry out the following steps.",
                                    "Make sure that you have         installed the latest version of the           CLI and that Docker is installed on your build machine.",
                                    "  1 : Edit the template.yaml file to specify the name of your             S3 buckets. S3 buckets must have globally unique names that meet the S3 bucket naming rules.Replace the bucket name EXAMPLE-BUCKET with a name of your choice consisting of lowercase letters, numbers, dots (.), and hyphens (-). For the destination           bucket, replace EXAMPLE-BUCKET-encrypted with <source-bucket-name>-encrypted, where <source-bucket> is the name           you chose for your source bucket.",
                                    "  1 : Edit the template.yaml file to specify the name of your             S3 buckets. S3 buckets must have globally unique names that meet the S3 bucket naming rules.Replace the bucket name EXAMPLE-BUCKET with a name of your choice consisting of lowercase letters, numbers, dots (.), and hyphens (-). For the destination           bucket, replace EXAMPLE-BUCKET-encrypted with <source-bucket-name>-encrypted, where <source-bucket> is the name           you chose for your source bucket.",
                                    "  1 : Edit the template.yaml file to specify the name of your             S3 buckets. S3 buckets must have globally unique names that meet the S3 bucket naming rules.Replace the bucket name EXAMPLE-BUCKET with a name of your choice consisting of lowercase letters, numbers, dots (.), and hyphens (-). For the destination           bucket, replace EXAMPLE-BUCKET-encrypted with <source-bucket-name>-encrypted, where <source-bucket> is the name           you chose for your source bucket.",
                                    "  1 : Edit the template.yaml file to specify the name of your             S3 buckets. S3 buckets must have globally unique names that meet the S3 bucket naming rules.Replace the bucket name EXAMPLE-BUCKET with a name of your choice consisting of lowercase letters, numbers, dots (.), and hyphens (-). For the destination           bucket, replace EXAMPLE-BUCKET-encrypted with <source-bucket-name>-encrypted, where <source-bucket> is the name           you chose for your source bucket.",
                                    "  1 : Edit the template.yaml file to specify the name of your             S3 buckets. S3 buckets must have globally unique names that meet the S3 bucket naming rules.Replace the bucket name EXAMPLE-BUCKET with a name of your choice consisting of lowercase letters, numbers, dots (.), and hyphens (-). For the destination           bucket, replace EXAMPLE-BUCKET-encrypted with <source-bucket-name>-encrypted, where <source-bucket> is the name           you chose for your source bucket.",
                                    "  \nRun the following command from the directory in which you saved your template.yaml, lambda_function.py, \n            and requirements.txtfiles.\n \nThis command gathers the build artifacts for your application and places them in the proper format and location to deploy them. Specifying \n          the --use-container option builds your function inside a Lambda-like Docker container. We use it here so you don't need to have Python 3.12\n          installed on your local machine for the build to work.\nDuring the build process, AWS SAM looks for the Lambda function code in the location you specified with the CodeUri \n          property in the template. In this case, we specified the current directory as the location (./).\nIf a requirements.txt file is present, AWS SAM uses it to gather the specified dependencies. By default, AWS SAM creates a .zip \n            deployment package with your function code and dependencies. You can also choose to deploy your function as a container image using the \n            PackageType \n          property.\n",
                                    {
                                        "code_example": "sam build --use-container"
                                    },
                                    "  \nRun the following command from the directory in which you saved your template.yaml, lambda_function.py, \n            and requirements.txtfiles.\n \nThis command gathers the build artifacts for your application and places them in the proper format and location to deploy them. Specifying \n          the --use-container option builds your function inside a Lambda-like Docker container. We use it here so you don't need to have Python 3.12\n          installed on your local machine for the build to work.\nDuring the build process, AWS SAM looks for the Lambda function code in the location you specified with the CodeUri \n          property in the template. In this case, we specified the current directory as the location (./).\nIf a requirements.txt file is present, AWS SAM uses it to gather the specified dependencies. By default, AWS SAM creates a .zip \n            deployment package with your function code and dependencies. You can also choose to deploy your function as a container image using the \n            PackageType \n          property.\n",
                                    {
                                        "code_example": "sam build --use-container"
                                    },
                                    "  \nRun the following command from the directory in which you saved your template.yaml, lambda_function.py, \n            and requirements.txtfiles.\n \nThis command gathers the build artifacts for your application and places them in the proper format and location to deploy them. Specifying \n          the --use-container option builds your function inside a Lambda-like Docker container. We use it here so you don't need to have Python 3.12\n          installed on your local machine for the build to work.\nDuring the build process, AWS SAM looks for the Lambda function code in the location you specified with the CodeUri \n          property in the template. In this case, we specified the current directory as the location (./).\nIf a requirements.txt file is present, AWS SAM uses it to gather the specified dependencies. By default, AWS SAM creates a .zip \n            deployment package with your function code and dependencies. You can also choose to deploy your function as a container image using the \n            PackageType \n          property.\n",
                                    {
                                        "code_example": "sam build --use-container"
                                    },
                                    "  \nRun the following command from the directory in which you saved your template.yaml, lambda_function.py, \n            and requirements.txtfiles.\n \nThis command gathers the build artifacts for your application and places them in the proper format and location to deploy them. Specifying \n          the --use-container option builds your function inside a Lambda-like Docker container. We use it here so you don't need to have Python 3.12\n          installed on your local machine for the build to work.\nDuring the build process, AWS SAM looks for the Lambda function code in the location you specified with the CodeUri \n          property in the template. In this case, we specified the current directory as the location (./).\nIf a requirements.txt file is present, AWS SAM uses it to gather the specified dependencies. By default, AWS SAM creates a .zip \n            deployment package with your function code and dependencies. You can also choose to deploy your function as a container image using the \n            PackageType \n          property.\n",
                                    {
                                        "code_example": "sam build --use-container"
                                    },
                                    "  \nRun the following command from the directory in which you saved your template.yaml, lambda_function.py, \n            and requirements.txtfiles.\n \nThis command gathers the build artifacts for your application and places them in the proper format and location to deploy them. Specifying \n          the --use-container option builds your function inside a Lambda-like Docker container. We use it here so you don't need to have Python 3.12\n          installed on your local machine for the build to work.\nDuring the build process, AWS SAM looks for the Lambda function code in the location you specified with the CodeUri \n          property in the template. In this case, we specified the current directory as the location (./).\nIf a requirements.txt file is present, AWS SAM uses it to gather the specified dependencies. By default, AWS SAM creates a .zip \n            deployment package with your function code and dependencies. You can also choose to deploy your function as a container image using the \n            PackageType \n          property.\n",
                                    {
                                        "code_example": "sam build --use-container"
                                    },
                                    "  \nRun the following command from the directory in which you saved your template.yaml, lambda_function.py, \n            and requirements.txtfiles.\n \nThis command gathers the build artifacts for your application and places them in the proper format and location to deploy them. Specifying \n          the --use-container option builds your function inside a Lambda-like Docker container. We use it here so you don't need to have Python 3.12\n          installed on your local machine for the build to work.\nDuring the build process, AWS SAM looks for the Lambda function code in the location you specified with the CodeUri \n          property in the template. In this case, we specified the current directory as the location (./).\nIf a requirements.txt file is present, AWS SAM uses it to gather the specified dependencies. By default, AWS SAM creates a .zip \n            deployment package with your function code and dependencies. You can also choose to deploy your function as a container image using the \n            PackageType \n          property.\n",
                                    {
                                        "code_example": "sam build --use-container"
                                    },
                                    "  \nRun the following command from the directory in which you saved your template.yaml, lambda_function.py, \n            and requirements.txtfiles.\n \nThis command gathers the build artifacts for your application and places them in the proper format and location to deploy them. Specifying \n          the --use-container option builds your function inside a Lambda-like Docker container. We use it here so you don't need to have Python 3.12\n          installed on your local machine for the build to work.\nDuring the build process, AWS SAM looks for the Lambda function code in the location you specified with the CodeUri \n          property in the template. In this case, we specified the current directory as the location (./).\nIf a requirements.txt file is present, AWS SAM uses it to gather the specified dependencies. By default, AWS SAM creates a .zip \n            deployment package with your function code and dependencies. You can also choose to deploy your function as a container image using the \n            PackageType \n          property.\n",
                                    {
                                        "code_example": "sam build --use-container"
                                    },
                                    "  \nRun the following command from the directory in which you saved your template.yaml, lambda_function.py, \n            and requirements.txtfiles.\n \nThis command gathers the build artifacts for your application and places them in the proper format and location to deploy them. Specifying \n          the --use-container option builds your function inside a Lambda-like Docker container. We use it here so you don't need to have Python 3.12\n          installed on your local machine for the build to work.\nDuring the build process, AWS SAM looks for the Lambda function code in the location you specified with the CodeUri \n          property in the template. In this case, we specified the current directory as the location (./).\nIf a requirements.txt file is present, AWS SAM uses it to gather the specified dependencies. By default, AWS SAM creates a .zip \n            deployment package with your function code and dependencies. You can also choose to deploy your function as a container image using the \n            PackageType \n          property.\n",
                                    {
                                        "code_example": "sam build --use-container"
                                    },
                                    "  \nRun the following command from the directory in which you saved your template.yaml, lambda_function.py, \n            and requirements.txtfiles.\n \nThis command gathers the build artifacts for your application and places them in the proper format and location to deploy them. Specifying \n          the --use-container option builds your function inside a Lambda-like Docker container. We use it here so you don't need to have Python 3.12\n          installed on your local machine for the build to work.\nDuring the build process, AWS SAM looks for the Lambda function code in the location you specified with the CodeUri \n          property in the template. In this case, we specified the current directory as the location (./).\nIf a requirements.txt file is present, AWS SAM uses it to gather the specified dependencies. By default, AWS SAM creates a .zip \n            deployment package with your function code and dependencies. You can also choose to deploy your function as a container image using the \n            PackageType \n          property.\n",
                                    {
                                        "code_example": "sam build --use-container"
                                    },
                                    "  \nRun the following command from the directory in which you saved your template.yaml, lambda_function.py, \n            and requirements.txtfiles.\n \nThis command gathers the build artifacts for your application and places them in the proper format and location to deploy them. Specifying \n          the --use-container option builds your function inside a Lambda-like Docker container. We use it here so you don't need to have Python 3.12\n          installed on your local machine for the build to work.\nDuring the build process, AWS SAM looks for the Lambda function code in the location you specified with the CodeUri \n          property in the template. In this case, we specified the current directory as the location (./).\nIf a requirements.txt file is present, AWS SAM uses it to gather the specified dependencies. By default, AWS SAM creates a .zip \n            deployment package with your function code and dependencies. You can also choose to deploy your function as a container image using the \n            PackageType \n          property.\n",
                                    {
                                        "code_example": "sam build --use-container"
                                    },
                                    "  \nRun the following command from the directory in which you saved your template.yaml, lambda_function.py, \n            and requirements.txtfiles.\n \nThis command gathers the build artifacts for your application and places them in the proper format and location to deploy them. Specifying \n          the --use-container option builds your function inside a Lambda-like Docker container. We use it here so you don't need to have Python 3.12\n          installed on your local machine for the build to work.\nDuring the build process, AWS SAM looks for the Lambda function code in the location you specified with the CodeUri \n          property in the template. In this case, we specified the current directory as the location (./).\nIf a requirements.txt file is present, AWS SAM uses it to gather the specified dependencies. By default, AWS SAM creates a .zip \n            deployment package with your function code and dependencies. You can also choose to deploy your function as a container image using the \n            PackageType \n          property.\n",
                                    {
                                        "code_example": "sam build --use-container"
                                    },
                                    "  \nTo deploy your application and create the Lambda and Amazon S3 resources specified in your AWS SAM template, run the following \n            command.\n \nUsing the --guided flag means that AWS SAM will show you prompts to guide you through the deployment process. For this \n            deployment, accept the default options by pressing Enter.\n",
                                    {
                                        "code_example": "sam deploy --guided"
                                    },
                                    "  \nTo deploy your application and create the Lambda and Amazon S3 resources specified in your AWS SAM template, run the following \n            command.\n \nUsing the --guided flag means that AWS SAM will show you prompts to guide you through the deployment process. For this \n            deployment, accept the default options by pressing Enter.\n",
                                    {
                                        "code_example": "sam deploy --guided"
                                    },
                                    "  \nTo deploy your application and create the Lambda and Amazon S3 resources specified in your AWS SAM template, run the following \n            command.\n \nUsing the --guided flag means that AWS SAM will show you prompts to guide you through the deployment process. For this \n            deployment, accept the default options by pressing Enter.\n",
                                    {
                                        "code_example": "sam deploy --guided"
                                    },
                                    "  \nTo deploy your application and create the Lambda and Amazon S3 resources specified in your AWS SAM template, run the following \n            command.\n \nUsing the --guided flag means that AWS SAM will show you prompts to guide you through the deployment process. For this \n            deployment, accept the default options by pressing Enter.\n",
                                    {
                                        "code_example": "sam deploy --guided"
                                    },
                                    "  \nTo deploy your application and create the Lambda and Amazon S3 resources specified in your AWS SAM template, run the following \n            command.\n \nUsing the --guided flag means that AWS SAM will show you prompts to guide you through the deployment process. For this \n            deployment, accept the default options by pressing Enter.\n",
                                    {
                                        "code_example": "sam deploy --guided"
                                    },
                                    "  \nTo deploy your application and create the Lambda and Amazon S3 resources specified in your AWS SAM template, run the following \n            command.\n \nUsing the --guided flag means that AWS SAM will show you prompts to guide you through the deployment process. For this \n            deployment, accept the default options by pressing Enter.\n",
                                    {
                                        "code_example": "sam deploy --guided"
                                    },
                                    "  \nTo deploy your application and create the Lambda and Amazon S3 resources specified in your AWS SAM template, run the following \n            command.\n \nUsing the --guided flag means that AWS SAM will show you prompts to guide you through the deployment process. For this \n            deployment, accept the default options by pressing Enter.\n",
                                    {
                                        "code_example": "sam deploy --guided"
                                    },
                                    "During the deployment process, AWS SAM creates the following resources in your AWS account:",
                                    "  1.An AWS CloudFormation stack             named sam-app",
                                    "  2.A Lambda function with the name EncryptPDF",
                                    "  3.Two S3 buckets with the names you chose when you edited the template.yaml AWS SAM template file",
                                    "  4.An IAM execution role for your function with the name format sam-app-EncryptPDFFunctionRole-2qGaapHFWOQ8",
                                    "When AWS SAM finishes creating your resources, you should see the following message:",
                                    "Successfully created/updated stack - sam-app in us-west-2"
                                ]
                            }
                        ]
                    },
                    {
                        "sub_header": "Testing the app",
                        "content": [
                            "To test your app, you upload a PDF file to your source bucket, and confirm that Lambda creates an encrypted version of the file in your       destination bucket. In this example, you can either test this manually using the console or the AWS CLI, or by using the provided test script.",
                            "For production applications, you can use traditional test methods and techniques, such as unit testing, to confirm the       correct functioning of your Lambda function code. Best practice is also to conduct tests like those in the provided test script which perform integration       testing with real, cloud-based resources. Integration testing in the cloud confirms that your infrastructure has been correctly deployed and that events flow       between different services as expected. To learn more, see How to test serverless functions and applications.",
                            {
                                "sub_header": "Testing the app manually",
                                "content": [
                                    "You can test your function manually by adding a PDF file to           your Amazon S3 source bucket. When you add your file to the source bucket, your Lambda function should be automatically invoked and should store an encrypted           version of the file in your target bucket.",
                                    "  1.Console : filename_encrypted.pdf",
                                    "  2.AWS CLI : --bucket",
                                    "ConsoleTo test your app by uploading a file (console)To upload a PDF file to your S3 bucket, do the following:Open the Buckets page of the Amazon S3 console and choose your source bucket.Choose Upload.Choose Add files and use the file selector to choose the PDF file you want to upload.Choose Open, then choose Upload.Verify that Lambda has saved an encrypted version of your PDF file in your target bucket by doing the following:Navigate back to the Buckets page of the Amazon S3 console and choose your destination bucket.In the Objects pane, you should now see a file with name format filename_encrypted.pdf (where                         filename.pdf was the name of the file you uploaded to your source bucket).                        To download your encrypted PDF, select the file, then choose Download.Confirm that you can open the downloaded file with the password your Lambda function protected it with (my-secret-password).AWS CLITo test your app by uploading a file (AWS CLI)From the directory containing the PDF file you want to upload, run the following CLI command. Replace the --bucket                     parameter with the name of your source bucket. For the --key and --body parameters, use the filename of                     your test file.aws s3api put-object --bucket SOURCEBUCKET --key test.pdf --body ./test.pdfVerify that your function has created an encrypted version of your file and saved it to your target S3 bucket. Run the                     following CLI command, replacing SOURCEBUCKET-encrypted with the name of your own destination bucket.aws s3api list-objects-v2 --bucket SOURCEBUCKET-encryptedIf your function runs successfully, you’ll see output similar to the following. Your target bucket should contain a file with the                     name format <your_test_file>_encrypted.pdf, where <your_test_file>                     is the name of the file you uploaded.{    \"Contents\": [        {            \"Key\": \"test_encrypted.pdf\",            \"LastModified\": \"2023-06-07T00:15:50+00:00\",            \"ETag\": \"\\\"7781a43e765a8301713f533d70968a1e\\\"\",            \"Size\": 2763,            \"StorageClass\": \"STANDARD\"        }    ]}To download the file that Lambda saved in your destination bucket, run the following CLI command. Replace the --bucket                     parameter with the name of your destination bucket. For the --key parameter, use the filename <your_test_file>_encrypted.pdf,                     where <your_test_file> is the name of the the test file you uploaded.aws s3api get-object --bucket SOURCEBUCKET-encrypted --key test_encrypted.pdf my_encrypted_file.pdfThis command downloads the file to your current directory and saves it as my_encrypted_file.pdf.Confirm the you can open the downloaded file with the password your Lambda function protected it with (my-secret-password).",
                                    "anchor",
                                    "anchor",
                                    "To test your app by uploading a file (console)",
                                    "  1 : To upload a PDF file to your S3 bucket, do the following:Open the Buckets page of the Amazon S3 console and choose your source bucket.Choose Upload.Choose Add files and use the file selector to choose the PDF file you want to upload.Choose Open, then choose Upload.",
                                    "  1 : To upload a PDF file to your S3 bucket, do the following:Open the Buckets page of the Amazon S3 console and choose your source bucket.Choose Upload.Choose Add files and use the file selector to choose the PDF file you want to upload.Choose Open, then choose Upload.",
                                    "  1 : To upload a PDF file to your S3 bucket, do the following:Open the Buckets page of the Amazon S3 console and choose your source bucket.Choose Upload.Choose Add files and use the file selector to choose the PDF file you want to upload.Choose Open, then choose Upload.",
                                    "  1 : Open the Buckets page of the Amazon S3 console and choose your source bucket.",
                                    "  1 : Open the Buckets page of the Amazon S3 console and choose your source bucket.",
                                    "  1 : Open the Buckets page of the Amazon S3 console and choose your source bucket.",
                                    "  2 : Choose Upload.",
                                    "  2 : Choose Upload.",
                                    "  2 : Choose Upload.",
                                    "  3 : Choose Add files and use the file selector to choose the PDF file you want to upload.",
                                    "  3 : Choose Add files and use the file selector to choose the PDF file you want to upload.",
                                    "  3 : Choose Add files and use the file selector to choose the PDF file you want to upload.",
                                    "  4 : Choose Open, then choose Upload.",
                                    "  4 : Choose Open, then choose Upload.",
                                    "  4 : Choose Open, then choose Upload.",
                                    "  1 : To upload a PDF file to your S3 bucket, do the following:Open the Buckets page of the Amazon S3 console and choose your source bucket.Choose Upload.Choose Add files and use the file selector to choose the PDF file you want to upload.Choose Open, then choose Upload.",
                                    "  2 : Verify that Lambda has saved an encrypted version of your PDF file in your target bucket by doing the following:Navigate back to the Buckets page of the Amazon S3 console and choose your destination bucket.In the Objects pane, you should now see a file with name format filename_encrypted.pdf (where                         filename.pdf was the name of the file you uploaded to your source bucket).                        To download your encrypted PDF, select the file, then choose Download.Confirm that you can open the downloaded file with the password your Lambda function protected it with (my-secret-password).",
                                    "  2 : Verify that Lambda has saved an encrypted version of your PDF file in your target bucket by doing the following:Navigate back to the Buckets page of the Amazon S3 console and choose your destination bucket.In the Objects pane, you should now see a file with name format filename_encrypted.pdf (where                         filename.pdf was the name of the file you uploaded to your source bucket).                        To download your encrypted PDF, select the file, then choose Download.Confirm that you can open the downloaded file with the password your Lambda function protected it with (my-secret-password).",
                                    "  2 : Verify that Lambda has saved an encrypted version of your PDF file in your target bucket by doing the following:Navigate back to the Buckets page of the Amazon S3 console and choose your destination bucket.In the Objects pane, you should now see a file with name format filename_encrypted.pdf (where                         filename.pdf was the name of the file you uploaded to your source bucket).                        To download your encrypted PDF, select the file, then choose Download.Confirm that you can open the downloaded file with the password your Lambda function protected it with (my-secret-password).",
                                    "  1 : Navigate back to the Buckets page of the Amazon S3 console and choose your destination bucket.",
                                    "  1 : Navigate back to the Buckets page of the Amazon S3 console and choose your destination bucket.",
                                    "  1 : Navigate back to the Buckets page of the Amazon S3 console and choose your destination bucket.",
                                    "  2 : In the Objects pane, you should now see a file with name format filename_encrypted.pdf (where                         filename.pdf was the name of the file you uploaded to your source bucket).                        To download your encrypted PDF, select the file, then choose Download.",
                                    "  2 : In the Objects pane, you should now see a file with name format filename_encrypted.pdf (where                         filename.pdf was the name of the file you uploaded to your source bucket).                        To download your encrypted PDF, select the file, then choose Download.",
                                    "  2 : In the Objects pane, you should now see a file with name format filename_encrypted.pdf (where                         filename.pdf was the name of the file you uploaded to your source bucket).                        To download your encrypted PDF, select the file, then choose Download.",
                                    "  3 : Confirm that you can open the downloaded file with the password your Lambda function protected it with (my-secret-password).",
                                    "  3 : Confirm that you can open the downloaded file with the password your Lambda function protected it with (my-secret-password).",
                                    "  3 : Confirm that you can open the downloaded file with the password your Lambda function protected it with (my-secret-password).",
                                    "  2 : Verify that Lambda has saved an encrypted version of your PDF file in your target bucket by doing the following:Navigate back to the Buckets page of the Amazon S3 console and choose your destination bucket.In the Objects pane, you should now see a file with name format filename_encrypted.pdf (where                         filename.pdf was the name of the file you uploaded to your source bucket).                        To download your encrypted PDF, select the file, then choose Download.Confirm that you can open the downloaded file with the password your Lambda function protected it with (my-secret-password)."
                                ]
                            },
                            {
                                "sub_header": "Testing the app with the automated script",
                                "content": [
                                    "To test your app using the provided test script, first ensure that the pytest module is installed in your local environment. You can install       pytest by running the following command:",
                                    "pip install pytest",
                                    "You also need to edit the code in the test_pdf_encrypt.py file to replace the placeholder bucket names with the names of           your Amazon S3 source and destination buckets. Make the following changes to test_pdf_encrypt.py:",
                                    "  1.In the test_source_bucket_available function, replace EXAMPLE-BUCKET with the name of your source bucket.",
                                    "  2.In the test_encrypted_file_in_bucket function, replace EXAMPLE-BUCKET-encrypted with <source-bucket>-encrypted,            where <source-bucket> is the name of your source bucket.",
                                    "  3.In the cleanup function, replace EXAMPLE-BUCKET with the name of your source bucket, and replace               EXAMPLE-BUCKET-encrypted with ≪source-bucket>-encrypted, where <source-bucket> is the name of your source bucket.",
                                    "To run the tests do the following:",
                                    "  1.Save a PDF file named test.pdfin the directory containing the test_pdf_encrypt.py and pytest.ini             files.",
                                    "  2.Open a terminal or shell program and run the following command from the directory containing the test files.pytest -s -v",
                                    {
                                        "code_example": "pytest -s -v"
                                    },
                                    "When the test completes, you should see output like the following:",
                                    "============================================================== test session starts =========================================================platform linux -- Python 3.12.2, pytest-7.2.2, pluggy-1.0.0 -- /usr/bin/python3cachedir: .pytest_cachehypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/pdf_encrypt_app/.hypothesis/examples')Test order randomisation NOT enabled. Enable with --random-order or --random-order-bucket=<bucket_type>rootdir: /home/pdf_encrypt_app, configfile: pytest.iniplugins: anyio-3.7.1, hypothesis-6.70.0, localserver-0.7.1, random-order-1.1.0collected 4 itemstest_pdf_encrypt.py::test_source_bucket_available PASSEDtest_pdf_encrypt.py::test_lambda_invoked PASSEDtest_pdf_encrypt.py::test_encrypted_file_in_bucket PASSEDtest_pdf_encrypt.py::test_cleanup PASSEDDeleted test.pdf from EXAMPLE-BUCKETDeleted test_encrypted.pdf from EXAMPLE-BUCKET-encrypted=============================================================== 4 passed in 7.32s =========================================================="
                                ]
                            }
                        ]
                    },
                    {
                        "sub_header": "Next steps",
                        "content": [
                            "Now you've created this example app, you can use the provided code as a basis to create other types of file-processing application. Modify the     code in the lambda_function.py file to implement the file-processing logic for your use case.",
                            "Many typical file-processing use cases involve image processing. When using Python, the most popular image-processing libraries like       pillow typically contain C or C++ components. In order to ensure that your function's deployment package is     compatible with the Lambda execution environment, it's important to use the correct source distribution binary.",
                            "When deploying your resources with AWS SAM, you need to take some extra steps to include the right source distribution in your deployment package. Because AWS SAM won't install dependencies     for a different platform than your build machine, specifying the correct source distribution (.whl file) in your requirements.txt       file won't work if your build machine uses an operating system or architecture that's different from the Lambda execution environment. Instead, you should do one of the following:",
                            "  1.Use the --use-container option when running sam build. When you specify this option, AWS SAM downloads a container base image that's         compatible with the Lambda execution environment and builds your function's deployment package in a Docker container using that image. To learn more, see           Building a Lambda           function inside of a provided container.",
                            "  2.Build your function's .zip deployment package yourself using the correct source distribution binary and save the .zip file in the directory you specify as the           CodeUri in the AWS SAM template. To learn more about building .zip deployment packages for Python using binary distributions, see           Creating a .zip deployment package with dependencies and Creating .zip deployment packages with native libraries."
                        ]
                    }
                ]
            },
            {
                "title": "Scheduled-maintenance app",
                "href": "https://docs.aws.amazon.com/lambda/latest/dg/scheduled-task-app.html",
                "sections": [
                    "You can use AWS Lambda to replace scheduled processes such as automated system backups, file conversions, and maintenance tasks.   In this example, you create a serverless application that performs regular scheduled maintenance on a DynamoDB table by deleting old entries. The app uses EventBridge Scheduler to invoke a   Lambda function on a cron schedule. When invoked, the function queries the table for items older than one year, and deletes them. The function logs each deleted item  in CloudWatch Logs.",
                    "To implement this example, first create a DynamoDB table and populate it with some test data for your function to query. Then, create a Python Lambda function with   an EventBridge Scheduler trigger and an IAM execution role that gives the function permission to read, and delete, items from your table.",
                    "Tip",
                    "If you’re new to Lambda, we recommend that you complete the tutorial Create your first Lambda function before      creating this example app.",
                    "You can deploy your app manually by creating and configuring resources with the AWS Management Console. You can     also deploy the app by using the AWS Serverless Application Model (AWS SAM). AWS SAM is an infrastructure as code (IaC) tool. With IaC, you don’t create     resources manually, but define them in code and then deploy them automatically.",
                    "If you want to learn more about using Lambda with IaC before deploying this example app, see Using Lambda with infrastructure as code (IaC).",
                    {
                        "sub_header": "Prerequisites",
                        "content": [
                            "Before you can create the example app, make sure you have the required command line tools and programs installed.",
                            "  1.Python : To populate the DynamoDB table you create to test your app, this example uses a  script and a CSV file to write data into the table. Make sure you have          version 3.8 or later installed on your machine.",
                            "  2.AWS SAM CLI : If you want to create the DynamoDB table and deploy the example app using AWS SAM, you need to install the .           Follow the installation instructions           in the AWS SAM User Guide.",
                            "  3.AWS CLI : To use the provided Python script to populate your test table, you need to have installed and configured the . This is because the script uses           the AWS SDK for Python (Boto3), which needs access to your AWS Identity and Access Management (IAM) credentials. You also need the  installed to deploy resources using AWS SAM. Install the CLI by following           the installation instructions in the AWS Command Line Interface User Guide.",
                            "  4.Docker : To deploy the app using AWS SAM,  must also be installed on your build machine. Follow the instructions in Install  Engine         on the  documentation website."
                        ]
                    },
                    {
                        "sub_header": "Downloading the example app files",
                        "content": [
                            "To create the example database and the scheduled-maintenance app, you need to create the following files in your project directory:",
                            "Example database files",
                            "  1.template.yaml - an AWS SAM template you can use to create the DynamoDB table",
                            "  2.sample_data.csv - a CSV file containing sample data to load into your table",
                            "  3.load_sample_data.py - a Python script that writes the data in the CSV file into the table",
                            "Scheduled-maintenance app files",
                            "  1.lambda_function.py - the Python function code for the Lambda function that performs the database maintenance",
                            "  2.requirements.txt - a manifest file defining the dependencies that your Python function code requires",
                            "  3.template.yaml - an AWS SAM template you can use to deploy the app",
                            "Test file",
                            "  1.test_app.py - a Python script that scans the table and confirms successful operation of your function by outputting all records older than one year",
                            "Expand the following sections to view the code and to learn more about the role of each file in creating and testing your app. To create the       files on your local machine, copy and paste the code below.",
                            "Copy and paste the following code into a file named template.yaml.",
                            {
                                "code_example": "AWSTemplateFormatVersion: '2010-09-09'\nTransform: AWS::Serverless-2016-10-31\nDescription: SAM Template for DynamoDB Table with Order_number as Partition Key and Date as Sort Key\n\nResources:\n  MyDynamoDBTable:\n    Type: AWS::DynamoDB::Table\n    DeletionPolicy: Retain\n    UpdateReplacePolicy: Retain\n    Properties:\n      TableName: MyOrderTable\n      BillingMode: PAY_PER_REQUEST\n      AttributeDefinitions:\n        - AttributeName: Order_number\n          AttributeType: S\n        - AttributeName: Date\n          AttributeType: S\n      KeySchema:\n        - AttributeName: Order_number\n          KeyType: HASH\n        - AttributeName: Date\n          KeyType: RANGE\n      SSESpecification:\n        SSEEnabled: true\n      GlobalSecondaryIndexes:\n        - IndexName: Date-index\n          KeySchema:\n            - AttributeName: Date\n              KeyType: HASH\n          Projection:\n            ProjectionType: ALL\n      PointInTimeRecoverySpecification:\n        PointInTimeRecoveryEnabled: true\n\nOutputs:\n  TableName:\n    Description: DynamoDB Table Name\n    Value: !Ref MyDynamoDBTable\n  TableArn:\n    Description: DynamoDB Table ARN\n    Value: !GetAtt MyDynamoDBTable.Arn"
                            },
                            "Note",
                            "AWS SAM templates use a standard naming convention of template.yaml. In this example, you have two template files - one to create the           example database and another to create the app itself. Save them in separate sub-directories in your project folder.",
                            "This AWS SAM template defines the DynamoDB table resource you create to test your app. The table uses a primary key of Order_number with a sort           key of Date. In order for your Lambda function to find items directly by date, we also define a Global Secondary Index         named Date-index.",
                            "To learn more about creating and configuring a DynamoDB table using the AWS::DynamoDB::Table resource, see AWS::DynamoDB::Table in           the AWS CloudFormation User Guide.",
                            "AWS SAM template (example DynamoDB table)",
                            "Copy and paste the following code into a file named template.yaml.",
                            {
                                "code_example": "AWSTemplateFormatVersion: '2010-09-09'\nTransform: AWS::Serverless-2016-10-31\nDescription: SAM Template for DynamoDB Table with Order_number as Partition Key and Date as Sort Key\n\nResources:\n  MyDynamoDBTable:\n    Type: AWS::DynamoDB::Table\n    DeletionPolicy: Retain\n    UpdateReplacePolicy: Retain\n    Properties:\n      TableName: MyOrderTable\n      BillingMode: PAY_PER_REQUEST\n      AttributeDefinitions:\n        - AttributeName: Order_number\n          AttributeType: S\n        - AttributeName: Date\n          AttributeType: S\n      KeySchema:\n        - AttributeName: Order_number\n          KeyType: HASH\n        - AttributeName: Date\n          KeyType: RANGE\n      SSESpecification:\n        SSEEnabled: true\n      GlobalSecondaryIndexes:\n        - IndexName: Date-index\n          KeySchema:\n            - AttributeName: Date\n              KeyType: HASH\n          Projection:\n            ProjectionType: ALL\n      PointInTimeRecoverySpecification:\n        PointInTimeRecoveryEnabled: true\n\nOutputs:\n  TableName:\n    Description: DynamoDB Table Name\n    Value: !Ref MyDynamoDBTable\n  TableArn:\n    Description: DynamoDB Table ARN\n    Value: !GetAtt MyDynamoDBTable.Arn"
                            },
                            "Note",
                            "AWS SAM templates use a standard naming convention of template.yaml. In this example, you have two template files - one to create the           example database and another to create the app itself. Save them in separate sub-directories in your project folder.",
                            "This AWS SAM template defines the DynamoDB table resource you create to test your app. The table uses a primary key of Order_number with a sort           key of Date. In order for your Lambda function to find items directly by date, we also define a Global Secondary Index         named Date-index.",
                            "To learn more about creating and configuring a DynamoDB table using the AWS::DynamoDB::Table resource, see AWS::DynamoDB::Table in           the AWS CloudFormation User Guide.",
                            "Copy and paste the following code into a file named template.yaml.AWSTemplateFormatVersion: '2010-09-09'Transform: AWS::Serverless-2016-10-31Description: SAM Template for DynamoDB Table with Order_number as Partition Key and Date as Sort KeyResources:  MyDynamoDBTable:    Type: AWS::DynamoDB::Table    DeletionPolicy: Retain    UpdateReplacePolicy: Retain    Properties:      TableName: MyOrderTable      BillingMode: PAY_PER_REQUEST      AttributeDefinitions:        - AttributeName: Order_number          AttributeType: S        - AttributeName: Date          AttributeType: S      KeySchema:        - AttributeName: Order_number          KeyType: HASH        - AttributeName: Date          KeyType: RANGE      SSESpecification:        SSEEnabled: true      GlobalSecondaryIndexes:        - IndexName: Date-index          KeySchema:            - AttributeName: Date              KeyType: HASH          Projection:            ProjectionType: ALL      PointInTimeRecoverySpecification:        PointInTimeRecoveryEnabled: trueOutputs:  TableName:    Description: DynamoDB Table Name    Value: !Ref MyDynamoDBTable  TableArn:    Description: DynamoDB Table ARN    Value: !GetAtt MyDynamoDBTable.ArnNoteAWS SAM templates use a standard naming convention of template.yaml. In this example, you have two template files - one to create the           example database and another to create the app itself. Save them in separate sub-directories in your project folder.This AWS SAM template defines the DynamoDB table resource you create to test your app. The table uses a primary key of Order_number with a sort           key of Date. In order for your Lambda function to find items directly by date, we also define a Global Secondary Index         named Date-index.To learn more about creating and configuring a DynamoDB table using the AWS::DynamoDB::Table resource, see AWS::DynamoDB::Table in           the AWS CloudFormation User Guide.AWS SAM template (example DynamoDB table)Copy and paste the following code into a file named template.yaml.AWSTemplateFormatVersion: '2010-09-09'Transform: AWS::Serverless-2016-10-31Description: SAM Template for DynamoDB Table with Order_number as Partition Key and Date as Sort KeyResources:  MyDynamoDBTable:    Type: AWS::DynamoDB::Table    DeletionPolicy: Retain    UpdateReplacePolicy: Retain    Properties:      TableName: MyOrderTable      BillingMode: PAY_PER_REQUEST      AttributeDefinitions:        - AttributeName: Order_number          AttributeType: S        - AttributeName: Date          AttributeType: S      KeySchema:        - AttributeName: Order_number          KeyType: HASH        - AttributeName: Date          KeyType: RANGE      SSESpecification:        SSEEnabled: true      GlobalSecondaryIndexes:        - IndexName: Date-index          KeySchema:            - AttributeName: Date              KeyType: HASH          Projection:            ProjectionType: ALL      PointInTimeRecoverySpecification:        PointInTimeRecoveryEnabled: trueOutputs:  TableName:    Description: DynamoDB Table Name    Value: !Ref MyDynamoDBTable  TableArn:    Description: DynamoDB Table ARN    Value: !GetAtt MyDynamoDBTable.ArnNoteAWS SAM templates use a standard naming convention of template.yaml. In this example, you have two template files - one to create the           example database and another to create the app itself. Save them in separate sub-directories in your project folder.This AWS SAM template defines the DynamoDB table resource you create to test your app. The table uses a primary key of Order_number with a sort           key of Date. In order for your Lambda function to find items directly by date, we also define a Global Secondary Index         named Date-index.To learn more about creating and configuring a DynamoDB table using the AWS::DynamoDB::Table resource, see AWS::DynamoDB::Table in           the AWS CloudFormation User Guide.",
                            "Copy and paste the following code into a file named sample_data.csv.",
                            {
                                "code_example": "Date,Order_number,CustomerName,ProductID,Quantity,TotalAmount\n2023-09-01,ORD001,Alejandro Rosalez,PROD123,2,199.98\n2023-09-01,ORD002,Akua Mansa,PROD456,1,49.99\n2023-09-02,ORD003,Ana Carolina Silva,PROD789,3,149.97\n2023-09-03,ORD004,Arnav Desai,PROD123,1,99.99\n2023-10-01,ORD005,Carlos Salazar,PROD456,2,99.98\n2023-10-02,ORD006,Diego Ramirez,PROD789,1,49.99\n2023-10-03,ORD007,Efua Owusu,PROD123,4,399.96\n2023-10-04,ORD008,John Stiles,PROD456,2,99.98\n2023-10-05,ORD009,Jorge Souza,PROD789,3,149.97\n2023-10-06,ORD010,Kwaku Mensah,PROD123,1,99.99\n2023-11-01,ORD011,Li Juan,PROD456,5,249.95\n2023-11-02,ORD012,Marcia Oliveria,PROD789,2,99.98\n2023-11-03,ORD013,Maria Garcia,PROD123,3,299.97\n2023-11-04,ORD014,Martha Rivera,PROD456,1,49.99\n2023-11-05,ORD015,Mary Major,PROD789,4,199.96\n2023-12-01,ORD016,Mateo Jackson,PROD123,2,199.99\n2023-12-02,ORD017,Nikki Wolf,PROD456,3,149.97\n2023-12-03,ORD018,Pat Candella,PROD789,1,49.99\n2023-12-04,ORD019,Paulo Santos,PROD123,5,499.95\n2023-12-05,ORD020,Richard Roe,PROD456,2,99.98\n2024-01-01,ORD021,Saanvi Sarkar,PROD789,3,149.97\n2024-01-02,ORD022,Shirley Rodriguez,PROD123,1,99.99\n2024-01-03,ORD023,Sofia Martinez,PROD456,4,199.96\n2024-01-04,ORD024,Terry Whitlock,PROD789,2,99.98\n2024-01-05,ORD025,Wang Xiulan,PROD123,3,299.97"
                            },
                            "This file contains some example test data to populate your DynamoDB table with in a standard comma-separated values (CSV) format.",
                            "Sample database data file",
                            "Copy and paste the following code into a file named sample_data.csv.",
                            {
                                "code_example": "Date,Order_number,CustomerName,ProductID,Quantity,TotalAmount\n2023-09-01,ORD001,Alejandro Rosalez,PROD123,2,199.98\n2023-09-01,ORD002,Akua Mansa,PROD456,1,49.99\n2023-09-02,ORD003,Ana Carolina Silva,PROD789,3,149.97\n2023-09-03,ORD004,Arnav Desai,PROD123,1,99.99\n2023-10-01,ORD005,Carlos Salazar,PROD456,2,99.98\n2023-10-02,ORD006,Diego Ramirez,PROD789,1,49.99\n2023-10-03,ORD007,Efua Owusu,PROD123,4,399.96\n2023-10-04,ORD008,John Stiles,PROD456,2,99.98\n2023-10-05,ORD009,Jorge Souza,PROD789,3,149.97\n2023-10-06,ORD010,Kwaku Mensah,PROD123,1,99.99\n2023-11-01,ORD011,Li Juan,PROD456,5,249.95\n2023-11-02,ORD012,Marcia Oliveria,PROD789,2,99.98\n2023-11-03,ORD013,Maria Garcia,PROD123,3,299.97\n2023-11-04,ORD014,Martha Rivera,PROD456,1,49.99\n2023-11-05,ORD015,Mary Major,PROD789,4,199.96\n2023-12-01,ORD016,Mateo Jackson,PROD123,2,199.99\n2023-12-02,ORD017,Nikki Wolf,PROD456,3,149.97\n2023-12-03,ORD018,Pat Candella,PROD789,1,49.99\n2023-12-04,ORD019,Paulo Santos,PROD123,5,499.95\n2023-12-05,ORD020,Richard Roe,PROD456,2,99.98\n2024-01-01,ORD021,Saanvi Sarkar,PROD789,3,149.97\n2024-01-02,ORD022,Shirley Rodriguez,PROD123,1,99.99\n2024-01-03,ORD023,Sofia Martinez,PROD456,4,199.96\n2024-01-04,ORD024,Terry Whitlock,PROD789,2,99.98\n2024-01-05,ORD025,Wang Xiulan,PROD123,3,299.97"
                            },
                            "This file contains some example test data to populate your DynamoDB table with in a standard comma-separated values (CSV) format.",
                            "Copy and paste the following code into a file named sample_data.csv.Date,Order_number,CustomerName,ProductID,Quantity,TotalAmount2023-09-01,ORD001,Alejandro Rosalez,PROD123,2,199.982023-09-01,ORD002,Akua Mansa,PROD456,1,49.992023-09-02,ORD003,Ana Carolina Silva,PROD789,3,149.972023-09-03,ORD004,Arnav Desai,PROD123,1,99.992023-10-01,ORD005,Carlos Salazar,PROD456,2,99.982023-10-02,ORD006,Diego Ramirez,PROD789,1,49.992023-10-03,ORD007,Efua Owusu,PROD123,4,399.962023-10-04,ORD008,John Stiles,PROD456,2,99.982023-10-05,ORD009,Jorge Souza,PROD789,3,149.972023-10-06,ORD010,Kwaku Mensah,PROD123,1,99.992023-11-01,ORD011,Li Juan,PROD456,5,249.952023-11-02,ORD012,Marcia Oliveria,PROD789,2,99.982023-11-03,ORD013,Maria Garcia,PROD123,3,299.972023-11-04,ORD014,Martha Rivera,PROD456,1,49.992023-11-05,ORD015,Mary Major,PROD789,4,199.962023-12-01,ORD016,Mateo Jackson,PROD123,2,199.992023-12-02,ORD017,Nikki Wolf,PROD456,3,149.972023-12-03,ORD018,Pat Candella,PROD789,1,49.992023-12-04,ORD019,Paulo Santos,PROD123,5,499.952023-12-05,ORD020,Richard Roe,PROD456,2,99.982024-01-01,ORD021,Saanvi Sarkar,PROD789,3,149.972024-01-02,ORD022,Shirley Rodriguez,PROD123,1,99.992024-01-03,ORD023,Sofia Martinez,PROD456,4,199.962024-01-04,ORD024,Terry Whitlock,PROD789,2,99.982024-01-05,ORD025,Wang Xiulan,PROD123,3,299.97This file contains some example test data to populate your DynamoDB table with in a standard comma-separated values (CSV) format.Sample database data fileCopy and paste the following code into a file named sample_data.csv.Date,Order_number,CustomerName,ProductID,Quantity,TotalAmount2023-09-01,ORD001,Alejandro Rosalez,PROD123,2,199.982023-09-01,ORD002,Akua Mansa,PROD456,1,49.992023-09-02,ORD003,Ana Carolina Silva,PROD789,3,149.972023-09-03,ORD004,Arnav Desai,PROD123,1,99.992023-10-01,ORD005,Carlos Salazar,PROD456,2,99.982023-10-02,ORD006,Diego Ramirez,PROD789,1,49.992023-10-03,ORD007,Efua Owusu,PROD123,4,399.962023-10-04,ORD008,John Stiles,PROD456,2,99.982023-10-05,ORD009,Jorge Souza,PROD789,3,149.972023-10-06,ORD010,Kwaku Mensah,PROD123,1,99.992023-11-01,ORD011,Li Juan,PROD456,5,249.952023-11-02,ORD012,Marcia Oliveria,PROD789,2,99.982023-11-03,ORD013,Maria Garcia,PROD123,3,299.972023-11-04,ORD014,Martha Rivera,PROD456,1,49.992023-11-05,ORD015,Mary Major,PROD789,4,199.962023-12-01,ORD016,Mateo Jackson,PROD123,2,199.992023-12-02,ORD017,Nikki Wolf,PROD456,3,149.972023-12-03,ORD018,Pat Candella,PROD789,1,49.992023-12-04,ORD019,Paulo Santos,PROD123,5,499.952023-12-05,ORD020,Richard Roe,PROD456,2,99.982024-01-01,ORD021,Saanvi Sarkar,PROD789,3,149.972024-01-02,ORD022,Shirley Rodriguez,PROD123,1,99.992024-01-03,ORD023,Sofia Martinez,PROD456,4,199.962024-01-04,ORD024,Terry Whitlock,PROD789,2,99.982024-01-05,ORD025,Wang Xiulan,PROD123,3,299.97This file contains some example test data to populate your DynamoDB table with in a standard comma-separated values (CSV) format.",
                            "Copy and paste the following code into a file named load_sample_data.py.",
                            {
                                "code_example": "import boto3\nimport csv\nfrom decimal import Decimal\n\n# Initialize the DynamoDB client\ndynamodb = boto3.resource('dynamodb')\ntable = dynamodb.Table('MyOrderTable') \nprint(\"DDB client initialized.\")\n\ndef load_data_from_csv(filename):\n    with open(filename, 'r') as file:\n        csv_reader = csv.DictReader(file)\n        for row in csv_reader:\n            item = {\n                'Order_number': row['Order_number'],\n                'Date': row['Date'],\n                'CustomerName': row['CustomerName'],\n                'ProductID': row['ProductID'],\n                'Quantity': int(row['Quantity']),\n                'TotalAmount': Decimal(str(row['TotalAmount']))\n            }\n            table.put_item(Item=item)\n            print(f\"Added item: {item['Order_number']} - {item['Date']}\")\n\nif __name__ == \"__main__\":\n    load_data_from_csv('sample_data.csv')\n    print(\"Data loading completed.\")"
                            },
                            "This Python script first uses the AWS SDK for Python (Boto3) to create a connection to your DynamoDB table. It then iterates over each row in the example-data CSV file, creates an item from that row, and            writes the item to the DynamoDB table using the boto3 SDK.",
                            "Python script to load sample data",
                            "Copy and paste the following code into a file named load_sample_data.py.",
                            {
                                "code_example": "import boto3\nimport csv\nfrom decimal import Decimal\n\n# Initialize the DynamoDB client\ndynamodb = boto3.resource('dynamodb')\ntable = dynamodb.Table('MyOrderTable') \nprint(\"DDB client initialized.\")\n\ndef load_data_from_csv(filename):\n    with open(filename, 'r') as file:\n        csv_reader = csv.DictReader(file)\n        for row in csv_reader:\n            item = {\n                'Order_number': row['Order_number'],\n                'Date': row['Date'],\n                'CustomerName': row['CustomerName'],\n                'ProductID': row['ProductID'],\n                'Quantity': int(row['Quantity']),\n                'TotalAmount': Decimal(str(row['TotalAmount']))\n            }\n            table.put_item(Item=item)\n            print(f\"Added item: {item['Order_number']} - {item['Date']}\")\n\nif __name__ == \"__main__\":\n    load_data_from_csv('sample_data.csv')\n    print(\"Data loading completed.\")"
                            },
                            "This Python script first uses the AWS SDK for Python (Boto3) to create a connection to your DynamoDB table. It then iterates over each row in the example-data CSV file, creates an item from that row, and            writes the item to the DynamoDB table using the boto3 SDK.",
                            "Copy and paste the following code into a file named load_sample_data.py.import boto3import csvfrom decimal import Decimal# Initialize the DynamoDB clientdynamodb = boto3.resource('dynamodb')table = dynamodb.Table('MyOrderTable') print(\"DDB client initialized.\")def load_data_from_csv(filename):    with open(filename, 'r') as file:        csv_reader = csv.DictReader(file)        for row in csv_reader:            item = {                'Order_number': row['Order_number'],                'Date': row['Date'],                'CustomerName': row['CustomerName'],                'ProductID': row['ProductID'],                'Quantity': int(row['Quantity']),                'TotalAmount': Decimal(str(row['TotalAmount']))            }            table.put_item(Item=item)            print(f\"Added item: {item['Order_number']} - {item['Date']}\")if __name__ == \"__main__\":    load_data_from_csv('sample_data.csv')    print(\"Data loading completed.\")This Python script first uses the AWS SDK for Python (Boto3) to create a connection to your DynamoDB table. It then iterates over each row in the example-data CSV file, creates an item from that row, and            writes the item to the DynamoDB table using the boto3 SDK.Python script to load sample dataCopy and paste the following code into a file named load_sample_data.py.import boto3import csvfrom decimal import Decimal# Initialize the DynamoDB clientdynamodb = boto3.resource('dynamodb')table = dynamodb.Table('MyOrderTable') print(\"DDB client initialized.\")def load_data_from_csv(filename):    with open(filename, 'r') as file:        csv_reader = csv.DictReader(file)        for row in csv_reader:            item = {                'Order_number': row['Order_number'],                'Date': row['Date'],                'CustomerName': row['CustomerName'],                'ProductID': row['ProductID'],                'Quantity': int(row['Quantity']),                'TotalAmount': Decimal(str(row['TotalAmount']))            }            table.put_item(Item=item)            print(f\"Added item: {item['Order_number']} - {item['Date']}\")if __name__ == \"__main__\":    load_data_from_csv('sample_data.csv')    print(\"Data loading completed.\")This Python script first uses the AWS SDK for Python (Boto3) to create a connection to your DynamoDB table. It then iterates over each row in the example-data CSV file, creates an item from that row, and            writes the item to the DynamoDB table using the boto3 SDK.",
                            "Copy and paste the following code into a file named lambda_function.py.",
                            {
                                "code_example": "import boto3\nfrom datetime import datetime, timedelta\nfrom boto3.dynamodb.conditions import Key, Attr\nimport logging\n\nlogger = logging.getLogger()\nlogger.setLevel(\"INFO\")\n\ndef lambda_handler(event, context):\n    # Initialize the DynamoDB client\n    dynamodb = boto3.resource('dynamodb')\n    \n    # Specify the table name\n    table_name = 'MyOrderTable'\n    table = dynamodb.Table(table_name)\n    \n    # Get today's date\n    today = datetime.now()\n    \n    # Calculate the date one year ago\n    one_year_ago = (today - timedelta(days=365)).strftime('%Y-%m-%d')\n    \n    # Scan the table using a global secondary index\n    response = table.scan(\n        IndexName='Date-index',\n        FilterExpression='#date < :one_year_ago',\n        ExpressionAttributeNames={\n            '#date': 'Date'\n        },\n        ExpressionAttributeValues={\n            ':one_year_ago': one_year_ago\n        }\n    )\n    \n     # Delete old items\n    with table.batch_writer() as batch:\n        for item in response['Items']:\n            Order_number = item['Order_number']\n            batch.delete_item(\n                Key={\n                    'Order_number': Order_number,\n                    'Date': item['Date']\n                }\n            )\n            logger.info(f'deleted order number {Order_number}')\n    \n    # Check if there are more items to scan\n    while 'LastEvaluatedKey' in response:\n        response = table.scan(\n            IndexName='DateIndex',\n            FilterExpression='#date < :one_year_ago',\n            ExpressionAttributeNames={\n                '#date': 'Date'\n            },\n            ExpressionAttributeValues={\n                ':one_year_ago': one_year_ago\n            },\n            ExclusiveStartKey=response['LastEvaluatedKey']\n        )\n        \n        # Delete old items\n        with table.batch_writer() as batch:\n            for item in response['Items']:\n                batch.delete_item(\n                    Key={\n                        'Order_number': item['Order_number'],\n                        'Date': item['Date']\n                    }\n                )\n    \n    return {\n        'statusCode': 200,\n        'body': 'Cleanup completed successfully'\n    }"
                            },
                            "The Python function code contains the handler function (lambda_handler) that Lambda runs when your function is     invoked.",
                            "When the function is invoked by EventBridge Scheduler, it uses the AWS SDK for Python (Boto3) to create a connection to the DynamoDB table on which the scheduled maintenance task is to be performed.     It then uses the Python datetime library to calculate the date one year ago, before scanning the table for items older than this and deleting them.",
                            "Note that responses from DynamoDB query and scan operations are limited to a maximum of 1 MB in size. If the response is larger than 1 MB, DynamoDB paginates the       data and returns a LastEvaluatedKey element in the response. To ensure that our function processes all the records in the table, we check for the presence of this key       and continue performing table scans from the last evaluated position until the whole table has been scanned.",
                            "Python function code",
                            "Copy and paste the following code into a file named lambda_function.py.",
                            {
                                "code_example": "import boto3\nfrom datetime import datetime, timedelta\nfrom boto3.dynamodb.conditions import Key, Attr\nimport logging\n\nlogger = logging.getLogger()\nlogger.setLevel(\"INFO\")\n\ndef lambda_handler(event, context):\n    # Initialize the DynamoDB client\n    dynamodb = boto3.resource('dynamodb')\n    \n    # Specify the table name\n    table_name = 'MyOrderTable'\n    table = dynamodb.Table(table_name)\n    \n    # Get today's date\n    today = datetime.now()\n    \n    # Calculate the date one year ago\n    one_year_ago = (today - timedelta(days=365)).strftime('%Y-%m-%d')\n    \n    # Scan the table using a global secondary index\n    response = table.scan(\n        IndexName='Date-index',\n        FilterExpression='#date < :one_year_ago',\n        ExpressionAttributeNames={\n            '#date': 'Date'\n        },\n        ExpressionAttributeValues={\n            ':one_year_ago': one_year_ago\n        }\n    )\n    \n     # Delete old items\n    with table.batch_writer() as batch:\n        for item in response['Items']:\n            Order_number = item['Order_number']\n            batch.delete_item(\n                Key={\n                    'Order_number': Order_number,\n                    'Date': item['Date']\n                }\n            )\n            logger.info(f'deleted order number {Order_number}')\n    \n    # Check if there are more items to scan\n    while 'LastEvaluatedKey' in response:\n        response = table.scan(\n            IndexName='DateIndex',\n            FilterExpression='#date < :one_year_ago',\n            ExpressionAttributeNames={\n                '#date': 'Date'\n            },\n            ExpressionAttributeValues={\n                ':one_year_ago': one_year_ago\n            },\n            ExclusiveStartKey=response['LastEvaluatedKey']\n        )\n        \n        # Delete old items\n        with table.batch_writer() as batch:\n            for item in response['Items']:\n                batch.delete_item(\n                    Key={\n                        'Order_number': item['Order_number'],\n                        'Date': item['Date']\n                    }\n                )\n    \n    return {\n        'statusCode': 200,\n        'body': 'Cleanup completed successfully'\n    }"
                            },
                            "The Python function code contains the handler function (lambda_handler) that Lambda runs when your function is     invoked.",
                            "When the function is invoked by EventBridge Scheduler, it uses the AWS SDK for Python (Boto3) to create a connection to the DynamoDB table on which the scheduled maintenance task is to be performed.     It then uses the Python datetime library to calculate the date one year ago, before scanning the table for items older than this and deleting them.",
                            "Note that responses from DynamoDB query and scan operations are limited to a maximum of 1 MB in size. If the response is larger than 1 MB, DynamoDB paginates the       data and returns a LastEvaluatedKey element in the response. To ensure that our function processes all the records in the table, we check for the presence of this key       and continue performing table scans from the last evaluated position until the whole table has been scanned.",
                            "Copy and paste the following code into a file named lambda_function.py.import boto3from datetime import datetime, timedeltafrom boto3.dynamodb.conditions import Key, Attrimport logginglogger = logging.getLogger()logger.setLevel(\"INFO\")def lambda_handler(event, context):    # Initialize the DynamoDB client    dynamodb = boto3.resource('dynamodb')        # Specify the table name    table_name = 'MyOrderTable'    table = dynamodb.Table(table_name)        # Get today's date    today = datetime.now()        # Calculate the date one year ago    one_year_ago = (today - timedelta(days=365)).strftime('%Y-%m-%d')        # Scan the table using a global secondary index    response = table.scan(        IndexName='Date-index',        FilterExpression='#date < :one_year_ago',        ExpressionAttributeNames={            '#date': 'Date'        },        ExpressionAttributeValues={            ':one_year_ago': one_year_ago        }    )         # Delete old items    with table.batch_writer() as batch:        for item in response['Items']:            Order_number = item['Order_number']            batch.delete_item(                Key={                    'Order_number': Order_number,                    'Date': item['Date']                }            )            logger.info(f'deleted order number {Order_number}')        # Check if there are more items to scan    while 'LastEvaluatedKey' in response:        response = table.scan(            IndexName='DateIndex',            FilterExpression='#date < :one_year_ago',            ExpressionAttributeNames={                '#date': 'Date'            },            ExpressionAttributeValues={                ':one_year_ago': one_year_ago            },            ExclusiveStartKey=response['LastEvaluatedKey']        )                # Delete old items        with table.batch_writer() as batch:            for item in response['Items']:                batch.delete_item(                    Key={                        'Order_number': item['Order_number'],                        'Date': item['Date']                    }                )        return {        'statusCode': 200,        'body': 'Cleanup completed successfully'    }The Python function code contains the handler function (lambda_handler) that Lambda runs when your function is     invoked.When the function is invoked by EventBridge Scheduler, it uses the AWS SDK for Python (Boto3) to create a connection to the DynamoDB table on which the scheduled maintenance task is to be performed.     It then uses the Python datetime library to calculate the date one year ago, before scanning the table for items older than this and deleting them.Note that responses from DynamoDB query and scan operations are limited to a maximum of 1 MB in size. If the response is larger than 1 MB, DynamoDB paginates the       data and returns a LastEvaluatedKey element in the response. To ensure that our function processes all the records in the table, we check for the presence of this key       and continue performing table scans from the last evaluated position until the whole table has been scanned.Python function codeCopy and paste the following code into a file named lambda_function.py.import boto3from datetime import datetime, timedeltafrom boto3.dynamodb.conditions import Key, Attrimport logginglogger = logging.getLogger()logger.setLevel(\"INFO\")def lambda_handler(event, context):    # Initialize the DynamoDB client    dynamodb = boto3.resource('dynamodb')        # Specify the table name    table_name = 'MyOrderTable'    table = dynamodb.Table(table_name)        # Get today's date    today = datetime.now()        # Calculate the date one year ago    one_year_ago = (today - timedelta(days=365)).strftime('%Y-%m-%d')        # Scan the table using a global secondary index    response = table.scan(        IndexName='Date-index',        FilterExpression='#date < :one_year_ago',        ExpressionAttributeNames={            '#date': 'Date'        },        ExpressionAttributeValues={            ':one_year_ago': one_year_ago        }    )         # Delete old items    with table.batch_writer() as batch:        for item in response['Items']:            Order_number = item['Order_number']            batch.delete_item(                Key={                    'Order_number': Order_number,                    'Date': item['Date']                }            )            logger.info(f'deleted order number {Order_number}')        # Check if there are more items to scan    while 'LastEvaluatedKey' in response:        response = table.scan(            IndexName='DateIndex',            FilterExpression='#date < :one_year_ago',            ExpressionAttributeNames={                '#date': 'Date'            },            ExpressionAttributeValues={                ':one_year_ago': one_year_ago            },            ExclusiveStartKey=response['LastEvaluatedKey']        )                # Delete old items        with table.batch_writer() as batch:            for item in response['Items']:                batch.delete_item(                    Key={                        'Order_number': item['Order_number'],                        'Date': item['Date']                    }                )        return {        'statusCode': 200,        'body': 'Cleanup completed successfully'    }The Python function code contains the handler function (lambda_handler) that Lambda runs when your function is     invoked.When the function is invoked by EventBridge Scheduler, it uses the AWS SDK for Python (Boto3) to create a connection to the DynamoDB table on which the scheduled maintenance task is to be performed.     It then uses the Python datetime library to calculate the date one year ago, before scanning the table for items older than this and deleting them.Note that responses from DynamoDB query and scan operations are limited to a maximum of 1 MB in size. If the response is larger than 1 MB, DynamoDB paginates the       data and returns a LastEvaluatedKey element in the response. To ensure that our function processes all the records in the table, we check for the presence of this key       and continue performing table scans from the last evaluated position until the whole table has been scanned.",
                            "Copy and paste the following code into a file named requirements.txt.",
                            {
                                "code_example": "boto3"
                            },
                            "For this example, your function code has only one dependency that isn't part of the standard Python library -           the SDK for Python (Boto3) that the function uses to scan and delete items from the DynamoDB table.",
                            "Note",
                            "A version of the SDK for Python (Boto3) is included as part of the Lambda runtime, so your code would run without adding Boto3 to your             function's deployment package. However, to maintain full control of your function's dependencies and avoid possible issues with             version misalignment, best practice for Python is to include all function dependencies in your function's deployment package.             See Runtime dependencies in Python to learn more.",
                            "requirements.txt manifest file",
                            "Copy and paste the following code into a file named requirements.txt.",
                            {
                                "code_example": "boto3"
                            },
                            "For this example, your function code has only one dependency that isn't part of the standard Python library -           the SDK for Python (Boto3) that the function uses to scan and delete items from the DynamoDB table.",
                            "Note",
                            "A version of the SDK for Python (Boto3) is included as part of the Lambda runtime, so your code would run without adding Boto3 to your             function's deployment package. However, to maintain full control of your function's dependencies and avoid possible issues with             version misalignment, best practice for Python is to include all function dependencies in your function's deployment package.             See Runtime dependencies in Python to learn more.",
                            "Copy and paste the following code into a file named requirements.txt.boto3For this example, your function code has only one dependency that isn't part of the standard Python library -           the SDK for Python (Boto3) that the function uses to scan and delete items from the DynamoDB table.NoteA version of the SDK for Python (Boto3) is included as part of the Lambda runtime, so your code would run without adding Boto3 to your             function's deployment package. However, to maintain full control of your function's dependencies and avoid possible issues with             version misalignment, best practice for Python is to include all function dependencies in your function's deployment package.             See Runtime dependencies in Python to learn more.requirements.txt manifest fileCopy and paste the following code into a file named requirements.txt.boto3For this example, your function code has only one dependency that isn't part of the standard Python library -           the SDK for Python (Boto3) that the function uses to scan and delete items from the DynamoDB table.NoteA version of the SDK for Python (Boto3) is included as part of the Lambda runtime, so your code would run without adding Boto3 to your             function's deployment package. However, to maintain full control of your function's dependencies and avoid possible issues with             version misalignment, best practice for Python is to include all function dependencies in your function's deployment package.             See Runtime dependencies in Python to learn more.",
                            "Copy and paste the following code into a file named template.yaml.",
                            {
                                "code_example": "AWSTemplateFormatVersion: '2010-09-09'\nTransform: AWS::Serverless-2016-10-31\nDescription: SAM Template for Lambda function and EventBridge Scheduler rule\n\nResources:\n  MyLambdaFunction:\n    Type: AWS::Serverless::Function\n    Properties:\n      FunctionName: ScheduledDBMaintenance\n      CodeUri: ./\n      Handler: lambda_function.lambda_handler\n      Runtime: python3.11\n      Architectures:\n        - x86_64\n      Events:\n        ScheduleEvent:\n          Type: ScheduleV2\n          Properties:\n            ScheduleExpression: cron(0 3 1 * ? *)\n            Description: Run on the first day of every month at 03:00 AM\n      Policies:\n        - CloudWatchLogsFullAccess\n        - Statement:\n            - Effect: Allow\n              Action:\n                - dynamodb:Scan\n                - dynamodb:BatchWriteItem\n              Resource: !Sub 'arn:aws:dynamodb:${AWS::Region}:${AWS::AccountId}:table/MyOrderTable'\n\n  LambdaLogGroup:\n    Type: AWS::Logs::LogGroup\n    Properties:\n      LogGroupName: !Sub /aws/lambda/${MyLambdaFunction}\n      RetentionInDays: 30\n\nOutputs:\n  LambdaFunctionName:\n    Description: Lambda Function Name\n    Value: !Ref MyLambdaFunction\n  LambdaFunctionArn:\n    Description: Lambda Function ARN\n    Value: !GetAtt MyLambdaFunction.Arn"
                            },
                            "Note",
                            "AWS SAM templates use a standard naming convention of template.yaml. In this example, you have two template files - one to create the             example database and another to create the app itself. Save them in separate sub-directories in your project folder.",
                            "This AWS SAM template defines the resources for your app. We define the Lambda function using the AWS::Serverless::Function resource. The EventBridge Scheduler schedule and the           trigger to invoke the Lambda function are created by using the Events property of this resource using a type of ScheduleV2. To learn more about defining EventBridge Scheduler schedules in AWS SAM templates,           see ScheduleV2 in the           AWS Serverless Application Model Developer Guide.",
                            "In addition to the Lambda function and the EventBridge Scheduler schedule, we also define a CloudWatch log group for your function to send records of deleted items to.",
                            "AWS SAM template (scheduled-maintenance app)",
                            "Copy and paste the following code into a file named template.yaml.",
                            {
                                "code_example": "AWSTemplateFormatVersion: '2010-09-09'\nTransform: AWS::Serverless-2016-10-31\nDescription: SAM Template for Lambda function and EventBridge Scheduler rule\n\nResources:\n  MyLambdaFunction:\n    Type: AWS::Serverless::Function\n    Properties:\n      FunctionName: ScheduledDBMaintenance\n      CodeUri: ./\n      Handler: lambda_function.lambda_handler\n      Runtime: python3.11\n      Architectures:\n        - x86_64\n      Events:\n        ScheduleEvent:\n          Type: ScheduleV2\n          Properties:\n            ScheduleExpression: cron(0 3 1 * ? *)\n            Description: Run on the first day of every month at 03:00 AM\n      Policies:\n        - CloudWatchLogsFullAccess\n        - Statement:\n            - Effect: Allow\n              Action:\n                - dynamodb:Scan\n                - dynamodb:BatchWriteItem\n              Resource: !Sub 'arn:aws:dynamodb:${AWS::Region}:${AWS::AccountId}:table/MyOrderTable'\n\n  LambdaLogGroup:\n    Type: AWS::Logs::LogGroup\n    Properties:\n      LogGroupName: !Sub /aws/lambda/${MyLambdaFunction}\n      RetentionInDays: 30\n\nOutputs:\n  LambdaFunctionName:\n    Description: Lambda Function Name\n    Value: !Ref MyLambdaFunction\n  LambdaFunctionArn:\n    Description: Lambda Function ARN\n    Value: !GetAtt MyLambdaFunction.Arn"
                            },
                            "Note",
                            "AWS SAM templates use a standard naming convention of template.yaml. In this example, you have two template files - one to create the             example database and another to create the app itself. Save them in separate sub-directories in your project folder.",
                            "This AWS SAM template defines the resources for your app. We define the Lambda function using the AWS::Serverless::Function resource. The EventBridge Scheduler schedule and the           trigger to invoke the Lambda function are created by using the Events property of this resource using a type of ScheduleV2. To learn more about defining EventBridge Scheduler schedules in AWS SAM templates,           see ScheduleV2 in the           AWS Serverless Application Model Developer Guide.",
                            "In addition to the Lambda function and the EventBridge Scheduler schedule, we also define a CloudWatch log group for your function to send records of deleted items to.",
                            "Copy and paste the following code into a file named template.yaml.AWSTemplateFormatVersion: '2010-09-09'Transform: AWS::Serverless-2016-10-31Description: SAM Template for Lambda function and EventBridge Scheduler ruleResources:  MyLambdaFunction:    Type: AWS::Serverless::Function    Properties:      FunctionName: ScheduledDBMaintenance      CodeUri: ./      Handler: lambda_function.lambda_handler      Runtime: python3.11      Architectures:        - x86_64      Events:        ScheduleEvent:          Type: ScheduleV2          Properties:            ScheduleExpression: cron(0 3 1 * ? *)            Description: Run on the first day of every month at 03:00 AM      Policies:        - CloudWatchLogsFullAccess        - Statement:            - Effect: Allow              Action:                - dynamodb:Scan                - dynamodb:BatchWriteItem              Resource: !Sub 'arn:aws:dynamodb:${AWS::Region}:${AWS::AccountId}:table/MyOrderTable'  LambdaLogGroup:    Type: AWS::Logs::LogGroup    Properties:      LogGroupName: !Sub /aws/lambda/${MyLambdaFunction}      RetentionInDays: 30Outputs:  LambdaFunctionName:    Description: Lambda Function Name    Value: !Ref MyLambdaFunction  LambdaFunctionArn:    Description: Lambda Function ARN    Value: !GetAtt MyLambdaFunction.ArnNoteAWS SAM templates use a standard naming convention of template.yaml. In this example, you have two template files - one to create the             example database and another to create the app itself. Save them in separate sub-directories in your project folder.This AWS SAM template defines the resources for your app. We define the Lambda function using the AWS::Serverless::Function resource. The EventBridge Scheduler schedule and the           trigger to invoke the Lambda function are created by using the Events property of this resource using a type of ScheduleV2. To learn more about defining EventBridge Scheduler schedules in AWS SAM templates,           see ScheduleV2 in the           AWS Serverless Application Model Developer Guide.In addition to the Lambda function and the EventBridge Scheduler schedule, we also define a CloudWatch log group for your function to send records of deleted items to.AWS SAM template (scheduled-maintenance app)Copy and paste the following code into a file named template.yaml.AWSTemplateFormatVersion: '2010-09-09'Transform: AWS::Serverless-2016-10-31Description: SAM Template for Lambda function and EventBridge Scheduler ruleResources:  MyLambdaFunction:    Type: AWS::Serverless::Function    Properties:      FunctionName: ScheduledDBMaintenance      CodeUri: ./      Handler: lambda_function.lambda_handler      Runtime: python3.11      Architectures:        - x86_64      Events:        ScheduleEvent:          Type: ScheduleV2          Properties:            ScheduleExpression: cron(0 3 1 * ? *)            Description: Run on the first day of every month at 03:00 AM      Policies:        - CloudWatchLogsFullAccess        - Statement:            - Effect: Allow              Action:                - dynamodb:Scan                - dynamodb:BatchWriteItem              Resource: !Sub 'arn:aws:dynamodb:${AWS::Region}:${AWS::AccountId}:table/MyOrderTable'  LambdaLogGroup:    Type: AWS::Logs::LogGroup    Properties:      LogGroupName: !Sub /aws/lambda/${MyLambdaFunction}      RetentionInDays: 30Outputs:  LambdaFunctionName:    Description: Lambda Function Name    Value: !Ref MyLambdaFunction  LambdaFunctionArn:    Description: Lambda Function ARN    Value: !GetAtt MyLambdaFunction.ArnNoteAWS SAM templates use a standard naming convention of template.yaml. In this example, you have two template files - one to create the             example database and another to create the app itself. Save them in separate sub-directories in your project folder.This AWS SAM template defines the resources for your app. We define the Lambda function using the AWS::Serverless::Function resource. The EventBridge Scheduler schedule and the           trigger to invoke the Lambda function are created by using the Events property of this resource using a type of ScheduleV2. To learn more about defining EventBridge Scheduler schedules in AWS SAM templates,           see ScheduleV2 in the           AWS Serverless Application Model Developer Guide.In addition to the Lambda function and the EventBridge Scheduler schedule, we also define a CloudWatch log group for your function to send records of deleted items to.",
                            "Copy and paste the following code into a file named test_app.py.",
                            {
                                "code_example": "import boto3\nfrom datetime import datetime, timedelta\nimport json\n\n# Initialize the DynamoDB client\ndynamodb = boto3.resource('dynamodb')\n\n# Specify your table name\ntable_name = 'YourTableName'\ntable = dynamodb.Table(table_name)\n\n# Get the current date\ncurrent_date = datetime.now()\n\n# Calculate the date one year ago\none_year_ago = current_date - timedelta(days=365)\n\n# Convert the date to string format (assuming the date in DynamoDB is stored as a string)\none_year_ago_str = one_year_ago.strftime('%Y-%m-%d')\n\n# Scan the table\nresponse = table.scan(\n    FilterExpression='#date < :one_year_ago',\n    ExpressionAttributeNames={\n        '#date': 'Date'\n    },\n    ExpressionAttributeValues={\n        ':one_year_ago': one_year_ago_str\n    }\n)\n\n# Process the results\nold_records = response['Items']\n\n# Continue scanning if we have more items (pagination)\nwhile 'LastEvaluatedKey' in response:\n    response = table.scan(\n        FilterExpression='#date < :one_year_ago',\n        ExpressionAttributeNames={\n            '#date': 'Date'\n        },\n        ExpressionAttributeValues={\n            ':one_year_ago': one_year_ago_str\n        },\n        ExclusiveStartKey=response['LastEvaluatedKey']\n    )\n    old_records.extend(response['Items'])\n\nfor record in old_records:\n    print(json.dumps(record))\n\n# The total number of old records should be zero.\nprint(f\"Total number of old records: {len(old_records)}\")\n"
                            },
                            "This test script uses the AWS SDK for Python (Boto3) to create a connection to your DynamoDB table and scan for items older than one year. To confirm if the Lambda function           has run successfully, at the end of the test, the function prints the number of records older than one year still in the table. If the Lambda function was successful,           the number of old records in the table should be zero.        ",
                            "Test script",
                            "Copy and paste the following code into a file named test_app.py.",
                            {
                                "code_example": "import boto3\nfrom datetime import datetime, timedelta\nimport json\n\n# Initialize the DynamoDB client\ndynamodb = boto3.resource('dynamodb')\n\n# Specify your table name\ntable_name = 'YourTableName'\ntable = dynamodb.Table(table_name)\n\n# Get the current date\ncurrent_date = datetime.now()\n\n# Calculate the date one year ago\none_year_ago = current_date - timedelta(days=365)\n\n# Convert the date to string format (assuming the date in DynamoDB is stored as a string)\none_year_ago_str = one_year_ago.strftime('%Y-%m-%d')\n\n# Scan the table\nresponse = table.scan(\n    FilterExpression='#date < :one_year_ago',\n    ExpressionAttributeNames={\n        '#date': 'Date'\n    },\n    ExpressionAttributeValues={\n        ':one_year_ago': one_year_ago_str\n    }\n)\n\n# Process the results\nold_records = response['Items']\n\n# Continue scanning if we have more items (pagination)\nwhile 'LastEvaluatedKey' in response:\n    response = table.scan(\n        FilterExpression='#date < :one_year_ago',\n        ExpressionAttributeNames={\n            '#date': 'Date'\n        },\n        ExpressionAttributeValues={\n            ':one_year_ago': one_year_ago_str\n        },\n        ExclusiveStartKey=response['LastEvaluatedKey']\n    )\n    old_records.extend(response['Items'])\n\nfor record in old_records:\n    print(json.dumps(record))\n\n# The total number of old records should be zero.\nprint(f\"Total number of old records: {len(old_records)}\")\n"
                            },
                            "This test script uses the AWS SDK for Python (Boto3) to create a connection to your DynamoDB table and scan for items older than one year. To confirm if the Lambda function           has run successfully, at the end of the test, the function prints the number of records older than one year still in the table. If the Lambda function was successful,           the number of old records in the table should be zero.        ",
                            "Copy and paste the following code into a file named test_app.py.import boto3from datetime import datetime, timedeltaimport json# Initialize the DynamoDB clientdynamodb = boto3.resource('dynamodb')# Specify your table nametable_name = 'YourTableName'table = dynamodb.Table(table_name)# Get the current datecurrent_date = datetime.now()# Calculate the date one year agoone_year_ago = current_date - timedelta(days=365)# Convert the date to string format (assuming the date in DynamoDB is stored as a string)one_year_ago_str = one_year_ago.strftime('%Y-%m-%d')# Scan the tableresponse = table.scan(    FilterExpression='#date < :one_year_ago',    ExpressionAttributeNames={        '#date': 'Date'    },    ExpressionAttributeValues={        ':one_year_ago': one_year_ago_str    })# Process the resultsold_records = response['Items']# Continue scanning if we have more items (pagination)while 'LastEvaluatedKey' in response:    response = table.scan(        FilterExpression='#date < :one_year_ago',        ExpressionAttributeNames={            '#date': 'Date'        },        ExpressionAttributeValues={            ':one_year_ago': one_year_ago_str        },        ExclusiveStartKey=response['LastEvaluatedKey']    )    old_records.extend(response['Items'])for record in old_records:    print(json.dumps(record))# The total number of old records should be zero.print(f\"Total number of old records: {len(old_records)}\")This test script uses the AWS SDK for Python (Boto3) to create a connection to your DynamoDB table and scan for items older than one year. To confirm if the Lambda function           has run successfully, at the end of the test, the function prints the number of records older than one year still in the table. If the Lambda function was successful,           the number of old records in the table should be zero.        Test scriptCopy and paste the following code into a file named test_app.py.import boto3from datetime import datetime, timedeltaimport json# Initialize the DynamoDB clientdynamodb = boto3.resource('dynamodb')# Specify your table nametable_name = 'YourTableName'table = dynamodb.Table(table_name)# Get the current datecurrent_date = datetime.now()# Calculate the date one year agoone_year_ago = current_date - timedelta(days=365)# Convert the date to string format (assuming the date in DynamoDB is stored as a string)one_year_ago_str = one_year_ago.strftime('%Y-%m-%d')# Scan the tableresponse = table.scan(    FilterExpression='#date < :one_year_ago',    ExpressionAttributeNames={        '#date': 'Date'    },    ExpressionAttributeValues={        ':one_year_ago': one_year_ago_str    })# Process the resultsold_records = response['Items']# Continue scanning if we have more items (pagination)while 'LastEvaluatedKey' in response:    response = table.scan(        FilterExpression='#date < :one_year_ago',        ExpressionAttributeNames={            '#date': 'Date'        },        ExpressionAttributeValues={            ':one_year_ago': one_year_ago_str        },        ExclusiveStartKey=response['LastEvaluatedKey']    )    old_records.extend(response['Items'])for record in old_records:    print(json.dumps(record))# The total number of old records should be zero.print(f\"Total number of old records: {len(old_records)}\")This test script uses the AWS SDK for Python (Boto3) to create a connection to your DynamoDB table and scan for items older than one year. To confirm if the Lambda function           has run successfully, at the end of the test, the function prints the number of records older than one year still in the table. If the Lambda function was successful,           the number of old records in the table should be zero.        "
                        ]
                    },
                    {
                        "sub_header": "Creating and populating the example DynamoDB table",
                        "content": [
                            "To test your scheduled-maintenance app, you first create a DynamoDB table and populate it with some sample data. You can create the table either manually using the       AWS Management Console or by using AWS SAM. We recommend that you use AWS SAM to quickly create and configure the table using a few AWS CLI commands.",
                            "  1.Console : MyOrderTable",
                            "  2.AWS SAM : template.yaml",
                            "ConsoleTo create the DynamoDB tableOpen the Tables page of the DynamoDB console.Choose Create table.Create the table by doing the following:Under Table details, for Table name, enter MyOrderTable.For Partition key, enter Order_number and leave the type as String.For Sort key, enter Date and leave the type as String.Leave Table settings set to Default settings and choose Create table.When your table has finished creating and its Status shows as Active, create a global secondary index (GSI) by doing the               following. Your app will use this GSI to search for items directly by date to determine what to delete.Choose MyOrderTable from the list of tables.Choose the Indexes tab.Under Global secondary indexes, choose Create index.Under Index details, enter Date for the Partition key and leave the                   Data type set to String.For Index name, enter Date-index.Leave all other parameters set to their default values, scroll to the bottom of the page, and choose Create index.AWS SAMTo create the DynamoDB tableNavigate to the folder you saved the template.yaml file for the DynamoDB table in. Note that this example uses two                 template.yaml files. Make sure they are saved in separate sub-folders and that you are in the correct folder containing                 the template to create your DynamoDB table.Run the following command.sam buildThis command gathers the build artifacts for the resources you want to deploy and places them in the proper format and                 location to deploy them.To create the DynamoDB resource specified in the template.yaml file, run the following command.sam deploy --guidedUsing the --guided flag means that AWS SAM will show you prompts to guide you through the deployment process. For this deployment,                 enter a Stack name of cron-app-test-db, and accept the defaults for all other options by using Enter.When AWS SAM has finished creating the DynamoDB resource, you should see the following message.Successfully created/updated stack - cron-app-test-db in us-west-2You can additionally confirm that the DynamoDB table has been created by opening the Tables page of the DynamoDB console.                 You should see a table named MyOrderTable.",
                            "anchor",
                            "anchor",
                            "To create the DynamoDB table",
                            "  1 : Open the Tables page of the DynamoDB console.",
                            "  1 : Open the Tables page of the DynamoDB console.",
                            "  1 : Open the Tables page of the DynamoDB console.",
                            "  2 : Choose Create table.",
                            "  2 : Choose Create table.",
                            "  2 : Choose Create table.",
                            "  3 : Create the table by doing the following:Under Table details, for Table name, enter MyOrderTable.For Partition key, enter Order_number and leave the type as String.For Sort key, enter Date and leave the type as String.Leave Table settings set to Default settings and choose Create table.",
                            "  3 : Create the table by doing the following:Under Table details, for Table name, enter MyOrderTable.For Partition key, enter Order_number and leave the type as String.For Sort key, enter Date and leave the type as String.Leave Table settings set to Default settings and choose Create table.",
                            "  3 : Create the table by doing the following:Under Table details, for Table name, enter MyOrderTable.For Partition key, enter Order_number and leave the type as String.For Sort key, enter Date and leave the type as String.Leave Table settings set to Default settings and choose Create table.",
                            "  1 : Under Table details, for Table name, enter MyOrderTable.",
                            "  1 : Under Table details, for Table name, enter MyOrderTable.",
                            "  1 : Under Table details, for Table name, enter MyOrderTable.",
                            "  2 : For Partition key, enter Order_number and leave the type as String.",
                            "  2 : For Partition key, enter Order_number and leave the type as String.",
                            "  2 : For Partition key, enter Order_number and leave the type as String.",
                            "  3 : For Sort key, enter Date and leave the type as String.",
                            "  3 : For Sort key, enter Date and leave the type as String.",
                            "  3 : For Sort key, enter Date and leave the type as String.",
                            "  4 : Leave Table settings set to Default settings and choose Create table.",
                            "  4 : Leave Table settings set to Default settings and choose Create table.",
                            "  4 : Leave Table settings set to Default settings and choose Create table.",
                            "  3 : Create the table by doing the following:Under Table details, for Table name, enter MyOrderTable.For Partition key, enter Order_number and leave the type as String.For Sort key, enter Date and leave the type as String.Leave Table settings set to Default settings and choose Create table.",
                            "  4 : When your table has finished creating and its Status shows as Active, create a global secondary index (GSI) by doing the               following. Your app will use this GSI to search for items directly by date to determine what to delete.Choose MyOrderTable from the list of tables.Choose the Indexes tab.Under Global secondary indexes, choose Create index.Under Index details, enter Date for the Partition key and leave the                   Data type set to String.For Index name, enter Date-index.Leave all other parameters set to their default values, scroll to the bottom of the page, and choose Create index.",
                            "  4 : When your table has finished creating and its Status shows as Active, create a global secondary index (GSI) by doing the               following. Your app will use this GSI to search for items directly by date to determine what to delete.Choose MyOrderTable from the list of tables.Choose the Indexes tab.Under Global secondary indexes, choose Create index.Under Index details, enter Date for the Partition key and leave the                   Data type set to String.For Index name, enter Date-index.Leave all other parameters set to their default values, scroll to the bottom of the page, and choose Create index.",
                            "  4 : When your table has finished creating and its Status shows as Active, create a global secondary index (GSI) by doing the               following. Your app will use this GSI to search for items directly by date to determine what to delete.Choose MyOrderTable from the list of tables.Choose the Indexes tab.Under Global secondary indexes, choose Create index.Under Index details, enter Date for the Partition key and leave the                   Data type set to String.For Index name, enter Date-index.Leave all other parameters set to their default values, scroll to the bottom of the page, and choose Create index.",
                            "  1 : Choose MyOrderTable from the list of tables.",
                            "  1 : Choose MyOrderTable from the list of tables.",
                            "  1 : Choose MyOrderTable from the list of tables.",
                            "  2 : Choose the Indexes tab.",
                            "  2 : Choose the Indexes tab.",
                            "  2 : Choose the Indexes tab.",
                            "  3 : Under Global secondary indexes, choose Create index.",
                            "  3 : Under Global secondary indexes, choose Create index.",
                            "  3 : Under Global secondary indexes, choose Create index.",
                            "  4 : Under Index details, enter Date for the Partition key and leave the                   Data type set to String.",
                            "  4 : Under Index details, enter Date for the Partition key and leave the                   Data type set to String.",
                            "  4 : Under Index details, enter Date for the Partition key and leave the                   Data type set to String.",
                            "  5 : For Index name, enter Date-index.",
                            "  5 : For Index name, enter Date-index.",
                            "  5 : For Index name, enter Date-index.",
                            "  6 : Leave all other parameters set to their default values, scroll to the bottom of the page, and choose Create index.",
                            "  6 : Leave all other parameters set to their default values, scroll to the bottom of the page, and choose Create index.",
                            "  6 : Leave all other parameters set to their default values, scroll to the bottom of the page, and choose Create index.",
                            "  4 : When your table has finished creating and its Status shows as Active, create a global secondary index (GSI) by doing the               following. Your app will use this GSI to search for items directly by date to determine what to delete.Choose MyOrderTable from the list of tables.Choose the Indexes tab.Under Global secondary indexes, choose Create index.Under Index details, enter Date for the Partition key and leave the                   Data type set to String.For Index name, enter Date-index.Leave all other parameters set to their default values, scroll to the bottom of the page, and choose Create index.",
                            "After you've created your table, you next add some sample data to test your app. The CSV file sample_data.csv you downloaded     earlier contains a number of example entries comprised of order numbers, dates, and customer and order information. Use the provided python script     load_sample_data.py to add this data to your table.",
                            "To add the sample data to the table",
                            "  1 : Navigate to the directory containing the sample_data.csv and load_sample_data.py files. If these files are in         separate directories, move them so they're saved in the same location.",
                            "  1 : Navigate to the directory containing the sample_data.csv and load_sample_data.py files. If these files are in         separate directories, move them so they're saved in the same location.",
                            "  1 : Navigate to the directory containing the sample_data.csv and load_sample_data.py files. If these files are in         separate directories, move them so they're saved in the same location.",
                            "  \nCreate a Python virtual environment to run the script in by running the following command. We recommend that you use a virtual environment because in a following \n        step you'll need to install the AWS SDK for Python (Boto3).\n \n",
                            {
                                "code_example": "python -m venv venv"
                            },
                            "  \nCreate a Python virtual environment to run the script in by running the following command. We recommend that you use a virtual environment because in a following \n        step you'll need to install the AWS SDK for Python (Boto3).\n \n",
                            {
                                "code_example": "python -m venv venv"
                            },
                            "  \nCreate a Python virtual environment to run the script in by running the following command. We recommend that you use a virtual environment because in a following \n        step you'll need to install the AWS SDK for Python (Boto3).\n \n",
                            {
                                "code_example": "python -m venv venv"
                            },
                            "  \nCreate a Python virtual environment to run the script in by running the following command. We recommend that you use a virtual environment because in a following \n        step you'll need to install the AWS SDK for Python (Boto3).\n \n",
                            {
                                "code_example": "python -m venv venv"
                            },
                            "  \nCreate a Python virtual environment to run the script in by running the following command. We recommend that you use a virtual environment because in a following \n        step you'll need to install the AWS SDK for Python (Boto3).\n \n",
                            {
                                "code_example": "python -m venv venv"
                            },
                            "  \nActivate the virtual environment by running the following command.\n \n",
                            {
                                "code_example": "source venv/bin/activate"
                            },
                            "  \nActivate the virtual environment by running the following command.\n \n",
                            {
                                "code_example": "source venv/bin/activate"
                            },
                            "  \nActivate the virtual environment by running the following command.\n \n",
                            {
                                "code_example": "source venv/bin/activate"
                            },
                            "  \nActivate the virtual environment by running the following command.\n \n",
                            {
                                "code_example": "source venv/bin/activate"
                            },
                            "  \nActivate the virtual environment by running the following command.\n \n",
                            {
                                "code_example": "source venv/bin/activate"
                            },
                            "  \nInstall the SDK for Python (Boto3) in your virtual environment by running the following command. The script uses this library to connect to your DynamoDB table and add the items.\n \n",
                            {
                                "code_example": "pip install boto3"
                            },
                            "  \nInstall the SDK for Python (Boto3) in your virtual environment by running the following command. The script uses this library to connect to your DynamoDB table and add the items.\n \n",
                            {
                                "code_example": "pip install boto3"
                            },
                            "  \nInstall the SDK for Python (Boto3) in your virtual environment by running the following command. The script uses this library to connect to your DynamoDB table and add the items.\n \n",
                            {
                                "code_example": "pip install boto3"
                            },
                            "  \nInstall the SDK for Python (Boto3) in your virtual environment by running the following command. The script uses this library to connect to your DynamoDB table and add the items.\n \n",
                            {
                                "code_example": "pip install boto3"
                            },
                            "  \nInstall the SDK for Python (Boto3) in your virtual environment by running the following command. The script uses this library to connect to your DynamoDB table and add the items.\n \n",
                            {
                                "code_example": "pip install boto3"
                            },
                            "  \nRun the script to populate the table by running the following command.\n \nIf the script runs successfully, it should print each item to the console as it loads it and report Data loading completed.\n",
                            {
                                "code_example": "python load_sample_data.py"
                            },
                            "  \nRun the script to populate the table by running the following command.\n \nIf the script runs successfully, it should print each item to the console as it loads it and report Data loading completed.\n",
                            {
                                "code_example": "python load_sample_data.py"
                            },
                            "  \nRun the script to populate the table by running the following command.\n \nIf the script runs successfully, it should print each item to the console as it loads it and report Data loading completed.\n",
                            {
                                "code_example": "python load_sample_data.py"
                            },
                            "  \nRun the script to populate the table by running the following command.\n \nIf the script runs successfully, it should print each item to the console as it loads it and report Data loading completed.\n",
                            {
                                "code_example": "python load_sample_data.py"
                            },
                            "  \nRun the script to populate the table by running the following command.\n \nIf the script runs successfully, it should print each item to the console as it loads it and report Data loading completed.\n",
                            {
                                "code_example": "python load_sample_data.py"
                            },
                            "  \nRun the script to populate the table by running the following command.\n \nIf the script runs successfully, it should print each item to the console as it loads it and report Data loading completed.\n",
                            {
                                "code_example": "python load_sample_data.py"
                            },
                            "  \nRun the script to populate the table by running the following command.\n \nIf the script runs successfully, it should print each item to the console as it loads it and report Data loading completed.\n",
                            {
                                "code_example": "python load_sample_data.py"
                            },
                            "  \nDeactivate the virtual environment by running the following command.\n \n",
                            {
                                "code_example": "deactivate"
                            },
                            "  \nDeactivate the virtual environment by running the following command.\n \n",
                            {
                                "code_example": "deactivate"
                            },
                            "  \nDeactivate the virtual environment by running the following command.\n \n",
                            {
                                "code_example": "deactivate"
                            },
                            "  \nDeactivate the virtual environment by running the following command.\n \n",
                            {
                                "code_example": "deactivate"
                            },
                            "  \nDeactivate the virtual environment by running the following command.\n \n",
                            {
                                "code_example": "deactivate"
                            },
                            "  7 : You can verify that the data has been loaded to your DynamoDB table by doing the following:Open the Explore items page of the DynamoDB console and select your table (MyOrderTable).In the Items returned pane, you should see the 25 items from the CSV file that the script added to the table.",
                            "  7 : You can verify that the data has been loaded to your DynamoDB table by doing the following:Open the Explore items page of the DynamoDB console and select your table (MyOrderTable).In the Items returned pane, you should see the 25 items from the CSV file that the script added to the table.",
                            "  7 : You can verify that the data has been loaded to your DynamoDB table by doing the following:Open the Explore items page of the DynamoDB console and select your table (MyOrderTable).In the Items returned pane, you should see the 25 items from the CSV file that the script added to the table.",
                            "  1 : Open the Explore items page of the DynamoDB console and select your table (MyOrderTable).",
                            "  1 : Open the Explore items page of the DynamoDB console and select your table (MyOrderTable).",
                            "  1 : Open the Explore items page of the DynamoDB console and select your table (MyOrderTable).",
                            "  2 : In the Items returned pane, you should see the 25 items from the CSV file that the script added to the table.",
                            "  2 : In the Items returned pane, you should see the 25 items from the CSV file that the script added to the table.",
                            "  2 : In the Items returned pane, you should see the 25 items from the CSV file that the script added to the table.",
                            "  7 : You can verify that the data has been loaded to your DynamoDB table by doing the following:Open the Explore items page of the DynamoDB console and select your table (MyOrderTable).In the Items returned pane, you should see the 25 items from the CSV file that the script added to the table."
                        ]
                    },
                    {
                        "sub_header": "Creating the scheduled-maintenance app",
                        "content": [
                            "You can create and deploy the resources for this example app step by step using the AWS Management Console or by using AWS SAM. In a production environment, we       recommend that you use an Infrustracture-as-Code (IaC) tool like AWS SAM to repeatably deploy serverless applications without using manual processes.",
                            "For this example, follow the console instructions to learn how to configure each AWS resource separately, or follow the AWS SAM instructions     to quickly deploy the app using AWS CLI commands.",
                            "  1.Console : .zip",
                            "  2.AWS SAM : template.yaml",
                            "ConsoleTo create the function using the AWS Management ConsoleFirst, create a function containing basic starter code. You then                 replace this code with your own function code by either copying and pasting the code directly in the Lambda code editor, or by uploading your code                as a .zip package. For this task, we recommend simply copying and pasting the code.Open the Functions page of the Lambda console.Choose Create function.Choose Author from scratch.Under Basic information, do the following:For Function name, enter ScheduledDBMaintenance.For Runtime choose the latest Python version.For Architecture, choose x86_64.Choose Create function.After your function is created, you can configure your function with the provided function code.In the Code source pane, replace the Hello world code that Lambda created with the Python function code from                        the lambda_function.py file that you saved earlier.In the DEPLOY section, choose Deploy to update your function's code:To configure the function memory and timeout (console)Select the Configuration tab for your function.In the General configuration pane, choose Edit.Set Memory to 256 MB and Timeout to 15 seconds.                  If you are processing a large table with many records, for example in the case of a production environment,                  you might consider setting Timeout to a larger number. This gives your function                  more time to scan, and clean the database.Choose Save.To configure the log format (console)You can configure Lambda functions to output logs in either unstructured text or JSON format. We recommend that you use JSON format for logs to                   make it easier to search and filter log data. To learn more about Lambda log configuration options, see Configuring advanced logging controls for Lambda functions.Select the Configuration tab for your function.Select Monitoring and operations tools.In the Logging configuration pane, choose Edit.For Logging configuration, select JSON.Choose Save.To set Up IAM permissionsTo give your function the permissions it needs to read and delete DynamoDB items, you need to add a policy to your function's                     execution role defining the necessary permissions.Open the Configuration tab, then choose                        Permissions from the left navigation bar.Choose the role name under Execution role.In the IAM console, choose Add permissions, then                        Create inline policy.Use the JSON editor and enter the following policy:{    \"Version\": \"2012-10-17\",    \"Statement\": [        {            \"Effect\": \"Allow\",            \"Action\": [                \"dynamodb:Scan\",                \"dynamodb:DeleteItem\",                \"dynamodb:BatchWriteItem\"            ],            \"Resource\": \"arn:aws:dynamodb:*:*:table/MyOrderTable\"        }    ]}Name the policy DynamoDBCleanupPolicy, then create it.To set up EventBridge Scheduler as a trigger (console)Open the EventBridge console.In the left navigation pane, choose Schedulers under the                        Scheduler section.Choose Create schedule.Configure the schedule by doing the following:Under Schedule name, enter a name for your schedule (for example, DynamoDBCleanupSchedule).Under Schedule pattern, choose Recurring schedule.For Schedule type leave the default as Cron-based schedule,                          then enter the following schedule details:Minutes: 0Hours: 3Day of month: 1Month: *Day of the week: ?Year: *When evaluated, this cron expression runs on the first day of every month at 03:00 AM.For Flexible time window, select Off.Choose Next.Configure the trigger for your Lambda function by doing the following:In the Target detail pane, leave Target API set to Templated targets,                         then select AWS Lambda Invoke.Under Invoke, select your Lambda function (ScheduledDBMaintenance) from the dropdown list.Leave the Payload empty and choose Next.Scroll down to Permissions and select Create a new role for this schedule.                            When you create a new EventBridge Scheduler schedule using the console, EventBridge Scheduler creates a new policy with the required                          permissions the schedule needs to invoke your function. For more information about managing your schedule permissions, see                          Cron-based schedules.                          in the EventBridge Scheduler User Guide.Choose Next.Review your settings and choose Create schedule to complete creation of the schedule and Lambda trigger.AWS SAMTo deploy the app using AWS SAMNavigate to the folder you saved the template.yaml file for the app in. Note that this example uses two template.yaml files.               Make sure they are saved in separate sub-folders and that you are in the correct folder containing the template to create the app.Copy the lambda_function.py and requirements.txt files you downloaded earlier to the same folder. The code location specified in the                 AWS SAM template is ./, meaning the current location. AWS SAM will search in this folder for the Lambda function code when you try to deploy the app.Run the following command.sam build --use-containerThis command gathers the build artifacts for the resources you want to deploy and places them in the proper format and                 location to deploy them. Specifying the --use-container option builds your function inside a Lambda-like Docker container.                 We use it here so you don't need to have Python 3.12 installed on your local machine for the build to work.To create the Lambda and EventBridge Scheduler resources specified in the template.yaml file, run the following command.sam deploy --guidedUsing the --guided flag means that AWS SAM will show you prompts to guide you through the deployment process. For this deployment,                 enter a Stack name of cron-maintenance-app, and accept the defaults for all other options by using Enter.When AWS SAM has finished creating the Lambda and EventBridge Scheduler resources, you should see the following message.Successfully created/updated stack - cron-maintenance-app in us-west-2You can additionally confirm that the Lambda function has been created by opening the Functions page of the Lambda console.                 You should see a function named ScheduledDBMaintenance.",
                            "anchor",
                            "anchor",
                            "To create the function using the AWS Management Console",
                            "First, create a function containing basic starter code. You then                 replace this code with your own function code by either copying and pasting the code directly in the Lambda code editor, or by uploading your code                as a .zip package. For this task, we recommend simply copying and pasting the code.",
                            "  1 : Open the Functions page of the Lambda console.",
                            "  1 : Open the Functions page of the Lambda console.",
                            "  1 : Open the Functions page of the Lambda console.",
                            "  2 : Choose Create function.",
                            "  2 : Choose Create function.",
                            "  2 : Choose Create function.",
                            "  3 : Choose Author from scratch.",
                            "  3 : Choose Author from scratch.",
                            "  3 : Choose Author from scratch.",
                            "  4 : Under Basic information, do the following:For Function name, enter ScheduledDBMaintenance.For Runtime choose the latest Python version.For Architecture, choose x86_64.",
                            "  4 : Under Basic information, do the following:For Function name, enter ScheduledDBMaintenance.For Runtime choose the latest Python version.For Architecture, choose x86_64.",
                            "  4 : Under Basic information, do the following:For Function name, enter ScheduledDBMaintenance.For Runtime choose the latest Python version.For Architecture, choose x86_64.",
                            "  1 : For Function name, enter ScheduledDBMaintenance.",
                            "  1 : For Function name, enter ScheduledDBMaintenance.",
                            "  1 : For Function name, enter ScheduledDBMaintenance.",
                            "  2 : For Runtime choose the latest Python version.",
                            "  2 : For Runtime choose the latest Python version.",
                            "  2 : For Runtime choose the latest Python version.",
                            "  3 : For Architecture, choose x86_64.",
                            "  3 : For Architecture, choose x86_64.",
                            "  3 : For Architecture, choose x86_64.",
                            "  4 : Under Basic information, do the following:For Function name, enter ScheduledDBMaintenance.For Runtime choose the latest Python version.For Architecture, choose x86_64.",
                            "  5 : Choose Create function.",
                            "  5 : Choose Create function.",
                            "  5 : Choose Create function.",
                            "  6 : After your function is created, you can configure your function with the provided function code.In the Code source pane, replace the Hello world code that Lambda created with the Python function code from                        the lambda_function.py file that you saved earlier.In the DEPLOY section, choose Deploy to update your function's code:",
                            "  6 : After your function is created, you can configure your function with the provided function code.In the Code source pane, replace the Hello world code that Lambda created with the Python function code from                        the lambda_function.py file that you saved earlier.In the DEPLOY section, choose Deploy to update your function's code:",
                            "  6 : After your function is created, you can configure your function with the provided function code.In the Code source pane, replace the Hello world code that Lambda created with the Python function code from                        the lambda_function.py file that you saved earlier.In the DEPLOY section, choose Deploy to update your function's code:",
                            "  1 : In the Code source pane, replace the Hello world code that Lambda created with the Python function code from                        the lambda_function.py file that you saved earlier.",
                            "  1 : In the Code source pane, replace the Hello world code that Lambda created with the Python function code from                        the lambda_function.py file that you saved earlier.",
                            "  1 : In the Code source pane, replace the Hello world code that Lambda created with the Python function code from                        the lambda_function.py file that you saved earlier.",
                            "  2 : In the DEPLOY section, choose Deploy to update your function's code:",
                            "  2 : In the DEPLOY section, choose Deploy to update your function's code:",
                            "  2 : In the DEPLOY section, choose Deploy to update your function's code:",
                            "  2 : In the DEPLOY section, choose Deploy to update your function's code:",
                            "  2 : In the DEPLOY section, choose Deploy to update your function's code:",
                            "  6 : After your function is created, you can configure your function with the provided function code.In the Code source pane, replace the Hello world code that Lambda created with the Python function code from                        the lambda_function.py file that you saved earlier.In the DEPLOY section, choose Deploy to update your function's code:",
                            "To configure the function memory and timeout (console)",
                            "  1 : Select the Configuration tab for your function.",
                            "  1 : Select the Configuration tab for your function.",
                            "  1 : Select the Configuration tab for your function.",
                            "  2 : In the General configuration pane, choose Edit.",
                            "  2 : In the General configuration pane, choose Edit.",
                            "  2 : In the General configuration pane, choose Edit.",
                            "  3 : Set Memory to 256 MB and Timeout to 15 seconds.                  If you are processing a large table with many records, for example in the case of a production environment,                  you might consider setting Timeout to a larger number. This gives your function                  more time to scan, and clean the database.",
                            "  3 : Set Memory to 256 MB and Timeout to 15 seconds.                  If you are processing a large table with many records, for example in the case of a production environment,                  you might consider setting Timeout to a larger number. This gives your function                  more time to scan, and clean the database.",
                            "  3 : Set Memory to 256 MB and Timeout to 15 seconds.                  If you are processing a large table with many records, for example in the case of a production environment,                  you might consider setting Timeout to a larger number. This gives your function                  more time to scan, and clean the database.",
                            "  4 : Choose Save.",
                            "  4 : Choose Save.",
                            "  4 : Choose Save.",
                            "To configure the log format (console)",
                            "You can configure Lambda functions to output logs in either unstructured text or JSON format. We recommend that you use JSON format for logs to                   make it easier to search and filter log data. To learn more about Lambda log configuration options, see Configuring advanced logging controls for Lambda functions.",
                            "  1 : Select the Configuration tab for your function.",
                            "  1 : Select the Configuration tab for your function.",
                            "  1 : Select the Configuration tab for your function.",
                            "  2 : Select Monitoring and operations tools.",
                            "  2 : Select Monitoring and operations tools.",
                            "  2 : Select Monitoring and operations tools.",
                            "  3 : In the Logging configuration pane, choose Edit.",
                            "  3 : In the Logging configuration pane, choose Edit.",
                            "  3 : In the Logging configuration pane, choose Edit.",
                            "  4 : For Logging configuration, select JSON.",
                            "  4 : For Logging configuration, select JSON.",
                            "  4 : For Logging configuration, select JSON.",
                            "  5 : Choose Save.",
                            "  5 : Choose Save.",
                            "  5 : Choose Save.",
                            "To set Up IAM permissions",
                            "To give your function the permissions it needs to read and delete DynamoDB items, you need to add a policy to your function's                     execution role defining the necessary permissions.",
                            "  1 : Open the Configuration tab, then choose                        Permissions from the left navigation bar.",
                            "  1 : Open the Configuration tab, then choose                        Permissions from the left navigation bar.",
                            "  1 : Open the Configuration tab, then choose                        Permissions from the left navigation bar.",
                            "  2 : Choose the role name under Execution role.",
                            "  2 : Choose the role name under Execution role.",
                            "  2 : Choose the role name under Execution role.",
                            "  3 : In the IAM console, choose Add permissions, then                        Create inline policy.",
                            "  3 : In the IAM console, choose Add permissions, then                        Create inline policy.",
                            "  3 : In the IAM console, choose Add permissions, then                        Create inline policy.",
                            "  \nUse the JSON editor and enter the following policy:\n \n",
                            {
                                "code_example": "{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"dynamodb:Scan\",\n                \"dynamodb:DeleteItem\",\n                \"dynamodb:BatchWriteItem\"\n            ],\n            \"Resource\": \"arn:aws:dynamodb:*:*:table/MyOrderTable\"\n        }\n    ]\n}"
                            },
                            "  \nUse the JSON editor and enter the following policy:\n \n",
                            {
                                "code_example": "{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"dynamodb:Scan\",\n                \"dynamodb:DeleteItem\",\n                \"dynamodb:BatchWriteItem\"\n            ],\n            \"Resource\": \"arn:aws:dynamodb:*:*:table/MyOrderTable\"\n        }\n    ]\n}"
                            },
                            "  \nUse the JSON editor and enter the following policy:\n \n",
                            {
                                "code_example": "{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"dynamodb:Scan\",\n                \"dynamodb:DeleteItem\",\n                \"dynamodb:BatchWriteItem\"\n            ],\n            \"Resource\": \"arn:aws:dynamodb:*:*:table/MyOrderTable\"\n        }\n    ]\n}"
                            },
                            "  \nUse the JSON editor and enter the following policy:\n \n",
                            {
                                "code_example": "{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"dynamodb:Scan\",\n                \"dynamodb:DeleteItem\",\n                \"dynamodb:BatchWriteItem\"\n            ],\n            \"Resource\": \"arn:aws:dynamodb:*:*:table/MyOrderTable\"\n        }\n    ]\n}"
                            },
                            "  \nUse the JSON editor and enter the following policy:\n \n",
                            {
                                "code_example": "{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"dynamodb:Scan\",\n                \"dynamodb:DeleteItem\",\n                \"dynamodb:BatchWriteItem\"\n            ],\n            \"Resource\": \"arn:aws:dynamodb:*:*:table/MyOrderTable\"\n        }\n    ]\n}"
                            },
                            "  5 : Name the policy DynamoDBCleanupPolicy, then create it.",
                            "  5 : Name the policy DynamoDBCleanupPolicy, then create it.",
                            "  5 : Name the policy DynamoDBCleanupPolicy, then create it.",
                            "To set up EventBridge Scheduler as a trigger (console)",
                            "  1 : Open the EventBridge console.",
                            "  1 : Open the EventBridge console.",
                            "  1 : Open the EventBridge console.",
                            "  2 : In the left navigation pane, choose Schedulers under the                        Scheduler section.",
                            "  2 : In the left navigation pane, choose Schedulers under the                        Scheduler section.",
                            "  2 : In the left navigation pane, choose Schedulers under the                        Scheduler section.",
                            "  3 : Choose Create schedule.",
                            "  3 : Choose Create schedule.",
                            "  3 : Choose Create schedule.",
                            "  4 : Configure the schedule by doing the following:Under Schedule name, enter a name for your schedule (for example, DynamoDBCleanupSchedule).Under Schedule pattern, choose Recurring schedule.For Schedule type leave the default as Cron-based schedule,                          then enter the following schedule details:Minutes: 0Hours: 3Day of month: 1Month: *Day of the week: ?Year: *When evaluated, this cron expression runs on the first day of every month at 03:00 AM.For Flexible time window, select Off.",
                            "  4 : Configure the schedule by doing the following:Under Schedule name, enter a name for your schedule (for example, DynamoDBCleanupSchedule).Under Schedule pattern, choose Recurring schedule.For Schedule type leave the default as Cron-based schedule,                          then enter the following schedule details:Minutes: 0Hours: 3Day of month: 1Month: *Day of the week: ?Year: *When evaluated, this cron expression runs on the first day of every month at 03:00 AM.For Flexible time window, select Off.",
                            "  4 : Configure the schedule by doing the following:Under Schedule name, enter a name for your schedule (for example, DynamoDBCleanupSchedule).Under Schedule pattern, choose Recurring schedule.For Schedule type leave the default as Cron-based schedule,                          then enter the following schedule details:Minutes: 0Hours: 3Day of month: 1Month: *Day of the week: ?Year: *When evaluated, this cron expression runs on the first day of every month at 03:00 AM.For Flexible time window, select Off.",
                            "  1 : Under Schedule name, enter a name for your schedule (for example, DynamoDBCleanupSchedule).",
                            "  1 : Under Schedule name, enter a name for your schedule (for example, DynamoDBCleanupSchedule).",
                            "  1 : Under Schedule name, enter a name for your schedule (for example, DynamoDBCleanupSchedule).",
                            "  2 : Under Schedule pattern, choose Recurring schedule.",
                            "  2 : Under Schedule pattern, choose Recurring schedule.",
                            "  2 : Under Schedule pattern, choose Recurring schedule.",
                            "  3 : For Schedule type leave the default as Cron-based schedule,                          then enter the following schedule details:Minutes: 0Hours: 3Day of month: 1Month: *Day of the week: ?Year: *When evaluated, this cron expression runs on the first day of every month at 03:00 AM.",
                            "  3 : For Schedule type leave the default as Cron-based schedule,                          then enter the following schedule details:Minutes: 0Hours: 3Day of month: 1Month: *Day of the week: ?Year: *When evaluated, this cron expression runs on the first day of every month at 03:00 AM.",
                            "  3 : For Schedule type leave the default as Cron-based schedule,                          then enter the following schedule details:Minutes: 0Hours: 3Day of month: 1Month: *Day of the week: ?Year: *When evaluated, this cron expression runs on the first day of every month at 03:00 AM.",
                            "  3 : For Schedule type leave the default as Cron-based schedule,                          then enter the following schedule details:Minutes: 0Hours: 3Day of month: 1Month: *Day of the week: ?Year: *When evaluated, this cron expression runs on the first day of every month at 03:00 AM.",
                            "  3 : For Schedule type leave the default as Cron-based schedule,                          then enter the following schedule details:Minutes: 0Hours: 3Day of month: 1Month: *Day of the week: ?Year: *When evaluated, this cron expression runs on the first day of every month at 03:00 AM.",
                            "  3 : For Schedule type leave the default as Cron-based schedule,                          then enter the following schedule details:Minutes: 0Hours: 3Day of month: 1Month: *Day of the week: ?Year: *When evaluated, this cron expression runs on the first day of every month at 03:00 AM.",
                            "  3 : For Schedule type leave the default as Cron-based schedule,                          then enter the following schedule details:Minutes: 0Hours: 3Day of month: 1Month: *Day of the week: ?Year: *When evaluated, this cron expression runs on the first day of every month at 03:00 AM.",
                            "  4 : For Flexible time window, select Off.",
                            "  4 : For Flexible time window, select Off.",
                            "  4 : For Flexible time window, select Off.",
                            "  4 : Configure the schedule by doing the following:Under Schedule name, enter a name for your schedule (for example, DynamoDBCleanupSchedule).Under Schedule pattern, choose Recurring schedule.For Schedule type leave the default as Cron-based schedule,                          then enter the following schedule details:Minutes: 0Hours: 3Day of month: 1Month: *Day of the week: ?Year: *When evaluated, this cron expression runs on the first day of every month at 03:00 AM.For Flexible time window, select Off.",
                            "  5 : Choose Next.",
                            "  5 : Choose Next.",
                            "  5 : Choose Next.",
                            "  6 : Configure the trigger for your Lambda function by doing the following:In the Target detail pane, leave Target API set to Templated targets,                         then select AWS Lambda Invoke.Under Invoke, select your Lambda function (ScheduledDBMaintenance) from the dropdown list.Leave the Payload empty and choose Next.Scroll down to Permissions and select Create a new role for this schedule.                            When you create a new EventBridge Scheduler schedule using the console, EventBridge Scheduler creates a new policy with the required                          permissions the schedule needs to invoke your function. For more information about managing your schedule permissions, see                          Cron-based schedules.                          in the EventBridge Scheduler User Guide.Choose Next.",
                            "  6 : Configure the trigger for your Lambda function by doing the following:In the Target detail pane, leave Target API set to Templated targets,                         then select AWS Lambda Invoke.Under Invoke, select your Lambda function (ScheduledDBMaintenance) from the dropdown list.Leave the Payload empty and choose Next.Scroll down to Permissions and select Create a new role for this schedule.                            When you create a new EventBridge Scheduler schedule using the console, EventBridge Scheduler creates a new policy with the required                          permissions the schedule needs to invoke your function. For more information about managing your schedule permissions, see                          Cron-based schedules.                          in the EventBridge Scheduler User Guide.Choose Next.",
                            "  6 : Configure the trigger for your Lambda function by doing the following:In the Target detail pane, leave Target API set to Templated targets,                         then select AWS Lambda Invoke.Under Invoke, select your Lambda function (ScheduledDBMaintenance) from the dropdown list.Leave the Payload empty and choose Next.Scroll down to Permissions and select Create a new role for this schedule.                            When you create a new EventBridge Scheduler schedule using the console, EventBridge Scheduler creates a new policy with the required                          permissions the schedule needs to invoke your function. For more information about managing your schedule permissions, see                          Cron-based schedules.                          in the EventBridge Scheduler User Guide.Choose Next.",
                            "  1 : In the Target detail pane, leave Target API set to Templated targets,                         then select AWS Lambda Invoke.",
                            "  1 : In the Target detail pane, leave Target API set to Templated targets,                         then select AWS Lambda Invoke.",
                            "  1 : In the Target detail pane, leave Target API set to Templated targets,                         then select AWS Lambda Invoke.",
                            "  2 : Under Invoke, select your Lambda function (ScheduledDBMaintenance) from the dropdown list.",
                            "  2 : Under Invoke, select your Lambda function (ScheduledDBMaintenance) from the dropdown list.",
                            "  2 : Under Invoke, select your Lambda function (ScheduledDBMaintenance) from the dropdown list.",
                            "  3 : Leave the Payload empty and choose Next.",
                            "  3 : Leave the Payload empty and choose Next.",
                            "  3 : Leave the Payload empty and choose Next.",
                            "  4 : Scroll down to Permissions and select Create a new role for this schedule.                            When you create a new EventBridge Scheduler schedule using the console, EventBridge Scheduler creates a new policy with the required                          permissions the schedule needs to invoke your function. For more information about managing your schedule permissions, see                          Cron-based schedules.                          in the EventBridge Scheduler User Guide.",
                            "  4 : Scroll down to Permissions and select Create a new role for this schedule.                            When you create a new EventBridge Scheduler schedule using the console, EventBridge Scheduler creates a new policy with the required                          permissions the schedule needs to invoke your function. For more information about managing your schedule permissions, see                          Cron-based schedules.                          in the EventBridge Scheduler User Guide.",
                            "  4 : Scroll down to Permissions and select Create a new role for this schedule.                            When you create a new EventBridge Scheduler schedule using the console, EventBridge Scheduler creates a new policy with the required                          permissions the schedule needs to invoke your function. For more information about managing your schedule permissions, see                          Cron-based schedules.                          in the EventBridge Scheduler User Guide.",
                            "  5 : Choose Next.",
                            "  5 : Choose Next.",
                            "  5 : Choose Next.",
                            "  6 : Configure the trigger for your Lambda function by doing the following:In the Target detail pane, leave Target API set to Templated targets,                         then select AWS Lambda Invoke.Under Invoke, select your Lambda function (ScheduledDBMaintenance) from the dropdown list.Leave the Payload empty and choose Next.Scroll down to Permissions and select Create a new role for this schedule.                            When you create a new EventBridge Scheduler schedule using the console, EventBridge Scheduler creates a new policy with the required                          permissions the schedule needs to invoke your function. For more information about managing your schedule permissions, see                          Cron-based schedules.                          in the EventBridge Scheduler User Guide.Choose Next.",
                            "  7 : Review your settings and choose Create schedule to complete creation of the schedule and Lambda trigger.",
                            "  7 : Review your settings and choose Create schedule to complete creation of the schedule and Lambda trigger.",
                            "  7 : Review your settings and choose Create schedule to complete creation of the schedule and Lambda trigger."
                        ]
                    },
                    {
                        "sub_header": "Testing the app",
                        "content": [
                            "      To test that your schedule correctly triggers your function, and that your function correctly cleans records      from the database, you can temporarily modify your schedule to run once at a specific time. You can then run sam deploy again to      reset your recurrence schedule to run once a month.    ",
                            "To run the application using the AWS Management Console",
                            "  1 : Navigate back to the EventBridge Scheduler console page.",
                            "  1 : Navigate back to the EventBridge Scheduler console page.",
                            "  1 : Navigate back to the EventBridge Scheduler console page.",
                            "  2 : Choose your schedule, then choose Edit.",
                            "  2 : Choose your schedule, then choose Edit.",
                            "  2 : Choose your schedule, then choose Edit.",
                            "  3 : In the Schedule pattern section, under Recurrence, choose One-time schedule.",
                            "  3 : In the Schedule pattern section, under Recurrence, choose One-time schedule.",
                            "  3 : In the Schedule pattern section, under Recurrence, choose One-time schedule.",
                            "  4 :           Set your invocation time to a few minutes from now, review your settings, then choose Save.        ",
                            "  4 :           Set your invocation time to a few minutes from now, review your settings, then choose Save.        ",
                            "  4 :           Set your invocation time to a few minutes from now, review your settings, then choose Save.        ",
                            "      After the schedule runs and invokes its target, you run the test_app.py script to verify that your function successfully removed all old records      from the DynamoDB table.    ",
                            "To verify that old records are deleted using a Python script",
                            "  1 :           In your command line windown, navigate to the folder where you saved test_app.py.        ",
                            "  1 :           In your command line windown, navigate to the folder where you saved test_app.py.        ",
                            "  1 :           In your command line windown, navigate to the folder where you saved test_app.py.        ",
                            "  \n\n          Run the script.\n        \n \n\n          If successful, you will see the following output.\n        \nTotal number of old records: 0\n",
                            {
                                "code_example": "python test_app.py"
                            },
                            "  \n\n          Run the script.\n        \n \n\n          If successful, you will see the following output.\n        \nTotal number of old records: 0\n",
                            {
                                "code_example": "python test_app.py"
                            },
                            "  \n\n          Run the script.\n        \n \n\n          If successful, you will see the following output.\n        \nTotal number of old records: 0\n",
                            {
                                "code_example": "python test_app.py"
                            },
                            "  \n\n          Run the script.\n        \n \n\n          If successful, you will see the following output.\n        \nTotal number of old records: 0\n",
                            {
                                "code_example": "python test_app.py"
                            },
                            "  \n\n          Run the script.\n        \n \n\n          If successful, you will see the following output.\n        \nTotal number of old records: 0\n",
                            {
                                "code_example": "python test_app.py"
                            },
                            "  \n\n          Run the script.\n        \n \n\n          If successful, you will see the following output.\n        \nTotal number of old records: 0\n",
                            {
                                "code_example": "python test_app.py"
                            },
                            "  \n\n          Run the script.\n        \n \n\n          If successful, you will see the following output.\n        \nTotal number of old records: 0\n",
                            {
                                "code_example": "python test_app.py"
                            },
                            "  \n\n          Run the script.\n        \n \n\n          If successful, you will see the following output.\n        \nTotal number of old records: 0\n",
                            {
                                "code_example": "python test_app.py"
                            },
                            "  \n\n          Run the script.\n        \n \n\n          If successful, you will see the following output.\n        \nTotal number of old records: 0\n",
                            {
                                "code_example": "python test_app.py"
                            }
                        ]
                    },
                    {
                        "sub_header": "Next steps",
                        "content": [
                            "      You can now modify the EventBridge Scheduler schedule to meet your partifuclar application requirements. EventBridge Scheduler supports the following schedule expressions: cron, rate, and one-time schedules.    ",
                            "      For more information about EventBridge Scheduler schedule expresssions, see Schedule types in the      EventBridge Scheduler User Guide.    "
                        ]
                    }
                ]
            }
        ],
        "sections": [
            "The following examples provide function code and  infrastructure as code (IaC) templates to quickly create and deploy serverless apps that implement some common Lambda uses cases. The  examples also include code examples and instructions to test the apps after you deploy them.",
            "For each of the example apps, we provide instructions to either create and configure resources manually using the AWS Management Console, or to   use the AWS Serverless Application Model to deploy the resources using IaC. Follow the console intructions to learn more about configuring the individual AWS   resources for each app, or use to AWS SAM to quickly deploy resources as you would in a production environment.",
            "You can use the provided examples as a basis for your own serverless applications by modifying the provided function code and templates   for your own use case.",
            "We're continuing to create new examples, so check back again to find more severless apps for common Lambda use cases.",
            {
                "sub_header": "Example apps",
                "content": [
                    "  1.Example serverless file-processing appCreate a serverless app to automatically perform a file-processing task when an object is uploaded to an Amazon S3 bucket. In this         example, when a PDF file is uploaded, the app encrypts the file and saves it to another S3 bucket.",
                    "  2.Example scheduled cron task appCreate an app to perform a scheduled task using a cron schedule. In this example, the app performs maintenance on a         Amazon DynamoDB table by deleting entries more than 12 months old."
                ]
            }
        ]
    },
    {
        "title": "Building with TypeScript",
        "href": "https://docs.aws.amazon.com/lambda/latest/dg/lambda-typescript.html",
        "contents": [
            {
                "title": "Handler",
                "href": "https://docs.aws.amazon.com/lambda/latest/dg/typescript-handler.html",
                "sections": []
            },
            {
                "title": "Deploy .zip file archives",
                "href": "https://docs.aws.amazon.com/lambda/latest/dg/typescript-package.html",
                "sections": []
            },
            {
                "title": "Deploy container images",
                "href": "https://docs.aws.amazon.com/lambda/latest/dg/typescript-image.html",
                "sections": []
            },
            {
                "title": "Layers",
                "href": "https://docs.aws.amazon.com/lambda/latest/dg/typescript-layers.html",
                "sections": []
            },
            {
                "title": "Context",
                "href": "https://docs.aws.amazon.com/lambda/latest/dg/typescript-context.html",
                "sections": []
            },
            {
                "title": "Logging",
                "href": "https://docs.aws.amazon.com/lambda/latest/dg/typescript-logging.html",
                "sections": []
            },
            {
                "title": "Tracing",
                "href": "https://docs.aws.amazon.com/lambda/latest/dg/typescript-tracing.html",
                "sections": []
            }
        ],
        "sections": []
    },
    {
        "title": "Integrating other services",
        "href": "https://docs.aws.amazon.com/lambda/latest/dg/lambda-services.html",
        "contents": [
            {
                "title": "Apache Kafka",
                "href": "https://docs.aws.amazon.com/lambda/latest/dg/with-kafka.html",
                "contents": [
                    {
                        "title": "Configure event source",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/with-kafka-configure.html",
                        "sections": []
                    },
                    {
                        "title": "Process messages",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/with-kafka-process.html",
                        "sections": []
                    },
                    {
                        "title": "Event filtering",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/with-kafka-filtering.html",
                        "sections": []
                    },
                    {
                        "title": "On-failure destinations",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/with-kafka-on-failure.html",
                        "sections": []
                    },
                    {
                        "title": "Troubleshooting",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/with-kafka-troubleshoot.html",
                        "sections": []
                    }
                ],
                "source": "Aws",
                "sections": []
            },
            {
                "title": "API Gateway",
                "href": "https://docs.aws.amazon.com/lambda/latest/dg/services-apigateway.html",
                "contents": [
                    {
                        "title": "Tutorial",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/services-apigateway-tutorial.html",
                        "sections": []
                    },
                    {
                        "title": "Errors",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/services-apigateway-errors.html",
                        "sections": []
                    },
                    {
                        "title": "Select an HTTP invoke method for Lambda",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/apig-http-invoke-decision.html",
                        "sections": []
                    }
                ],
                "source": "Aws",
                "sections": []
            },
            {
                "title": "Infrastructure Composer",
                "href": "https://docs.aws.amazon.com/lambda/latest/dg/services-appcomposer.html",
                "sections": []
            },
            {
                "title": "CloudFormation",
                "href": "https://docs.aws.amazon.com/lambda/latest/dg/services-cloudformation.html",
                "sections": []
            },
            {
                "title": "Amazon DocumentDB",
                "href": "https://docs.aws.amazon.com/lambda/latest/dg/with-documentdb.html",
                "contents": [
                    {
                        "title": "Tutorial",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/with-documentdb-tutorial.html",
                        "sections": []
                    }
                ],
                "source": "Aws",
                "sections": []
            },
            {
                "title": "DynamoDB",
                "href": "https://docs.aws.amazon.com/lambda/latest/dg/with-ddb.html",
                "contents": [
                    {
                        "title": "Create mapping",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/services-dynamodb-eventsourcemapping.html",
                        "sections": []
                    },
                    {
                        "title": "Batch item failures",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/services-ddb-batchfailurereporting.html",
                        "sections": []
                    },
                    {
                        "title": "Error handling",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/services-dynamodb-errors.html",
                        "sections": []
                    },
                    {
                        "title": "Stateful processing",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/services-ddb-windows.html",
                        "sections": []
                    },
                    {
                        "title": "Parameters",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/services-ddb-params.html",
                        "sections": []
                    },
                    {
                        "title": "Event filtering",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/with-ddb-filtering.html",
                        "sections": []
                    },
                    {
                        "title": "Tutorial",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/with-ddb-example.html",
                        "sections": []
                    }
                ],
                "source": "Aws",
                "sections": []
            },
            {
                "title": "EC2",
                "href": "https://docs.aws.amazon.com/lambda/latest/dg/services-ec2.html",
                "sections": []
            },
            {
                "title": "Elastic Load Balancing (Application Load Balancer)",
                "href": "https://docs.aws.amazon.com/lambda/latest/dg/services-alb.html",
                "sections": []
            },
            {
                "title": "Invoke using an EventBridge Scheduler",
                "href": "https://docs.aws.amazon.com/lambda/latest/dg/with-eventbridge-scheduler.html",
                "sections": []
            },
            {
                "title": "IoT",
                "href": "https://docs.aws.amazon.com/lambda/latest/dg/services-iot.html",
                "sections": []
            },
            {
                "title": "Kinesis Data Streams",
                "href": "https://docs.aws.amazon.com/lambda/latest/dg/with-kinesis.html",
                "contents": [
                    {
                        "title": "Create mapping",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/services-kinesis-create.html",
                        "sections": []
                    },
                    {
                        "title": "Batch item failures",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/services-kinesis-batchfailurereporting.html",
                        "sections": []
                    },
                    {
                        "title": "Error handling",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/kinesis-on-failure-destination.html",
                        "sections": []
                    },
                    {
                        "title": "Stateful processing",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/services-kinesis-windows.html",
                        "sections": []
                    },
                    {
                        "title": "Parameters",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/services-kinesis-parameters.html",
                        "sections": []
                    },
                    {
                        "title": "Event filtering",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/with-kinesis-filtering.html",
                        "sections": []
                    },
                    {
                        "title": "Tutorial",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/with-kinesis-example.html",
                        "sections": []
                    }
                ],
                "source": "Aws",
                "sections": []
            },
            {
                "title": "MQ",
                "href": "https://docs.aws.amazon.com/lambda/latest/dg/with-mq.html",
                "contents": [
                    {
                        "title": "Configure event source",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/process-mq-messages-with-lambda.html",
                        "sections": []
                    },
                    {
                        "title": "Parameters",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/services-mq-params.html",
                        "sections": []
                    },
                    {
                        "title": "Event filtering",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/with-mq-filtering.html",
                        "sections": []
                    },
                    {
                        "title": "Troubleshoot",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/services-mq-errors.html",
                        "sections": []
                    }
                ],
                "source": "Aws",
                "sections": []
            },
            {
                "title": "MSK",
                "href": "https://docs.aws.amazon.com/lambda/latest/dg/with-msk.html",
                "contents": [
                    {
                        "title": "Configure event source",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/with-msk-configure.html",
                        "sections": []
                    },
                    {
                        "title": "Process messages",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/with-msk-process.html",
                        "sections": []
                    },
                    {
                        "title": "Event filtering",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/with-msk-filtering.html",
                        "sections": []
                    },
                    {
                        "title": "On-failure destinations",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/with-msk-on-failure.html",
                        "sections": []
                    },
                    {
                        "title": "Tutorial",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/services-msk-tutorial.html",
                        "sections": []
                    }
                ],
                "source": "Aws",
                "sections": []
            },
            {
                "title": "RDS",
                "href": "https://docs.aws.amazon.com/lambda/latest/dg/services-rds.html",
                "sections": []
            },
            {
                "title": "S3",
                "href": "https://docs.aws.amazon.com/lambda/latest/dg/with-s3.html",
                "contents": [
                    {
                        "title": "Tutorial: Use an S3 trigger",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/with-s3-example.html",
                        "sections": []
                    },
                    {
                        "title": "Tutorial: Use an Amazon S3 trigger to create thumbnails",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/with-s3-tutorial.html",
                        "sections": []
                    }
                ],
                "source": "Aws",
                "sections": []
            },
            {
                "title": "SQS",
                "href": "https://docs.aws.amazon.com/lambda/latest/dg/with-sqs.html",
                "contents": [
                    {
                        "title": "Create mapping",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/services-sqs-configure.html",
                        "sections": []
                    },
                    {
                        "title": "Scaling behavior",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/services-sqs-scaling.html",
                        "sections": []
                    },
                    {
                        "title": "Error handling",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/services-sqs-errorhandling.html",
                        "sections": []
                    },
                    {
                        "title": "Parameters",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/services-sqs-parameters.html",
                        "sections": []
                    },
                    {
                        "title": "Event filtering",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/with-sqs-filtering.html",
                        "sections": []
                    },
                    {
                        "title": "Tutorial",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/with-sqs-example.html",
                        "sections": []
                    },
                    {
                        "title": "SQS cross-account tutorial",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/with-sqs-cross-account-example.html",
                        "sections": []
                    }
                ],
                "source": "Aws",
                "sections": []
            },
            {
                "title": "S3 Batch",
                "href": "https://docs.aws.amazon.com/lambda/latest/dg/services-s3-batch.html",
                "sections": []
            },
            {
                "title": "SNS",
                "href": "https://docs.aws.amazon.com/lambda/latest/dg/with-sns.html",
                "contents": [
                    {
                        "title": "Tutorial",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/with-sns-example.html",
                        "sections": []
                    }
                ],
                "source": "Aws",
                "sections": []
            }
        ],
        "sections": []
    },
    {
        "title": "Code examples",
        "href": "https://docs.aws.amazon.com/lambda/latest/dg/service_code_examples.html",
        "contents": [
            {
                "title": "Basics",
                "href": "https://docs.aws.amazon.com/lambda/latest/dg/service_code_examples_basics.html",
                "contents": [
                    {
                        "title": "Hello Lambda",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_lambda_Hello_section.html",
                        "sections": []
                    },
                    {
                        "title": "Learn the basics",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_lambda_Scenario_GettingStartedFunctions_section.html",
                        "sections": []
                    },
                    {
                        "title": "Actions",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/service_code_examples_actions.html",
                        "contents": [
                            {
                                "title": "CreateAlias",
                                "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_lambda_CreateAlias_section.html",
                                "sections": []
                            },
                            {
                                "title": "CreateFunction",
                                "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_lambda_CreateFunction_section.html",
                                "sections": []
                            },
                            {
                                "title": "DeleteAlias",
                                "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_lambda_DeleteAlias_section.html",
                                "sections": []
                            },
                            {
                                "title": "DeleteFunction",
                                "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_lambda_DeleteFunction_section.html",
                                "sections": []
                            },
                            {
                                "title": "DeleteFunctionConcurrency",
                                "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_lambda_DeleteFunctionConcurrency_section.html",
                                "sections": []
                            },
                            {
                                "title": "DeleteProvisionedConcurrencyConfig",
                                "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_lambda_DeleteProvisionedConcurrencyConfig_section.html",
                                "sections": []
                            },
                            {
                                "title": "GetAccountSettings",
                                "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_lambda_GetAccountSettings_section.html",
                                "sections": []
                            },
                            {
                                "title": "GetAlias",
                                "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_lambda_GetAlias_section.html",
                                "sections": []
                            },
                            {
                                "title": "GetFunction",
                                "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_lambda_GetFunction_section.html",
                                "sections": []
                            },
                            {
                                "title": "GetFunctionConcurrency",
                                "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_lambda_GetFunctionConcurrency_section.html",
                                "sections": []
                            },
                            {
                                "title": "GetFunctionConfiguration",
                                "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_lambda_GetFunctionConfiguration_section.html",
                                "sections": []
                            },
                            {
                                "title": "GetPolicy",
                                "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_lambda_GetPolicy_section.html",
                                "sections": []
                            },
                            {
                                "title": "GetProvisionedConcurrencyConfig",
                                "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_lambda_GetProvisionedConcurrencyConfig_section.html",
                                "sections": []
                            },
                            {
                                "title": "Invoke",
                                "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_lambda_Invoke_section.html",
                                "sections": []
                            },
                            {
                                "title": "ListFunctions",
                                "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_lambda_ListFunctions_section.html",
                                "sections": []
                            },
                            {
                                "title": "ListProvisionedConcurrencyConfigs",
                                "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_lambda_ListProvisionedConcurrencyConfigs_section.html",
                                "sections": []
                            },
                            {
                                "title": "ListTags",
                                "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_lambda_ListTags_section.html",
                                "sections": []
                            },
                            {
                                "title": "ListVersionsByFunction",
                                "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_lambda_ListVersionsByFunction_section.html",
                                "sections": []
                            },
                            {
                                "title": "PublishVersion",
                                "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_lambda_PublishVersion_section.html",
                                "sections": []
                            },
                            {
                                "title": "PutFunctionConcurrency",
                                "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_lambda_PutFunctionConcurrency_section.html",
                                "sections": []
                            },
                            {
                                "title": "PutProvisionedConcurrencyConfig",
                                "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_lambda_PutProvisionedConcurrencyConfig_section.html",
                                "sections": []
                            },
                            {
                                "title": "RemovePermission",
                                "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_lambda_RemovePermission_section.html",
                                "sections": []
                            },
                            {
                                "title": "TagResource",
                                "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_lambda_TagResource_section.html",
                                "sections": []
                            },
                            {
                                "title": "UntagResource",
                                "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_lambda_UntagResource_section.html",
                                "sections": []
                            },
                            {
                                "title": "UpdateAlias",
                                "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_lambda_UpdateAlias_section.html",
                                "sections": []
                            },
                            {
                                "title": "UpdateFunctionCode",
                                "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_lambda_UpdateFunctionCode_section.html",
                                "sections": []
                            },
                            {
                                "title": "UpdateFunctionConfiguration",
                                "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_lambda_UpdateFunctionConfiguration_section.html",
                                "sections": []
                            }
                        ],
                        "source": "Aws",
                        "sections": []
                    }
                ],
                "source": "Aws",
                "sections": []
            },
            {
                "title": "Scenarios",
                "href": "https://docs.aws.amazon.com/lambda/latest/dg/service_code_examples_scenarios.html",
                "contents": [
                    {
                        "title": "Automatically confirm known users with a Lambda function",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_cross_CognitoAutoConfirmUser_section.html",
                        "sections": []
                    },
                    {
                        "title": "Automatically migrate known users with a Lambda function",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_cross_CognitoAutoMigrateUser_section.html",
                        "sections": []
                    },
                    {
                        "title": "Create a REST API to track COVID-19 data",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_cross_ApiGatewayDataTracker_section.html",
                        "sections": []
                    },
                    {
                        "title": "Create a lending library REST API",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_cross_AuroraRestLendingLibrary_section.html",
                        "sections": []
                    },
                    {
                        "title": "Create a messenger application",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_cross_StepFunctionsMessenger_section.html",
                        "sections": []
                    },
                    {
                        "title": "Create a serverless application to manage photos",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_cross_PAM_section.html",
                        "sections": []
                    },
                    {
                        "title": "Create a websocket chat application",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_cross_ApiGatewayWebsocketChat_section.html",
                        "sections": []
                    },
                    {
                        "title": "Create an application to analyze customer feedback",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_cross_FSA_section.html",
                        "sections": []
                    },
                    {
                        "title": "Invoke a Lambda function from a browser",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_cross_LambdaForBrowser_section.html",
                        "sections": []
                    },
                    {
                        "title": "Transform data with S3 Object Lambda",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_cross_ServerlessS3DataTransformation_section.html",
                        "sections": []
                    },
                    {
                        "title": "Use API Gateway to invoke a Lambda function",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_cross_LambdaAPIGateway_section.html",
                        "sections": []
                    },
                    {
                        "title": "Use Step Functions to invoke Lambda functions",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_cross_ServerlessWorkflows_section.html",
                        "sections": []
                    },
                    {
                        "title": "Use scheduled events to invoke a Lambda function",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_cross_LambdaScheduledEvents_section.html",
                        "sections": []
                    },
                    {
                        "title": "Write custom activity data with a Lambda function after Amazon Cognito user authentication",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_cross_CognitoCustomActivityLog_section.html",
                        "sections": []
                    }
                ],
                "source": "Aws",
                "sections": []
            },
            {
                "title": "Serverless examples",
                "href": "https://docs.aws.amazon.com/lambda/latest/dg/service_code_examples_serverless_examples.html",
                "contents": [
                    {
                        "title": "Connecting to an Amazon RDS database in a Lambda function",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_serverless_connect_RDS_Lambda_section.html",
                        "sections": []
                    },
                    {
                        "title": "Invoke a Lambda function from a Kinesis trigger",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_serverless_Kinesis_Lambda_section.html",
                        "sections": []
                    },
                    {
                        "title": "Invoke a Lambda function from a DynamoDB trigger",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_serverless_DynamoDB_Lambda_section.html",
                        "sections": []
                    },
                    {
                        "title": "Invoke a Lambda function from a Amazon DocumentDB trigger",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_serverless_DocumentDB_Lambda_section.html",
                        "sections": []
                    },
                    {
                        "title": "Invoke a Lambda function from an Amazon MSK trigger",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_serverless_MSK_Lambda_section.html",
                        "sections": []
                    },
                    {
                        "title": "Invoke a Lambda function from an Amazon S3 trigger",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_serverless_S3_Lambda_section.html",
                        "sections": []
                    },
                    {
                        "title": "Invoke a Lambda function from an Amazon SNS trigger",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_serverless_SNS_Lambda_section.html",
                        "sections": []
                    },
                    {
                        "title": "Invoke a Lambda function from an Amazon SQS trigger",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_serverless_SQS_Lambda_section.html",
                        "sections": []
                    },
                    {
                        "title": "Reporting batch item failures for Lambda functions with a Kinesis trigger",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_serverless_Kinesis_Lambda_batch_item_failures_section.html",
                        "sections": []
                    },
                    {
                        "title": "Reporting batch item failures for Lambda functions with a DynamoDB trigger",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_serverless_DynamoDB_Lambda_batch_item_failures_section.html",
                        "sections": []
                    },
                    {
                        "title": "Reporting batch item failures for Lambda functions with an Amazon SQS trigger",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_serverless_SQS_Lambda_batch_item_failures_section.html",
                        "sections": []
                    }
                ],
                "source": "Aws",
                "sections": []
            }
        ],
        "sections": []
    }
]