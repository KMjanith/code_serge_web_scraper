[
    {
        "title": "What is AWS Lambda?",
        "href": "https://docs.aws.amazon.com/lambda/latest/dg/welcome.html",
        "source": "Aws",
        "parent_content": [
            "You can use AWS Lambda to run code without provisioning or managing servers.",
            " Lambda runs your code    on a high-availability compute infrastructure and performs all of the administration of the compute resources,    including server and operating system maintenance, capacity provisioning and automatic scaling, and    logging. With Lambda, all you need to do is supply your code in one of the language runtimes that Lambda supports.",
            "You organize your code into Lambda functions. The Lambda service runs your function only when needed and scales automatically. You only pay for the compute time that you consume—there is no charge when your code is not running. For more information, see AWS Lambda Pricing.",
            "Tip",
            "To learn how to build serverless solutions, check out the Serverless Developer Guide.",
            {
                "sub_header": "When to use Lambda",
                "content": [
                    "Lambda is an ideal compute service for application scenarios that need to scale up rapidly, and scale down to      zero when not in demand. For example, you can use Lambda for:",
                    "  1.File processing: :  Use Amazon Simple Storage Service (Amazon S3) to trigger Lambda data processing in real time after an upload.",
                    "  2.Stream processing: :  Use Lambda and Amazon Kinesis to process real-time streaming data for application activity tracking, transaction order processing, clickstream analysis, data cleansing, log filtering, indexing, social media analysis, Internet of Things (IoT) device data telemetry, and metering.",
                    "  3.Web applications: :  Combine Lambda with other AWS services to build powerful web applications that automatically scale up and down and run in a highly available configuration across multiple data centers.",
                    "  4.IoT backends: :  Build serverless backends using Lambda to handle web, mobile, IoT, and third-party API requests.",
                    "  5.Mobile backends: :  Build backends using Lambda and Amazon API Gateway  to authenticate and process API requests. Use AWS Amplify to easily integrate with your iOS, Android, Web, and React Native frontends.",
                    "When using Lambda, you are responsible only for your code. Lambda manages the compute fleet that offers a      balance of memory, CPU, network, and other resources to run your code. Because Lambda manages these resources, you      cannot log in to compute instances or customize the operating system on provided        runtimes. Lambda performs operational and administrative activities on your behalf, including managing      capacity, monitoring, and logging your Lambda functions."
                ]
            },
            {
                "sub_header": "Key features",
                "content": [
                    "The following key features help you develop Lambda applications that are scalable, secure, and easily      extensible:",
                    "  1. Environment variables : \nUse environment variables to adjust your function's behavior without updating code.\n",
                    "  2.Versions : \nManage the deployment of your functions with versions, so that, for example, a new function can be used for beta testing without affecting users of the stable production version.\n",
                    "  3.Container images : \nCreate a container image for a Lambda function by using an AWS provided base image or an alternative base\n            image so that you can reuse your existing container tooling or deploy larger workloads that rely on sizable dependencies, such as machine learning.\n",
                    "  4.Layers : \nPackage libraries and other dependencies to reduce the size of deployment archives and makes it faster to deploy your code.\n",
                    "  5.Lambda extensions : \nAugment your Lambda functions with tools for monitoring, observability, security, and governance.\n",
                    "  6.Function URLs : \nAdd a dedicated HTTP(S) endpoint to your Lambda function.\n",
                    "  7.Response streaming : \nConfigure your Lambda function URLs to stream response payloads back to clients from Node.js functions, to improve time to first byte (TTFB) performance or to return larger payloads.\n",
                    "  8.Concurrency and scaling controls : \nApply fine-grained control over the scaling and responsiveness of your production applications.\n",
                    "  9.Code signing : \nVerify that only approved developers publish unaltered, trusted code in your Lambda functions \n",
                    "  10.Private networking : \nCreate a private network for resources such as databases, cache instances, or internal services.\n",
                    "  11.File system access : \nConfigure a function to mount an Amazon Elastic File System (Amazon EFS) to a local directory, so that your function code can access and modify shared resources safely and at high concurrency.\n",
                    "  12.Lambda SnapStart for Java : \nImprove startup performance for Java runtimes by up to 10x at no extra cost, typically with no changes to your function code.\n"
                ]
            }
        ]
    },
    {
        "title": "Example apps",
        "href": "https://docs.aws.amazon.com/lambda/latest/dg/example-apps.html",
        "contents": [
            {
                "title": "File-processing app",
                "href": "https://docs.aws.amazon.com/lambda/latest/dg/file-processing-app.html",
                "source": "Aws",
                "parent_content": [
                    "One of the most common use cases for Lambda is to perform file processing tasks. For example, you might use a Lambda function     to automatically create PDF files from HTML files or images, or to create thumbnails when a user uploads an image.",
                    "In this example, you create an app which automatically encrypts PDF files when they are uploaded to an Amazon Simple Storage Service (Amazon S3) bucket.     To implement this app, you create the following resources:",
                    "  1.An S3 bucket for users to upload PDF files to",
                    "  2.A Lambda function in Python which reads the uploaded file and creates an encrypted, password-protected version of it",
                    "  3.A second S3 bucket for Lambda to save the encrypted file in",
                    "You also create an AWS Identity and Access Management (IAM) policy to give your Lambda function permission to perform read and write operations     on your S3 buckets.",
                    "Tip",
                    "If you’re brand new to Lambda, we recommend that you carry out the tutorial Create your first Lambda function before      creating this example app.",
                    "You can deploy your app manually by creating and configuring resources with the AWS Management Console or the AWS Command Line Interface (AWS CLI). You can     also deploy the app by using the AWS Serverless Application Model (AWS SAM). AWS SAM is an infrastructure as code (IaC) tool. With IaC, you don’t create     resources manually, but define them in code and then deploy them automatically.",
                    "If you want to learn more about using Lambda with IaC before deploying this example app, see Using Lambda with infrastructure as code (IaC).",
                    {
                        "sub_header": "Prerequisites",
                        "content": [
                            "Before you can create the example app, make sure you have the required command line tools installed.",
                            "  1.AWS CLI : You can manually deploy the resources for your app using either the AWS Management Console or the . To use the CLI, install it by following               the installation                 instructions in the AWS Command Line Interface User Guide.",
                            "  2.AWS SAM CLI : If you want to deploy the example app using AWS SAM, you need to install both the AWS CLI and the . To install the ,               follow the installation instructions               in the AWS SAM User Guide.",
                            "  3.pytest module : After you’ve deployed your app, you can test it using an automated Python test script that we provide. To use this script, install the               pytest package in you local development environment by running the following command:pip install pytest",
                            {
                                "code_example": "pip install pytest"
                            },
                            "To deploy the app using AWS SAM, Docker must also be installed on your build machine."
                        ]
                    },
                    {
                        "sub_header": "Downloading the example app files",
                        "content": [
                            "To create and test the example app, you create the following files in your project directory:",
                            "  1.lambda_function.py - the Python function code for the Lambda function that performs the file encryption",
                            "  2.requirements.txt - a manifest file defining the dependencies that your Python function code requires",
                            "  3.template.yaml - an AWS SAM template you can use to deploy the app",
                            "  4.test_pdf_encrypt.py - a test script you can use to automatically test your application",
                            "  5.pytest.ini - a configuration file for the the test script",
                            "Expand the following sections to view the code and to learn more about the role of each file in creating and testing your app. To create the       files on your local machine, either copy and paste the code below, or download the files from the aws-lambda-developer-guide GitHub repo.",
                            "Copy and paste the following code into a file named lambda_function.py.",
                            {
                                "code_example": "from pypdf import PdfReader, PdfWriter\nimport uuid\nimport os\nfrom urllib.parse import unquote_plus\nimport boto3\n\n# Create the S3 client to download and upload objects from S3\ns3_client = boto3.client('s3')\n\ndef lambda_handler(event, context):\n    # Iterate over the S3 event object and get the key for all uploaded files\n    for record in event['Records']:\n        bucket = record['s3']['bucket']['name']\n        key = unquote_plus(record['s3']['object']['key']) # Decode the S3 object key to remove any URL-encoded characters\n        download_path = f'/tmp/{uuid.uuid4()}.pdf' # Create a path in the Lambda tmp directory to save the file to \n        upload_path = f'/tmp/converted-{uuid.uuid4()}.pdf' # Create another path to save the encrypted file to\n        \n        # If the file is a PDF, encrypt it and upload it to the destination S3 bucket\n        if key.lower().endswith('.pdf'):\n            s3_client.download_file(bucket, key, download_path)\n            encrypt_pdf(download_path, upload_path)\n            encrypted_key = add_encrypted_suffix(key)\n            s3_client.upload_file(upload_path, f'{bucket}-encrypted', encrypted_key)\n\n# Define the function to encrypt the PDF file with a password\ndef encrypt_pdf(file_path, encrypted_file_path):\n    reader = PdfReader(file_path)\n    writer = PdfWriter()\n    \n    for page in reader.pages:\n        writer.add_page(page)\n\n    # Add a password to the new PDF\n    writer.encrypt(\"my-secret-password\")\n\n    # Save the new PDF to a file\n    with open(encrypted_file_path, \"wb\") as file:\n        writer.write(file)\n\n# Define a function to add a suffix to the original filename after encryption\ndef add_encrypted_suffix(original_key):\n    filename, extension = original_key.rsplit('.', 1)\n    return f'{filename}_encrypted.{extension}'"
                            },
                            "Note",
                            "In this example code, a password for the encrypted file (my-secret-password) is hardcoded into the           function code. In a production application, don't include sensitive information like passwords in your function code. Use           AWS Secrets Manager to securely store sensitive parameters.",
                            "The python function code contains three functions - the handler function that Lambda runs           when your function is invoked, and two separate function named add_encrypted_suffix and encrypt_pdf that the handler calls to perform the PDF encryption.",
                            "When your function is invoked by Amazon S3, Lambda passes a JSON formatted event argument to the function that contains details about the           event that caused the invocation. In this case, the information includes name of the S3 bucket and the object keys for the uploaded files.           To learn more about the format of event object for Amazon S3, see Process Amazon S3 event notifications with Lambda.",
                            "Your function then uses the AWS SDK for Python (Boto3) to download the PDF files specified in the event object to its local temporary storage directory, before           encrypting them using the pypdf library.",
                            "Finally, the function uses the Boto3 SDK to store the encrypted file in your S3 destination bucket.",
                            "Python function code",
                            "Copy and paste the following code into a file named lambda_function.py.",
                            {
                                "code_example": "from pypdf import PdfReader, PdfWriter\nimport uuid\nimport os\nfrom urllib.parse import unquote_plus\nimport boto3\n\n# Create the S3 client to download and upload objects from S3\ns3_client = boto3.client('s3')\n\ndef lambda_handler(event, context):\n    # Iterate over the S3 event object and get the key for all uploaded files\n    for record in event['Records']:\n        bucket = record['s3']['bucket']['name']\n        key = unquote_plus(record['s3']['object']['key']) # Decode the S3 object key to remove any URL-encoded characters\n        download_path = f'/tmp/{uuid.uuid4()}.pdf' # Create a path in the Lambda tmp directory to save the file to \n        upload_path = f'/tmp/converted-{uuid.uuid4()}.pdf' # Create another path to save the encrypted file to\n        \n        # If the file is a PDF, encrypt it and upload it to the destination S3 bucket\n        if key.lower().endswith('.pdf'):\n            s3_client.download_file(bucket, key, download_path)\n            encrypt_pdf(download_path, upload_path)\n            encrypted_key = add_encrypted_suffix(key)\n            s3_client.upload_file(upload_path, f'{bucket}-encrypted', encrypted_key)\n\n# Define the function to encrypt the PDF file with a password\ndef encrypt_pdf(file_path, encrypted_file_path):\n    reader = PdfReader(file_path)\n    writer = PdfWriter()\n    \n    for page in reader.pages:\n        writer.add_page(page)\n\n    # Add a password to the new PDF\n    writer.encrypt(\"my-secret-password\")\n\n    # Save the new PDF to a file\n    with open(encrypted_file_path, \"wb\") as file:\n        writer.write(file)\n\n# Define a function to add a suffix to the original filename after encryption\ndef add_encrypted_suffix(original_key):\n    filename, extension = original_key.rsplit('.', 1)\n    return f'{filename}_encrypted.{extension}'"
                            },
                            "Note",
                            "In this example code, a password for the encrypted file (my-secret-password) is hardcoded into the           function code. In a production application, don't include sensitive information like passwords in your function code. Use           AWS Secrets Manager to securely store sensitive parameters.",
                            "The python function code contains three functions - the handler function that Lambda runs           when your function is invoked, and two separate function named add_encrypted_suffix and encrypt_pdf that the handler calls to perform the PDF encryption.",
                            "When your function is invoked by Amazon S3, Lambda passes a JSON formatted event argument to the function that contains details about the           event that caused the invocation. In this case, the information includes name of the S3 bucket and the object keys for the uploaded files.           To learn more about the format of event object for Amazon S3, see Process Amazon S3 event notifications with Lambda.",
                            "Your function then uses the AWS SDK for Python (Boto3) to download the PDF files specified in the event object to its local temporary storage directory, before           encrypting them using the pypdf library.",
                            "Finally, the function uses the Boto3 SDK to store the encrypted file in your S3 destination bucket.",
                            "Copy and paste the following code into a file named requirements.txt.",
                            {
                                "code_example": "boto3\npypdf"
                            },
                            "For this example, your function code has only two dependencies that aren't part of the standard Python library -           the SDK for Python (Boto3) and the pypdf package the function uses to perform the PDF encryption.",
                            "Note",
                            "A version of the SDK for Python (Boto3) is included as part of the Lambda runtime, so your code would run without adding Boto3 to your             function's deployment package. However, to maintain full control of your function's dependencies and avoid possible issues with             version misalignment, best practice for Python is to include all function dependencies in your function's deployment package.           See Runtime dependencies in Python to learn more.",
                            "requirements.txt manifest file",
                            "Copy and paste the following code into a file named requirements.txt.",
                            {
                                "code_example": "boto3\npypdf"
                            },
                            "For this example, your function code has only two dependencies that aren't part of the standard Python library -           the SDK for Python (Boto3) and the pypdf package the function uses to perform the PDF encryption.",
                            "Note",
                            "A version of the SDK for Python (Boto3) is included as part of the Lambda runtime, so your code would run without adding Boto3 to your             function's deployment package. However, to maintain full control of your function's dependencies and avoid possible issues with             version misalignment, best practice for Python is to include all function dependencies in your function's deployment package.           See Runtime dependencies in Python to learn more.",
                            "Copy and paste the following code into a file named template.yaml.",
                            {
                                "code_example": "AWSTemplateFormatVersion: '2010-09-09'\nTransform: AWS::Serverless-2016-10-31\n\nResources:\n  EncryptPDFFunction:\n    Type: AWS::Serverless::Function\n    Properties:\n      FunctionName: EncryptPDF\n      Architectures: [x86_64]\n      CodeUri: ./\n      Handler: lambda_function.lambda_handler\n      Runtime: python3.12\n      Timeout: 15\n      MemorySize: 256\n      LoggingConfig:\n        LogFormat: JSON\n      Policies:\n        - AmazonS3FullAccess\n      Events:\n        S3Event:\n          Type: S3\n          Properties:\n            Bucket: !Ref PDFSourceBucket\n            Events: s3:ObjectCreated:*\n\n  PDFSourceBucket:\n    Type: AWS::S3::Bucket\n    Properties:\n      BucketName: EXAMPLE-BUCKET\n\n  EncryptedPDFBucket:\n    Type: AWS::S3::Bucket\n    Properties:\n      BucketName: EXAMPLE-BUCKET-encrypted"
                            },
                            "The AWS SAM template defines the resources you create for your app. In this example, the template defines a Lambda function using the         AWS::Serverless::Function type and two S3 buckets using the AWS::S3::Bucket type. The bucket names specified in the         template are placeholders. Before you deploy the app using AWS SAM, you need to edit the template to rename the buckets with globally unique names that          meet the S3 bucket naming rules. This step is           explained further in Deploy the resources using AWS SAM.",
                            "The definition of the Lambda function resource configures a trigger for the function using the S3Event event property. This         trigger causes your function to be invoked whenever an object is created in your source bucket.",
                            "The function definition also specifies an AWS Identity and Access Management (IAM) policy to be attached to the function's execution role.           The AWS managed policyAmazonS3FullAccess gives your function the permissions it needs to read and write objects to Amazon S3.",
                            "AWS SAM template",
                            "Copy and paste the following code into a file named template.yaml.",
                            {
                                "code_example": "AWSTemplateFormatVersion: '2010-09-09'\nTransform: AWS::Serverless-2016-10-31\n\nResources:\n  EncryptPDFFunction:\n    Type: AWS::Serverless::Function\n    Properties:\n      FunctionName: EncryptPDF\n      Architectures: [x86_64]\n      CodeUri: ./\n      Handler: lambda_function.lambda_handler\n      Runtime: python3.12\n      Timeout: 15\n      MemorySize: 256\n      LoggingConfig:\n        LogFormat: JSON\n      Policies:\n        - AmazonS3FullAccess\n      Events:\n        S3Event:\n          Type: S3\n          Properties:\n            Bucket: !Ref PDFSourceBucket\n            Events: s3:ObjectCreated:*\n\n  PDFSourceBucket:\n    Type: AWS::S3::Bucket\n    Properties:\n      BucketName: EXAMPLE-BUCKET\n\n  EncryptedPDFBucket:\n    Type: AWS::S3::Bucket\n    Properties:\n      BucketName: EXAMPLE-BUCKET-encrypted"
                            },
                            "The AWS SAM template defines the resources you create for your app. In this example, the template defines a Lambda function using the         AWS::Serverless::Function type and two S3 buckets using the AWS::S3::Bucket type. The bucket names specified in the         template are placeholders. Before you deploy the app using AWS SAM, you need to edit the template to rename the buckets with globally unique names that          meet the S3 bucket naming rules. This step is           explained further in Deploy the resources using AWS SAM.",
                            "The definition of the Lambda function resource configures a trigger for the function using the S3Event event property. This         trigger causes your function to be invoked whenever an object is created in your source bucket.",
                            "The function definition also specifies an AWS Identity and Access Management (IAM) policy to be attached to the function's execution role.           The AWS managed policyAmazonS3FullAccess gives your function the permissions it needs to read and write objects to Amazon S3.",
                            "Copy and paste the following code into a file named test_pdf_encrypt.py.",
                            {
                                "code_example": "import boto3\nimport json\nimport pytest\nimport time\nimport os\n\n@pytest.fixture\ndef lambda_client():\n    return boto3.client('lambda')\n    \n@pytest.fixture\ndef s3_client():\n    return boto3.client('s3')\n\n@pytest.fixture\ndef logs_client():\n    return boto3.client('logs')\n\n@pytest.fixture(scope='session')\ndef cleanup():\n    # Create a new S3 client for cleanup\n    s3_client = boto3.client('s3')\n\n    yield\n    # Cleanup code will be executed after all tests have finished\n\n    # Delete test.pdf from the source bucket\n    source_bucket = 'EXAMPLE-BUCKET'\n    source_file_key = 'test.pdf'\n    s3_client.delete_object(Bucket=source_bucket, Key=source_file_key)\n    print(f\"\\nDeleted {source_file_key} from {source_bucket}\")\n\n    # Delete test_encrypted.pdf from the destination bucket\n    destination_bucket = 'EXAMPLE-BUCKET-encrypted'\n    destination_file_key = 'test_encrypted.pdf'\n    s3_client.delete_object(Bucket=destination_bucket, Key=destination_file_key)\n    print(f\"Deleted {destination_file_key} from {destination_bucket}\")\n        \n\n@pytest.mark.order(1)\ndef test_source_bucket_available(s3_client):\n    s3_bucket_name = 'EXAMPLE-BUCKET'\n    file_name = 'test.pdf'\n    file_path = os.path.join(os.path.dirname(__file__), file_name)\n\n    file_uploaded = False\n    try:\n        s3_client.upload_file(file_path, s3_bucket_name, file_name)\n        file_uploaded = True\n    except:\n        print(\"Error: couldn't upload file\")\n\n    assert file_uploaded, \"Could not upload file to S3 bucket\"\n\n    \n\n@pytest.mark.order(2)\ndef test_lambda_invoked(logs_client):\n\n    # Wait for a few seconds to make sure the logs are available\n    time.sleep(5)\n\n    # Get the latest log stream for the specified log group\n    log_streams = logs_client.describe_log_streams(\n        logGroupName='/aws/lambda/EncryptPDF',\n        orderBy='LastEventTime',\n        descending=True,\n        limit=1\n    )\n\n    latest_log_stream_name = log_streams['logStreams'][0]['logStreamName']\n\n    # Retrieve the log events from the latest log stream\n    log_events = logs_client.get_log_events(\n        logGroupName='/aws/lambda/EncryptPDF',\n        logStreamName=latest_log_stream_name\n    )\n\n    success_found = False\n    for event in log_events['events']:\n        message = json.loads(event['message'])\n        status = message.get('record', {}).get('status')\n        if status == 'success':\n            success_found = True\n            break\n\n    assert success_found, \"Lambda function execution did not report 'success' status in logs.\"\n\n@pytest.mark.order(3)\ndef test_encrypted_file_in_bucket(s3_client):\n    # Specify the destination S3 bucket and the expected converted file key\n    destination_bucket = 'EXAMPLE-BUCKET-encrypted'\n    converted_file_key = 'test_encrypted.pdf'\n\n    try:\n        # Attempt to retrieve the metadata of the converted file from the destination S3 bucket\n        s3_client.head_object(Bucket=destination_bucket, Key=converted_file_key)\n    except s3_client.exceptions.ClientError as e:\n        # If the file is not found, the test will fail\n        pytest.fail(f\"Converted file '{converted_file_key}' not found in the destination bucket: {str(e)}\")\n\ndef test_cleanup(cleanup):\n    # This test uses the cleanup fixture and will be executed last\n    pass"
                            },
                            "The automated test script executes three test functions to confirm correct operation of your app:",
                            "  1.The test test_source_bucket_available confirms that your source bucket has been successfully created             by uploading a test PDF file to the bucket.",
                            "  2.The test test_lambda_invoked interrogates the latest CloudWatch Logs log stream for your function to confirm that             when you uploaded the test file, your Lambda function ran and reported success.",
                            "  3.The test test_encrypted_file_in_bucket confirms that your destination bucket contains the encrypted test_encrypted.pdf             file.",
                            "After all these tests have run, the script runs an additional cleanup step to delete the test.pdf and test_encrypted.pdf files from         both your source and destination buckets.",
                            "As with the AWS SAM template, the bucket names specified in this file are placeholders. Before running the test, you need to edit this file           with your app's real bucket names. This step is explained further in Testing the app with the automated script",
                            "Automated test script",
                            "Copy and paste the following code into a file named test_pdf_encrypt.py.",
                            {
                                "code_example": "import boto3\nimport json\nimport pytest\nimport time\nimport os\n\n@pytest.fixture\ndef lambda_client():\n    return boto3.client('lambda')\n    \n@pytest.fixture\ndef s3_client():\n    return boto3.client('s3')\n\n@pytest.fixture\ndef logs_client():\n    return boto3.client('logs')\n\n@pytest.fixture(scope='session')\ndef cleanup():\n    # Create a new S3 client for cleanup\n    s3_client = boto3.client('s3')\n\n    yield\n    # Cleanup code will be executed after all tests have finished\n\n    # Delete test.pdf from the source bucket\n    source_bucket = 'EXAMPLE-BUCKET'\n    source_file_key = 'test.pdf'\n    s3_client.delete_object(Bucket=source_bucket, Key=source_file_key)\n    print(f\"\\nDeleted {source_file_key} from {source_bucket}\")\n\n    # Delete test_encrypted.pdf from the destination bucket\n    destination_bucket = 'EXAMPLE-BUCKET-encrypted'\n    destination_file_key = 'test_encrypted.pdf'\n    s3_client.delete_object(Bucket=destination_bucket, Key=destination_file_key)\n    print(f\"Deleted {destination_file_key} from {destination_bucket}\")\n        \n\n@pytest.mark.order(1)\ndef test_source_bucket_available(s3_client):\n    s3_bucket_name = 'EXAMPLE-BUCKET'\n    file_name = 'test.pdf'\n    file_path = os.path.join(os.path.dirname(__file__), file_name)\n\n    file_uploaded = False\n    try:\n        s3_client.upload_file(file_path, s3_bucket_name, file_name)\n        file_uploaded = True\n    except:\n        print(\"Error: couldn't upload file\")\n\n    assert file_uploaded, \"Could not upload file to S3 bucket\"\n\n    \n\n@pytest.mark.order(2)\ndef test_lambda_invoked(logs_client):\n\n    # Wait for a few seconds to make sure the logs are available\n    time.sleep(5)\n\n    # Get the latest log stream for the specified log group\n    log_streams = logs_client.describe_log_streams(\n        logGroupName='/aws/lambda/EncryptPDF',\n        orderBy='LastEventTime',\n        descending=True,\n        limit=1\n    )\n\n    latest_log_stream_name = log_streams['logStreams'][0]['logStreamName']\n\n    # Retrieve the log events from the latest log stream\n    log_events = logs_client.get_log_events(\n        logGroupName='/aws/lambda/EncryptPDF',\n        logStreamName=latest_log_stream_name\n    )\n\n    success_found = False\n    for event in log_events['events']:\n        message = json.loads(event['message'])\n        status = message.get('record', {}).get('status')\n        if status == 'success':\n            success_found = True\n            break\n\n    assert success_found, \"Lambda function execution did not report 'success' status in logs.\"\n\n@pytest.mark.order(3)\ndef test_encrypted_file_in_bucket(s3_client):\n    # Specify the destination S3 bucket and the expected converted file key\n    destination_bucket = 'EXAMPLE-BUCKET-encrypted'\n    converted_file_key = 'test_encrypted.pdf'\n\n    try:\n        # Attempt to retrieve the metadata of the converted file from the destination S3 bucket\n        s3_client.head_object(Bucket=destination_bucket, Key=converted_file_key)\n    except s3_client.exceptions.ClientError as e:\n        # If the file is not found, the test will fail\n        pytest.fail(f\"Converted file '{converted_file_key}' not found in the destination bucket: {str(e)}\")\n\ndef test_cleanup(cleanup):\n    # This test uses the cleanup fixture and will be executed last\n    pass"
                            },
                            "The automated test script executes three test functions to confirm correct operation of your app:",
                            "  1.The test test_source_bucket_available confirms that your source bucket has been successfully created             by uploading a test PDF file to the bucket.",
                            "  2.The test test_lambda_invoked interrogates the latest CloudWatch Logs log stream for your function to confirm that             when you uploaded the test file, your Lambda function ran and reported success.",
                            "  3.The test test_encrypted_file_in_bucket confirms that your destination bucket contains the encrypted test_encrypted.pdf             file.",
                            "After all these tests have run, the script runs an additional cleanup step to delete the test.pdf and test_encrypted.pdf files from         both your source and destination buckets.",
                            "As with the AWS SAM template, the bucket names specified in this file are placeholders. Before running the test, you need to edit this file           with your app's real bucket names. This step is explained further in Testing the app with the automated script",
                            "Copy and paste the following code into a file named pytest.ini.",
                            {
                                "code_example": "[pytest]\nmarkers =\n    order: specify test execution order"
                            },
                            "This is needed to specify the order in which the tests in the test_pdf_encrypt.py script run.",
                            "Test script configuration file",
                            "Copy and paste the following code into a file named pytest.ini.",
                            {
                                "code_example": "[pytest]\nmarkers =\n    order: specify test execution order"
                            },
                            "This is needed to specify the order in which the tests in the test_pdf_encrypt.py script run."
                        ]
                    },
                    {
                        "sub_header": "Deploying the app",
                        "content": [
                            "You can create and deploy the resources for this example app either manually or by using AWS SAM. In a production environment, we       recommend that you use an IaC tool like AWS SAM to quickly and repeatably deploy whole serverless applications without using manual processes.",
                            "For this example, follow the console or AWS CLI instructions to learn how to configure each AWS resource separately, or skip ahead to        Deploy the resources using AWS SAM to quickly deploy the app using a few CLI commands.",
                            {
                                "sub_header": "Deploy the resources manually",
                                "content": [
                                    "To deploy your app manually, you carry out the following steps:",
                                    "  1.Create source and destination Amazon S3 buckets",
                                    "  2.Create a Lambda function that encrypts a PDF file and saves the encrypted version to an S3 bucket",
                                    "  3.Configure a Lambda trigger that invokes your function when objects are uploaded to your source bucket",
                                    "Follow the instructions in the following paragraphs to create and configure your resources.",
                                    {
                                        "sub_header": "Create two S3 buckets",
                                        "content": [
                                            "First create two S3 buckets. The first bucket is the source bucket you will upload your PDF files to. The second bucket is used by     Lambda to save the encrypted file when you invoke your function.",
                                            "  1.Console : SOURCEBUCKET-encrypted",
                                            "  2.AWS CLI : region",
                                            "anchor",
                                            "anchor",
                                            "To create the S3 buckets (console)",
                                            "  1 : Open the Buckets page of the Amazon S3 console.",
                                            "  2 : Choose Create bucket.",
                                            "  3 : Under General configuration, do the following:For Bucket name, enter a globally unique name that meets the Amazon S3 Bucket naming rules.                     Bucket names can contain only lower case letters, numbers, dots (.), and hyphens (-). For AWS Region, choose the AWS Region                     closest to your geographical location. Later in the deployment process, you must create your Lambda function in the same AWS Region, so                     make a note of the region you chose.",
                                            "  4 : Leave all other options set to their default values and choose Create bucket.",
                                            "  5 : Repeat steps 1 to 4 to create your destination bucket. For Bucket name, enter SOURCEBUCKET-encrypted,                 where SOURCEBUCKET is the name of the source bucket you just created."
                                        ]
                                    },
                                    {
                                        "sub_header": "Create an execution role (AWS CLI only)",
                                        "content": [
                                            "An execution role is an IAM role that grants a Lambda function permission to access AWS services and resources. When you create a function       using the Lambda console, Lambda automatically creates an execution role. You only need to create a role manually if you choose to deploy the app       using the AWS CLI. To give your function read and write access to Amazon S3, you attach the       AWS managed policyAmazonS3FullAccess.",
                                            "  1.Console : \nThis step is only required if you choose to deploy your app using the AWS CLI.\n",
                                            "  2.AWS CLI : AmazonS3FullAccess",
                                            "anchor",
                                            "anchor",
                                            "This step is only required if you choose to deploy your app using the AWS CLI."
                                        ]
                                    },
                                    {
                                        "sub_header": "Create the function deployment package",
                                        "content": [
                                            "To create your function, you create a deployment package containing your function code and its dependencies. For this       application, your function code uses a separate library for the PDF encryption.",
                                            "To create the deployment package",
                                            "  1 : Navigate to the project directory containing the lambda_function.py and requirements.txt           files you created or downloaded from GitHub earlier and create a new directory named package.",
                                            " 2 : Install the dependencies specified in the requirements.txt file in your package directory by running the following command. ",
                                            {
                                                "code_example": "pip install -r requirements.txt --target ./package/"
                                            },
                                            " 3 : Create a .zip file containing your application code and its dependencies. In Linux or MacOS, run the following commands from your           command line interface.  In Windows, use your preferred zip tool to create the lambda_function.zip file. Make sure that your         lambda_function.py file and the folders containing your dependencies are all at the root of the .zip file.",
                                            {
                                                "code_example": "cd package\nzip -r ../lambda_function.zip .\ncd ..\nzip lambda_function.zip lambda_function.py"
                                            },
                                            "You can also create your deployment package using a Python virtual environment. See Working with .zip file archives for Python Lambda functions"
                                        ]
                                    },
                                    {
                                        "sub_header": "Create the Lambda function",
                                        "content": [
                                            "You now use the deployment package you created in the previous step to deploy your Lambda function.",
                                            "  1.Console : EncryptPDF",
                                            "  2.AWS CLI : lambda_function.zip",
                                            "anchor",
                                            "anchor",
                                            "To create the function (console)",
                                            "To create your Lambda function using the console, you first create a basic function containing some ‘Hello world’ code. You then           replace this code with your own function code by uploading the.zip file you created in the previous step.",
                                            "To ensure that your function doesn't time out when encrypting large PDF files, you configure the function's memory and timeout settings.           You also set the function's log format to JSON. Configuring JSON formatted logs is necessary when using the provided test script so it can read the           function's invocation status from CloudWatch Logs to confirm successful invocation.",
                                            "  1 : Open the Functions page of the Lambda console.",
                                            "  2 : Make sure you're working in the same AWS Region you created your S3 bucket in. You can change your region using the drop-down             list at the top of the screen.",
                                            "  3 : Choose Create function.",
                                            "  4 : Choose Author from scratch.",
                                            "  5 : Under Basic information, do the following:For Function name, enter EncryptPDF.For Runtime choose Python 3.12.For Architecture, choose x86_64.",
                                            "  6 : Choose Create function.",
                                            "To upload the function code (console)",
                                            "  1 : In the Code source pane, choose Upload from.",
                                            "  2 : Choose .zip file.",
                                            "  3 : Choose Upload.",
                                            "  4 : In the file selector, select your .zip file and choose Open.",
                                            "  5 : Choose Save.",
                                            "To configure the function memory and timeout (console)",
                                            "  1 : Select the Configuration tab for your function.",
                                            "  2 : In the General configuration pane, choose Edit.",
                                            "  3 : Set Memory to 256 MB and Timeout to 15 seconds.",
                                            "  4 : Choose Save.",
                                            "To configure the log format (console)",
                                            "  1 : Select the Configuration tab for your function.",
                                            "  2 : Select Monitoring and operations tools.",
                                            "  3 : In the Logging configuration pane, choose Edit.",
                                            "  4 : For Logging configuration, select JSON.",
                                            "  5 : Choose Save."
                                        ]
                                    },
                                    {
                                        "sub_header": "Configure an Amazon S3 trigger to invoke the function",
                                        "content": [
                                            "For your Lambda function to run when you upload a file to your source bucket, you need to configure a trigger for your function. You can     configure the Amazon S3 trigger using either the console or the AWS CLI.",
                                            "Important",
                                            "This procedure configures the S3 bucket to invoke your function every time that an object is created in the bucket. Be sure to       configure this only on the source bucket. If your Lambda function creates objects in the same bucket that invokes it, your function can be         invoked continuously in a loop. This can result         in un expected charges being billed to your AWS account.",
                                            "  1.Console : EncryptPDF",
                                            "  2.AWS CLI : source-account",
                                            "anchor",
                                            "anchor",
                                            "To configure the Amazon S3 trigger (console)",
                                            "  1 : Open the Functions page of the Lambda console and choose your function (EncryptPDF).",
                                            "  2 : Choose Add trigger.",
                                            "  3 : Select S3.",
                                            "  4 : Under Bucket, select your source bucket.",
                                            "  5 : Under Event types, select All object create events.",
                                            "  6 : Under Recursive invocation, select the check box to acknowledge that using the same S3 bucket for input                 and output is not recommended. You can learn more about recursive invocation patterns in Lambda by reading                Recursive patterns that cause run-away Lambda functions                in Serverless Land.",
                                            "  7 : Choose Add.When you create a trigger using the Lambda console, Lambda automatically creates a resource based policy                 to give the service you select permission to invoke your function. "
                                        ]
                                    }
                                ]
                            },
                            {
                                "sub_header": "Deploy the resources using AWS SAM",
                                "content": [
                                    "To deploy the example app using the AWS SAM CLI, carry out the following steps.",
                                    "Make sure that you have         installed the latest version of the           CLI and that Docker is installed on your build machine.",
                                    "  1 : Edit the template.yaml file to specify the name of your             S3 buckets. S3 buckets must have globally unique names that meet the S3 bucket naming rules.Replace the bucket name EXAMPLE-BUCKET with a name of your choice consisting of lowercase letters, numbers, dots (.), and hyphens (-). For the destination           bucket, replace EXAMPLE-BUCKET-encrypted with <source-bucket-name>-encrypted, where <source-bucket> is the name           you chose for your source bucket.",
                                    " 2 : Run the following command from the directory in which you saved your template.yaml, lambda_function.py,             and requirements.txtfiles. This command gathers the build artifacts for your application and places them in the proper format and location to deploy them. Specifying           the --use-container option builds your function inside a Lambda-like Docker container. We use it here so you don't need to have Python 3.12          installed on your local machine for the build to work.During the build process, AWS SAM looks for the Lambda function code in the location you specified with the CodeUri           property in the template. In this case, we specified the current directory as the location (./).If a requirements.txt file is present, AWS SAM uses it to gather the specified dependencies. By default, AWS SAM creates a .zip             deployment package with your function code and dependencies. You can also choose to deploy your function as a container image using the             PackageType           property.",
                                    {
                                        "code_example": "sam build --use-container"
                                    },
                                    " 3 : To deploy your application and create the Lambda and Amazon S3 resources specified in your AWS SAM template, run the following             command. Using the --guided flag means that AWS SAM will show you prompts to guide you through the deployment process. For this             deployment, accept the default options by pressing Enter.",
                                    {
                                        "code_example": "sam deploy --guided"
                                    },
                                    "During the deployment process, AWS SAM creates the following resources in your AWS account:",
                                    "  1.An AWS CloudFormation stack             named sam-app",
                                    "  2.A Lambda function with the name EncryptPDF",
                                    "  3.Two S3 buckets with the names you chose when you edited the template.yaml AWS SAM template file",
                                    "  4.An IAM execution role for your function with the name format sam-app-EncryptPDFFunctionRole-2qGaapHFWOQ8",
                                    "When AWS SAM finishes creating your resources, you should see the following message:",
                                    "Successfully created/updated stack - sam-app in us-west-2"
                                ]
                            }
                        ]
                    },
                    {
                        "sub_header": "Testing the app",
                        "content": [
                            "To test your app, you upload a PDF file to your source bucket, and confirm that Lambda creates an encrypted version of the file in your       destination bucket. In this example, you can either test this manually using the console or the AWS CLI, or by using the provided test script.",
                            "For production applications, you can use traditional test methods and techniques, such as unit testing, to confirm the       correct functioning of your Lambda function code. Best practice is also to conduct tests like those in the provided test script which perform integration       testing with real, cloud-based resources. Integration testing in the cloud confirms that your infrastructure has been correctly deployed and that events flow       between different services as expected. To learn more, see How to test serverless functions and applications.",
                            {
                                "sub_header": "Testing the app manually",
                                "content": [
                                    "You can test your function manually by adding a PDF file to           your Amazon S3 source bucket. When you add your file to the source bucket, your Lambda function should be automatically invoked and should store an encrypted           version of the file in your target bucket.",
                                    "  1.Console : filename_encrypted.pdf",
                                    "  2.AWS CLI : --bucket",
                                    "anchor",
                                    "anchor",
                                    "To test your app by uploading a file (console)",
                                    "  1 : To upload a PDF file to your S3 bucket, do the following:Open the Buckets page of the Amazon S3 console and choose your source bucket.Choose Upload.Choose Add files and use the file selector to choose the PDF file you want to upload.Choose Open, then choose Upload.",
                                    "  2 : Verify that Lambda has saved an encrypted version of your PDF file in your target bucket by doing the following:Navigate back to the Buckets page of the Amazon S3 console and choose your destination bucket.In the Objects pane, you should now see a file with name format filename_encrypted.pdf (where                         filename.pdf was the name of the file you uploaded to your source bucket).                        To download your encrypted PDF, select the file, then choose Download.Confirm that you can open the downloaded file with the password your Lambda function protected it with (my-secret-password)."
                                ]
                            },
                            {
                                "sub_header": "Testing the app with the automated script",
                                "content": [
                                    "To test your app using the provided test script, first ensure that the pytest module is installed in your local environment. You can install       pytest by running the following command:",
                                    "pip install pytest",
                                    "You also need to edit the code in the test_pdf_encrypt.py file to replace the placeholder bucket names with the names of           your Amazon S3 source and destination buckets. Make the following changes to test_pdf_encrypt.py:",
                                    "  1.In the test_source_bucket_available function, replace EXAMPLE-BUCKET with the name of your source bucket.",
                                    "  2.In the test_encrypted_file_in_bucket function, replace EXAMPLE-BUCKET-encrypted with <source-bucket>-encrypted,            where <source-bucket> is the name of your source bucket.",
                                    "  3.In the cleanup function, replace EXAMPLE-BUCKET with the name of your source bucket, and replace               EXAMPLE-BUCKET-encrypted with ≪source-bucket>-encrypted, where <source-bucket> is the name of your source bucket.",
                                    "To run the tests do the following:",
                                    "  1.Save a PDF file named test.pdfin the directory containing the test_pdf_encrypt.py and pytest.ini             files.",
                                    "  2.Open a terminal or shell program and run the following command from the directory containing the test files.pytest -s -v",
                                    {
                                        "code_example": "pytest -s -v"
                                    },
                                    "When the test completes, you should see output like the following:",
                                    "============================================================== test session starts =========================================================platform linux -- Python 3.12.2, pytest-7.2.2, pluggy-1.0.0 -- /usr/bin/python3cachedir: .pytest_cachehypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/pdf_encrypt_app/.hypothesis/examples')Test order randomisation NOT enabled. Enable with --random-order or --random-order-bucket=<bucket_type>rootdir: /home/pdf_encrypt_app, configfile: pytest.iniplugins: anyio-3.7.1, hypothesis-6.70.0, localserver-0.7.1, random-order-1.1.0collected 4 itemstest_pdf_encrypt.py::test_source_bucket_available PASSEDtest_pdf_encrypt.py::test_lambda_invoked PASSEDtest_pdf_encrypt.py::test_encrypted_file_in_bucket PASSEDtest_pdf_encrypt.py::test_cleanup PASSEDDeleted test.pdf from EXAMPLE-BUCKETDeleted test_encrypted.pdf from EXAMPLE-BUCKET-encrypted=============================================================== 4 passed in 7.32s =========================================================="
                                ]
                            }
                        ]
                    },
                    {
                        "sub_header": "Next steps",
                        "content": [
                            "Now you've created this example app, you can use the provided code as a basis to create other types of file-processing application. Modify the     code in the lambda_function.py file to implement the file-processing logic for your use case.",
                            "Many typical file-processing use cases involve image processing. When using Python, the most popular image-processing libraries like       pillow typically contain C or C++ components. In order to ensure that your function's deployment package is     compatible with the Lambda execution environment, it's important to use the correct source distribution binary.",
                            "When deploying your resources with AWS SAM, you need to take some extra steps to include the right source distribution in your deployment package. Because AWS SAM won't install dependencies     for a different platform than your build machine, specifying the correct source distribution (.whl file) in your requirements.txt       file won't work if your build machine uses an operating system or architecture that's different from the Lambda execution environment. Instead, you should do one of the following:",
                            "  1.Use the --use-container option when running sam build. When you specify this option, AWS SAM downloads a container base image that's         compatible with the Lambda execution environment and builds your function's deployment package in a Docker container using that image. To learn more, see           Building a Lambda           function inside of a provided container.",
                            "  2.Build your function's .zip deployment package yourself using the correct source distribution binary and save the .zip file in the directory you specify as the           CodeUri in the AWS SAM template. To learn more about building .zip deployment packages for Python using binary distributions, see           Creating a .zip deployment package with dependencies and Creating .zip deployment packages with native libraries."
                        ]
                    }
                ]
            },
            {
                "title": "Scheduled-maintenance app",
                "href": "https://docs.aws.amazon.com/lambda/latest/dg/scheduled-task-app.html",
                "source": "Aws",
                "parent_content": [
                    "You can use AWS Lambda to replace scheduled processes such as automated system backups, file conversions, and maintenance tasks.   In this example, you create a serverless application that performs regular scheduled maintenance on a DynamoDB table by deleting old entries. The app uses EventBridge Scheduler to invoke a   Lambda function on a cron schedule. When invoked, the function queries the table for items older than one year, and deletes them. The function logs each deleted item  in CloudWatch Logs.",
                    "To implement this example, first create a DynamoDB table and populate it with some test data for your function to query. Then, create a Python Lambda function with   an EventBridge Scheduler trigger and an IAM execution role that gives the function permission to read, and delete, items from your table.",
                    "Tip",
                    "If you’re new to Lambda, we recommend that you complete the tutorial Create your first Lambda function before      creating this example app.",
                    "You can deploy your app manually by creating and configuring resources with the AWS Management Console. You can     also deploy the app by using the AWS Serverless Application Model (AWS SAM). AWS SAM is an infrastructure as code (IaC) tool. With IaC, you don’t create     resources manually, but define them in code and then deploy them automatically.",
                    "If you want to learn more about using Lambda with IaC before deploying this example app, see Using Lambda with infrastructure as code (IaC).",
                    {
                        "sub_header": "Prerequisites",
                        "content": [
                            "Before you can create the example app, make sure you have the required command line tools and programs installed.",
                            "  1.Python : To populate the DynamoDB table you create to test your app, this example uses a  script and a CSV file to write data into the table. Make sure you have          version 3.8 or later installed on your machine.",
                            "  2.AWS SAM CLI : If you want to create the DynamoDB table and deploy the example app using AWS SAM, you need to install the .           Follow the installation instructions           in the AWS SAM User Guide.",
                            "  3.AWS CLI : To use the provided Python script to populate your test table, you need to have installed and configured the . This is because the script uses           the AWS SDK for Python (Boto3), which needs access to your AWS Identity and Access Management (IAM) credentials. You also need the  installed to deploy resources using AWS SAM. Install the CLI by following           the installation instructions in the AWS Command Line Interface User Guide.",
                            "  4.Docker : To deploy the app using AWS SAM,  must also be installed on your build machine. Follow the instructions in Install  Engine         on the  documentation website."
                        ]
                    },
                    {
                        "sub_header": "Downloading the example app files",
                        "content": [
                            "To create the example database and the scheduled-maintenance app, you need to create the following files in your project directory:",
                            "Example database files",
                            "  1.template.yaml - an AWS SAM template you can use to create the DynamoDB table",
                            "  2.sample_data.csv - a CSV file containing sample data to load into your table",
                            "  3.load_sample_data.py - a Python script that writes the data in the CSV file into the table",
                            "Scheduled-maintenance app files",
                            "  1.lambda_function.py - the Python function code for the Lambda function that performs the database maintenance",
                            "  2.requirements.txt - a manifest file defining the dependencies that your Python function code requires",
                            "  3.template.yaml - an AWS SAM template you can use to deploy the app",
                            "Test file",
                            "  1.test_app.py - a Python script that scans the table and confirms successful operation of your function by outputting all records older than one year",
                            "Expand the following sections to view the code and to learn more about the role of each file in creating and testing your app. To create the       files on your local machine, copy and paste the code below.",
                            "Copy and paste the following code into a file named template.yaml.",
                            {
                                "code_example": "AWSTemplateFormatVersion: '2010-09-09'\nTransform: AWS::Serverless-2016-10-31\nDescription: SAM Template for DynamoDB Table with Order_number as Partition Key and Date as Sort Key\n\nResources:\n  MyDynamoDBTable:\n    Type: AWS::DynamoDB::Table\n    DeletionPolicy: Retain\n    UpdateReplacePolicy: Retain\n    Properties:\n      TableName: MyOrderTable\n      BillingMode: PAY_PER_REQUEST\n      AttributeDefinitions:\n        - AttributeName: Order_number\n          AttributeType: S\n        - AttributeName: Date\n          AttributeType: S\n      KeySchema:\n        - AttributeName: Order_number\n          KeyType: HASH\n        - AttributeName: Date\n          KeyType: RANGE\n      SSESpecification:\n        SSEEnabled: true\n      GlobalSecondaryIndexes:\n        - IndexName: Date-index\n          KeySchema:\n            - AttributeName: Date\n              KeyType: HASH\n          Projection:\n            ProjectionType: ALL\n      PointInTimeRecoverySpecification:\n        PointInTimeRecoveryEnabled: true\n\nOutputs:\n  TableName:\n    Description: DynamoDB Table Name\n    Value: !Ref MyDynamoDBTable\n  TableArn:\n    Description: DynamoDB Table ARN\n    Value: !GetAtt MyDynamoDBTable.Arn"
                            },
                            "Note",
                            "AWS SAM templates use a standard naming convention of template.yaml. In this example, you have two template files - one to create the           example database and another to create the app itself. Save them in separate sub-directories in your project folder.",
                            "This AWS SAM template defines the DynamoDB table resource you create to test your app. The table uses a primary key of Order_number with a sort           key of Date. In order for your Lambda function to find items directly by date, we also define a Global Secondary Index         named Date-index.",
                            "To learn more about creating and configuring a DynamoDB table using the AWS::DynamoDB::Table resource, see AWS::DynamoDB::Table in           the AWS CloudFormation User Guide.",
                            "AWS SAM template (example DynamoDB table)",
                            "Copy and paste the following code into a file named template.yaml.",
                            {
                                "code_example": "AWSTemplateFormatVersion: '2010-09-09'\nTransform: AWS::Serverless-2016-10-31\nDescription: SAM Template for DynamoDB Table with Order_number as Partition Key and Date as Sort Key\n\nResources:\n  MyDynamoDBTable:\n    Type: AWS::DynamoDB::Table\n    DeletionPolicy: Retain\n    UpdateReplacePolicy: Retain\n    Properties:\n      TableName: MyOrderTable\n      BillingMode: PAY_PER_REQUEST\n      AttributeDefinitions:\n        - AttributeName: Order_number\n          AttributeType: S\n        - AttributeName: Date\n          AttributeType: S\n      KeySchema:\n        - AttributeName: Order_number\n          KeyType: HASH\n        - AttributeName: Date\n          KeyType: RANGE\n      SSESpecification:\n        SSEEnabled: true\n      GlobalSecondaryIndexes:\n        - IndexName: Date-index\n          KeySchema:\n            - AttributeName: Date\n              KeyType: HASH\n          Projection:\n            ProjectionType: ALL\n      PointInTimeRecoverySpecification:\n        PointInTimeRecoveryEnabled: true\n\nOutputs:\n  TableName:\n    Description: DynamoDB Table Name\n    Value: !Ref MyDynamoDBTable\n  TableArn:\n    Description: DynamoDB Table ARN\n    Value: !GetAtt MyDynamoDBTable.Arn"
                            },
                            "Note",
                            "AWS SAM templates use a standard naming convention of template.yaml. In this example, you have two template files - one to create the           example database and another to create the app itself. Save them in separate sub-directories in your project folder.",
                            "This AWS SAM template defines the DynamoDB table resource you create to test your app. The table uses a primary key of Order_number with a sort           key of Date. In order for your Lambda function to find items directly by date, we also define a Global Secondary Index         named Date-index.",
                            "To learn more about creating and configuring a DynamoDB table using the AWS::DynamoDB::Table resource, see AWS::DynamoDB::Table in           the AWS CloudFormation User Guide.",
                            "Copy and paste the following code into a file named sample_data.csv.",
                            {
                                "code_example": "Date,Order_number,CustomerName,ProductID,Quantity,TotalAmount\n2023-09-01,ORD001,Alejandro Rosalez,PROD123,2,199.98\n2023-09-01,ORD002,Akua Mansa,PROD456,1,49.99\n2023-09-02,ORD003,Ana Carolina Silva,PROD789,3,149.97\n2023-09-03,ORD004,Arnav Desai,PROD123,1,99.99\n2023-10-01,ORD005,Carlos Salazar,PROD456,2,99.98\n2023-10-02,ORD006,Diego Ramirez,PROD789,1,49.99\n2023-10-03,ORD007,Efua Owusu,PROD123,4,399.96\n2023-10-04,ORD008,John Stiles,PROD456,2,99.98\n2023-10-05,ORD009,Jorge Souza,PROD789,3,149.97\n2023-10-06,ORD010,Kwaku Mensah,PROD123,1,99.99\n2023-11-01,ORD011,Li Juan,PROD456,5,249.95\n2023-11-02,ORD012,Marcia Oliveria,PROD789,2,99.98\n2023-11-03,ORD013,Maria Garcia,PROD123,3,299.97\n2023-11-04,ORD014,Martha Rivera,PROD456,1,49.99\n2023-11-05,ORD015,Mary Major,PROD789,4,199.96\n2023-12-01,ORD016,Mateo Jackson,PROD123,2,199.99\n2023-12-02,ORD017,Nikki Wolf,PROD456,3,149.97\n2023-12-03,ORD018,Pat Candella,PROD789,1,49.99\n2023-12-04,ORD019,Paulo Santos,PROD123,5,499.95\n2023-12-05,ORD020,Richard Roe,PROD456,2,99.98\n2024-01-01,ORD021,Saanvi Sarkar,PROD789,3,149.97\n2024-01-02,ORD022,Shirley Rodriguez,PROD123,1,99.99\n2024-01-03,ORD023,Sofia Martinez,PROD456,4,199.96\n2024-01-04,ORD024,Terry Whitlock,PROD789,2,99.98\n2024-01-05,ORD025,Wang Xiulan,PROD123,3,299.97"
                            },
                            "This file contains some example test data to populate your DynamoDB table with in a standard comma-separated values (CSV) format.",
                            "Sample database data file",
                            "Copy and paste the following code into a file named sample_data.csv.",
                            {
                                "code_example": "Date,Order_number,CustomerName,ProductID,Quantity,TotalAmount\n2023-09-01,ORD001,Alejandro Rosalez,PROD123,2,199.98\n2023-09-01,ORD002,Akua Mansa,PROD456,1,49.99\n2023-09-02,ORD003,Ana Carolina Silva,PROD789,3,149.97\n2023-09-03,ORD004,Arnav Desai,PROD123,1,99.99\n2023-10-01,ORD005,Carlos Salazar,PROD456,2,99.98\n2023-10-02,ORD006,Diego Ramirez,PROD789,1,49.99\n2023-10-03,ORD007,Efua Owusu,PROD123,4,399.96\n2023-10-04,ORD008,John Stiles,PROD456,2,99.98\n2023-10-05,ORD009,Jorge Souza,PROD789,3,149.97\n2023-10-06,ORD010,Kwaku Mensah,PROD123,1,99.99\n2023-11-01,ORD011,Li Juan,PROD456,5,249.95\n2023-11-02,ORD012,Marcia Oliveria,PROD789,2,99.98\n2023-11-03,ORD013,Maria Garcia,PROD123,3,299.97\n2023-11-04,ORD014,Martha Rivera,PROD456,1,49.99\n2023-11-05,ORD015,Mary Major,PROD789,4,199.96\n2023-12-01,ORD016,Mateo Jackson,PROD123,2,199.99\n2023-12-02,ORD017,Nikki Wolf,PROD456,3,149.97\n2023-12-03,ORD018,Pat Candella,PROD789,1,49.99\n2023-12-04,ORD019,Paulo Santos,PROD123,5,499.95\n2023-12-05,ORD020,Richard Roe,PROD456,2,99.98\n2024-01-01,ORD021,Saanvi Sarkar,PROD789,3,149.97\n2024-01-02,ORD022,Shirley Rodriguez,PROD123,1,99.99\n2024-01-03,ORD023,Sofia Martinez,PROD456,4,199.96\n2024-01-04,ORD024,Terry Whitlock,PROD789,2,99.98\n2024-01-05,ORD025,Wang Xiulan,PROD123,3,299.97"
                            },
                            "This file contains some example test data to populate your DynamoDB table with in a standard comma-separated values (CSV) format.",
                            "Copy and paste the following code into a file named load_sample_data.py.",
                            {
                                "code_example": "import boto3\nimport csv\nfrom decimal import Decimal\n\n# Initialize the DynamoDB client\ndynamodb = boto3.resource('dynamodb')\ntable = dynamodb.Table('MyOrderTable') \nprint(\"DDB client initialized.\")\n\ndef load_data_from_csv(filename):\n    with open(filename, 'r') as file:\n        csv_reader = csv.DictReader(file)\n        for row in csv_reader:\n            item = {\n                'Order_number': row['Order_number'],\n                'Date': row['Date'],\n                'CustomerName': row['CustomerName'],\n                'ProductID': row['ProductID'],\n                'Quantity': int(row['Quantity']),\n                'TotalAmount': Decimal(str(row['TotalAmount']))\n            }\n            table.put_item(Item=item)\n            print(f\"Added item: {item['Order_number']} - {item['Date']}\")\n\nif __name__ == \"__main__\":\n    load_data_from_csv('sample_data.csv')\n    print(\"Data loading completed.\")"
                            },
                            "This Python script first uses the AWS SDK for Python (Boto3) to create a connection to your DynamoDB table. It then iterates over each row in the example-data CSV file, creates an item from that row, and            writes the item to the DynamoDB table using the boto3 SDK.",
                            "Python script to load sample data",
                            "Copy and paste the following code into a file named load_sample_data.py.",
                            {
                                "code_example": "import boto3\nimport csv\nfrom decimal import Decimal\n\n# Initialize the DynamoDB client\ndynamodb = boto3.resource('dynamodb')\ntable = dynamodb.Table('MyOrderTable') \nprint(\"DDB client initialized.\")\n\ndef load_data_from_csv(filename):\n    with open(filename, 'r') as file:\n        csv_reader = csv.DictReader(file)\n        for row in csv_reader:\n            item = {\n                'Order_number': row['Order_number'],\n                'Date': row['Date'],\n                'CustomerName': row['CustomerName'],\n                'ProductID': row['ProductID'],\n                'Quantity': int(row['Quantity']),\n                'TotalAmount': Decimal(str(row['TotalAmount']))\n            }\n            table.put_item(Item=item)\n            print(f\"Added item: {item['Order_number']} - {item['Date']}\")\n\nif __name__ == \"__main__\":\n    load_data_from_csv('sample_data.csv')\n    print(\"Data loading completed.\")"
                            },
                            "This Python script first uses the AWS SDK for Python (Boto3) to create a connection to your DynamoDB table. It then iterates over each row in the example-data CSV file, creates an item from that row, and            writes the item to the DynamoDB table using the boto3 SDK.",
                            "Copy and paste the following code into a file named lambda_function.py.",
                            {
                                "code_example": "import boto3\nfrom datetime import datetime, timedelta\nfrom boto3.dynamodb.conditions import Key, Attr\nimport logging\n\nlogger = logging.getLogger()\nlogger.setLevel(\"INFO\")\n\ndef lambda_handler(event, context):\n    # Initialize the DynamoDB client\n    dynamodb = boto3.resource('dynamodb')\n    \n    # Specify the table name\n    table_name = 'MyOrderTable'\n    table = dynamodb.Table(table_name)\n    \n    # Get today's date\n    today = datetime.now()\n    \n    # Calculate the date one year ago\n    one_year_ago = (today - timedelta(days=365)).strftime('%Y-%m-%d')\n    \n    # Scan the table using a global secondary index\n    response = table.scan(\n        IndexName='Date-index',\n        FilterExpression='#date < :one_year_ago',\n        ExpressionAttributeNames={\n            '#date': 'Date'\n        },\n        ExpressionAttributeValues={\n            ':one_year_ago': one_year_ago\n        }\n    )\n    \n     # Delete old items\n    with table.batch_writer() as batch:\n        for item in response['Items']:\n            Order_number = item['Order_number']\n            batch.delete_item(\n                Key={\n                    'Order_number': Order_number,\n                    'Date': item['Date']\n                }\n            )\n            logger.info(f'deleted order number {Order_number}')\n    \n    # Check if there are more items to scan\n    while 'LastEvaluatedKey' in response:\n        response = table.scan(\n            IndexName='DateIndex',\n            FilterExpression='#date < :one_year_ago',\n            ExpressionAttributeNames={\n                '#date': 'Date'\n            },\n            ExpressionAttributeValues={\n                ':one_year_ago': one_year_ago\n            },\n            ExclusiveStartKey=response['LastEvaluatedKey']\n        )\n        \n        # Delete old items\n        with table.batch_writer() as batch:\n            for item in response['Items']:\n                batch.delete_item(\n                    Key={\n                        'Order_number': item['Order_number'],\n                        'Date': item['Date']\n                    }\n                )\n    \n    return {\n        'statusCode': 200,\n        'body': 'Cleanup completed successfully'\n    }"
                            },
                            "The Python function code contains the handler function (lambda_handler) that Lambda runs when your function is     invoked.",
                            "When the function is invoked by EventBridge Scheduler, it uses the AWS SDK for Python (Boto3) to create a connection to the DynamoDB table on which the scheduled maintenance task is to be performed.     It then uses the Python datetime library to calculate the date one year ago, before scanning the table for items older than this and deleting them.",
                            "Note that responses from DynamoDB query and scan operations are limited to a maximum of 1 MB in size. If the response is larger than 1 MB, DynamoDB paginates the       data and returns a LastEvaluatedKey element in the response. To ensure that our function processes all the records in the table, we check for the presence of this key       and continue performing table scans from the last evaluated position until the whole table has been scanned.",
                            "Python function code",
                            "Copy and paste the following code into a file named lambda_function.py.",
                            {
                                "code_example": "import boto3\nfrom datetime import datetime, timedelta\nfrom boto3.dynamodb.conditions import Key, Attr\nimport logging\n\nlogger = logging.getLogger()\nlogger.setLevel(\"INFO\")\n\ndef lambda_handler(event, context):\n    # Initialize the DynamoDB client\n    dynamodb = boto3.resource('dynamodb')\n    \n    # Specify the table name\n    table_name = 'MyOrderTable'\n    table = dynamodb.Table(table_name)\n    \n    # Get today's date\n    today = datetime.now()\n    \n    # Calculate the date one year ago\n    one_year_ago = (today - timedelta(days=365)).strftime('%Y-%m-%d')\n    \n    # Scan the table using a global secondary index\n    response = table.scan(\n        IndexName='Date-index',\n        FilterExpression='#date < :one_year_ago',\n        ExpressionAttributeNames={\n            '#date': 'Date'\n        },\n        ExpressionAttributeValues={\n            ':one_year_ago': one_year_ago\n        }\n    )\n    \n     # Delete old items\n    with table.batch_writer() as batch:\n        for item in response['Items']:\n            Order_number = item['Order_number']\n            batch.delete_item(\n                Key={\n                    'Order_number': Order_number,\n                    'Date': item['Date']\n                }\n            )\n            logger.info(f'deleted order number {Order_number}')\n    \n    # Check if there are more items to scan\n    while 'LastEvaluatedKey' in response:\n        response = table.scan(\n            IndexName='DateIndex',\n            FilterExpression='#date < :one_year_ago',\n            ExpressionAttributeNames={\n                '#date': 'Date'\n            },\n            ExpressionAttributeValues={\n                ':one_year_ago': one_year_ago\n            },\n            ExclusiveStartKey=response['LastEvaluatedKey']\n        )\n        \n        # Delete old items\n        with table.batch_writer() as batch:\n            for item in response['Items']:\n                batch.delete_item(\n                    Key={\n                        'Order_number': item['Order_number'],\n                        'Date': item['Date']\n                    }\n                )\n    \n    return {\n        'statusCode': 200,\n        'body': 'Cleanup completed successfully'\n    }"
                            },
                            "The Python function code contains the handler function (lambda_handler) that Lambda runs when your function is     invoked.",
                            "When the function is invoked by EventBridge Scheduler, it uses the AWS SDK for Python (Boto3) to create a connection to the DynamoDB table on which the scheduled maintenance task is to be performed.     It then uses the Python datetime library to calculate the date one year ago, before scanning the table for items older than this and deleting them.",
                            "Note that responses from DynamoDB query and scan operations are limited to a maximum of 1 MB in size. If the response is larger than 1 MB, DynamoDB paginates the       data and returns a LastEvaluatedKey element in the response. To ensure that our function processes all the records in the table, we check for the presence of this key       and continue performing table scans from the last evaluated position until the whole table has been scanned.",
                            "Copy and paste the following code into a file named requirements.txt.",
                            {
                                "code_example": "boto3"
                            },
                            "For this example, your function code has only one dependency that isn't part of the standard Python library -           the SDK for Python (Boto3) that the function uses to scan and delete items from the DynamoDB table.",
                            "Note",
                            "A version of the SDK for Python (Boto3) is included as part of the Lambda runtime, so your code would run without adding Boto3 to your             function's deployment package. However, to maintain full control of your function's dependencies and avoid possible issues with             version misalignment, best practice for Python is to include all function dependencies in your function's deployment package.             See Runtime dependencies in Python to learn more.",
                            "requirements.txt manifest file",
                            "Copy and paste the following code into a file named requirements.txt.",
                            {
                                "code_example": "boto3"
                            },
                            "For this example, your function code has only one dependency that isn't part of the standard Python library -           the SDK for Python (Boto3) that the function uses to scan and delete items from the DynamoDB table.",
                            "Note",
                            "A version of the SDK for Python (Boto3) is included as part of the Lambda runtime, so your code would run without adding Boto3 to your             function's deployment package. However, to maintain full control of your function's dependencies and avoid possible issues with             version misalignment, best practice for Python is to include all function dependencies in your function's deployment package.             See Runtime dependencies in Python to learn more.",
                            "Copy and paste the following code into a file named template.yaml.",
                            {
                                "code_example": "AWSTemplateFormatVersion: '2010-09-09'\nTransform: AWS::Serverless-2016-10-31\nDescription: SAM Template for Lambda function and EventBridge Scheduler rule\n\nResources:\n  MyLambdaFunction:\n    Type: AWS::Serverless::Function\n    Properties:\n      FunctionName: ScheduledDBMaintenance\n      CodeUri: ./\n      Handler: lambda_function.lambda_handler\n      Runtime: python3.11\n      Architectures:\n        - x86_64\n      Events:\n        ScheduleEvent:\n          Type: ScheduleV2\n          Properties:\n            ScheduleExpression: cron(0 3 1 * ? *)\n            Description: Run on the first day of every month at 03:00 AM\n      Policies:\n        - CloudWatchLogsFullAccess\n        - Statement:\n            - Effect: Allow\n              Action:\n                - dynamodb:Scan\n                - dynamodb:BatchWriteItem\n              Resource: !Sub 'arn:aws:dynamodb:${AWS::Region}:${AWS::AccountId}:table/MyOrderTable'\n\n  LambdaLogGroup:\n    Type: AWS::Logs::LogGroup\n    Properties:\n      LogGroupName: !Sub /aws/lambda/${MyLambdaFunction}\n      RetentionInDays: 30\n\nOutputs:\n  LambdaFunctionName:\n    Description: Lambda Function Name\n    Value: !Ref MyLambdaFunction\n  LambdaFunctionArn:\n    Description: Lambda Function ARN\n    Value: !GetAtt MyLambdaFunction.Arn"
                            },
                            "Note",
                            "AWS SAM templates use a standard naming convention of template.yaml. In this example, you have two template files - one to create the             example database and another to create the app itself. Save them in separate sub-directories in your project folder.",
                            "This AWS SAM template defines the resources for your app. We define the Lambda function using the AWS::Serverless::Function resource. The EventBridge Scheduler schedule and the           trigger to invoke the Lambda function are created by using the Events property of this resource using a type of ScheduleV2. To learn more about defining EventBridge Scheduler schedules in AWS SAM templates,           see ScheduleV2 in the           AWS Serverless Application Model Developer Guide.",
                            "In addition to the Lambda function and the EventBridge Scheduler schedule, we also define a CloudWatch log group for your function to send records of deleted items to.",
                            "AWS SAM template (scheduled-maintenance app)",
                            "Copy and paste the following code into a file named template.yaml.",
                            {
                                "code_example": "AWSTemplateFormatVersion: '2010-09-09'\nTransform: AWS::Serverless-2016-10-31\nDescription: SAM Template for Lambda function and EventBridge Scheduler rule\n\nResources:\n  MyLambdaFunction:\n    Type: AWS::Serverless::Function\n    Properties:\n      FunctionName: ScheduledDBMaintenance\n      CodeUri: ./\n      Handler: lambda_function.lambda_handler\n      Runtime: python3.11\n      Architectures:\n        - x86_64\n      Events:\n        ScheduleEvent:\n          Type: ScheduleV2\n          Properties:\n            ScheduleExpression: cron(0 3 1 * ? *)\n            Description: Run on the first day of every month at 03:00 AM\n      Policies:\n        - CloudWatchLogsFullAccess\n        - Statement:\n            - Effect: Allow\n              Action:\n                - dynamodb:Scan\n                - dynamodb:BatchWriteItem\n              Resource: !Sub 'arn:aws:dynamodb:${AWS::Region}:${AWS::AccountId}:table/MyOrderTable'\n\n  LambdaLogGroup:\n    Type: AWS::Logs::LogGroup\n    Properties:\n      LogGroupName: !Sub /aws/lambda/${MyLambdaFunction}\n      RetentionInDays: 30\n\nOutputs:\n  LambdaFunctionName:\n    Description: Lambda Function Name\n    Value: !Ref MyLambdaFunction\n  LambdaFunctionArn:\n    Description: Lambda Function ARN\n    Value: !GetAtt MyLambdaFunction.Arn"
                            },
                            "Note",
                            "AWS SAM templates use a standard naming convention of template.yaml. In this example, you have two template files - one to create the             example database and another to create the app itself. Save them in separate sub-directories in your project folder.",
                            "This AWS SAM template defines the resources for your app. We define the Lambda function using the AWS::Serverless::Function resource. The EventBridge Scheduler schedule and the           trigger to invoke the Lambda function are created by using the Events property of this resource using a type of ScheduleV2. To learn more about defining EventBridge Scheduler schedules in AWS SAM templates,           see ScheduleV2 in the           AWS Serverless Application Model Developer Guide.",
                            "In addition to the Lambda function and the EventBridge Scheduler schedule, we also define a CloudWatch log group for your function to send records of deleted items to.",
                            "Copy and paste the following code into a file named test_app.py.",
                            {
                                "code_example": "import boto3\nfrom datetime import datetime, timedelta\nimport json\n\n# Initialize the DynamoDB client\ndynamodb = boto3.resource('dynamodb')\n\n# Specify your table name\ntable_name = 'YourTableName'\ntable = dynamodb.Table(table_name)\n\n# Get the current date\ncurrent_date = datetime.now()\n\n# Calculate the date one year ago\none_year_ago = current_date - timedelta(days=365)\n\n# Convert the date to string format (assuming the date in DynamoDB is stored as a string)\none_year_ago_str = one_year_ago.strftime('%Y-%m-%d')\n\n# Scan the table\nresponse = table.scan(\n    FilterExpression='#date < :one_year_ago',\n    ExpressionAttributeNames={\n        '#date': 'Date'\n    },\n    ExpressionAttributeValues={\n        ':one_year_ago': one_year_ago_str\n    }\n)\n\n# Process the results\nold_records = response['Items']\n\n# Continue scanning if we have more items (pagination)\nwhile 'LastEvaluatedKey' in response:\n    response = table.scan(\n        FilterExpression='#date < :one_year_ago',\n        ExpressionAttributeNames={\n            '#date': 'Date'\n        },\n        ExpressionAttributeValues={\n            ':one_year_ago': one_year_ago_str\n        },\n        ExclusiveStartKey=response['LastEvaluatedKey']\n    )\n    old_records.extend(response['Items'])\n\nfor record in old_records:\n    print(json.dumps(record))\n\n# The total number of old records should be zero.\nprint(f\"Total number of old records: {len(old_records)}\")\n"
                            },
                            "This test script uses the AWS SDK for Python (Boto3) to create a connection to your DynamoDB table and scan for items older than one year. To confirm if the Lambda function           has run successfully, at the end of the test, the function prints the number of records older than one year still in the table. If the Lambda function was successful,           the number of old records in the table should be zero.        ",
                            "Test script",
                            "Copy and paste the following code into a file named test_app.py.",
                            {
                                "code_example": "import boto3\nfrom datetime import datetime, timedelta\nimport json\n\n# Initialize the DynamoDB client\ndynamodb = boto3.resource('dynamodb')\n\n# Specify your table name\ntable_name = 'YourTableName'\ntable = dynamodb.Table(table_name)\n\n# Get the current date\ncurrent_date = datetime.now()\n\n# Calculate the date one year ago\none_year_ago = current_date - timedelta(days=365)\n\n# Convert the date to string format (assuming the date in DynamoDB is stored as a string)\none_year_ago_str = one_year_ago.strftime('%Y-%m-%d')\n\n# Scan the table\nresponse = table.scan(\n    FilterExpression='#date < :one_year_ago',\n    ExpressionAttributeNames={\n        '#date': 'Date'\n    },\n    ExpressionAttributeValues={\n        ':one_year_ago': one_year_ago_str\n    }\n)\n\n# Process the results\nold_records = response['Items']\n\n# Continue scanning if we have more items (pagination)\nwhile 'LastEvaluatedKey' in response:\n    response = table.scan(\n        FilterExpression='#date < :one_year_ago',\n        ExpressionAttributeNames={\n            '#date': 'Date'\n        },\n        ExpressionAttributeValues={\n            ':one_year_ago': one_year_ago_str\n        },\n        ExclusiveStartKey=response['LastEvaluatedKey']\n    )\n    old_records.extend(response['Items'])\n\nfor record in old_records:\n    print(json.dumps(record))\n\n# The total number of old records should be zero.\nprint(f\"Total number of old records: {len(old_records)}\")\n"
                            },
                            "This test script uses the AWS SDK for Python (Boto3) to create a connection to your DynamoDB table and scan for items older than one year. To confirm if the Lambda function           has run successfully, at the end of the test, the function prints the number of records older than one year still in the table. If the Lambda function was successful,           the number of old records in the table should be zero.        "
                        ]
                    },
                    {
                        "sub_header": "Creating and populating the example DynamoDB table",
                        "content": [
                            "To test your scheduled-maintenance app, you first create a DynamoDB table and populate it with some sample data. You can create the table either manually using the       AWS Management Console or by using AWS SAM. We recommend that you use AWS SAM to quickly create and configure the table using a few AWS CLI commands.",
                            "  1.Console : MyOrderTable",
                            "  2.AWS SAM : template.yaml",
                            "anchor",
                            "anchor",
                            "To create the DynamoDB table",
                            "  1 : Open the Tables page of the DynamoDB console.",
                            "  2 : Choose Create table.",
                            "  3 : Create the table by doing the following:Under Table details, for Table name, enter MyOrderTable.For Partition key, enter Order_number and leave the type as String.For Sort key, enter Date and leave the type as String.Leave Table settings set to Default settings and choose Create table.",
                            "  4 : When your table has finished creating and its Status shows as Active, create a global secondary index (GSI) by doing the               following. Your app will use this GSI to search for items directly by date to determine what to delete.Choose MyOrderTable from the list of tables.Choose the Indexes tab.Under Global secondary indexes, choose Create index.Under Index details, enter Date for the Partition key and leave the                   Data type set to String.For Index name, enter Date-index.Leave all other parameters set to their default values, scroll to the bottom of the page, and choose Create index.",
                            "After you've created your table, you next add some sample data to test your app. The CSV file sample_data.csv you downloaded     earlier contains a number of example entries comprised of order numbers, dates, and customer and order information. Use the provided python script     load_sample_data.py to add this data to your table.",
                            "To add the sample data to the table",
                            "  1 : Navigate to the directory containing the sample_data.csv and load_sample_data.py files. If these files are in         separate directories, move them so they're saved in the same location.",
                            " 2 : Create a Python virtual environment to run the script in by running the following command. We recommend that you use a virtual environment because in a following         step you'll need to install the AWS SDK for Python (Boto3). ",
                            {
                                "code_example": "python -m venv venv"
                            },
                            " 3 : Activate the virtual environment by running the following command. ",
                            {
                                "code_example": "source venv/bin/activate"
                            },
                            " 4 : Install the SDK for Python (Boto3) in your virtual environment by running the following command. The script uses this library to connect to your DynamoDB table and add the items. ",
                            {
                                "code_example": "pip install boto3"
                            },
                            " 5 : Run the script to populate the table by running the following command. If the script runs successfully, it should print each item to the console as it loads it and report Data loading completed.",
                            {
                                "code_example": "python load_sample_data.py"
                            },
                            " 6 : Deactivate the virtual environment by running the following command. ",
                            {
                                "code_example": "deactivate"
                            },
                            "  7 : You can verify that the data has been loaded to your DynamoDB table by doing the following:Open the Explore items page of the DynamoDB console and select your table (MyOrderTable).In the Items returned pane, you should see the 25 items from the CSV file that the script added to the table."
                        ]
                    },
                    {
                        "sub_header": "Creating the scheduled-maintenance app",
                        "content": [
                            "You can create and deploy the resources for this example app step by step using the AWS Management Console or by using AWS SAM. In a production environment, we       recommend that you use an Infrustracture-as-Code (IaC) tool like AWS SAM to repeatably deploy serverless applications without using manual processes.",
                            "For this example, follow the console instructions to learn how to configure each AWS resource separately, or follow the AWS SAM instructions     to quickly deploy the app using AWS CLI commands.",
                            "  1.Console : .zip",
                            "  2.AWS SAM : template.yaml",
                            "anchor",
                            "anchor",
                            "To create the function using the AWS Management Console",
                            "First, create a function containing basic starter code. You then                 replace this code with your own function code by either copying and pasting the code directly in the Lambda code editor, or by uploading your code                as a .zip package. For this task, we recommend simply copying and pasting the code.",
                            "  1 : Open the Functions page of the Lambda console.",
                            "  2 : Choose Create function.",
                            "  3 : Choose Author from scratch.",
                            "  4 : Under Basic information, do the following:For Function name, enter ScheduledDBMaintenance.For Runtime choose the latest Python version.For Architecture, choose x86_64.",
                            "  5 : Choose Create function.",
                            "  6 : After your function is created, you can configure your function with the provided function code.In the Code source pane, replace the Hello world code that Lambda created with the Python function code from                        the lambda_function.py file that you saved earlier.In the DEPLOY section, choose Deploy to update your function's code:",
                            "To configure the function memory and timeout (console)",
                            "  1 : Select the Configuration tab for your function.",
                            "  2 : In the General configuration pane, choose Edit.",
                            "  3 : Set Memory to 256 MB and Timeout to 15 seconds.                  If you are processing a large table with many records, for example in the case of a production environment,                  you might consider setting Timeout to a larger number. This gives your function                  more time to scan, and clean the database.",
                            "  4 : Choose Save.",
                            "To configure the log format (console)",
                            "You can configure Lambda functions to output logs in either unstructured text or JSON format. We recommend that you use JSON format for logs to                   make it easier to search and filter log data. To learn more about Lambda log configuration options, see Configuring advanced logging controls for Lambda functions.",
                            "  1 : Select the Configuration tab for your function.",
                            "  2 : Select Monitoring and operations tools.",
                            "  3 : In the Logging configuration pane, choose Edit.",
                            "  4 : For Logging configuration, select JSON.",
                            "  5 : Choose Save.",
                            "To set Up IAM permissions",
                            "To give your function the permissions it needs to read and delete DynamoDB items, you need to add a policy to your function's                     execution role defining the necessary permissions.",
                            "  1 : Open the Configuration tab, then choose                        Permissions from the left navigation bar.",
                            "  2 : Choose the role name under Execution role.",
                            "  3 : In the IAM console, choose Add permissions, then                        Create inline policy.",
                            " 4 : Use the JSON editor and enter the following policy: ",
                            {
                                "code_example": "{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"dynamodb:Scan\",\n                \"dynamodb:DeleteItem\",\n                \"dynamodb:BatchWriteItem\"\n            ],\n            \"Resource\": \"arn:aws:dynamodb:*:*:table/MyOrderTable\"\n        }\n    ]\n}"
                            },
                            "  5 : Name the policy DynamoDBCleanupPolicy, then create it.",
                            "To set up EventBridge Scheduler as a trigger (console)",
                            "  1 : Open the EventBridge console.",
                            "  2 : In the left navigation pane, choose Schedulers under the                        Scheduler section.",
                            "  3 : Choose Create schedule.",
                            "  4 : Configure the schedule by doing the following:Under Schedule name, enter a name for your schedule (for example, DynamoDBCleanupSchedule).Under Schedule pattern, choose Recurring schedule.For Schedule type leave the default as Cron-based schedule,                          then enter the following schedule details:Minutes: 0Hours: 3Day of month: 1Month: *Day of the week: ?Year: *When evaluated, this cron expression runs on the first day of every month at 03:00 AM.For Flexible time window, select Off.",
                            "  5 : Choose Next.",
                            "  6 : Configure the trigger for your Lambda function by doing the following:In the Target detail pane, leave Target API set to Templated targets,                         then select AWS Lambda Invoke.Under Invoke, select your Lambda function (ScheduledDBMaintenance) from the dropdown list.Leave the Payload empty and choose Next.Scroll down to Permissions and select Create a new role for this schedule.                            When you create a new EventBridge Scheduler schedule using the console, EventBridge Scheduler creates a new policy with the required                          permissions the schedule needs to invoke your function. For more information about managing your schedule permissions, see                          Cron-based schedules.                          in the EventBridge Scheduler User Guide.Choose Next.",
                            "  7 : Review your settings and choose Create schedule to complete creation of the schedule and Lambda trigger."
                        ]
                    },
                    {
                        "sub_header": "Testing the app",
                        "content": [
                            "      To test that your schedule correctly triggers your function, and that your function correctly cleans records      from the database, you can temporarily modify your schedule to run once at a specific time. You can then run sam deploy again to      reset your recurrence schedule to run once a month.    ",
                            "To run the application using the AWS Management Console",
                            "  1 : Navigate back to the EventBridge Scheduler console page.",
                            "  2 : Choose your schedule, then choose Edit.",
                            "  3 : In the Schedule pattern section, under Recurrence, choose One-time schedule.",
                            "  4 :           Set your invocation time to a few minutes from now, review your settings, then choose Save.        ",
                            "      After the schedule runs and invokes its target, you run the test_app.py script to verify that your function successfully removed all old records      from the DynamoDB table.    ",
                            "To verify that old records are deleted using a Python script",
                            "  1 :           In your command line windown, navigate to the folder where you saved test_app.py.        ",
                            " 2 :           Run the script.                   If successful, you will see the following output.        Total number of old records: 0",
                            {
                                "code_example": "python test_app.py"
                            }
                        ]
                    },
                    {
                        "sub_header": "Next steps",
                        "content": [
                            "      You can now modify the EventBridge Scheduler schedule to meet your partifuclar application requirements. EventBridge Scheduler supports the following schedule expressions: cron, rate, and one-time schedules.    ",
                            "      For more information about EventBridge Scheduler schedule expresssions, see Schedule types in the      EventBridge Scheduler User Guide.    "
                        ]
                    }
                ]
            }
        ],
        "source": "Aws",
        "parent_content": [
            "The following examples provide function code and  infrastructure as code (IaC) templates to quickly create and deploy serverless apps that implement some common Lambda uses cases. The  examples also include code examples and instructions to test the apps after you deploy them.",
            "For each of the example apps, we provide instructions to either create and configure resources manually using the AWS Management Console, or to   use the AWS Serverless Application Model to deploy the resources using IaC. Follow the console intructions to learn more about configuring the individual AWS   resources for each app, or use to AWS SAM to quickly deploy resources as you would in a production environment.",
            "You can use the provided examples as a basis for your own serverless applications by modifying the provided function code and templates   for your own use case.",
            "We're continuing to create new examples, so check back again to find more severless apps for common Lambda use cases.",
            {
                "sub_header": "Example apps",
                "content": [
                    "  1.Example serverless file-processing appCreate a serverless app to automatically perform a file-processing task when an object is uploaded to an Amazon S3 bucket. In this         example, when a PDF file is uploaded, the app encrypts the file and saves it to another S3 bucket.",
                    "  2.Example scheduled cron task appCreate an app to perform a scheduled task using a cron schedule. In this example, the app performs maintenance on a         Amazon DynamoDB table by deleting entries more than 12 months old."
                ]
            }
        ]
    },
    {
        "title": "Building with TypeScript",
        "href": "https://docs.aws.amazon.com/lambda/latest/dg/lambda-typescript.html",
        "contents": [
            {
                "title": "Handler",
                "href": "https://docs.aws.amazon.com/lambda/latest/dg/typescript-handler.html",
                "source": "Aws",
                "parent_content": [
                    "The Lambda function handler is the method in your function code that processes events. When your function is  invoked, Lambda runs the handler method. Your function runs until the handler returns a response, exits, or times out.",
                    "Topics",
                    {
                        "sub_header": "Typescript handler basics",
                        "content": [
                            "Example TypeScript handler",
                            "This example function logs the contents of the event object and returns the location of the        logs. Note the following:",
                            "  1.Before using this code in a Lambda function, you must add the @types/aws-lambda package as a development dependency. This package contains the type definitions for Lambda. When @types/aws-lambda is installed, the import statement (import ... from 'aws-lambda') imports the type definitions. It does not import the aws-lambda NPM package, which is an unrelated third-party tool. For more information, see aws-lambda in the DefinitelyTyped GitHub repository.",
                            "  2.The handler in this example is an ES module and must be designated as such in your package.json file or by using the .mjs file extension. For more information, see  see Designating a function handler as an ES module.",
                            {
                                "code_example": "import { Handler } from 'aws-lambda';\n\nexport const handler: Handler = async (event, context) => {\n    console.log('EVENT: \\n' + JSON.stringify(event, null, 2));\n    return context.logStreamName;\n};"
                            },
                            "The runtime passes arguments to the handler method. The first argument is the event object,      which contains information from the invoker. The invoker passes this information as a JSON-formatted string when it      calls Invoke, and the runtime converts it to an object. When an AWS service invokes your function, the event      structure varies by service. With TypeScript, we recommend using type      annotations for the event object. For more information, see Using types for the event object.",
                            "The second argument is the context object, which contains information      about the invocation, function, and execution environment. In the preceding example, the function gets the name of      the log stream from the context object and returns it to the invoker.",
                            "You can also use a callback argument, which is a function that you can call in non-async handlers to send a response.      We recommend that you use async/await instead of callbacks. Async/await provides improved readability, error handling,      and efficiency. For more information about the differences between async/await and callbacks, see      Using callbacks."
                        ]
                    },
                    {
                        "sub_header": "Using async/await",
                        "content": [
                            "If your code performs an asynchronous task, use the async/await pattern to make sure that the handler finishes running. Async/await is a concise and readable way to write asynchronous code in Node.js, without the need for nested callbacks or chaining promises. With async/await, you can write code that reads like synchronous code, while still being asynchronous and non-blocking.",
                            "The async keyword marks a function as asynchronous, and the await keyword pauses the execution of the function until a Promise is resolved.",
                            "Example  TypeScript function – asynchronous",
                            "This example uses fetch, which is available in the nodejs18.x runtime. Note the following:",
                            "  1.Before using this code in a Lambda function, you must add the @types/aws-lambda package as a development dependency. This package contains the type definitions for Lambda. When @types/aws-lambda is installed, the import statement (import ... from 'aws-lambda') imports the type definitions. It does not import the aws-lambda NPM package, which is an unrelated third-party tool. For more information, see aws-lambda in the DefinitelyTyped GitHub repository.",
                            "  2.The handler in this example is an ES module and must be designated as such in your package.json file or by using the .mjs file extension. For more information, see  see Designating a function handler as an ES module.",
                            {
                                "code_example": "import { APIGatewayProxyEvent, APIGatewayProxyResult } from 'aws-lambda';\nconst url = 'https://aws.amazon.com/';\nexport const lambdaHandler = async (event: APIGatewayProxyEvent): Promise<APIGatewayProxyResult> => {\n    try {\n        // fetch is available with Node.js 18\n        const res = await fetch(url);\n        return {\n            statusCode: res.status,\n            body: JSON.stringify({\n                message: await res.text(),\n            }),\n        };\n    } catch (err) {\n        console.log(err);\n        return {\n            statusCode: 500,\n            body: JSON.stringify({\n                message: 'some error happened',\n            }),\n        };\n    }\n};"
                            }
                        ]
                    },
                    {
                        "sub_header": "Using callbacks",
                        "content": [
                            "We recommend that you use async/await to declare the function handler instead of using callbacks. Async/await is a better choice for several reasons:",
                            "  1.Readability: :  Async/await code is easier to read and understand than callback code, which can quickly become difficult to follow and result in callback hell.",
                            "  2.Debugging and error handling: :  Debugging callback-based code can be difficult. The call stack can become hard to follow and errors can easily be swallowed. With async/await, you can use try/catch blocks to handle errors.",
                            "  3.Efficiency: :  Callbacks often require switching between different parts of the code. Async/await can reduce the number of context switches, resulting in more efficient code.",
                            "When you use callbacks in your handler, the function continues to execute until the event loop is empty or the    function times out. The response isn't sent to the invoker until all event loop tasks are finished. If the    function times out, an error is returned instead. You can configure the runtime to send the response immediately    by setting context.callbackWaitsForEmptyEventLoop to false.",
                            "The callback function takes two      arguments: an Error and a response. The response object must be compatible with        JSON.stringify.",
                            "Example TypeScript function with callback",
                            "This sample function receives an event from Amazon API Gateway, logs the event and context objects, and then returns a response to API Gateway. Note the following:",
                            "  1.Before using this code in a Lambda function, you must add the @types/aws-lambda package as a development dependency. This package contains the type definitions for Lambda. When @types/aws-lambda is installed, the import statement (import ... from 'aws-lambda') imports the type definitions. It does not import the aws-lambda NPM package, which is an unrelated third-party tool. For more information, see aws-lambda in the DefinitelyTyped GitHub repository.",
                            "  2.The handler in this example is an ES module and must be designated as such in your package.json file or by using the .mjs file extension. For more information, see  see Designating a function handler as an ES module.",
                            {
                                "code_example": "import { Context, APIGatewayProxyCallback, APIGatewayEvent } from 'aws-lambda';\n\nexport const lambdaHandler = (event: APIGatewayEvent, context: Context, callback: APIGatewayProxyCallback): void => {\n    console.log(`Event: ${JSON.stringify(event, null, 2)}`);\n    console.log(`Context: ${JSON.stringify(context, null, 2)}`);\n    callback(null, {\n        statusCode: 200,\n        body: JSON.stringify({\n            message: 'hello world',\n        }),\n    });\n};"
                            }
                        ]
                    },
                    {
                        "sub_header": "Using types for the event object",
                        "content": [
                            "We recommend that you don’t use the any type for the handler arguments and return type because you lose the ability to check types. Instead, generate an event using the sam local generate-event AWS Serverless Application Model CLI command, or use an open-source definition from the @types/aws-lambda package.",
                            "Generating an event using the sam local generate-event command",
                            " 1 : Generate an  Amazon Simple Storage Service (Amazon S3) proxy event. ",
                            {
                                "code_example": "sam local generate-event s3 put >> S3PutEvent.json"
                            },
                            " 2 : Use the quicktype utility to generate type definitions from the S3PutEvent.json file. ",
                            {
                                "code_example": "npm install -g quicktype\nquicktype S3PutEvent.json -o S3PutEvent.ts"
                            },
                            " 3 : Use the generated types in your code. ",
                            {
                                "code_example": "import { S3PutEvent } from './S3PutEvent';\n\nexport const lambdaHandler = async (event: S3PutEvent): Promise<void> => {\n  event.Records.map((record) => console.log(record.s3.object.key));\n};"
                            },
                            "Generating an event using an open-source definition from the @types/aws-lambda package",
                            " 1 : Add the @types/aws-lambda package as a development dependency. ",
                            {
                                "code_example": "npm install -D @types/aws-lambda"
                            },
                            " 2 : Use the types in your code. ",
                            {
                                "code_example": "import { S3Event } from \"aws-lambda\";\n\nexport const lambdaHandler = async (event: S3Event): Promise<void> => {\n  event.Records.map((record) => console.log(record.s3.object.key));\n};"
                            }
                        ]
                    },
                    {
                        "sub_header": "Code best practices for Typescript Lambda functions",
                        "content": [
                            "Adhere to the guidelines in the following list to use best coding practices when building your Lambda functions:",
                            "  1.Separate the Lambda handler from your core logic. :  This allows you to make          a more unit-testable function. In Node.js this may look like: exports.myHandler = function(event, context, callback) {\tvar foo = event.foo;\tvar bar = event.bar;\tvar result = MyLambdaFunction (foo, bar);\tcallback(null, result);}function MyLambdaFunction (foo, bar) {\t// MyLambdaFunction logic here}",
                            {
                                "code_example": "exports.myHandler = function(event, context, callback) {\n\tvar foo = event.foo;\n\tvar bar = event.bar;\n\tvar result = MyLambdaFunction (foo, bar);\n\n\tcallback(null, result);\n}\n\nfunction MyLambdaFunction (foo, bar) {\n\t// MyLambdaFunction logic here\n}"
                            },
                            "  2.Control the dependencies in your function's deployment package.  :  The          AWS Lambda execution environment contains a number of libraries. For the Node.js and Python runtimes, these include the AWS SDKs.          To enable the latest set of features and security updates, Lambda will periodically update these libraries.          These updates may introduce subtle changes to the behavior of your Lambda function. To have full control of the          dependencies your function uses, package all of your dependencies with your deployment package. ",
                            "  3.Minimize the complexity of your dependencies. :  Prefer simpler frameworks          that load quickly on execution environment startup.",
                            "  4.Minimize your deployment package size to its runtime necessities.  :  This          will reduce the amount of time that it takes for your deployment package to be downloaded and unpacked ahead          of invocation.",
                            "  1.Take advantage of execution environment reuse to improve the performance of your\n        function. :  Initialize SDK clients and database connections outside of the function handler, and        cache static assets locally in the /tmp directory. Subsequent invocations processed by        the same instance of your function can reuse these resources. This saves cost by reducing function run time.To avoid potential data leaks across invocations, don’t use the execution environment to store user data,        events, or other information with security implications. If your function relies on a mutable state that can’t        be stored in memory within the handler, consider creating a separate function or separate versions of a        function for each user.",
                            "  2.Use a keep-alive directive to maintain persistent connections. :  Lambda purges idle connections over time. Attempting to reuse an idle connection when invoking a function will result in a connection error. To maintain your persistent connection, use the keep-alive directive associated with your runtime. For an example, see Reusing Connections with Keep-Alive in Node.js.",
                            "  3.Use environment variables to pass operational parameters to your function. :  For example, if        you are writing to an Amazon S3 bucket, instead of hard-coding the bucket name you are writing to, configure the        bucket name as an environment variable.",
                            "  4.Avoid using recursive invocations :  in your Lambda function, where the function        invokes itself or initiates a process that may invoke the function again. This could lead to unintended volume of        function invocations and escalated costs. If you see an unintended volume of invocations, set the function reserved concurrency        to 0 immediately to throttle all invocations to the function, while you update the        code.",
                            "  5.Do not use non-documented, non-public APIs :  in your Lambda function code.        For AWS Lambda managed runtimes, Lambda periodically applies security and functional updates to Lambda's internal APIs.        These internal API updates may be backwards-incompatible, leading to unintended consequences such as invocation        failures if your function has a dependency on these non-public APIs. See        the API reference for a list of        publicly available APIs.",
                            "  6.Write idempotent code. :  Writing idempotent code for your functions ensures that        duplicate events are handled the same way. Your code should properly validate events and gracefully handle        duplicate events. For more information, see        How do I make          my Lambda function idempotent?."
                        ]
                    }
                ]
            },
            {
                "title": "Deploy .zip file archives",
                "href": "https://docs.aws.amazon.com/lambda/latest/dg/typescript-package.html",
                "source": "Aws",
                "parent_content": [
                    "Before you can deploy TypeScript code to AWS Lambda, you need to transpile it into JavaScript. This page explains three ways to build and deploy TypeScript code to Lambda with .zip file archives:",
                    "  1.Using AWS Serverless Application Model (AWS SAM)",
                    "  2.Using the AWS Cloud Development Kit (AWS CDK)",
                    "  3.Using the AWS Command Line Interface (AWS CLI) and esbuild",
                    "AWS SAM and AWS CDK simplify building and deploying TypeScript functions. The AWS SAM template specification provides a simple and clean syntax to describe the Lambda functions, APIs, permissions, configurations, and events that make up your serverless application. The AWS CDK lets you build reliable, scalable, cost-effective applications in the cloud with the considerable expressive power of a programming language. The AWS CDK is intended for moderately to highly experienced AWS users. Both the AWS CDK and the AWS SAM use esbuild to transpile TypeScript code into JavaScript.",
                    {
                        "sub_header": "Using AWS SAM to deploy TypeScript code to Lambda",
                        "content": [
                            "Follow the steps below to download, build, and deploy a sample Hello World TypeScript application using the AWS SAM. This application implements a basic API backend. It consists of an Amazon API Gateway endpoint and a Lambda function. When you send a GET request to the API Gateway endpoint, the Lambda function is invoked. The function returns a hello world message.",
                            "Note",
                            "AWS SAM uses esbuild to create Node.js Lambda functions from TypeScript code. esbuild support is currently in public preview. During public preview, esbuild support may be subject to backwards incompatible changes.",
                            "Prerequisites",
                            "To complete the steps in this section, you must have the following:",
                            "  1.AWS CLI version 2",
                            "  2.AWS SAM CLI version 1.75 or later",
                            "  3.Node.js 18.x",
                            "Deploy a sample AWS SAM application",
                            " 1 : Initialize the application using the Hello World TypeScript template. ",
                            {
                                "code_example": "sam init --app-template hello-world-typescript --name sam-app --package-type Zip --runtime nodejs18.x"
                            },
                            " 2 : (Optional) The sample application includes configurations for commonly used tools, such as ESLlint for code linting and Jest for unit testing. To run lint and test commands: ",
                            {
                                "code_example": "cd sam-app/hello-world\nnpm install\nnpm run lint\nnpm run test"
                            },
                            " 3 : Build the app. ",
                            {
                                "code_example": "cd sam-app\nsam build"
                            },
                            " 4 : Deploy the app. ",
                            {
                                "code_example": "sam deploy --guided"
                            },
                            "  5 : Follow the on-screen prompts. To accept the default options provided in the interactive experience, respond with Enter.",
                            " 6 : The output shows the endpoint for the REST API. Open the endpoint in a browser to test the function. You should see this response: ",
                            {
                                "code_example": "{\"message\":\"hello world\"}"
                            },
                            " 7 : This is a public API endpoint that is accessible over the internet. We recommend that you delete the endpoint after testing. ",
                            {
                                "code_example": "sam delete"
                            }
                        ]
                    },
                    {
                        "sub_header": "Using the AWS CDK to deploy TypeScript code to Lambda",
                        "content": [
                            "Follow the steps below to build and deploy a sample TypeScript application using the AWS CDK. This application implements a basic API backend. It consists of an API Gateway endpoint and a Lambda function. When you send a GET request to the API Gateway endpoint, the Lambda function is invoked. The function returns a hello world message.",
                            "Prerequisites",
                            "To complete the steps in this section, you must have the following:",
                            "  1.AWS CLI version 2",
                            "  2.AWS CDK version 2",
                            "  3.Node.js 18.x",
                            "  4.Either Docker or esbuild",
                            "Deploy a sample AWS CDK application",
                            " 1 : Create a project directory for your new application. ",
                            {
                                "code_example": "mkdir hello-world\ncd hello-world"
                            },
                            " 2 : Initialize the app. ",
                            {
                                "code_example": "cdk init app --language typescript"
                            },
                            " 3 : Add the @types/aws-lambda package as a development dependency. This package contains the type definitions for Lambda. ",
                            {
                                "code_example": "npm install -D @types/aws-lambda"
                            },
                            "  4 : Open the lib directory. You should see a file called hello-world-stack.ts. Create two new files in this directory: hello-world.function.ts and hello-world.ts.",
                            " 5 : Open hello-world.function.ts and add the following code to the file. This is the code for the Lambda function.NoteThe import statement imports the type definitions from @types/aws-lambda. It does not import the aws-lambda NPM package, which is an unrelated third-party tool. For more information, see aws-lambda in the DefinitelyTyped GitHub repository. ",
                            {
                                "code_example": "import { Context, APIGatewayProxyResult, APIGatewayEvent } from 'aws-lambda';\n\nexport const handler = async (event: APIGatewayEvent, context: Context): Promise<APIGatewayProxyResult> => {\n    console.log(`Event: ${JSON.stringify(event, null, 2)}`);\n    console.log(`Context: ${JSON.stringify(context, null, 2)}`);\n    return {\n        statusCode: 200,\n        body: JSON.stringify({\n            message: 'hello world',\n        }),\n    };\n};"
                            },
                            " 6 : Open hello-world.ts and add the following code to the file. This contains the NodejsFunction construct, which creates the Lambda function, and the LambdaRestApi construct, which creates the REST API. The NodejsFunction construct assumes the following by default:Your function handler is called handler.The .ts file that contains the function code (hello-world.function.ts) is in the same directory as the .ts file that contains the construct (hello-world.ts). The construct uses the construct's ID (\"hello-world\") and the name of the Lambda handler file (\"function\") to find the function code. For example, if your function code is in a file called hello-world.my-function.ts, the hello-world.ts file must reference the function code like this:const helloFunction = new NodejsFunction(this, 'my-function');You can change this behavior and configure other esbuild parameters. For more information, see Configuring esbuild in the AWS CDK API reference.",
                            {
                                "code_example": "import { Construct } from 'constructs';\nimport { NodejsFunction } from 'aws-cdk-lib/aws-lambda-nodejs';\nimport { LambdaRestApi } from 'aws-cdk-lib/aws-apigateway';\n  \nexport class HelloWorld extends Construct {\n  constructor(scope: Construct, id: string) {\n    super(scope, id);\n    const helloFunction = new NodejsFunction(this, 'function');\n    new LambdaRestApi(this, 'apigw', {\n      handler: helloFunction,\n    });\n  }\n}"
                            },
                            " 7 : Open hello-world-stack.ts. This is the code that defines your AWS CDK stack. Replace the code with the following: ",
                            {
                                "code_example": "import { Stack, StackProps } from 'aws-cdk-lib';\nimport { Construct } from 'constructs';\nimport { HelloWorld } from './hello-world';\n  \nexport class HelloWorldStack extends Stack {\n  constructor(scope: Construct, id: string, props?: StackProps) {\n    super(scope, id, props);\n    new HelloWorld(this, 'hello-world');\n  }\n}"
                            },
                            " 8 : from the hello-world directory containing your cdk.json file, deploy your           application. ",
                            {
                                "code_example": "cdk deploy"
                            },
                            " 9 : The AWS CDK builds and packages the Lambda function using esbuild, and then deploys the function to the Lambda runtime. The output shows the endpoint for the REST API. Open the endpoint in a browser to test the function. You should see this response: This is a public API endpoint that is accessible over the internet. We recommend that you delete the endpoint after testing.",
                            {
                                "code_example": "{\"message\":\"hello world\"}"
                            }
                        ]
                    },
                    {
                        "sub_header": "Using the AWS CLI and esbuild to deploy TypeScript code to Lambda",
                        "content": [
                            "The following example demonstrates how to transpile and deploy TypeScript code to Lambda using esbuild and the AWS CLI. esbuild produces one JavaScript file with all dependencies. This is the only file that you need to add to the .zip archive.",
                            "Prerequisites",
                            "To complete the steps in this section, you must have the following:",
                            "  1.AWS CLI version 2",
                            "  2.Node.js 18.x",
                            "  3.An execution role for the Lambda function",
                            "  4.For Windows users, a zip file utility such as 7zip.",
                            "Deploy a sample function",
                            "  1 : On your local machine, create a project directory for your new function. ",
                            " 2 : Create a new Node.js project with npm or a package manager of your choice. ",
                            {
                                "code_example": "npm init"
                            },
                            " 3 : Add the @types/aws-lambda and esbuild packages as development dependencies. The @types/aws-lambda package contains the type definitions for Lambda. ",
                            {
                                "code_example": "npm install -D @types/aws-lambda esbuild"
                            },
                            " 4 : Create a new file called index.ts. Add the following code to the new file. This is the code for the Lambda function. The function returns a hello world message. The function doesn’t create any API Gateway resources.NoteThe import statement imports the type definitions from @types/aws-lambda. It does not import the aws-lambda NPM package, which is an unrelated third-party tool. For more information, see aws-lambda in the DefinitelyTyped GitHub repository. ",
                            {
                                "code_example": "import { Context, APIGatewayProxyResult, APIGatewayEvent } from 'aws-lambda';\n\nexport const handler = async (event: APIGatewayEvent, context: Context): Promise<APIGatewayProxyResult> => {\n  console.log(`Event: ${JSON.stringify(event, null, 2)}`);\n  console.log(`Context: ${JSON.stringify(context, null, 2)}`);\n  return {\n      statusCode: 200,\n      body: JSON.stringify({\n          message: 'hello world',\n      }),\n   };\n};"
                            },
                            " 5 : Add a build script to the package.json file. This configures esbuild to automatically create the .zip deployment package. For more information, see Build scripts in the esbuild documentation.Linux and MacOS WindowsIn this example, the \"postbuild\" command uses the 7zip utility to       create your .zip file. Use your own preferred Windows zip utility and modify the command as necessary.\"scripts\": {  \"prebuild\": \"del /q dist\",  \"build\": \"esbuild index.ts --bundle --minify --sourcemap --platform=node --target=es2020 --outfile=dist/index.js\",  \"postbuild\": \"cd dist && 7z a -tzip index.zip index.js*\"},anchoranchorLinux and MacOSWindows ",
                            {
                                "code_example": "\"scripts\": {\n  \"prebuild\": \"rm -rf dist\",\n  \"build\": \"esbuild index.ts --bundle --minify --sourcemap --platform=node --target=es2020 --outfile=dist/index.js\",\n  \"postbuild\": \"cd dist && zip -r index.zip index.js*\"\n},"
                            },
                            " 6 : Build the package. ",
                            {
                                "code_example": "npm run build"
                            },
                            " 7 : Create a Lambda function using the .zip deployment package. Replace the highlighted text with the Amazon Resource Name (ARN) of your execution role. ",
                            {
                                "code_example": "aws lambda create-function --function-name hello-world --runtime \"nodejs18.x\" --role arn:aws:iam::123456789012:role/lambda-ex --zip-file \"fileb://dist/index.zip\" --handler index.handler"
                            },
                            " 8 : Run a test event to confirm that the function returns the following response. If you want to invoke this function using API Gateway, create and configure a REST API. ",
                            {
                                "code_example": "{\n  \"statusCode\": 200,\n  \"body\": \"{\\\"message\\\":\\\"hello world\\\"}\"\n}"
                            }
                        ]
                    }
                ]
            },
            {
                "title": "Deploy container images",
                "href": "https://docs.aws.amazon.com/lambda/latest/dg/typescript-image.html",
                "source": "Aws",
                "parent_content": [
                    "You can deploy your TypeScript code to an AWS Lambda function as a Node.js container image. AWS provides base images for Node.js to help you build the container image. These base images are preloaded with a language runtime and other components that are required to run the image on Lambda. AWS provides a Dockerfile for each of the base images to help with building your container image.",
                    "If you use a community or private enterprise base image, you must add the Node.js runtime interface client (RIC) to the base image to make it compatible with Lambda.",
                    "Lambda provides a runtime interface emulator for local testing. The AWS base images for Node.js include the runtime interface emulator. If you use an alternative base image, such as an Alpine Linux or Debian image, you can build the emulator into your image or install it on your local machine.",
                    {
                        "sub_header": "Using a Node.js base image to build and package TypeScript function code",
                        "content": [
                            "To complete the steps in this section, you must have the following:",
                            "  1.AWS CLI version 2",
                            "  2.Docker",
                            "  3.Node.js 22.x",
                            "Prerequisites",
                            "To complete the steps in this section, you must have the following:",
                            "  1.AWS CLI version 2",
                            "  2.Docker",
                            "  3.Node.js 22.x",
                            "To create an image from an AWS base image for Lambda",
                            "  1 : On your local machine, create a project directory for your new function.",
                            " 2 : Create a new Node.js project with npm or a package manager of your choice. ",
                            {
                                "code_example": "npm init"
                            },
                            " 3 : Add the @types/aws-lambda and esbuild packages as development dependencies. The @types/aws-lambda package contains the type definitions for Lambda. ",
                            {
                                "code_example": "npm install -D @types/aws-lambda esbuild"
                            },
                            " 4 : Add a build script to the package.json file. ",
                            {
                                "code_example": "  \"scripts\": {\n  \"build\": \"esbuild index.ts --bundle --minify --sourcemap --platform=node --target=es2020 --outfile=dist/index.js\"\n}"
                            },
                            " 5 : Create a new file called index.ts. Add the following sample code to the new file. This is the code for the Lambda function. The function returns a hello world message.NoteThe import statement imports the type definitions from @types/aws-lambda. It does not import the aws-lambda NPM package, which is an unrelated third-party tool. For more information, see aws-lambda in the DefinitelyTyped GitHub repository. ",
                            {
                                "code_example": "import { Context, APIGatewayProxyResult, APIGatewayEvent } from 'aws-lambda';\n\nexport const handler = async (event: APIGatewayEvent, context: Context): Promise<APIGatewayProxyResult> => {\n    console.log(`Event: ${JSON.stringify(event, null, 2)}`);\n    console.log(`Context: ${JSON.stringify(context, null, 2)}`);\n    return {\n        statusCode: 200,\n        body: JSON.stringify({\n            message: 'hello world',\n        }),\n    };\n};"
                            },
                            " 6 : Create a new Dockerfile with the following configuration:Set the FROM property to the URI of the base image.Set the CMD argument to specify the Lambda function handler.The following example Dockerfile uses a multi-stage build. The first step transpiles the TypeScript code into JavaScript. The second step produces a container image that contains only JavaScript files and production dependencies.Note that the example Dockerfile does not include a USER instruction. When you deploy a container image to Lambda, Lambda automatically defines a default Linux user with least-privileged permissions. This is different from standard Docker behavior which defaults to the root user when no USER instruction is provided.Example  Dockerfile ",
                            {
                                "code_example": "FROM public.ecr.aws/lambda/nodejs:22 as builder\nWORKDIR /usr/app\nCOPY package.json index.ts  ./\nRUN npm install\nRUN npm run build\n    \nFROM public.ecr.aws/lambda/nodejs:22\nWORKDIR ${LAMBDA_TASK_ROOT}\nCOPY --from=builder /usr/app/dist/* ./\nCMD [\"index.handler\"]"
                            },
                            " 7 : Build the Docker image with the docker build command. The          following example names the image docker-image and gives it the test tag. NoteThe command specifies the --platform linux/amd64 option to ensure that your container is compatible with the Lambda execution environment regardless of the         architecture of your build machine. If you intend to create a Lambda function using the ARM64 instruction set architecture, be sure to change the command to use the --platform linux/arm64        option instead.",
                            {
                                "code_example": "docker build --platform linux/amd64 -t docker-image:test ."
                            },
                            "Creating an image from a base image",
                            "To create an image from an AWS base image for Lambda",
                            "  1 : On your local machine, create a project directory for your new function.",
                            " 2 : Create a new Node.js project with npm or a package manager of your choice. ",
                            {
                                "code_example": "npm init"
                            },
                            " 3 : Add the @types/aws-lambda and esbuild packages as development dependencies. The @types/aws-lambda package contains the type definitions for Lambda. ",
                            {
                                "code_example": "npm install -D @types/aws-lambda esbuild"
                            },
                            " 4 : Add a build script to the package.json file. ",
                            {
                                "code_example": "  \"scripts\": {\n  \"build\": \"esbuild index.ts --bundle --minify --sourcemap --platform=node --target=es2020 --outfile=dist/index.js\"\n}"
                            },
                            " 5 : Create a new file called index.ts. Add the following sample code to the new file. This is the code for the Lambda function. The function returns a hello world message.NoteThe import statement imports the type definitions from @types/aws-lambda. It does not import the aws-lambda NPM package, which is an unrelated third-party tool. For more information, see aws-lambda in the DefinitelyTyped GitHub repository. ",
                            {
                                "code_example": "import { Context, APIGatewayProxyResult, APIGatewayEvent } from 'aws-lambda';\n\nexport const handler = async (event: APIGatewayEvent, context: Context): Promise<APIGatewayProxyResult> => {\n    console.log(`Event: ${JSON.stringify(event, null, 2)}`);\n    console.log(`Context: ${JSON.stringify(context, null, 2)}`);\n    return {\n        statusCode: 200,\n        body: JSON.stringify({\n            message: 'hello world',\n        }),\n    };\n};"
                            },
                            " 6 : Create a new Dockerfile with the following configuration:Set the FROM property to the URI of the base image.Set the CMD argument to specify the Lambda function handler.The following example Dockerfile uses a multi-stage build. The first step transpiles the TypeScript code into JavaScript. The second step produces a container image that contains only JavaScript files and production dependencies.Note that the example Dockerfile does not include a USER instruction. When you deploy a container image to Lambda, Lambda automatically defines a default Linux user with least-privileged permissions. This is different from standard Docker behavior which defaults to the root user when no USER instruction is provided.Example  Dockerfile ",
                            {
                                "code_example": "FROM public.ecr.aws/lambda/nodejs:22 as builder\nWORKDIR /usr/app\nCOPY package.json index.ts  ./\nRUN npm install\nRUN npm run build\n    \nFROM public.ecr.aws/lambda/nodejs:22\nWORKDIR ${LAMBDA_TASK_ROOT}\nCOPY --from=builder /usr/app/dist/* ./\nCMD [\"index.handler\"]"
                            },
                            " 7 : Build the Docker image with the docker build command. The          following example names the image docker-image and gives it the test tag. NoteThe command specifies the --platform linux/amd64 option to ensure that your container is compatible with the Lambda execution environment regardless of the         architecture of your build machine. If you intend to create a Lambda function using the ARM64 instruction set architecture, be sure to change the command to use the --platform linux/arm64        option instead.",
                            {
                                "code_example": "docker build --platform linux/amd64 -t docker-image:test ."
                            },
                            " 1 : Start the Docker image with the docker run command. In this example,              docker-image is the image name and test is the tag. This command runs the image as a container and creates a local endpoint at            localhost:9000/2015-03-31/functions/function/invocations.NoteIf you built the Docker image for the ARM64 instruction set architecture, be sure to use the --platform linux/arm64 option instead of --platform linux/amd64.",
                            {
                                "code_example": "docker run --platform linux/amd64 -p 9000:8080 docker-image:test"
                            },
                            " 2 : From a new terminal window, post an event to the local endpoint.Linux/macOSIn Linux and macOS, run the following curl command: This command invokes the function with an empty event and returns a response. If you're using your own function code rather than the sample function code, you might want to invoke the function with a JSON payload. Example:curl \"http://localhost:9000/2015-03-31/functions/function/invocations\" -d '{\"payload\":\"hello world!\"}'PowerShellIn PowerShell, run the following Invoke-WebRequest command:Invoke-WebRequest -Uri \"http://localhost:9000/2015-03-31/functions/function/invocations\" -Method Post -Body '{}' -ContentType \"application/json\"This command invokes the function with an empty event and returns a response. If you're using your own function code rather than the sample function code, you might want to invoke the function with a JSON payload. Example:Invoke-WebRequest -Uri \"http://localhost:9000/2015-03-31/functions/function/invocations\" -Method Post -Body '{\"payload\":\"hello world!\"}' -ContentType \"application/json\"",
                            {
                                "code_example": "curl \"http://localhost:9000/2015-03-31/functions/function/invocations\" -d '{}'"
                            },
                            " 3 : Get the container ID. ",
                            {
                                "code_example": "docker ps"
                            },
                            " 4 : Use the docker kill command to stop the container. In this command, replace 3766c4ab331c with the container ID from the previous step. ",
                            {
                                "code_example": "docker kill 3766c4ab331c"
                            },
                            "(Optional) Test the image locally",
                            " 1 : Start the Docker image with the docker run command. In this example,              docker-image is the image name and test is the tag. This command runs the image as a container and creates a local endpoint at            localhost:9000/2015-03-31/functions/function/invocations.NoteIf you built the Docker image for the ARM64 instruction set architecture, be sure to use the --platform linux/arm64 option instead of --platform linux/amd64.",
                            {
                                "code_example": "docker run --platform linux/amd64 -p 9000:8080 docker-image:test"
                            },
                            " 2 : From a new terminal window, post an event to the local endpoint.Linux/macOSIn Linux and macOS, run the following curl command: This command invokes the function with an empty event and returns a response. If you're using your own function code rather than the sample function code, you might want to invoke the function with a JSON payload. Example:curl \"http://localhost:9000/2015-03-31/functions/function/invocations\" -d '{\"payload\":\"hello world!\"}'PowerShellIn PowerShell, run the following Invoke-WebRequest command:Invoke-WebRequest -Uri \"http://localhost:9000/2015-03-31/functions/function/invocations\" -Method Post -Body '{}' -ContentType \"application/json\"This command invokes the function with an empty event and returns a response. If you're using your own function code rather than the sample function code, you might want to invoke the function with a JSON payload. Example:Invoke-WebRequest -Uri \"http://localhost:9000/2015-03-31/functions/function/invocations\" -Method Post -Body '{\"payload\":\"hello world!\"}' -ContentType \"application/json\"anchoranchorLinux/macOSPowerShellIn Linux and macOS, run the following curl command: This command invokes the function with an empty event and returns a response. If you're using your own function code rather than the sample function code, you might want to invoke the function with a JSON payload. Example:curl \"http://localhost:9000/2015-03-31/functions/function/invocations\" -d '{\"payload\":\"hello world!\"}'",
                            {
                                "code_example": "curl \"http://localhost:9000/2015-03-31/functions/function/invocations\" -d '{}'"
                            },
                            " 3 : Get the container ID. ",
                            {
                                "code_example": "docker ps"
                            },
                            " 4 : Use the docker kill command to stop the container. In this command, replace 3766c4ab331c with the container ID from the previous step. ",
                            {
                                "code_example": "docker kill 3766c4ab331c"
                            },
                            "To upload the image to Amazon ECR and create the Lambda function",
                            " 1 : Run the get-login-password command to authenticate the Docker CLI to your Amazon ECR registry.Set the --region value to the AWS Region where you want to create the Amazon ECR repository.Replace 111122223333 with your AWS account ID. ",
                            {
                                "code_example": "aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin 111122223333.dkr.ecr.us-east-1.amazonaws.com"
                            },
                            " 2 : Create a repository in Amazon ECR using the create-repository command. NoteThe Amazon ECR repository must be in the same AWS Region as the Lambda function.If successful, you see a response like this:{    \"repository\": {        \"repositoryArn\": \"arn:aws:ecr:us-east-1:111122223333:repository/hello-world\",        \"registryId\": \"111122223333\",        \"repositoryName\": \"hello-world\",        \"repositoryUri\": \"111122223333.dkr.ecr.us-east-1.amazonaws.com/hello-world\",        \"createdAt\": \"2023-03-09T10:39:01+00:00\",        \"imageTagMutability\": \"MUTABLE\",        \"imageScanningConfiguration\": {            \"scanOnPush\": true        },        \"encryptionConfiguration\": {            \"encryptionType\": \"AES256\"        }    }}",
                            {
                                "code_example": "aws ecr create-repository --repository-name hello-world --region us-east-1 --image-scanning-configuration scanOnPush=true --image-tag-mutability MUTABLE"
                            },
                            "  3 : Copy the repositoryUri from the output in the previous step.",
                            " 4 : Run the docker tag command to tag your local image into your Amazon ECR repository as the latest version. In this command:docker-image:test is the name and tag of your Docker image. This is the image name and tag that you specified in the docker build command.Replace <ECRrepositoryUri> with the repositoryUri that you copied. Make sure to include :latest at the end of the URI. Example:docker tag docker-image:test 111122223333.dkr.ecr.us-east-1.amazonaws.com/hello-world:latest",
                            {
                                "code_example": "docker tag docker-image:test <ECRrepositoryUri>:latest"
                            },
                            " 5 : Run the docker push command to deploy your local image to the Amazon ECR repository. Make sure to include :latest at the end of the repository URI. ",
                            {
                                "code_example": "docker push 111122223333.dkr.ecr.us-east-1.amazonaws.com/hello-world:latest"
                            },
                            "  6 : Create an execution role for the function, if you don't already have one. You need the Amazon Resource Name (ARN) of the role in the next step.",
                            " 7 : Create the Lambda function. For ImageUri, specify the repository URI from earlier. Make sure to include :latest at the end of the URI. NoteYou can create a function using an image in a different AWS account, as long as the image is in the same Region as the Lambda function. For more information, see  Amazon ECR cross-account permissions.",
                            {
                                "code_example": "aws lambda create-function \\\n  --function-name hello-world \\\n  --package-type Image \\\n  --code ImageUri=111122223333.dkr.ecr.us-east-1.amazonaws.com/hello-world:latest \\\n  --role arn:aws:iam::111122223333:role/lambda-ex"
                            },
                            " 8 : Invoke the function. You should see a response like this:{  \"ExecutedVersion\": \"$LATEST\",   \"StatusCode\": 200}",
                            {
                                "code_example": "aws lambda invoke --function-name hello-world response.json"
                            },
                            "  9 : To see the output of the function, check the response.json file.",
                            "To update the function code, you must build the image again, upload the new image to the Amazon ECR repository, and then use the update-function-code command to deploy the image to the Lambda function.",
                            "Lambda resolves the image tag to a specific image digest. This means that if you point the image tag that was used to deploy the function to a new image in Amazon ECR, Lambda doesn't automatically update the function to use the new image.",
                            "To deploy the new image to the same Lambda function, you must use the update-function-code command, even if the image tag in Amazon ECR remains the same. In the following example, the --publish option creates a new version of the function using the updated container image.",
                            {
                                "code_example": "aws lambda update-function-code \\\n  --function-name hello-world \\\n  --image-uri 111122223333.dkr.ecr.us-east-1.amazonaws.com/hello-world:latest \\\n  --publish"
                            },
                            "Deploying the image",
                            "To upload the image to Amazon ECR and create the Lambda function",
                            " 1 : Run the get-login-password command to authenticate the Docker CLI to your Amazon ECR registry.Set the --region value to the AWS Region where you want to create the Amazon ECR repository.Replace 111122223333 with your AWS account ID. ",
                            {
                                "code_example": "aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin 111122223333.dkr.ecr.us-east-1.amazonaws.com"
                            },
                            " 2 : Create a repository in Amazon ECR using the create-repository command. NoteThe Amazon ECR repository must be in the same AWS Region as the Lambda function.If successful, you see a response like this:{    \"repository\": {        \"repositoryArn\": \"arn:aws:ecr:us-east-1:111122223333:repository/hello-world\",        \"registryId\": \"111122223333\",        \"repositoryName\": \"hello-world\",        \"repositoryUri\": \"111122223333.dkr.ecr.us-east-1.amazonaws.com/hello-world\",        \"createdAt\": \"2023-03-09T10:39:01+00:00\",        \"imageTagMutability\": \"MUTABLE\",        \"imageScanningConfiguration\": {            \"scanOnPush\": true        },        \"encryptionConfiguration\": {            \"encryptionType\": \"AES256\"        }    }}",
                            {
                                "code_example": "aws ecr create-repository --repository-name hello-world --region us-east-1 --image-scanning-configuration scanOnPush=true --image-tag-mutability MUTABLE"
                            },
                            "  3 : Copy the repositoryUri from the output in the previous step.",
                            " 4 : Run the docker tag command to tag your local image into your Amazon ECR repository as the latest version. In this command:docker-image:test is the name and tag of your Docker image. This is the image name and tag that you specified in the docker build command.Replace <ECRrepositoryUri> with the repositoryUri that you copied. Make sure to include :latest at the end of the URI. Example:docker tag docker-image:test 111122223333.dkr.ecr.us-east-1.amazonaws.com/hello-world:latest",
                            {
                                "code_example": "docker tag docker-image:test <ECRrepositoryUri>:latest"
                            },
                            " 5 : Run the docker push command to deploy your local image to the Amazon ECR repository. Make sure to include :latest at the end of the repository URI. ",
                            {
                                "code_example": "docker push 111122223333.dkr.ecr.us-east-1.amazonaws.com/hello-world:latest"
                            },
                            "  6 : Create an execution role for the function, if you don't already have one. You need the Amazon Resource Name (ARN) of the role in the next step.",
                            " 7 : Create the Lambda function. For ImageUri, specify the repository URI from earlier. Make sure to include :latest at the end of the URI. NoteYou can create a function using an image in a different AWS account, as long as the image is in the same Region as the Lambda function. For more information, see  Amazon ECR cross-account permissions.",
                            {
                                "code_example": "aws lambda create-function \\\n  --function-name hello-world \\\n  --package-type Image \\\n  --code ImageUri=111122223333.dkr.ecr.us-east-1.amazonaws.com/hello-world:latest \\\n  --role arn:aws:iam::111122223333:role/lambda-ex"
                            },
                            " 8 : Invoke the function. You should see a response like this:{  \"ExecutedVersion\": \"$LATEST\",   \"StatusCode\": 200}",
                            {
                                "code_example": "aws lambda invoke --function-name hello-world response.json"
                            },
                            "  9 : To see the output of the function, check the response.json file.",
                            "To update the function code, you must build the image again, upload the new image to the Amazon ECR repository, and then use the update-function-code command to deploy the image to the Lambda function.",
                            "Lambda resolves the image tag to a specific image digest. This means that if you point the image tag that was used to deploy the function to a new image in Amazon ECR, Lambda doesn't automatically update the function to use the new image.",
                            "To deploy the new image to the same Lambda function, you must use the update-function-code command, even if the image tag in Amazon ECR remains the same. In the following example, the --publish option creates a new version of the function using the updated container image.",
                            {
                                "code_example": "aws lambda update-function-code \\\n  --function-name hello-world \\\n  --image-uri 111122223333.dkr.ecr.us-east-1.amazonaws.com/hello-world:latest \\\n  --publish"
                            }
                        ]
                    }
                ]
            },
            {
                "title": "Layers",
                "href": "https://docs.aws.amazon.com/lambda/latest/dg/typescript-layers.html",
                "source": "Aws",
                "parent_content": []
            },
            {
                "title": "Context",
                "href": "https://docs.aws.amazon.com/lambda/latest/dg/typescript-context.html",
                "source": "Aws",
                "parent_content": []
            },
            {
                "title": "Logging",
                "href": "https://docs.aws.amazon.com/lambda/latest/dg/typescript-logging.html",
                "source": "Aws",
                "parent_content": []
            },
            {
                "title": "Tracing",
                "href": "https://docs.aws.amazon.com/lambda/latest/dg/typescript-tracing.html",
                "source": "Aws",
                "parent_content": []
            }
        ],
        "source": "Aws",
        "parent_content": [
            "You can use the Node.js runtime to run TypeScript code in AWS Lambda. Because Node.js doesn't run TypeScript code natively, you must first     transpile your TypeScript code into JavaScript. Then, use the JavaScript files to deploy your function code to Lambda. Your code runs in an     environment that includes the AWS SDK for JavaScript, with credentials from an AWS Identity and Access Management (IAM) role that you manage. To learn more     about the SDK versions included with the Node.js runtimes, see Runtime-included SDK versions.",
            "Lambda supports the following Node.js runtimes.",
            "NameIdentifierOperating systemDeprecation dateBlock function createBlock function updateNode.js 22nodejs22.xAmazon Linux 2023            Not scheduled                        Not scheduled                        Not scheduled            Node.js 20nodejs20.xAmazon Linux 2023            Not scheduled                        Not scheduled                        Not scheduled            Node.js 18nodejs18.xAmazon Linux 2            Jul 31, 2025                        Sep 1, 2025                        Oct 1, 2025            ",
            "Topics",
            {
                "sub_header": "Setting up a TypeScript development environment",
                "content": [
                    "Use a local integrated development environment (IDE), text editor, or AWS Cloud9 to write your TypeScript function code. You can’t create TypeScript code on the Lambda console.",
                    "To transpile your TypeScript code, set up a compiler such as esbuild or Microsoft's TypeScript compiler (tsc) , which is bundled with the TypeScript distribution. You can use the AWS Serverless Application Model (AWS SAM) or the AWS Cloud Development Kit (AWS CDK) to simplify building and deploying TypeScript code. Both tools use esbuild to transpile TypeScript code into JavaScript.",
                    "When using esbuild, consider the following:",
                    "  1.There are several TypeScript caveats.",
                    "  2.tsconfig.json : You must configure your TypeScript transpilation settings to match the Node.js runtime that you plan to use. For more information, see Target in the esbuild documentation. For an example of a  file that demonstrates how to target a specific Node.js version supported by Lambda, refer to the TypeScript GitHub repository.",
                    "  3.tsconfig.json : esbuild doesn’t perform type checks. To check types, use the tsc compiler. Run tsc -noEmit or add a \"noEmit\" parameter to your  file, as shown in the following example. This configures tsc to not emit JavaScript files. After checking types, use esbuild to convert the TypeScript files into JavaScript.",
                    "Example  tsconfig.json",
                    {
                        "code_example": " {\n  \"compilerOptions\": {\n    \"target\": \"es2020\",\n    \"strict\": true,\n    \"preserveConstEnums\": true,\n    \"noEmit\": true,\n    \"sourceMap\": false,\n    \"module\":\"commonjs\",\n    \"moduleResolution\":\"node\",\n    \"esModuleInterop\": true, \n    \"skipLibCheck\": true,\n    \"forceConsistentCasingInFileNames\": true, \n    \"isolatedModules\": true, \n  },\n  \"exclude\": [\"node_modules\", \"**/*.test.ts\"]\n}"
                    }
                ]
            }
        ]
    },
    {
        "title": "Integrating other services",
        "href": "https://docs.aws.amazon.com/lambda/latest/dg/lambda-services.html",
        "contents": [
            {
                "title": "Apache Kafka",
                "href": "https://docs.aws.amazon.com/lambda/latest/dg/with-kafka.html",
                "contents": [
                    {
                        "title": "Configure event source",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/with-kafka-configure.html",
                        "source": "Aws",
                        "parent_content": []
                    },
                    {
                        "title": "Process messages",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/with-kafka-process.html",
                        "source": "Aws",
                        "parent_content": []
                    },
                    {
                        "title": "Event filtering",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/with-kafka-filtering.html",
                        "source": "Aws",
                        "parent_content": []
                    },
                    {
                        "title": "On-failure destinations",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/with-kafka-on-failure.html",
                        "source": "Aws",
                        "parent_content": []
                    },
                    {
                        "title": "Troubleshooting",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/with-kafka-troubleshoot.html",
                        "source": "Aws",
                        "parent_content": []
                    }
                ],
                "source": "Aws",
                "parent_content": []
            },
            {
                "title": "API Gateway",
                "href": "https://docs.aws.amazon.com/lambda/latest/dg/services-apigateway.html",
                "contents": [
                    {
                        "title": "Tutorial",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/services-apigateway-tutorial.html",
                        "source": "Aws",
                        "parent_content": []
                    },
                    {
                        "title": "Errors",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/services-apigateway-errors.html",
                        "source": "Aws",
                        "parent_content": []
                    },
                    {
                        "title": "Select an HTTP invoke method for Lambda",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/apig-http-invoke-decision.html",
                        "source": "Aws",
                        "parent_content": []
                    }
                ],
                "source": "Aws",
                "parent_content": []
            },
            {
                "title": "Infrastructure Composer",
                "href": "https://docs.aws.amazon.com/lambda/latest/dg/services-appcomposer.html",
                "source": "Aws",
                "parent_content": []
            },
            {
                "title": "CloudFormation",
                "href": "https://docs.aws.amazon.com/lambda/latest/dg/services-cloudformation.html",
                "source": "Aws",
                "parent_content": []
            },
            {
                "title": "Amazon DocumentDB",
                "href": "https://docs.aws.amazon.com/lambda/latest/dg/with-documentdb.html",
                "contents": [
                    {
                        "title": "Tutorial",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/with-documentdb-tutorial.html",
                        "source": "Aws",
                        "parent_content": []
                    }
                ],
                "source": "Aws",
                "parent_content": []
            },
            {
                "title": "DynamoDB",
                "href": "https://docs.aws.amazon.com/lambda/latest/dg/with-ddb.html",
                "contents": [
                    {
                        "title": "Create mapping",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/services-dynamodb-eventsourcemapping.html",
                        "source": "Aws",
                        "parent_content": []
                    },
                    {
                        "title": "Batch item failures",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/services-ddb-batchfailurereporting.html",
                        "source": "Aws",
                        "parent_content": []
                    },
                    {
                        "title": "Error handling",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/services-dynamodb-errors.html",
                        "source": "Aws",
                        "parent_content": []
                    },
                    {
                        "title": "Stateful processing",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/services-ddb-windows.html",
                        "source": "Aws",
                        "parent_content": []
                    },
                    {
                        "title": "Parameters",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/services-ddb-params.html",
                        "source": "Aws",
                        "parent_content": []
                    },
                    {
                        "title": "Event filtering",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/with-ddb-filtering.html",
                        "source": "Aws",
                        "parent_content": []
                    },
                    {
                        "title": "Tutorial",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/with-ddb-example.html",
                        "source": "Aws",
                        "parent_content": []
                    }
                ],
                "source": "Aws",
                "parent_content": []
            },
            {
                "title": "EC2",
                "href": "https://docs.aws.amazon.com/lambda/latest/dg/services-ec2.html",
                "source": "Aws",
                "parent_content": []
            },
            {
                "title": "Elastic Load Balancing (Application Load Balancer)",
                "href": "https://docs.aws.amazon.com/lambda/latest/dg/services-alb.html",
                "source": "Aws",
                "parent_content": []
            },
            {
                "title": "Invoke using an EventBridge Scheduler",
                "href": "https://docs.aws.amazon.com/lambda/latest/dg/with-eventbridge-scheduler.html",
                "source": "Aws",
                "parent_content": []
            },
            {
                "title": "IoT",
                "href": "https://docs.aws.amazon.com/lambda/latest/dg/services-iot.html",
                "source": "Aws",
                "parent_content": []
            },
            {
                "title": "Kinesis Data Streams",
                "href": "https://docs.aws.amazon.com/lambda/latest/dg/with-kinesis.html",
                "contents": [
                    {
                        "title": "Create mapping",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/services-kinesis-create.html",
                        "source": "Aws",
                        "parent_content": []
                    },
                    {
                        "title": "Batch item failures",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/services-kinesis-batchfailurereporting.html",
                        "source": "Aws",
                        "parent_content": []
                    },
                    {
                        "title": "Error handling",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/kinesis-on-failure-destination.html",
                        "source": "Aws",
                        "parent_content": []
                    },
                    {
                        "title": "Stateful processing",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/services-kinesis-windows.html",
                        "source": "Aws",
                        "parent_content": []
                    },
                    {
                        "title": "Parameters",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/services-kinesis-parameters.html",
                        "source": "Aws",
                        "parent_content": []
                    },
                    {
                        "title": "Event filtering",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/with-kinesis-filtering.html",
                        "source": "Aws",
                        "parent_content": []
                    },
                    {
                        "title": "Tutorial",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/with-kinesis-example.html",
                        "source": "Aws",
                        "parent_content": []
                    }
                ],
                "source": "Aws",
                "parent_content": []
            },
            {
                "title": "MQ",
                "href": "https://docs.aws.amazon.com/lambda/latest/dg/with-mq.html",
                "contents": [
                    {
                        "title": "Configure event source",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/process-mq-messages-with-lambda.html",
                        "source": "Aws",
                        "parent_content": []
                    },
                    {
                        "title": "Parameters",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/services-mq-params.html",
                        "source": "Aws",
                        "parent_content": []
                    },
                    {
                        "title": "Event filtering",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/with-mq-filtering.html",
                        "source": "Aws",
                        "parent_content": []
                    },
                    {
                        "title": "Troubleshoot",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/services-mq-errors.html",
                        "source": "Aws",
                        "parent_content": []
                    }
                ],
                "source": "Aws",
                "parent_content": []
            },
            {
                "title": "MSK",
                "href": "https://docs.aws.amazon.com/lambda/latest/dg/with-msk.html",
                "contents": [
                    {
                        "title": "Configure event source",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/with-msk-configure.html",
                        "source": "Aws",
                        "parent_content": []
                    },
                    {
                        "title": "Process messages",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/with-msk-process.html",
                        "source": "Aws",
                        "parent_content": []
                    },
                    {
                        "title": "Event filtering",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/with-msk-filtering.html",
                        "source": "Aws",
                        "parent_content": []
                    },
                    {
                        "title": "On-failure destinations",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/with-msk-on-failure.html",
                        "source": "Aws",
                        "parent_content": []
                    },
                    {
                        "title": "Tutorial",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/services-msk-tutorial.html",
                        "source": "Aws",
                        "parent_content": []
                    }
                ],
                "source": "Aws",
                "parent_content": []
            },
            {
                "title": "RDS",
                "href": "https://docs.aws.amazon.com/lambda/latest/dg/services-rds.html",
                "source": "Aws",
                "parent_content": []
            },
            {
                "title": "S3",
                "href": "https://docs.aws.amazon.com/lambda/latest/dg/with-s3.html",
                "contents": [
                    {
                        "title": "Tutorial: Use an S3 trigger",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/with-s3-example.html",
                        "source": "Aws",
                        "parent_content": []
                    },
                    {
                        "title": "Tutorial: Use an Amazon S3 trigger to create thumbnails",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/with-s3-tutorial.html",
                        "source": "Aws",
                        "parent_content": []
                    }
                ],
                "source": "Aws",
                "parent_content": []
            },
            {
                "title": "SQS",
                "href": "https://docs.aws.amazon.com/lambda/latest/dg/with-sqs.html",
                "contents": [
                    {
                        "title": "Create mapping",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/services-sqs-configure.html",
                        "source": "Aws",
                        "parent_content": []
                    },
                    {
                        "title": "Scaling behavior",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/services-sqs-scaling.html",
                        "source": "Aws",
                        "parent_content": []
                    },
                    {
                        "title": "Error handling",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/services-sqs-errorhandling.html",
                        "source": "Aws",
                        "parent_content": []
                    },
                    {
                        "title": "Parameters",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/services-sqs-parameters.html",
                        "source": "Aws",
                        "parent_content": []
                    },
                    {
                        "title": "Event filtering",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/with-sqs-filtering.html",
                        "source": "Aws",
                        "parent_content": []
                    },
                    {
                        "title": "Tutorial",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/with-sqs-example.html",
                        "source": "Aws",
                        "parent_content": []
                    },
                    {
                        "title": "SQS cross-account tutorial",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/with-sqs-cross-account-example.html",
                        "source": "Aws",
                        "parent_content": []
                    }
                ],
                "source": "Aws",
                "parent_content": []
            },
            {
                "title": "S3 Batch",
                "href": "https://docs.aws.amazon.com/lambda/latest/dg/services-s3-batch.html",
                "source": "Aws",
                "parent_content": []
            },
            {
                "title": "SNS",
                "href": "https://docs.aws.amazon.com/lambda/latest/dg/with-sns.html",
                "contents": [
                    {
                        "title": "Tutorial",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/with-sns-example.html",
                        "source": "Aws",
                        "parent_content": []
                    }
                ],
                "source": "Aws",
                "parent_content": []
            }
        ],
        "source": "Aws",
        "parent_content": []
    },
    {
        "title": "Code examples",
        "href": "https://docs.aws.amazon.com/lambda/latest/dg/service_code_examples.html",
        "contents": [
            {
                "title": "Basics",
                "href": "https://docs.aws.amazon.com/lambda/latest/dg/service_code_examples_basics.html",
                "contents": [
                    {
                        "title": "Hello Lambda",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_lambda_Hello_section.html",
                        "source": "Aws",
                        "parent_content": []
                    },
                    {
                        "title": "Learn the basics",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_lambda_Scenario_GettingStartedFunctions_section.html",
                        "source": "Aws",
                        "parent_content": []
                    },
                    {
                        "title": "Actions",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/service_code_examples_actions.html",
                        "contents": [
                            {
                                "title": "CreateAlias",
                                "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_lambda_CreateAlias_section.html",
                                "source": "Aws",
                                "parent_content": []
                            },
                            {
                                "title": "CreateFunction",
                                "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_lambda_CreateFunction_section.html",
                                "source": "Aws",
                                "parent_content": []
                            },
                            {
                                "title": "DeleteAlias",
                                "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_lambda_DeleteAlias_section.html",
                                "source": "Aws",
                                "parent_content": []
                            },
                            {
                                "title": "DeleteFunction",
                                "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_lambda_DeleteFunction_section.html",
                                "source": "Aws",
                                "parent_content": []
                            },
                            {
                                "title": "DeleteFunctionConcurrency",
                                "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_lambda_DeleteFunctionConcurrency_section.html",
                                "source": "Aws",
                                "parent_content": []
                            },
                            {
                                "title": "DeleteProvisionedConcurrencyConfig",
                                "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_lambda_DeleteProvisionedConcurrencyConfig_section.html",
                                "source": "Aws",
                                "parent_content": []
                            },
                            {
                                "title": "GetAccountSettings",
                                "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_lambda_GetAccountSettings_section.html",
                                "source": "Aws",
                                "parent_content": []
                            },
                            {
                                "title": "GetAlias",
                                "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_lambda_GetAlias_section.html",
                                "source": "Aws",
                                "parent_content": []
                            },
                            {
                                "title": "GetFunction",
                                "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_lambda_GetFunction_section.html",
                                "source": "Aws",
                                "parent_content": []
                            },
                            {
                                "title": "GetFunctionConcurrency",
                                "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_lambda_GetFunctionConcurrency_section.html",
                                "source": "Aws",
                                "parent_content": []
                            },
                            {
                                "title": "GetFunctionConfiguration",
                                "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_lambda_GetFunctionConfiguration_section.html",
                                "source": "Aws",
                                "parent_content": []
                            },
                            {
                                "title": "GetPolicy",
                                "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_lambda_GetPolicy_section.html",
                                "source": "Aws",
                                "parent_content": []
                            },
                            {
                                "title": "GetProvisionedConcurrencyConfig",
                                "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_lambda_GetProvisionedConcurrencyConfig_section.html",
                                "source": "Aws",
                                "parent_content": []
                            },
                            {
                                "title": "Invoke",
                                "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_lambda_Invoke_section.html",
                                "source": "Aws",
                                "parent_content": []
                            },
                            {
                                "title": "ListFunctions",
                                "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_lambda_ListFunctions_section.html",
                                "source": "Aws",
                                "parent_content": []
                            },
                            {
                                "title": "ListProvisionedConcurrencyConfigs",
                                "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_lambda_ListProvisionedConcurrencyConfigs_section.html",
                                "source": "Aws",
                                "parent_content": []
                            },
                            {
                                "title": "ListTags",
                                "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_lambda_ListTags_section.html",
                                "source": "Aws",
                                "parent_content": []
                            },
                            {
                                "title": "ListVersionsByFunction",
                                "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_lambda_ListVersionsByFunction_section.html",
                                "source": "Aws",
                                "parent_content": []
                            },
                            {
                                "title": "PublishVersion",
                                "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_lambda_PublishVersion_section.html",
                                "source": "Aws",
                                "parent_content": []
                            },
                            {
                                "title": "PutFunctionConcurrency",
                                "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_lambda_PutFunctionConcurrency_section.html",
                                "source": "Aws",
                                "parent_content": []
                            },
                            {
                                "title": "PutProvisionedConcurrencyConfig",
                                "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_lambda_PutProvisionedConcurrencyConfig_section.html",
                                "source": "Aws",
                                "parent_content": []
                            },
                            {
                                "title": "RemovePermission",
                                "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_lambda_RemovePermission_section.html",
                                "source": "Aws",
                                "parent_content": []
                            },
                            {
                                "title": "TagResource",
                                "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_lambda_TagResource_section.html",
                                "source": "Aws",
                                "parent_content": []
                            },
                            {
                                "title": "UntagResource",
                                "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_lambda_UntagResource_section.html",
                                "source": "Aws",
                                "parent_content": []
                            },
                            {
                                "title": "UpdateAlias",
                                "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_lambda_UpdateAlias_section.html",
                                "source": "Aws",
                                "parent_content": []
                            },
                            {
                                "title": "UpdateFunctionCode",
                                "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_lambda_UpdateFunctionCode_section.html",
                                "source": "Aws",
                                "parent_content": []
                            },
                            {
                                "title": "UpdateFunctionConfiguration",
                                "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_lambda_UpdateFunctionConfiguration_section.html",
                                "source": "Aws",
                                "parent_content": []
                            }
                        ],
                        "source": "Aws",
                        "parent_content": []
                    }
                ],
                "source": "Aws",
                "parent_content": []
            },
            {
                "title": "Scenarios",
                "href": "https://docs.aws.amazon.com/lambda/latest/dg/service_code_examples_scenarios.html",
                "contents": [
                    {
                        "title": "Automatically confirm known users with a Lambda function",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_cross_CognitoAutoConfirmUser_section.html",
                        "source": "Aws",
                        "parent_content": []
                    },
                    {
                        "title": "Automatically migrate known users with a Lambda function",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_cross_CognitoAutoMigrateUser_section.html",
                        "source": "Aws",
                        "parent_content": []
                    },
                    {
                        "title": "Create a REST API to track COVID-19 data",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_cross_ApiGatewayDataTracker_section.html",
                        "source": "Aws",
                        "parent_content": []
                    },
                    {
                        "title": "Create a lending library REST API",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_cross_AuroraRestLendingLibrary_section.html",
                        "source": "Aws",
                        "parent_content": []
                    },
                    {
                        "title": "Create a messenger application",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_cross_StepFunctionsMessenger_section.html",
                        "source": "Aws",
                        "parent_content": []
                    },
                    {
                        "title": "Create a serverless application to manage photos",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_cross_PAM_section.html",
                        "source": "Aws",
                        "parent_content": []
                    },
                    {
                        "title": "Create a websocket chat application",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_cross_ApiGatewayWebsocketChat_section.html",
                        "source": "Aws",
                        "parent_content": []
                    },
                    {
                        "title": "Create an application to analyze customer feedback",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_cross_FSA_section.html",
                        "source": "Aws",
                        "parent_content": []
                    },
                    {
                        "title": "Invoke a Lambda function from a browser",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_cross_LambdaForBrowser_section.html",
                        "source": "Aws",
                        "parent_content": []
                    },
                    {
                        "title": "Transform data with S3 Object Lambda",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_cross_ServerlessS3DataTransformation_section.html",
                        "source": "Aws",
                        "parent_content": []
                    },
                    {
                        "title": "Use API Gateway to invoke a Lambda function",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_cross_LambdaAPIGateway_section.html",
                        "source": "Aws",
                        "parent_content": []
                    },
                    {
                        "title": "Use Step Functions to invoke Lambda functions",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_cross_ServerlessWorkflows_section.html",
                        "source": "Aws",
                        "parent_content": []
                    },
                    {
                        "title": "Use scheduled events to invoke a Lambda function",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_cross_LambdaScheduledEvents_section.html",
                        "source": "Aws",
                        "parent_content": []
                    },
                    {
                        "title": "Write custom activity data with a Lambda function after Amazon Cognito user authentication",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_cross_CognitoCustomActivityLog_section.html",
                        "source": "Aws",
                        "parent_content": []
                    }
                ],
                "source": "Aws",
                "parent_content": []
            },
            {
                "title": "Serverless examples",
                "href": "https://docs.aws.amazon.com/lambda/latest/dg/service_code_examples_serverless_examples.html",
                "contents": [
                    {
                        "title": "Connecting to an Amazon RDS database in a Lambda function",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_serverless_connect_RDS_Lambda_section.html",
                        "source": "Aws",
                        "parent_content": []
                    },
                    {
                        "title": "Invoke a Lambda function from a Kinesis trigger",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_serverless_Kinesis_Lambda_section.html",
                        "source": "Aws",
                        "parent_content": []
                    },
                    {
                        "title": "Invoke a Lambda function from a DynamoDB trigger",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_serverless_DynamoDB_Lambda_section.html",
                        "source": "Aws",
                        "parent_content": []
                    },
                    {
                        "title": "Invoke a Lambda function from a Amazon DocumentDB trigger",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_serverless_DocumentDB_Lambda_section.html",
                        "source": "Aws",
                        "parent_content": []
                    },
                    {
                        "title": "Invoke a Lambda function from an Amazon MSK trigger",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_serverless_MSK_Lambda_section.html",
                        "source": "Aws",
                        "parent_content": []
                    },
                    {
                        "title": "Invoke a Lambda function from an Amazon S3 trigger",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_serverless_S3_Lambda_section.html",
                        "source": "Aws",
                        "parent_content": []
                    },
                    {
                        "title": "Invoke a Lambda function from an Amazon SNS trigger",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_serverless_SNS_Lambda_section.html",
                        "source": "Aws",
                        "parent_content": []
                    },
                    {
                        "title": "Invoke a Lambda function from an Amazon SQS trigger",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_serverless_SQS_Lambda_section.html",
                        "source": "Aws",
                        "parent_content": []
                    },
                    {
                        "title": "Reporting batch item failures for Lambda functions with a Kinesis trigger",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_serverless_Kinesis_Lambda_batch_item_failures_section.html",
                        "source": "Aws",
                        "parent_content": []
                    },
                    {
                        "title": "Reporting batch item failures for Lambda functions with a DynamoDB trigger",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_serverless_DynamoDB_Lambda_batch_item_failures_section.html",
                        "source": "Aws",
                        "parent_content": []
                    },
                    {
                        "title": "Reporting batch item failures for Lambda functions with an Amazon SQS trigger",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_serverless_SQS_Lambda_batch_item_failures_section.html",
                        "source": "Aws",
                        "parent_content": []
                    }
                ],
                "source": "Aws",
                "parent_content": []
            }
        ],
        "source": "Aws",
        "parent_content": []
    }
]