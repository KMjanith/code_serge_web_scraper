[
    {
        "title": "What is AWS Lambda?",
        "href": "https://docs.aws.amazon.com/lambda/latest/dg/welcome.html",
        "sections": [
            "",
            "You can use AWS Lambda to run code without provisioning or managing servers.",
            " Lambda runs your code    on a high-availability compute infrastructure and performs all of the administration of the compute resources,    including server and operating system maintenance, capacity provisioning and automatic scaling, and    logging. With Lambda, all you need to do is supply your code in one of the language runtimes that Lambda supports.",
            "You organize your code into Lambda functions. The Lambda service runs your function only when needed and scales automatically. You only pay for the compute time that you consume—there is no charge when your code is not running. For more information, see AWS Lambda Pricing.",
            "TipTo learn how to build serverless solutions, check out the Serverless Developer Guide.",
            {
                "sub_header": "When to use Lambda",
                "content": [
                    "Lambda is an ideal compute service for application scenarios that need to scale up rapidly, and scale down to      zero when not in demand. For example, you can use Lambda for:",
                    "  1.File processing:  Use Amazon Simple Storage Service (Amazon S3) to trigger Lambda data processing in real time after an upload.",
                    "  2.Stream processing:  Use Lambda and Amazon Kinesis to process real-time streaming data for application activity tracking, transaction order processing, clickstream analysis, data cleansing, log filtering, indexing, social media analysis, Internet of Things (IoT) device data telemetry, and metering.",
                    "  3.Web applications:  Combine Lambda with other AWS services to build powerful web applications that automatically scale up and down and run in a highly available configuration across multiple data centers.",
                    "  4.IoT backends:  Build serverless backends using Lambda to handle web, mobile, IoT, and third-party API requests.",
                    "  5.Mobile backends:  Build backends using Lambda and Amazon API Gateway  to authenticate and process API requests. Use AWS Amplify to easily integrate with your iOS, Android, Web, and React Native frontends.",
                    "When using Lambda, you are responsible only for your code. Lambda manages the compute fleet that offers a      balance of memory, CPU, network, and other resources to run your code. Because Lambda manages these resources, you      cannot log in to compute instances or customize the operating system on provided        runtimes. Lambda performs operational and administrative activities on your behalf, including managing      capacity, monitoring, and logging your Lambda functions."
                ]
            },
            {
                "sub_header": "Key features",
                "content": [
                    "The following key features help you develop Lambda applications that are scalable, secure, and easily      extensible:",
                    "  1. Environment variables : \nUse environment variables to adjust your function's behavior without updating code.\n",
                    "  2.Versions : \nManage the deployment of your functions with versions, so that, for example, a new function can be used for beta testing without affecting users of the stable production version.\n",
                    "  3.Container images : \nCreate a container image for a Lambda function by using an AWS provided base image or an alternative base\n            image so that you can reuse your existing container tooling or deploy larger workloads that rely on sizable dependencies, such as machine learning.\n",
                    "  4.Layers : \nPackage libraries and other dependencies to reduce the size of deployment archives and makes it faster to deploy your code.\n",
                    "  5.Lambda extensions : \nAugment your Lambda functions with tools for monitoring, observability, security, and governance.\n",
                    "  6.Function URLs : \nAdd a dedicated HTTP(S) endpoint to your Lambda function.\n",
                    "  7.Response streaming : \nConfigure your Lambda function URLs to stream response payloads back to clients from Node.js functions, to improve time to first byte (TTFB) performance or to return larger payloads.\n",
                    "  8.Concurrency and scaling controls : \nApply fine-grained control over the scaling and responsiveness of your production applications.\n",
                    "  9.Code signing : \nVerify that only approved developers publish unaltered, trusted code in your Lambda functions \n",
                    "  10.Private networking : \nCreate a private network for resources such as databases, cache instances, or internal services.\n",
                    "  11.File system access : \nConfigure a function to mount an Amazon Elastic File System (Amazon EFS) to a local directory, so that your function code can access and modify shared resources safely and at high concurrency.\n",
                    "  12.Lambda SnapStart for Java : \nImprove startup performance for Java runtimes by up to 10x at no extra cost, typically with no changes to your function code.\n"
                ]
            }
        ]
    },
    {
        "title": "Example apps",
        "href": "https://docs.aws.amazon.com/lambda/latest/dg/example-apps.html",
        "contents": [
            {
                "title": "File-processing app",
                "href": "https://docs.aws.amazon.com/lambda/latest/dg/file-processing-app.html",
                "sections": [
                    "",
                    "One of the most common use cases for Lambda is to perform file processing tasks. For example, you might use a Lambda function     to automatically create PDF files from HTML files or images, or to create thumbnails when a user uploads an image.",
                    "In this example, you create an app which automatically encrypts PDF files when they are uploaded to an Amazon Simple Storage Service (Amazon S3) bucket.     To implement this app, you create the following resources:",
                    "You also create an AWS Identity and Access Management (IAM) policy to give your Lambda function permission to perform read and write operations     on your S3 buckets.",
                    "\n\n",
                    "TipIf you’re brand new to Lambda, we recommend that you carry out the tutorial Create your first Lambda function before\n      creating this example app.",
                    "You can deploy your app manually by creating and configuring resources with the AWS Management Console or the AWS Command Line Interface (AWS CLI). You can     also deploy the app by using the AWS Serverless Application Model (AWS SAM). AWS SAM is an infrastructure as code (IaC) tool. With IaC, you don’t create     resources manually, but define them in code and then deploy them automatically.",
                    "If you want to learn more about using Lambda with IaC before deploying this example app, see Using Lambda with infrastructure as code (IaC).",
                    {
                        "sub_header": "Prerequisites",
                        "content": [
                            "Before you can create the example app, make sure you have the required command line tools installed.",
                            {
                                "code_example": "pytest"
                            },
                            "To deploy the app using AWS SAM, Docker must also be installed on your build machine."
                        ]
                    },
                    {
                        "sub_header": "Downloading the example app files",
                        "content": [
                            "To create and test the example app, you create the following files in your project directory:",
                            {
                                "code_example": "lambda_function.py"
                            },
                            "Expand the following sections to view the code and to learn more about the role of each file in creating and testing your app. To create the       files on your local machine, either copy and paste the code below, or download the files from the aws-lambda-developer-guide GitHub repo.",
                            {
                                "code_example": "lambda_function.py"
                            }
                        ]
                    },
                    {
                        "sub_header": "Deploying the app",
                        "content": [
                            "You can create and deploy the resources for this example app either manually or by using AWS SAM. In a production environment, we       recommend that you use an IaC tool like AWS SAM to quickly and repeatably deploy whole serverless applications without using manual processes.",
                            "For this example, follow the console or AWS CLI instructions to learn how to configure each AWS resource separately, or skip ahead to        Deploy the resources using AWS SAM to quickly deploy the app using a few CLI commands.",
                            {
                                "sub_header": "Deploy the resources manually",
                                "content": [
                                    "To deploy your app manually, you carry out the following steps:",
                                    "Follow the instructions in the following paragraphs to create and configure your resources.",
                                    {
                                        "sub_header": "Create two S3 buckets",
                                        "content": [
                                            "First create two S3 buckets. The first bucket is the source bucket you will upload your PDF files to. The second bucket is used by     Lambda to save the encrypted file when you invoke your function.",
                                            "ConsoleTo create the S3 buckets (console)Open the Buckets page of the Amazon S3 console.Choose Create bucket.Under General configuration, do the following:For Bucket name, enter a globally unique name that meets the Amazon S3 Bucket naming rules.                     Bucket names can contain only lower case letters, numbers, dots (.), and hyphens (-). For AWS Region, choose the AWS Region                     closest to your geographical location. Later in the deployment process, you must create your Lambda function in the same AWS Region, so                     make a note of the region you chose.Leave all other options set to their default values and choose Create bucket.Repeat steps 1 to 4 to create your destination bucket. For Bucket name, enter SOURCEBUCKET-encrypted,                 where SOURCEBUCKET is the name of the source bucket you just created.AWS CLITo create the Amazon S3 buckets (AWS CLI)Run the following CLI command to create your source bucket. The name you choose for your bucket must be globally unique and                 follow the Amazon S3 Bucket naming rules.                 Names can only contain lower case letters, numbers, dots (.), and hyphens (-). For region and LocationConstraint,                 choose the AWS Region closest to your geographical                 location.aws s3api create-bucket --bucket SOURCEBUCKET --region us-west-2 \\--create-bucket-configuration LocationConstraint=us-west-2Later in the tutorial, you must create your Lambda function in the same AWS Region as your source bucket, so make a note of the                 region you chose.Run the following command to create your destination bucket. For the bucket name, you must use SOURCEBUCKET-encrypted,                 where SOURCEBUCKET is the name of the source bucket you created in step 1. For region                 and LocationConstraint, choose the same AWS Region you used to create your source bucket.aws s3api create-bucket --bucket SOURCEBUCKET-encrypted --region us-west-2 \\--create-bucket-configuration LocationConstraint=us-west-2anchoranchorConsoleAWS CLITo create the S3 buckets (console)Open the Buckets page of the Amazon S3 console.Choose Create bucket.Under General configuration, do the following:For Bucket name, enter a globally unique name that meets the Amazon S3 Bucket naming rules.                     Bucket names can contain only lower case letters, numbers, dots (.), and hyphens (-). For AWS Region, choose the AWS Region                     closest to your geographical location. Later in the deployment process, you must create your Lambda function in the same AWS Region, so                     make a note of the region you chose.Leave all other options set to their default values and choose Create bucket.Repeat steps 1 to 4 to create your destination bucket. For Bucket name, enter SOURCEBUCKET-encrypted,                 where SOURCEBUCKET is the name of the source bucket you just created."
                                        ]
                                    },
                                    {
                                        "sub_header": "Create an execution role (AWS CLI only)",
                                        "content": [
                                            "An execution role is an IAM role that grants a Lambda function permission to access AWS services and resources. When you create a function       using the Lambda console, Lambda automatically creates an execution role. You only need to create a role manually if you choose to deploy the app       using the AWS CLI. To give your function read and write access to Amazon S3, you attach the       AWS managed policyAmazonS3FullAccess.",
                                            "ConsoleThis step is only required if you choose to deploy your app using the AWS CLI.AWS CLITo create an execution role and attach the AmazonS3FullAccess managed policy (AWS CLI)Save the following JSON in a file named trust-policy.json. This trust policy allows Lambda to use the role’s                permissions by giving the service principal lambda.amazonaws.com permission to call the AWS Security Token Service (AWS STS) AssumeRole                action.{  \"Version\": \"2012-10-17\",  \"Statement\": [    {      \"Effect\": \"Allow\",      \"Principal\": {        \"Service\": \"lambda.amazonaws.com\"      },      \"Action\": \"sts:AssumeRole\"    }  ]}From the directory you saved the JSON trust policy document in, run the following CLI command to create the execution role.aws iam create-role --role-name LambdaS3Role --assume-role-policy-document file://trust-policy.jsonTo attach the AmazonS3FullAccess managed policy, run the following CLI command.aws iam attach-role-policy --role-name LambdaS3Role --policy-arn arn:aws:iam::aws:policy/AmazonS3FullAccessanchoranchorConsoleAWS CLIThis step is only required if you choose to deploy your app using the AWS CLI."
                                        ]
                                    },
                                    {
                                        "sub_header": "Create the function deployment package",
                                        "content": [
                                            "To create your function, you create a deployment package containing your function code and its dependencies. For this       application, your function code uses a separate library for the PDF encryption.",
                                            {
                                                "code_example": "lambda_function.py"
                                            },
                                            "You can also create your deployment package using a Python virtual environment. See Working with .zip file archives for Python Lambda functions"
                                        ]
                                    },
                                    {
                                        "sub_header": "Create the Lambda function",
                                        "content": [
                                            "You now use the deployment package you created in the previous step to deploy your Lambda function.",
                                            "ConsoleTo create the function (console)To create your Lambda function using the console, you first create a basic function containing some ‘Hello world’ code. You then           replace this code with your own function code by uploading the.zip file you created in the previous step.To ensure that your function doesn't time out when encrypting large PDF files, you configure the function's memory and timeout settings.           You also set the function's log format to JSON. Configuring JSON formatted logs is necessary when using the provided test script so it can read the           function's invocation status from CloudWatch Logs to confirm successful invocation.Open the Functions page of the Lambda console.Make sure you're working in the same AWS Region you created your S3 bucket in. You can change your region using the drop-down             list at the top of the screen.Choose Create function.Choose Author from scratch.Under Basic information, do the following:For Function name, enter EncryptPDF.For Runtime choose Python 3.12.For Architecture, choose x86_64.Choose Create function.To upload the function code (console)In the Code source pane, choose Upload from.Choose .zip file.Choose Upload.In the file selector, select your .zip file and choose Open.Choose Save.To configure the function memory and timeout (console)Select the Configuration tab for your function.In the General configuration pane, choose Edit.Set Memory to 256 MB and Timeout to 15 seconds.Choose Save.To configure the log format (console)Select the Configuration tab for your function.Select Monitoring and operations tools.In the Logging configuration pane, choose Edit.For Logging configuration, select JSON.Choose Save.AWS CLITo create the function (AWS CLI)Run the following command from the directory containing your lambda_function.zip             file.For the region parameter, replace us-west-2 with the region you created your             S3 buckets in.aws lambda create-function --function-name EncryptPDF \\--zip-file fileb://lambda_function.zip --handler lambda_function.lambda_handler \\--runtime python3.12 --timeout 15 --memory-size 256 \\--role arn:aws:iam::123456789012:role/LambdaS3Role --region us-west-2 \\--logging-config LogFormat=JSONanchoranchorConsoleAWS CLITo create the function (console)To create your Lambda function using the console, you first create a basic function containing some ‘Hello world’ code. You then           replace this code with your own function code by uploading the.zip file you created in the previous step.To ensure that your function doesn't time out when encrypting large PDF files, you configure the function's memory and timeout settings.           You also set the function's log format to JSON. Configuring JSON formatted logs is necessary when using the provided test script so it can read the           function's invocation status from CloudWatch Logs to confirm successful invocation.Open the Functions page of the Lambda console.Make sure you're working in the same AWS Region you created your S3 bucket in. You can change your region using the drop-down             list at the top of the screen.Choose Create function.Choose Author from scratch.Under Basic information, do the following:For Function name, enter EncryptPDF.For Runtime choose Python 3.12.For Architecture, choose x86_64.Choose Create function.To upload the function code (console)In the Code source pane, choose Upload from.Choose .zip file.Choose Upload.In the file selector, select your .zip file and choose Open.Choose Save.To configure the function memory and timeout (console)Select the Configuration tab for your function.In the General configuration pane, choose Edit.Set Memory to 256 MB and Timeout to 15 seconds.Choose Save.To configure the log format (console)Select the Configuration tab for your function.Select Monitoring and operations tools.In the Logging configuration pane, choose Edit.For Logging configuration, select JSON.Choose Save."
                                        ]
                                    },
                                    {
                                        "sub_header": "Configure an Amazon S3 trigger to invoke the function",
                                        "content": [
                                            "For your Lambda function to run when you upload a file to your source bucket, you need to configure a trigger for your function. You can     configure the Amazon S3 trigger using either the console or the AWS CLI.",
                                            "ImportantThis procedure configures the S3 bucket to invoke your function every time that an object is created in the bucket. Be sure to \n      configure this only on the source bucket. If your Lambda function creates objects in the same bucket that invokes it, your function can be \n        invoked continuously in a loop. This can result \n        in un expected charges being billed to your AWS account.",
                                            "ConsoleTo configure the Amazon S3 trigger (console)Open the Functions page of the Lambda console and choose your function (EncryptPDF).Choose Add trigger.Select S3.Under Bucket, select your source bucket.Under Event types, select All object create events.Under Recursive invocation, select the check box to acknowledge that using the same S3 bucket for input                 and output is not recommended. You can learn more about recursive invocation patterns in Lambda by reading                Recursive patterns that cause run-away Lambda functions                in Serverless Land.Choose Add.When you create a trigger using the Lambda console, Lambda automatically creates a resource based policy                 to give the service you select permission to invoke your function. AWS CLITo configure the Amazon S3 trigger (AWS CLI)For your Amazon S3 source bucket to invoke your function when you add a file, you first need to configure permissions for your                 function using a resource based policy.                 A resource-based policy statement gives other AWS services permission to invoke your function. To give Amazon S3 permission to invoke                 your function, run the following CLI command. Be sure to replace the source-account parameter with your own AWS account ID and to                 use your own source bucket name.aws lambda add-permission --function-name EncryptPDF \\--principal s3.amazonaws.com --statement-id s3invoke --action \"lambda:InvokeFunction\" \\--source-arn arn:aws:s3:::SOURCEBUCKET \\--source-account 123456789012The policy you define with this command allows Amazon S3 to invoke your function only when an action takes place on your source bucket.NoteAlthough S3 bucket names are globally unique, when using resource-based policies it is best practice to specify that the                 bucket must belong to your account.  This is because if you delete a bucket, it is possible for another AWS account to create a                 bucket with the same Amazon Resource Name (ARN).Save the following JSON in a file named notification.json. When applied to your source bucket, this JSON                 configures the bucket to send a notification to your Lambda function every time a new object is added. Replace the AWS account               number and AWS Region in the Lambda function ARN with your own account number and region.{\"LambdaFunctionConfigurations\": [    {      \"Id\": \"EncryptPDFEventConfiguration\",      \"LambdaFunctionArn\": \"arn:aws:lambda:us-west-2:123456789012:function:EncryptPDF\",      \"Events\": [ \"s3:ObjectCreated:Put\" ]    }  ]}Run the following CLI command to apply the notification settings in the JSON file you created to your source bucket. Replace                 SOURCEBUCKET with the name of your own source bucket.aws s3api put-bucket-notification-configuration --bucket SOURCEBUCKET \\--notification-configuration file://notification.jsonTo learn more about the put-bucket-notification-configuration command and the                 notification-configuration option, see put-bucket-notification-configuration                 in the AWS CLI Command Reference.anchoranchorConsoleAWS CLITo configure the Amazon S3 trigger (console)Open the Functions page of the Lambda console and choose your function (EncryptPDF).Choose Add trigger.Select S3.Under Bucket, select your source bucket.Under Event types, select All object create events.Under Recursive invocation, select the check box to acknowledge that using the same S3 bucket for input                 and output is not recommended. You can learn more about recursive invocation patterns in Lambda by reading                Recursive patterns that cause run-away Lambda functions                in Serverless Land.Choose Add.When you create a trigger using the Lambda console, Lambda automatically creates a resource based policy                 to give the service you select permission to invoke your function. "
                                        ]
                                    }
                                ]
                            },
                            {
                                "sub_header": "Deploy the resources using AWS SAM",
                                "content": [
                                    "To deploy the example app using the AWS SAM CLI, carry out the following steps.",
                                    "Make sure that you have         installed the latest version of the           CLI and that Docker is installed on your build machine.",
                                    {
                                        "code_example": "template.yaml"
                                    },
                                    "During the deployment process, AWS SAM creates the following resources in your AWS account:",
                                    {
                                        "code_example": "sam-app"
                                    },
                                    "When AWS SAM finishes creating your resources, you should see the following message:",
                                    "Successfully created/updated stack - sam-app in us-west-2"
                                ]
                            }
                        ]
                    },
                    {
                        "sub_header": "Testing the app",
                        "content": [
                            "To test your app, you upload a PDF file to your source bucket, and confirm that Lambda creates an encrypted version of the file in your       destination bucket. In this example, you can either test this manually using the console or the AWS CLI, or by using the provided test script.",
                            "For production applications, you can use traditional test methods and techniques, such as unit testing, to confirm the       correct functioning of your Lambda function code. Best practice is also to conduct tests like those in the provided test script which perform integration       testing with real, cloud-based resources. Integration testing in the cloud confirms that your infrastructure has been correctly deployed and that events flow       between different services as expected. To learn more, see How to test serverless functions and applications.",
                            {
                                "sub_header": "Testing the app manually",
                                "content": [
                                    "You can test your function manually by adding a PDF file to           your Amazon S3 source bucket. When you add your file to the source bucket, your Lambda function should be automatically invoked and should store an encrypted           version of the file in your target bucket.",
                                    "ConsoleTo test your app by uploading a file (console)To upload a PDF file to your S3 bucket, do the following:Open the Buckets page of the Amazon S3 console and choose your source bucket.Choose Upload.Choose Add files and use the file selector to choose the PDF file you want to upload.Choose Open, then choose Upload.Verify that Lambda has saved an encrypted version of your PDF file in your target bucket by doing the following:Navigate back to the Buckets page of the Amazon S3 console and choose your destination bucket.In the Objects pane, you should now see a file with name format filename_encrypted.pdf (where                         filename.pdf was the name of the file you uploaded to your source bucket).                        To download your encrypted PDF, select the file, then choose Download.Confirm that you can open the downloaded file with the password your Lambda function protected it with (my-secret-password).AWS CLITo test your app by uploading a file (AWS CLI)From the directory containing the PDF file you want to upload, run the following CLI command. Replace the --bucket                     parameter with the name of your source bucket. For the --key and --body parameters, use the filename of                     your test file.aws s3api put-object --bucket SOURCEBUCKET --key test.pdf --body ./test.pdfVerify that your function has created an encrypted version of your file and saved it to your target S3 bucket. Run the                     following CLI command, replacing SOURCEBUCKET-encrypted with the name of your own destination bucket.aws s3api list-objects-v2 --bucket SOURCEBUCKET-encryptedIf your function runs successfully, you’ll see output similar to the following. Your target bucket should contain a file with the                     name format <your_test_file>_encrypted.pdf, where <your_test_file>                     is the name of the file you uploaded.{    \"Contents\": [        {            \"Key\": \"test_encrypted.pdf\",            \"LastModified\": \"2023-06-07T00:15:50+00:00\",            \"ETag\": \"\\\"7781a43e765a8301713f533d70968a1e\\\"\",            \"Size\": 2763,            \"StorageClass\": \"STANDARD\"        }    ]}To download the file that Lambda saved in your destination bucket, run the following CLI command. Replace the --bucket                     parameter with the name of your destination bucket. For the --key parameter, use the filename <your_test_file>_encrypted.pdf,                     where <your_test_file> is the name of the the test file you uploaded.aws s3api get-object --bucket SOURCEBUCKET-encrypted --key test_encrypted.pdf my_encrypted_file.pdfThis command downloads the file to your current directory and saves it as my_encrypted_file.pdf.Confirm the you can open the downloaded file with the password your Lambda function protected it with (my-secret-password).anchoranchorConsoleAWS CLITo test your app by uploading a file (console)To upload a PDF file to your S3 bucket, do the following:Open the Buckets page of the Amazon S3 console and choose your source bucket.Choose Upload.Choose Add files and use the file selector to choose the PDF file you want to upload.Choose Open, then choose Upload.Verify that Lambda has saved an encrypted version of your PDF file in your target bucket by doing the following:Navigate back to the Buckets page of the Amazon S3 console and choose your destination bucket.In the Objects pane, you should now see a file with name format filename_encrypted.pdf (where                         filename.pdf was the name of the file you uploaded to your source bucket).                        To download your encrypted PDF, select the file, then choose Download.Confirm that you can open the downloaded file with the password your Lambda function protected it with (my-secret-password)."
                                ]
                            },
                            {
                                "sub_header": "Testing the app with the automated script",
                                "content": [
                                    "To test your app using the provided test script, first ensure that the pytest module is installed in your local environment. You can install       pytest by running the following command:",
                                    "pip install pytest",
                                    "You also need to edit the code in the test_pdf_encrypt.py file to replace the placeholder bucket names with the names of           your Amazon S3 source and destination buckets. Make the following changes to test_pdf_encrypt.py:",
                                    {
                                        "code_example": "test_source_bucket_available"
                                    },
                                    "To run the tests do the following:",
                                    {
                                        "code_example": "test.pdf"
                                    },
                                    "When the test completes, you should see output like the following:",
                                    "============================================================== test session starts =========================================================platform linux -- Python 3.12.2, pytest-7.2.2, pluggy-1.0.0 -- /usr/bin/python3cachedir: .pytest_cachehypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/pdf_encrypt_app/.hypothesis/examples')Test order randomisation NOT enabled. Enable with --random-order or --random-order-bucket=<bucket_type>rootdir: /home/pdf_encrypt_app, configfile: pytest.iniplugins: anyio-3.7.1, hypothesis-6.70.0, localserver-0.7.1, random-order-1.1.0collected 4 itemstest_pdf_encrypt.py::test_source_bucket_available PASSEDtest_pdf_encrypt.py::test_lambda_invoked PASSEDtest_pdf_encrypt.py::test_encrypted_file_in_bucket PASSEDtest_pdf_encrypt.py::test_cleanup PASSEDDeleted test.pdf from EXAMPLE-BUCKETDeleted test_encrypted.pdf from EXAMPLE-BUCKET-encrypted=============================================================== 4 passed in 7.32s =========================================================="
                                ]
                            }
                        ]
                    },
                    {
                        "sub_header": "Next steps",
                        "content": [
                            "Now you've created this example app, you can use the provided code as a basis to create other types of file-processing application. Modify the     code in the lambda_function.py file to implement the file-processing logic for your use case.",
                            "Many typical file-processing use cases involve image processing. When using Python, the most popular image-processing libraries like       pillow typically contain C or C++ components. In order to ensure that your function's deployment package is     compatible with the Lambda execution environment, it's important to use the correct source distribution binary.",
                            "When deploying your resources with AWS SAM, you need to take some extra steps to include the right source distribution in your deployment package. Because AWS SAM won't install dependencies     for a different platform than your build machine, specifying the correct source distribution (.whl file) in your requirements.txt       file won't work if your build machine uses an operating system or architecture that's different from the Lambda execution environment. Instead, you should do one of the following:",
                            {
                                "code_example": "--use-container"
                            }
                        ]
                    }
                ]
            },
            {
                "title": "Scheduled-maintenance app",
                "href": "https://docs.aws.amazon.com/lambda/latest/dg/scheduled-task-app.html",
                "sections": [
                    "",
                    "You can use AWS Lambda to replace scheduled processes such as automated system backups, file conversions, and maintenance tasks.   In this example, you create a serverless application that performs regular scheduled maintenance on a DynamoDB table by deleting old entries. The app uses EventBridge Scheduler to invoke a   Lambda function on a cron schedule. When invoked, the function queries the table for items older than one year, and deletes them. The function logs each deleted item  in CloudWatch Logs.",
                    "To implement this example, first create a DynamoDB table and populate it with some test data for your function to query. Then, create a Python Lambda function with   an EventBridge Scheduler trigger and an IAM execution role that gives the function permission to read, and delete, items from your table.",
                    "\n\n",
                    "TipIf you’re new to Lambda, we recommend that you complete the tutorial Create your first Lambda function before\n      creating this example app.",
                    "You can deploy your app manually by creating and configuring resources with the AWS Management Console. You can     also deploy the app by using the AWS Serverless Application Model (AWS SAM). AWS SAM is an infrastructure as code (IaC) tool. With IaC, you don’t create     resources manually, but define them in code and then deploy them automatically.",
                    "If you want to learn more about using Lambda with IaC before deploying this example app, see Using Lambda with infrastructure as code (IaC).",
                    {
                        "sub_header": "Prerequisites",
                        "content": [
                            "Before you can create the example app, make sure you have the required command line tools and programs installed.",
                            "  1.Python ",
                            "  2.AWS SAM CLI ",
                            "  3.AWS CLI ",
                            "  4.Docker "
                        ]
                    },
                    {
                        "sub_header": "Downloading the example app files",
                        "content": [
                            "To create the example database and the scheduled-maintenance app, you need to create the following files in your project directory:",
                            "Example database files",
                            {
                                "code_example": "template.yaml"
                            },
                            "Scheduled-maintenance app files",
                            {
                                "code_example": "lambda_function.py"
                            },
                            "Test file",
                            {
                                "code_example": "test_app.py"
                            },
                            "Expand the following sections to view the code and to learn more about the role of each file in creating and testing your app. To create the       files on your local machine, copy and paste the code below.",
                            {
                                "code_example": "template.yaml"
                            }
                        ]
                    },
                    {
                        "sub_header": "Creating and populating the example DynamoDB table",
                        "content": [
                            "To test your scheduled-maintenance app, you first create a DynamoDB table and populate it with some sample data. You can create the table either manually using the       AWS Management Console or by using AWS SAM. We recommend that you use AWS SAM to quickly create and configure the table using a few AWS CLI commands.",
                            "ConsoleTo create the DynamoDB tableOpen the Tables page of the DynamoDB console.Choose Create table.Create the table by doing the following:Under Table details, for Table name, enter MyOrderTable.For Partition key, enter Order_number and leave the type as String.For Sort key, enter Date and leave the type as String.Leave Table settings set to Default settings and choose Create table.When your table has finished creating and its Status shows as Active, create a global secondary index (GSI) by doing the               following. Your app will use this GSI to search for items directly by date to determine what to delete.Choose MyOrderTable from the list of tables.Choose the Indexes tab.Under Global secondary indexes, choose Create index.Under Index details, enter Date for the Partition key and leave the                   Data type set to String.For Index name, enter Date-index.Leave all other parameters set to their default values, scroll to the bottom of the page, and choose Create index.AWS SAMTo create the DynamoDB tableNavigate to the folder you saved the template.yaml file for the DynamoDB table in. Note that this example uses two                 template.yaml files. Make sure they are saved in separate sub-folders and that you are in the correct folder containing                 the template to create your DynamoDB table.Run the following command.sam buildThis command gathers the build artifacts for the resources you want to deploy and places them in the proper format and                 location to deploy them.To create the DynamoDB resource specified in the template.yaml file, run the following command.sam deploy --guidedUsing the --guided flag means that AWS SAM will show you prompts to guide you through the deployment process. For this deployment,                 enter a Stack name of cron-app-test-db, and accept the defaults for all other options by using Enter.When AWS SAM has finished creating the DynamoDB resource, you should see the following message.Successfully created/updated stack - cron-app-test-db in us-west-2You can additionally confirm that the DynamoDB table has been created by opening the Tables page of the DynamoDB console.                 You should see a table named MyOrderTable.anchoranchorConsoleAWS SAMTo create the DynamoDB tableOpen the Tables page of the DynamoDB console.Choose Create table.Create the table by doing the following:Under Table details, for Table name, enter MyOrderTable.For Partition key, enter Order_number and leave the type as String.For Sort key, enter Date and leave the type as String.Leave Table settings set to Default settings and choose Create table.When your table has finished creating and its Status shows as Active, create a global secondary index (GSI) by doing the               following. Your app will use this GSI to search for items directly by date to determine what to delete.Choose MyOrderTable from the list of tables.Choose the Indexes tab.Under Global secondary indexes, choose Create index.Under Index details, enter Date for the Partition key and leave the                   Data type set to String.For Index name, enter Date-index.Leave all other parameters set to their default values, scroll to the bottom of the page, and choose Create index.",
                            "After you've created your table, you next add some sample data to test your app. The CSV file sample_data.csv you downloaded     earlier contains a number of example entries comprised of order numbers, dates, and customer and order information. Use the provided python script     load_sample_data.py to add this data to your table.",
                            {
                                "code_example": "sample_data.csv"
                            }
                        ]
                    },
                    {
                        "sub_header": "Creating the scheduled-maintenance app",
                        "content": [
                            "You can create and deploy the resources for this example app step by step using the AWS Management Console or by using AWS SAM. In a production environment, we       recommend that you use an Infrustracture-as-Code (IaC) tool like AWS SAM to repeatably deploy serverless applications without using manual processes.",
                            "For this example, follow the console instructions to learn how to configure each AWS resource separately, or follow the AWS SAM instructions     to quickly deploy the app using AWS CLI commands.",
                            "ConsoleTo create the function using the AWS Management ConsoleFirst, create a function containing basic starter code. You then                 replace this code with your own function code by either copying and pasting the code directly in the Lambda code editor, or by uploading your code                as a .zip package. For this task, we recommend simply copying and pasting the code.Open the Functions page of the Lambda console.Choose Create function.Choose Author from scratch.Under Basic information, do the following:For Function name, enter ScheduledDBMaintenance.For Runtime choose the latest Python version.For Architecture, choose x86_64.Choose Create function.After your function is created, you can configure your function with the provided function code.In the Code source pane, replace the Hello world code that Lambda created with the Python function code from                        the lambda_function.py file that you saved earlier.In the DEPLOY section, choose Deploy to update your function's code:To configure the function memory and timeout (console)Select the Configuration tab for your function.In the General configuration pane, choose Edit.Set Memory to 256 MB and Timeout to 15 seconds.                  If you are processing a large table with many records, for example in the case of a production environment,                  you might consider setting Timeout to a larger number. This gives your function                  more time to scan, and clean the database.Choose Save.To configure the log format (console)You can configure Lambda functions to output logs in either unstructured text or JSON format. We recommend that you use JSON format for logs to                   make it easier to search and filter log data. To learn more about Lambda log configuration options, see Configuring advanced logging controls for Lambda functions.Select the Configuration tab for your function.Select Monitoring and operations tools.In the Logging configuration pane, choose Edit.For Logging configuration, select JSON.Choose Save.To set Up IAM permissionsTo give your function the permissions it needs to read and delete DynamoDB items, you need to add a policy to your function's                     execution role defining the necessary permissions.Open the Configuration tab, then choose                        Permissions from the left navigation bar.Choose the role name under Execution role.In the IAM console, choose Add permissions, then                        Create inline policy.Use the JSON editor and enter the following policy:{    \"Version\": \"2012-10-17\",    \"Statement\": [        {            \"Effect\": \"Allow\",            \"Action\": [                \"dynamodb:Scan\",                \"dynamodb:DeleteItem\",                \"dynamodb:BatchWriteItem\"            ],            \"Resource\": \"arn:aws:dynamodb:*:*:table/MyOrderTable\"        }    ]}Name the policy DynamoDBCleanupPolicy, then create it.To set up EventBridge Scheduler as a trigger (console)Open the EventBridge console.In the left navigation pane, choose Schedulers under the                        Scheduler section.Choose Create schedule.Configure the schedule by doing the following:Under Schedule name, enter a name for your schedule (for example, DynamoDBCleanupSchedule).Under Schedule pattern, choose Recurring schedule.For Schedule type leave the default as Cron-based schedule,                          then enter the following schedule details:Minutes: 0Hours: 3Day of month: 1Month: *Day of the week: ?Year: *When evaluated, this cron expression runs on the first day of every month at 03:00 AM.For Flexible time window, select Off.Choose Next.Configure the trigger for your Lambda function by doing the following:In the Target detail pane, leave Target API set to Templated targets,                         then select AWS Lambda Invoke.Under Invoke, select your Lambda function (ScheduledDBMaintenance) from the dropdown list.Leave the Payload empty and choose Next.Scroll down to Permissions and select Create a new role for this schedule.                            When you create a new EventBridge Scheduler schedule using the console, EventBridge Scheduler creates a new policy with the required                          permissions the schedule needs to invoke your function. For more information about managing your schedule permissions, see                          Cron-based schedules.                          in the EventBridge Scheduler User Guide.Choose Next.Review your settings and choose Create schedule to complete creation of the schedule and Lambda trigger.AWS SAMTo deploy the app using AWS SAMNavigate to the folder you saved the template.yaml file for the app in. Note that this example uses two template.yaml files.               Make sure they are saved in separate sub-folders and that you are in the correct folder containing the template to create the app.Copy the lambda_function.py and requirements.txt files you downloaded earlier to the same folder. The code location specified in the                 AWS SAM template is ./, meaning the current location. AWS SAM will search in this folder for the Lambda function code when you try to deploy the app.Run the following command.sam build --use-containerThis command gathers the build artifacts for the resources you want to deploy and places them in the proper format and                 location to deploy them. Specifying the --use-container option builds your function inside a Lambda-like Docker container.                 We use it here so you don't need to have Python 3.12 installed on your local machine for the build to work.To create the Lambda and EventBridge Scheduler resources specified in the template.yaml file, run the following command.sam deploy --guidedUsing the --guided flag means that AWS SAM will show you prompts to guide you through the deployment process. For this deployment,                 enter a Stack name of cron-maintenance-app, and accept the defaults for all other options by using Enter.When AWS SAM has finished creating the Lambda and EventBridge Scheduler resources, you should see the following message.Successfully created/updated stack - cron-maintenance-app in us-west-2You can additionally confirm that the Lambda function has been created by opening the Functions page of the Lambda console.                 You should see a function named ScheduledDBMaintenance.anchoranchorConsoleAWS SAMTo create the function using the AWS Management ConsoleFirst, create a function containing basic starter code. You then                 replace this code with your own function code by either copying and pasting the code directly in the Lambda code editor, or by uploading your code                as a .zip package. For this task, we recommend simply copying and pasting the code.Open the Functions page of the Lambda console.Choose Create function.Choose Author from scratch.Under Basic information, do the following:For Function name, enter ScheduledDBMaintenance.For Runtime choose the latest Python version.For Architecture, choose x86_64.Choose Create function.After your function is created, you can configure your function with the provided function code.In the Code source pane, replace the Hello world code that Lambda created with the Python function code from                        the lambda_function.py file that you saved earlier.In the DEPLOY section, choose Deploy to update your function's code:To configure the function memory and timeout (console)Select the Configuration tab for your function.In the General configuration pane, choose Edit.Set Memory to 256 MB and Timeout to 15 seconds.                  If you are processing a large table with many records, for example in the case of a production environment,                  you might consider setting Timeout to a larger number. This gives your function                  more time to scan, and clean the database.Choose Save.To configure the log format (console)You can configure Lambda functions to output logs in either unstructured text or JSON format. We recommend that you use JSON format for logs to                   make it easier to search and filter log data. To learn more about Lambda log configuration options, see Configuring advanced logging controls for Lambda functions.Select the Configuration tab for your function.Select Monitoring and operations tools.In the Logging configuration pane, choose Edit.For Logging configuration, select JSON.Choose Save.To set Up IAM permissionsTo give your function the permissions it needs to read and delete DynamoDB items, you need to add a policy to your function's                     execution role defining the necessary permissions.Open the Configuration tab, then choose                        Permissions from the left navigation bar.Choose the role name under Execution role.In the IAM console, choose Add permissions, then                        Create inline policy.Use the JSON editor and enter the following policy:{    \"Version\": \"2012-10-17\",    \"Statement\": [        {            \"Effect\": \"Allow\",            \"Action\": [                \"dynamodb:Scan\",                \"dynamodb:DeleteItem\",                \"dynamodb:BatchWriteItem\"            ],            \"Resource\": \"arn:aws:dynamodb:*:*:table/MyOrderTable\"        }    ]}Name the policy DynamoDBCleanupPolicy, then create it.To set up EventBridge Scheduler as a trigger (console)Open the EventBridge console.In the left navigation pane, choose Schedulers under the                        Scheduler section.Choose Create schedule.Configure the schedule by doing the following:Under Schedule name, enter a name for your schedule (for example, DynamoDBCleanupSchedule).Under Schedule pattern, choose Recurring schedule.For Schedule type leave the default as Cron-based schedule,                          then enter the following schedule details:Minutes: 0Hours: 3Day of month: 1Month: *Day of the week: ?Year: *When evaluated, this cron expression runs on the first day of every month at 03:00 AM.For Flexible time window, select Off.Choose Next.Configure the trigger for your Lambda function by doing the following:In the Target detail pane, leave Target API set to Templated targets,                         then select AWS Lambda Invoke.Under Invoke, select your Lambda function (ScheduledDBMaintenance) from the dropdown list.Leave the Payload empty and choose Next.Scroll down to Permissions and select Create a new role for this schedule.                            When you create a new EventBridge Scheduler schedule using the console, EventBridge Scheduler creates a new policy with the required                          permissions the schedule needs to invoke your function. For more information about managing your schedule permissions, see                          Cron-based schedules.                          in the EventBridge Scheduler User Guide.Choose Next.Review your settings and choose Create schedule to complete creation of the schedule and Lambda trigger."
                        ]
                    },
                    {
                        "sub_header": "Testing the app",
                        "content": [
                            "      To test that your schedule correctly triggers your function, and that your function correctly cleans records      from the database, you can temporarily modify your schedule to run once at a specific time. You can then run sam deploy again to      reset your recurrence schedule to run once a month.    ",
                            "To run the application using the AWS Management Console\nNavigate back to the EventBridge Scheduler console page.\n\nChoose your schedule, then choose Edit.\n\nIn the Schedule pattern section, under Recurrence, choose One-time schedule.\n\n\n          Set your invocation time to a few minutes from now, review your settings, then choose Save.\n        \n",
                            "      After the schedule runs and invokes its target, you run the test_app.py script to verify that your function successfully removed all old records      from the DynamoDB table.    ",
                            {
                                "code_example": "test_app.py"
                            }
                        ]
                    },
                    {
                        "sub_header": "Next steps",
                        "content": [
                            "      You can now modify the EventBridge Scheduler schedule to meet your partifuclar application requirements. EventBridge Scheduler supports the following schedule expressions: cron, rate, and one-time schedules.    ",
                            "      For more information about EventBridge Scheduler schedule expresssions, see Schedule types in the      EventBridge Scheduler User Guide.    "
                        ]
                    }
                ]
            }
        ],
        "sections": [
            "",
            "The following examples provide function code and  infrastructure as code (IaC) templates to quickly create and deploy serverless apps that implement some common Lambda uses cases. The  examples also include code examples and instructions to test the apps after you deploy them.",
            "For each of the example apps, we provide instructions to either create and configure resources manually using the AWS Management Console, or to   use the AWS Serverless Application Model to deploy the resources using IaC. Follow the console intructions to learn more about configuring the individual AWS   resources for each app, or use to AWS SAM to quickly deploy resources as you would in a production environment.",
            "You can use the provided examples as a basis for your own serverless applications by modifying the provided function code and templates   for your own use case.",
            "We're continuing to create new examples, so check back again to find more severless apps for common Lambda use cases.",
            {
                "sub_header": "Example apps",
                "content": []
            }
        ]
    },
    {
        "title": "Building with TypeScript",
        "href": "https://docs.aws.amazon.com/lambda/latest/dg/lambda-typescript.html",
        "contents": [
            {
                "title": "Handler",
                "href": "https://docs.aws.amazon.com/lambda/latest/dg/typescript-handler.html",
                "sections": [
                    "",
                    "The Lambda function handler is the method in your function code that processes events. When your function is  invoked, Lambda runs the handler method. Your function runs until the handler returns a response, exits, or times out.",
                    "  1.Typescript handler basics",
                    "  2.Using async/await",
                    "  3.Using callbacks",
                    "  4.Using types for the event object",
                    "  5.Code best practices for Typescript Lambda functions",
                    {
                        "sub_header": "Typescript handler basics",
                        "content": [
                            {
                                "code_example": "@types/aws-lambda"
                            },
                            "The runtime passes arguments to the handler method. The first argument is the event object,      which contains information from the invoker. The invoker passes this information as a JSON-formatted string when it      calls Invoke, and the runtime converts it to an object. When an AWS service invokes your function, the event      structure varies by service. With TypeScript, we recommend using type      annotations for the event object. For more information, see Using types for the event object.",
                            "The second argument is the context object, which contains information      about the invocation, function, and execution environment. In the preceding example, the function gets the name of      the log stream from the context object and returns it to the invoker.",
                            "You can also use a callback argument, which is a function that you can call in non-async handlers to send a response.      We recommend that you use async/await instead of callbacks. Async/await provides improved readability, error handling,      and efficiency. For more information about the differences between async/await and callbacks, see      Using callbacks."
                        ]
                    },
                    {
                        "sub_header": "Using async/await",
                        "content": [
                            "If your code performs an asynchronous task, use the async/await pattern to make sure that the handler finishes running. Async/await is a concise and readable way to write asynchronous code in Node.js, without the need for nested callbacks or chaining promises. With async/await, you can write code that reads like synchronous code, while still being asynchronous and non-blocking.",
                            "The async keyword marks a function as asynchronous, and the await keyword pauses the execution of the function until a Promise is resolved.",
                            {
                                "code_example": "fetch"
                            }
                        ]
                    },
                    {
                        "sub_header": "Using callbacks",
                        "content": [
                            "We recommend that you use async/await to declare the function handler instead of using callbacks. Async/await is a better choice for several reasons:",
                            "  1.Readability:  Async/await code is easier to read and understand than callback code, which can quickly become difficult to follow and result in callback hell.",
                            "  2.Debugging and error handling:  Debugging callback-based code can be difficult. The call stack can become hard to follow and errors can easily be swallowed. With async/await, you can use try/catch blocks to handle errors.",
                            "  3.Efficiency:  Callbacks often require switching between different parts of the code. Async/await can reduce the number of context switches, resulting in more efficient code.",
                            "When you use callbacks in your handler, the function continues to execute until the event loop is empty or the    function times out. The response isn't sent to the invoker until all event loop tasks are finished. If the    function times out, an error is returned instead. You can configure the runtime to send the response immediately    by setting context.callbackWaitsForEmptyEventLoop to false.",
                            "The callback function takes two      arguments: an Error and a response. The response object must be compatible with        JSON.stringify.",
                            {
                                "code_example": "@types/aws-lambda"
                            }
                        ]
                    },
                    {
                        "sub_header": "Using types for the event object",
                        "content": [
                            "We recommend that you don’t use the any type for the handler arguments and return type because you lose the ability to check types. Instead, generate an event using the sam local generate-event AWS Serverless Application Model CLI command, or use an open-source definition from the @types/aws-lambda package.",
                            {
                                "code_example": "sam local generate-event s3 put >> S3PutEvent.json"
                            },
                            {
                                "code_example": "npm install -D @types/aws-lambda"
                            }
                        ]
                    },
                    {
                        "sub_header": "Code best practices for Typescript Lambda functions",
                        "content": [
                            "Adhere to the guidelines in the following list to use best coding practices when building your Lambda functions:",
                            {
                                "code_example": "exports.myHandler = function(event, context, callback) {\n\tvar foo = event.foo;\n\tvar bar = event.bar;\n\tvar result = MyLambdaFunction (foo, bar);\n\n\tcallback(null, result);\n}\n\nfunction MyLambdaFunction (foo, bar) {\n\t// MyLambdaFunction logic here\n}"
                            },
                            {
                                "code_example": "/tmp"
                            }
                        ]
                    }
                ]
            },
            {
                "title": "Deploy .zip file archives",
                "href": "https://docs.aws.amazon.com/lambda/latest/dg/typescript-package.html",
                "sections": [
                    "",
                    "Before you can deploy TypeScript code to AWS Lambda, you need to transpile it into JavaScript. This page explains three ways to build and deploy TypeScript code to Lambda with .zip file archives:",
                    "AWS SAM and AWS CDK simplify building and deploying TypeScript functions. The AWS SAM template specification provides a simple and clean syntax to describe the Lambda functions, APIs, permissions, configurations, and events that make up your serverless application. The AWS CDK lets you build reliable, scalable, cost-effective applications in the cloud with the considerable expressive power of a programming language. The AWS CDK is intended for moderately to highly experienced AWS users. Both the AWS CDK and the AWS SAM use esbuild to transpile TypeScript code into JavaScript.",
                    {
                        "sub_header": "Using AWS SAM to deploy TypeScript code to Lambda",
                        "content": [
                            "Follow the steps below to download, build, and deploy a sample Hello World TypeScript application using the AWS SAM. This application implements a basic API backend. It consists of an Amazon API Gateway endpoint and a Lambda function. When you send a GET request to the API Gateway endpoint, the Lambda function is invoked. The function returns a hello world message.",
                            "NoteAWS SAM uses esbuild to create Node.js Lambda functions from TypeScript code. esbuild support is currently in public preview. During public preview, esbuild support may be subject to backwards incompatible changes.",
                            {
                                "code_example": "sam init --app-template hello-world-typescript --name sam-app --package-type Zip --runtime nodejs18.x"
                            }
                        ]
                    },
                    {
                        "sub_header": "Using the AWS CDK to deploy TypeScript code to Lambda",
                        "content": [
                            "Follow the steps below to build and deploy a sample TypeScript application using the AWS CDK. This application implements a basic API backend. It consists of an API Gateway endpoint and a Lambda function. When you send a GET request to the API Gateway endpoint, the Lambda function is invoked. The function returns a hello world message.",
                            {
                                "code_example": "mkdir hello-world\ncd hello-world"
                            }
                        ]
                    },
                    {
                        "sub_header": "Using the AWS CLI and esbuild to deploy TypeScript code to Lambda",
                        "content": [
                            "The following example demonstrates how to transpile and deploy TypeScript code to Lambda using esbuild and the AWS CLI. esbuild produces one JavaScript file with all dependencies. This is the only file that you need to add to the .zip archive.",
                            {
                                "code_example": "npm init"
                            }
                        ]
                    }
                ]
            },
            {
                "title": "Deploy container images",
                "href": "https://docs.aws.amazon.com/lambda/latest/dg/typescript-image.html",
                "sections": [
                    "",
                    "You can deploy your TypeScript code to an AWS Lambda function as a Node.js container image. AWS provides base images for Node.js to help you build the container image. These base images are preloaded with a language runtime and other components that are required to run the image on Lambda. AWS provides a Dockerfile for each of the base images to help with building your container image.",
                    "If you use a community or private enterprise base image, you must add the Node.js runtime interface client (RIC) to the base image to make it compatible with Lambda.",
                    "Lambda provides a runtime interface emulator for local testing. The AWS base images for Node.js include the runtime interface emulator. If you use an alternative base image, such as an Alpine Linux or Debian image, you can build the emulator into your image or install it on your local machine.",
                    {
                        "sub_header": "Using a Node.js base image to build and package TypeScript function code",
                        "content": [
                            {
                                "code_example": "npm"
                            }
                        ]
                    }
                ]
            },
            {
                "title": "Layers",
                "href": "https://docs.aws.amazon.com/lambda/latest/dg/typescript-layers.html",
                "sections": [
                    "",
                    "A Lambda layer is a .zip  file archive that contains supplementary code or data. Layers usually contain library dependencies, a  custom runtime, or configuration files. Creating a layer involves  three general steps:",
                    "\n\nPackage your layer content. This means creating a .zip file archive that contains the dependencies\n        you want to use in your functions.\n\nCreate the layer in Lambda.\n\nAdd the layer to your functions.\n",
                    "This topic contains steps and guidance on how to properly package and create a Node.js  Lambda layer with external library dependencies. Additionally, this topic explains how use your layer with a function written in TypeScript.",
                    {
                        "sub_header": "Prerequisites",
                        "content": [
                            "To follow the steps in this section, you must have the following:",
                            "Throughout this topic, we reference the layer-nodejs   sample application in the aws-lambda-developer-guide GitHub repository. This application contains scripts   that will package the lodash library into a Lambda layer. The layer   directory contains the scripts to generate the layer. The application also contains a TypeScript sample   function in the function-ts directory that uses the dependency from the layer.   After creating a layer, you can transpile, deploy and invoke the corresponding function to verify that everything   works. This document walks through how to create, package, deploy and test this layer using the TypeScript sample function.",
                            " This sample application uses the Node.js 20 runtime. If you add additional dependencies to your    layer, they must be compatible with Node.js 20."
                        ]
                    },
                    {
                        "sub_header": "Node.js layer compatibility with the Lambda runtime environment",
                        "content": [
                            "When you package code in a Node.js layer, you specify the Lambda runtime environments that the code is compatible with. To   assess code compatibility with a runtime, consider what versions of Node.js, what operating systems, and what   instruction set architectures the code is designed for.",
                            "Lambda Node.js runtimes specify their Node.js version and operating system. In this document, you will use the Node.js   20 runtime, which is based on AL2023. For more information about runtime versions, see Supported runtimes. When you create a Lambda function, you specify the instruction set architecture. In this   document, you will use the arm64 architecture. For more information about architectures in Lambda, see Selecting and configuring an instruction set architecture for your Lambda function.",
                            "When you use code provided in a package, each package maintainer independently defines their compatibility. Most   Node.js development is designed to work independently of operating system and instruction set architecture.   Additionally, breaking incompatibilities with new Node.js versions are not that common. Expect to spend more of your   time assessing compatibility between packages than assessing package compatibility with Node.js version,   operating system, or instruction set architecture.",
                            "Sometimes Node.js packages include compiled code, which require you to consider operating system and    instruction set architecture compatibility. If you do need to assess code compatibility for your packages, you will need to inspect the   packages and their documentation. Packages in NPM can specify their compatibility through the engines,   os, and cpu fields of their package.json manifest file. For more information   about package.json files, see package.json in the NPM documentation."
                        ]
                    },
                    {
                        "sub_header": "Layer paths for Node.js runtimes",
                        "content": [
                            "When you add a layer to a function, Lambda loads the layer content into the execution environment.    If your layer packages dependencies in specific folder paths,    the Node.js execution environment will recognize the modules, and you can reference the modules from your    function code.",
                            "To ensure that your modules are picked up, package them into your layer .zip file in one of the following folder paths.   These files are stored in /opt, and the folder paths are loaded into the PATH environment variable.",
                            {
                                "code_example": "nodejs/node_modules"
                            },
                            "For example, the resulting layer .zip file that you create in this tutorial has the   following directory structure:",
                            "layer_content.zip└ nodejs    └ node20        └ node_modules            └ lodash            └ <other potential dependencies>            └ ...",
                            "You will put the lodash library   in the nodejs/node20/node_modules directory. This   ensures that Lambda can locate the library during function invocations."
                        ]
                    },
                    {
                        "sub_header": "Packaging the layer content",
                        "content": [
                            "In this example, you package the lodash library in a layer .zip file. Complete the following steps  to install and package the layer content.",
                            {
                                "code_example": "sample-apps/layer-nodejs"
                            }
                        ]
                    },
                    {
                        "sub_header": "Creating the layer",
                        "content": [
                            "Take the layer_content.zip file that you   generated in the previous section and upload it as a Lambda layer. You can upload a layer using   the AWS Management Console or the Lambda API via the AWS Command Line Interface (AWS CLI). When you upload your layer .zip   file, in the following PublishLayerVersion AWS CLI command, specify   nodejs20.x as the compatible runtime and arm64 as the compatible   architecture.",
                            "aws lambda publish-layer-version --layer-name nodejs-lodash-layer \\    --zip-file fileb://layer_content.zip \\    --compatible-runtimes nodejs20.x \\    --compatible-architectures \"arm64\"",
                            "From the response, note the LayerVersionArn, which looks like   arn:aws:lambda:us-east-1:123456789012:layer:nodejs-lodash-layer:1.   You'll need this Amazon Resource Name (ARN) in the next step of this tutorial, when you add   the layer to your function."
                        ]
                    },
                    {
                        "sub_header": "Adding the layer to your function",
                        "content": [
                            "Deploy a sample Lambda function that uses the lodash library in its function code, then attach the   layer you created. To create a Lambda function using function code written in TypeScript, you must transpile the   TypeScript to JavaScript for use by the Node.js runtime. For more information about this process, see Define Lambda function handler in TypeScript. For better compatibility, use tsc to transpile your TypeScript module when you   distribute your dependencies with layers. If you bundle your dependencies, consider using esbuild. For more   information about bundling with esbuild, see Deploy transpiled TypeScript code in Lambda with .zip file archives.",
                            "To deploy the function, you need an execution role. For more   information, see   Defining Lambda function permissions with an execution role. If you don't have an existing execution role,   follow the steps in the collapsible section. Otherwise, skip to the next section to deploy the   function.",
                            {
                                "code_example": "lambda-role"
                            },
                            "The sample function code uses the lodash _.findLastIndex method to read through an array of objects. It   compares the objects against a criteria to find the index of a match. Then, it returns the index and value of the   object in the Lambda response.",
                            "import { Handler } from 'aws-lambda';import * as _ from 'lodash';type User = {  user: string;  active: boolean;}type UserResult = {  statusCode: number;  body: string;}const users: User[] = [  { 'user': 'Carlos', 'active': true },  { 'user': 'Gil-dong', 'active': false },  { 'user': 'Pat', 'active': false }];export const handler: Handler<any, UserResult> = async (): Promise<UserResult> => {  let out = _.findLastIndex(users, (user: User) => { return user.user == 'Pat'; });  const response = {    statusCode: 200,    body: JSON.stringify(out + \", \" + users[out].user),  };  return response;};",
                            {
                                "code_example": "function-ts/"
                            },
                            {
                                "code_example": "delete"
                            }
                        ]
                    }
                ]
            },
            {
                "title": "Context",
                "href": "https://docs.aws.amazon.com/lambda/latest/dg/typescript-context.html",
                "sections": [
                    "",
                    "When Lambda runs your function, it passes a context object to the handler.    This object provides methods and properties that provide information about the invocation, function, and execution    environment.",
                    {
                        "code_example": "getRemainingTimeInMillis()"
                    },
                    {
                        "code_example": "functionName"
                    },
                    "You can use the @types/aws-lambda npm package to work with the context object.",
                    {
                        "code_example": "@types/aws-lambda"
                    }
                ]
            },
            {
                "title": "Logging",
                "href": "https://docs.aws.amazon.com/lambda/latest/dg/typescript-logging.html",
                "sections": [
                    "",
                    "AWS Lambda automatically monitors Lambda functions and sends log entries to Amazon CloudWatch. Your Lambda function     comes with a CloudWatch Logs log group and a log stream for each instance of your function. The Lambda runtime environment     sends details about each invocation and other output from your function's     code to the log stream. For more information about CloudWatch Logs, see Using CloudWatch Logs with Lambda.",
                    "To output logs from your function code, you can use methods on the console object. For more detailed logging, you can use any logging library that writes to stdout or stderr.",
                    "  1.Using logging tools and libraries",
                    "  2.Using Powertools for AWS Lambda (TypeScript) and AWS SAM for structured logging",
                    "  3.Using Powertools for AWS Lambda (TypeScript) and the AWS CDK for structured logging",
                    "  4.Viewing logs in the Lambda console",
                    "  5.Viewing logs in the CloudWatch console",
                    {
                        "sub_header": "Using logging tools and libraries",
                        "content": [
                            "Powertools for AWS Lambda (TypeScript) is a developer toolkit to implement Serverless best       practices and increase developer velocity. The Logger utility       provides a Lambda optimized logger which includes additional information about function context across all your functions with output structured as JSON. Use this       utility to do the following:"
                        ]
                    },
                    {
                        "sub_header": "Using Powertools for AWS Lambda (TypeScript) and AWS SAM for structured logging",
                        "content": [
                            "Follow the steps below to download, build, and deploy a sample Hello World TypeScript application with integrated Powertools for AWS Lambda (TypeScript) modules using the AWS SAM. This application implements a       basic API backend and uses Powertools for emitting logs, metrics, and traces. It consists of an Amazon API Gateway endpoint and a Lambda function.       When you send a GET request to the API Gateway endpoint, the Lambda function invokes, sends logs and metrics using Embedded Metric Format to CloudWatch, and       sends traces to AWS X-Ray. The function returns a hello world message.",
                            {
                                "code_example": "sam init --app-template hello-world-powertools-typescript --name sam-app --package-type Zip --runtime nodejs18.x"
                            },
                            {
                                "sub_header": "Managing log retention",
                                "content": [
                                    "Log groups aren't deleted automatically when you delete a function. To avoid storing logs indefinitely, delete    the log group, or configure a retention period after which CloudWatch automatically deletes the logs. To set up log retention, add the following to your AWS SAM template:",
                                    "Resources:  HelloWorldFunction:    Type: AWS::Serverless::Function    Properties:    # Omitting other properties    LogGroup:    Type: AWS::Logs::LogGroup    Properties:      LogGroupName: !Sub \"/aws/lambda/${HelloWorldFunction}\"      RetentionInDays: 7"
                                ]
                            }
                        ]
                    },
                    {
                        "sub_header": "Using Powertools for AWS Lambda (TypeScript) and the AWS CDK for structured logging",
                        "content": [
                            "Follow the steps below to download, build, and deploy a sample Hello World TypeScript application with integrated Powertools for AWS Lambda (TypeScript) modules using the AWS CDK. This application implements a               basic API backend and uses Powertools for emitting logs, metrics, and traces. It consists of an Amazon API Gateway endpoint and a Lambda function.               When you send a GET request to the API Gateway endpoint, the Lambda function invokes, sends logs and metrics using Embedded Metric Format to CloudWatch, and               sends traces to AWS X-Ray. The function returns a hello world message.",
                            {
                                "code_example": "mkdir hello-world\ncd hello-world"
                            }
                        ]
                    },
                    {
                        "sub_header": "Viewing logs in the Lambda console",
                        "content": [
                            "You can use the Lambda console to view log output after you invoke a Lambda function.",
                            "If your code can be tested from the embedded Code editor, you will find logs in the execution results. When you use the console test feature to invoke a function, you'll find Log output in the Details section."
                        ]
                    },
                    {
                        "sub_header": "Viewing logs in the CloudWatch console",
                        "content": [
                            "You can use the Amazon CloudWatch console to view logs for all Lambda function invocations.",
                            {
                                "code_example": "your-function-name"
                            },
                            "Each log stream corresponds to an instance of your function. A log stream appears when you update your Lambda function, and when additional instances are created to handle multiple concurrent invocations. To find logs for a specific invocation, we recommend instrumenting your function with AWS X-Ray. X-Ray records details about the request and the log stream in the trace."
                        ]
                    }
                ]
            },
            {
                "title": "Tracing",
                "href": "https://docs.aws.amazon.com/lambda/latest/dg/typescript-tracing.html",
                "sections": [
                    "",
                    "Lambda integrates with AWS X-Ray to help you trace, debug, and optimize Lambda applications. You can use X-Ray    to trace a request as it traverses resources in your application, which may include Lambda functions and other AWS    services.",
                    "To send tracing data to X-Ray, you can use one of three SDK libraries:",
                    "Each of the SDKs offer ways to send your telemetry data to the X-Ray service.    You can then use X-Ray to view, filter, and gain insights into your application's performance metrics to identify    issues and opportunities for optimization.",
                    "ImportantThe X-Ray and Powertools for AWS Lambda SDKs are part of a tightly integrated instrumentation solution offered by AWS. \nThe ADOT Lambda Layers are part of an industry-wide standard for tracing instrumentation that collect more data in general, but may not be \nsuited for all use cases. You can implement end-to-end tracing in X-Ray using either solution. To learn more about choosing between them, see \nChoosing between the AWS \nDistro for Open Telemetry and X-Ray SDKs.",
                    "  1.Using Powertools for AWS Lambda (TypeScript) and AWS SAM for tracing",
                    "  2.Using Powertools for AWS Lambda (TypeScript) and the AWS CDK for tracing",
                    "  3.Interpreting an X-Ray trace",
                    {
                        "sub_header": "Using Powertools for AWS Lambda (TypeScript) and AWS SAM for tracing",
                        "content": [
                            "Follow the steps below to download, build, and deploy a sample Hello World TypeScript application with integrated Powertools for AWS Lambda (TypeScript) modules using the AWS SAM. This application implements a       basic API backend and uses Powertools for emitting logs, metrics, and traces. It consists of an Amazon API Gateway endpoint and a Lambda function.       When you send a GET request to the API Gateway endpoint, the Lambda function invokes, sends logs and metrics using Embedded Metric Format to CloudWatch, and       sends traces to AWS X-Ray. The function returns a hello world message.",
                            {
                                "code_example": "sam init --app-template hello-world-powertools-typescript --name sam-app --package-type Zip --runtime nodejs18.x --no-tracing"
                            },
                            "X-Ray doesn't trace all requests to your application. X-Ray applies a sampling algorithm    to ensure that tracing is efficient, while still providing a representative sample of all requests. The sampling rate is    1 request per second and 5 percent of additional requests. You can't configure the X-Ray sampling rate for your functions."
                        ]
                    },
                    {
                        "sub_header": "Using Powertools for AWS Lambda (TypeScript) and the AWS CDK for tracing",
                        "content": [
                            "Follow the steps below to download, build, and deploy a sample Hello World TypeScript application with integrated Powertools for AWS Lambda (TypeScript) modules using the AWS CDK. This application implements a               basic API backend and uses Powertools for emitting logs, metrics, and traces. It consists of an Amazon API Gateway endpoint and a Lambda function.               When you send a GET request to the API Gateway endpoint, the Lambda function invokes, sends logs and metrics using Embedded Metric Format to CloudWatch, and               sends traces to AWS X-Ray. The function returns a hello world message.",
                            {
                                "code_example": "mkdir hello-world\ncd hello-world"
                            }
                        ]
                    },
                    {
                        "sub_header": "Interpreting an X-Ray trace",
                        "content": [
                            "After you've configured active tracing, you can observe specific requests    through your application. The     X-Ray trace map provides information about your application and all its components. The following example shows a trace from    the sample application:",
                            "\n\n"
                        ]
                    }
                ]
            }
        ],
        "sections": [
            "",
            "You can use the Node.js runtime to run TypeScript code in AWS Lambda. Because Node.js doesn't run TypeScript code natively, you must first     transpile your TypeScript code into JavaScript. Then, use the JavaScript files to deploy your function code to Lambda. Your code runs in an     environment that includes the AWS SDK for JavaScript, with credentials from an AWS Identity and Access Management (IAM) role that you manage. To learn more     about the SDK versions included with the Node.js runtimes, see Runtime-included SDK versions.",
            "Lambda supports the following Node.js runtimes.",
            {
                "code_example": "nodejs22.x"
            },
            "  1.Setting up a TypeScript development environment",
            "  2.Define Lambda function handler in TypeScript",
            "  3.Deploy transpiled TypeScript code in Lambda with .zip file archives",
            "  4.Deploy transpiled TypeScript code in Lambda with container images",
            "  5.Working with layers for TypeScript Lambda functions",
            "  6.Using the Lambda context object to retrieve TypeScript function information",
            "  7.Log and monitor TypeScript Lambda functions",
            "  8.Tracing TypeScript code in AWS Lambda",
            {
                "sub_header": "Setting up a TypeScript development environment",
                "content": [
                    "Use a local integrated development environment (IDE), text editor, or AWS Cloud9 to write your TypeScript function code. You can’t create TypeScript code on the Lambda console.",
                    "To transpile your TypeScript code, set up a compiler such as esbuild or Microsoft's TypeScript compiler (tsc) , which is bundled with the TypeScript distribution. You can use the AWS Serverless Application Model (AWS SAM) or the AWS Cloud Development Kit (AWS CDK) to simplify building and deploying TypeScript code. Both tools use esbuild to transpile TypeScript code into JavaScript.",
                    "When using esbuild, consider the following:",
                    {
                        "code_example": "tsc"
                    },
                    {
                        "code_example": " {\n  \"compilerOptions\": {\n    \"target\": \"es2020\",\n    \"strict\": true,\n    \"preserveConstEnums\": true,\n    \"noEmit\": true,\n    \"sourceMap\": false,\n    \"module\":\"commonjs\",\n    \"moduleResolution\":\"node\",\n    \"esModuleInterop\": true, \n    \"skipLibCheck\": true,\n    \"forceConsistentCasingInFileNames\": true, \n    \"isolatedModules\": true, \n  },\n  \"exclude\": [\"node_modules\", \"**/*.test.ts\"]\n}"
                    }
                ]
            }
        ]
    },
    {
        "title": "Integrating other services",
        "href": "https://docs.aws.amazon.com/lambda/latest/dg/lambda-services.html",
        "contents": [
            {
                "title": "Apache Kafka",
                "href": "https://docs.aws.amazon.com/lambda/latest/dg/with-kafka.html",
                "contents": [
                    {
                        "title": "Configure event source",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/with-kafka-configure.html",
                        "sections": [
                            "",
                            "Before you create an event source mapping for your self-managed Apache Kafka cluster, you need to ensure that your cluster and the VPC     it resides in are correctly configured. You also need to make sure that your Lambda function's execution role has     the necessary IAM permissions.",
                            "Follow the instructions in the following sections to configure your self-managed Apache Kafka cluster and Lambda function. To learn how to     create the event source mapping, see Adding a Kafka cluster as an event source.",
                            "  1.Kafka cluster authentication",
                            "  2.API access and Lambda function permissions",
                            "  3.Configure network security",
                            {
                                "sub_header": "Kafka cluster authentication",
                                "content": [
                                    "Lambda supports several methods to authenticate with your self-managed Apache Kafka cluster. Make sure that you configure the      Kafka cluster to use one of these supported authentication methods. For more information about Kafka security, see      the Security section of the Kafka      documentation.",
                                    {
                                        "sub_header": "SASL/SCRAM authentication",
                                        "content": [
                                            "Lambda supports Simple Authentication and Security Layer/Salted Challenge Response Authentication Mechanism        (SASL/SCRAM) authentication with Transport Layer Security (TLS) encryption (SASL_SSL). Lambda sends the encrypted credentials to authenticate with        the cluster. Lambda doesn't support SASL/SCRAM with plaintext (SASL_PLAINTEXT). For more information about SASL/SCRAM authentication, see RFC 5802.",
                                            "Lambda also supports SASL/PLAIN authentication. Because this mechanism uses clear text credentials, the connection to the        server must use TLS encryption to ensure that the credentials are protected.",
                                            "For SASL authentication, you store the sign-in credentials as a secret in AWS Secrets Manager. For more information        about using Secrets Manager, see Tutorial: Create and retrieve a secret in the AWS Secrets Manager User Guide.",
                                            "ImportantTo use Secrets Manager for authentication, secrets must be stored in the same AWS region as your Lambda function."
                                        ]
                                    },
                                    {
                                        "sub_header": "Mutual TLS authentication",
                                        "content": [
                                            "Mutual TLS (mTLS) provides two-way authentication between the client and server. The client sends a        certificate to the server for the server to verify the client, and the server sends a certificate to the client        for the client to verify the server. ",
                                            "In self-managed Apache Kafka, Lambda acts as the client. You configure a client certificate (as a secret in Secrets Manager) to        authenticate Lambda with your Kafka brokers. The client certificate must be signed by a CA in the server's trust        store.",
                                            "The Kafka cluster sends a server certificate to Lambda to authenticate the Kafka brokers with Lambda. The        server certificate can be a public CA certificate or a private CA/self-signed certificate. The public CA        certificate must be signed by a certificate authority (CA) that's in the Lambda trust store. For a private        CA/self-signed certificate, you configure the server root CA certificate (as a secret in Secrets Manager). Lambda uses        the root certificate to verify the Kafka brokers.",
                                            "For more information about mTLS, see         Introducing mutual TLS authentication for Amazon MSK as an event source."
                                        ]
                                    },
                                    {
                                        "sub_header": "Configuring the client certificate secret",
                                        "content": [
                                            "The CLIENT_CERTIFICATE_TLS_AUTH secret requires a certificate field and a private key field. For an        encrypted private key, the secret requires a private key password. Both the certificate and private key must be        in PEM format.",
                                            "NoteLambda supports the PBES1 (but not\n        PBES2) private key encryption algorithms.",
                                            "The certificate field must contain a list of certificates, beginning with the client certificate, followed        by any intermediate certificates, and ending with the root certificate. Each certificate must start on a new        line with the following structure:",
                                            "-----BEGIN CERTIFICATE-----          <certificate contents>-----END CERTIFICATE-----      ",
                                            "Secrets Manager supports secrets up to 65,536 bytes, which is enough space for long certificate chains.",
                                            "The private key must be in PKCS #8        format, with the following structure:",
                                            "-----BEGIN PRIVATE KEY-----           <private key contents>-----END PRIVATE KEY-----            ",
                                            "For an encrypted private key, use the following structure:",
                                            "-----BEGIN ENCRYPTED PRIVATE KEY-----            <private key contents>-----END ENCRYPTED PRIVATE KEY-----           ",
                                            "The following example shows the contents of a secret for mTLS authentication using an encrypted private key.        For an encrypted private key, include the private key password in the secret.",
                                            "{\"privateKeyPassword\":\"testpassword\",\"certificate\":\"-----BEGIN CERTIFICATE-----MIIE5DCCAsygAwIBAgIRAPJdwaFaNRrytHBto0j5BA0wDQYJKoZIhvcNAQELBQAw...j0Lh4/+1HfgyE2KlmII36dg4IMzNjAFEBZiCRoPimO40s1cRqtFHXoal0QQbIlxkcmUuiAii9R0=-----END CERTIFICATE----------BEGIN CERTIFICATE-----MIIFgjCCA2qgAwIBAgIQdjNZd6uFf9hbNC5RdfmHrzANBgkqhkiG9w0BAQsFADBb...rQoiowbbk5wXCheYSANQIfTZ6weQTgiCHCCbuuMKNVS95FkXm0vqVD/YpXKwA/noc8PH3PSoAaRwMMgOSA2ALJvbRz8mpg==-----END CERTIFICATE-----\",\"privateKey\":\"-----BEGIN ENCRYPTED PRIVATE KEY-----MIIFKzBVBgkqhkiG9w0BBQ0wSDAnBgkqhkiG9w0BBQwwGgQUiAFcK5hT/X7Kjmgp...QrSekqF+kWzmB6nAfSzgO9IaoAaytLvNgGTckWeUkWn/V0Ck+LdGUXzAC4RxZnoQzp2mwJn2NYB7AZ7+imp0azDZb+8YG2aUCiyqb6PnnA==-----END ENCRYPTED PRIVATE KEY-----\"}"
                                        ]
                                    },
                                    {
                                        "sub_header": "Configuring the server root CA certificate secret",
                                        "content": [
                                            "You create this secret if your Kafka brokers use TLS encryption with certificates signed by a private CA.        You can use TLS encryption for VPC, SASL/SCRAM, SASL/PLAIN, or mTLS authentication.",
                                            "The server root CA certificate secret requires a field that contains the Kafka broker's root CA certificate        in PEM format. The following example shows the structure of the secret.",
                                            "{\"certificate\":\"-----BEGIN CERTIFICATE-----MIID7zCCAtegAwIBAgIBADANBgkqhkiG9w0BAQsFADCBmDELMAkGA1UEBhMCVVMxEDAOBgNVBAgTB0FyaXpvbmExEzARBgNVBAcTClNjb3R0c2RhbGUxJTAjBgNVBAoTHFN0YXJmaWVsZCBUZWNobm9sb2dpZXMsIEluYy4xOzA5BgNVBAMTMlN0YXJmaWVsZCBTZXJ2aWNlcyBSb290IENlcnRpZmljYXRlIEF1dG...-----END CERTIFICATE-----\"}"
                                        ]
                                    }
                                ]
                            },
                            {
                                "sub_header": "API access and Lambda function permissions",
                                "content": [
                                    "In addition to accessing your self-managed Kafka cluster, your Lambda function needs permissions to perform      various API actions. You add these permissions to the function's execution role. If your users need access to any API actions, add the required permissions to the      identity policy for the AWS Identity and Access Management (IAM) user or role.",
                                    {
                                        "sub_header": "Required Lambda function permissions",
                                        "content": [
                                            "To create and store logs in a log group in Amazon CloudWatch Logs, your Lambda function must have the following        permissions in its execution role:"
                                        ]
                                    },
                                    {
                                        "sub_header": "Optional Lambda function permissions",
                                        "content": [
                                            "Your Lambda function might also need permissions to:",
                                            {
                                                "sub_header": "Secrets Manager and AWS KMS permissions",
                                                "content": [
                                                    "Depending on the type of access control that you're configuring for your Kafka brokers, your Lambda function          might need permission to access your Secrets Manager secret or to decrypt your AWS KMS customer managed key. To access these          resources, your function's execution role must have the following permissions:"
                                                ]
                                            },
                                            {
                                                "sub_header": "VPC permissions",
                                                "content": [
                                                    "If only users within a VPC can access your self-managed Apache Kafka cluster, your Lambda function must have permission to          access your Amazon VPC resources. These resources include your VPC, subnets, security groups, and network          interfaces. To access these resources, your function's execution role must have the following          permissions:"
                                                ]
                                            }
                                        ]
                                    },
                                    {
                                        "sub_header": "Adding permissions to your execution role",
                                        "content": [
                                            "To access other AWS services that your self-managed Apache Kafka cluster uses, Lambda uses the permissions policies that you        define in your Lambda function's execution role.",
                                            "By default, Lambda is not permitted to perform the required or optional actions for a self-managed Apache Kafka cluster. You        must create and define these actions in an IAM trust policy for your execution role. This example shows how you might create a policy that allows        Lambda to access your Amazon VPC resources.",
                                            "{        \"Version\":\"2012-10-17\",        \"Statement\":[           {              \"Effect\":\"Allow\",              \"Action\":[                 \"ec2:CreateNetworkInterface\",                 \"ec2:DescribeNetworkInterfaces\",                 \"ec2:DescribeVpcs\",                 \"ec2:DeleteNetworkInterface\",                 \"ec2:DescribeSubnets\",                 \"ec2:DescribeSecurityGroups\"              ],              \"Resource\":\"*\"           }        ]     }"
                                        ]
                                    },
                                    {
                                        "sub_header": "Granting users access with an IAM policy",
                                        "content": [
                                            "By default, users and roles don't have permission to perform event source API operations. To grant access to users in your        organization or account, you create or update an identity-based policy. For more information, see Controlling access to AWS resources          using policies in the IAM User Guide."
                                        ]
                                    }
                                ]
                            },
                            {
                                "sub_header": "Configure network security",
                                "content": [
                                    "To give Lambda full access to self-managed Apache Kafka through your event source mapping, either your cluster must use a public endpoint             (public IP address), or you must provide access to the Amazon VPC you created the cluster in.",
                                    "When you use self-managed Apache Kafka with Lambda, create AWS PrivateLink VPC endpoints that provide your function            access to the resources in your Amazon VPC.",
                                    "NoteAWS PrivateLink VPC endpoints are required for functions with event source mappings that use the default (on-demand) mode\n                for event pollers. If your event source mapping uses \n                provisioned mode, you don't need to configure AWS PrivateLink VPC endpoints.",
                                    "Create an endpoint to provide access to the following resources:",
                                    "Alternatively, configure a NAT gateway on each public subnet in the Amazon VPC. For more information,             see Enable internet access for VPC-connected Lambda functions.",
                                    "When you create an event source mapping for self-managed Apache Kafka, Lambda checks whether Elastic Network Interfaces (ENIs)             are already present for the subnets and security groups configured for your Amazon VPC. If Lambda finds existing ENIs, it             attempts to re-use them. Otherwise, Lambda creates new ENIs to connect to the event source and invoke your function.",
                                    "NoteLambda functions always run inside VPCs owned by the Lambda service. Your function's VPC configuration\n                does not affect the event source mapping. Only the networking configuration of the event source's determines \n                how Lambda connects to your event source.",
                                    "Configure the security groups for the Amazon VPC containing your cluster. By default,            self-managed Apache Kafka uses the following ports: 9092.",
                                    {
                                        "code_example": "443"
                                    },
                                    "If your cluster uses authentication, you can also restrict the endpoint policy for the Secrets Manager endpoint.             To call the Secrets Manager API, Lambda uses your function role, not the Lambda service principal.",
                                    {
                                        "code_example": "{\n      \"Statement\": [\n          {\n              \"Action\": \"secretsmanager:GetSecretValue\",\n              \"Effect\": \"Allow\",\n              \"Principal\": {\n                  \"AWS\": [\n                      \"arn:aws::iam::123456789012:role/my-role\"\n                  ]\n              },\n              \"Resource\": \"arn:aws::secretsmanager:us-west-2:123456789012:secret:my-secret\"\n          }\n      ]\n  }"
                                    },
                                    "When you use Amazon VPC endpoints, AWS routes your API calls to invoke your function using the endpoint's Elastic Network Interface (ENI).            The Lambda service principal needs to call lambda:InvokeFunction on any roles and functions that use those ENIs.",
                                    "By default, Amazon VPC endpoints have open IAM policies that allow broad access to resources. Best practice is to restrict these            policies to perform the needed actions using that endpoint. To ensure that your event source mapping is able to invoke your Lambda            function, the VPC endpoint policy must allow the Lambda service principal to call sts:AssumeRole and            lambda:InvokeFunction. Restricting your VPC endpoint policies to allow only API calls originating within your organization            prevents the event source mapping from functioning properly, so \"Resource\": \"*\" is required in these policies.",
                                    "The following example VPC endpoint policies show how to grant the required access to the Lambda service principal for the            AWS STS and Lambda endpoints.",
                                    {
                                        "code_example": "{\n      \"Statement\": [\n          {\n              \"Action\": \"sts:AssumeRole\",\n              \"Effect\": \"Allow\",\n              \"Principal\": {\n                  \"Service\": [\n                      \"lambda.amazonaws.com\"\n                  ]\n              },\n              \"Resource\": \"*\"\n          }\n      ]\n    }"
                                    },
                                    {
                                        "code_example": "{\n      \"Statement\": [\n          {\n              \"Action\": \"lambda:InvokeFunction\",\n              \"Effect\": \"Allow\",\n              \"Principal\": {\n                  \"Service\": [\n                      \"lambda.amazonaws.com\"\n                  ]\n              },\n              \"Resource\": \"*\"\n          }\n      ]\n  }"
                                    }
                                ]
                            }
                        ]
                    },
                    {
                        "title": "Process messages",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/with-kafka-process.html",
                        "sections": [
                            "",
                            "NoteIf you want to send data to a target other than a Lambda function or enrich the data before sending it, see \n    Amazon EventBridge Pipes.",
                            "  1.Adding a Kafka cluster as an event source",
                            "  2.Self-managed Apache Kafka configuration parameters",
                            "  3.Using a Kafka cluster as an event source",
                            "  4.Polling and stream starting positions",
                            "  5.Message throughput scaling behavior for self-managed Apache Kafka event source mappings",
                            "  6.Amazon CloudWatch metrics",
                            {
                                "sub_header": "Adding a Kafka cluster as an event source",
                                "content": [
                                    "To create an event source mapping, add your Kafka      cluster as a Lambda function trigger using the Lambda      console, an AWS SDK, or the AWS Command Line Interface (AWS CLI).",
                                    "This section describes how to create an event source mapping using the Lambda console and the AWS CLI.",
                                    {
                                        "sub_header": "Prerequisites",
                                        "content": []
                                    },
                                    {
                                        "sub_header": "Customizable consumer group ID",
                                        "content": [
                                            "When setting up Kafka as an event source, you can specify a consumer group ID. This consumer group ID is an    existing identifier for the Kafka consumer group that you want your Lambda function to join. You can use this feature to seamlessly migrate any    ongoing Kafka record processing setups from other consumers to Lambda.",
                                            "If you specify a consumer group ID and there are other active pollers within that consumer group, Kafka distributes messages across      all consumers. In other words, Lambda doesn't receive all message for the Kafka topic. If you want Lambda to handle all messages in the      topic, turn off any other pollers in that consumer group.",
                                            "Additionally, if you specify a consumer group ID, and Kafka finds a valid existing consumer group with the same ID, Lambda ignores the      StartingPosition parameter for your event source mapping. Instead, Lambda begins processing records according to the committed      offset of the consumer group. If you specify a consumer group ID, and Kafka cannot find an existing consumer group, then Lambda configures your      event source with the specified StartingPosition.",
                                            "The consumer group ID that you specify must be unique among all your Kafka event sources. After creating a Kafka event source mapping      with the consumer group ID specified, you cannot update this value."
                                        ]
                                    },
                                    {
                                        "sub_header": "Adding a self-managed Kafka cluster (console)",
                                        "content": [
                                            "Follow these steps to add your self-managed Apache Kafka cluster and a Kafka topic as a trigger for your Lambda function.",
                                            "  3.Function overview Under , choose Add trigger.",
                                            "  4.Trigger configuration Under , do the following:",
                                            "  5.Apache Kafka Choose the  trigger type.",
                                            "  6.Bootstrap servers For , enter the host and port pair address of a Kafka broker\n                in your cluster, and then choose Add. Repeat for each Kafka broker in the\n                cluster.",
                                            "  7.Topic name For , enter the name of the Kafka topic used to store records in the\n                cluster.",
                                            "  8.Batch size (Optional) For , enter the maximum number of records to receive in a\n                single batch.",
                                            "  9.Batch window For , enter the maximum amount of seconds that Lambda spends\n                gathering records before invoking the function.",
                                            "  10.Consumer group ID (Optional) For , enter the ID of a Kafka consumer group to join.",
                                            "  11.Starting position (Optional) For , choose Latest to start\n                reading the stream from the latest record, Trim horizon to start at the\n                earliest available record, or At timestamp to specify a timestamp to start\n                reading from.",
                                            "  12.VPC (Optional) For , choose the Amazon  for your Kafka cluster. Then, choose the\n                 subnets and  security groups.",
                                            "  13.Authentication (Optional) For , choose Add, and then do the\n                following:",
                                            "  15.BASIC_AUTH If your Kafka broker uses SASL/PLAIN authentication, choose\n                        .",
                                            "  16.SASL_SCRAM If your broker uses SASL/SCRAM authentication, choose one of the\n                         protocols.",
                                            "  17.CLIENT_CERTIFICATE_TLS_AUTH If you're configuring mTLS authentication, choose the\n                         protocol.",
                                            "  19.Encryption (Optional) For , choose the Secrets Manager secret containing the root CA\n                certificate that your Kafka brokers use for TLS encryption, if your Kafka brokers use certificates\n                signed by a private CA.",
                                            "  20.Enable\n                trigger To create the trigger in a disabled state for testing (recommended), clear . Or, to enable the trigger immediately, select Enable\n                  trigger.",
                                            "  21.Add To create the trigger, choose ."
                                        ]
                                    },
                                    {
                                        "sub_header": "Adding a self-managed Kafka cluster (AWS CLI)",
                                        "content": [
                                            "Use the following example AWS CLI commands to create and view a self-managed Apache Kafka trigger for your Lambda function.",
                                            {
                                                "sub_header": "Using SASL/SCRAM",
                                                "content": [
                                                    "If Kafka users access your Kafka brokers over the internet, specify the Secrets Manager secret that you created          for SASL/SCRAM authentication. The following example uses the create-event-source-mapping AWS CLI command to map a Lambda function named my-kafka-function to a Kafka topic named AWSKafkaTopic.",
                                                    "aws lambda create-event-source-mapping \\   --topics AWSKafkaTopic \\  --source-access-configuration Type=SASL_SCRAM_512_AUTH,URI=arn:aws:secretsmanager:us-east-1:111122223333:secret:MyBrokerSecretName \\  --function-name arn:aws:lambda:us-east-1:111122223333:function:my-kafka-function \\  --self-managed-event-source '{\"Endpoints\":{\"KAFKA_BOOTSTRAP_SERVERS\":[\"abc3.xyz.com:9092\", \"abc2.xyz.com:9092\"]}}'"
                                                ]
                                            },
                                            {
                                                "sub_header": "Using a VPC",
                                                "content": [
                                                    "If only Kafka users within your VPC access your Kafka brokers, you must specify your VPC, subnets, and VPC          security group. The following example uses the create-event-source-mapping AWS CLI command to map a Lambda function named my-kafka-function to a Kafka topic named AWSKafkaTopic.",
                                                    "aws lambda create-event-source-mapping \\   --topics AWSKafkaTopic \\  --source-access-configuration '[{\"Type\": \"VPC_SUBNET\", \"URI\": \"subnet:subnet-0011001100\"}, {\"Type\": \"VPC_SUBNET\", \"URI\": \"subnet:subnet-0022002200\"}, {\"Type\": \"VPC_SECURITY_GROUP\", \"URI\": \"security_group:sg-0123456789\"}]' \\  --function-name arn:aws:lambda:us-east-1:111122223333:function:my-kafka-function \\  --self-managed-event-source '{\"Endpoints\":{\"KAFKA_BOOTSTRAP_SERVERS\":[\"abc3.xyz.com:9092\", \"abc2.xyz.com:9092\"]}}'"
                                                ]
                                            },
                                            {
                                                "sub_header": "Viewing the status using the AWS CLI",
                                                "content": [
                                                    "The following example uses the get-event-source-mapping AWS CLI command to describe the status of the event source mapping that you created.",
                                                    "aws lambda get-event-source-mapping          --uuid dh38738e-992b-343a-1077-3478934hjkfd7"
                                                ]
                                            }
                                        ]
                                    }
                                ]
                            },
                            {
                                "sub_header": "Self-managed Apache Kafka configuration parameters",
                                "content": [
                                    "All Lambda event source types share the same CreateEventSourceMapping and UpdateEventSourceMapping      API operations. However, only some of the parameters apply to Apache Kafka.",
                                    {
                                        "code_example": "MinimumPollers"
                                    }
                                ]
                            },
                            {
                                "sub_header": "Using a Kafka cluster as an event source",
                                "content": [
                                    "When you add your Apache Kafka or Amazon MSK cluster as a trigger for your Lambda function, the cluster is used as an event source.",
                                    "Lambda reads event data from the Kafka topics that you specify as Topics in a      CreateEventSourceMapping request, based on the StartingPosition that you specify. After      successful processing, your Kafka topic is committed to your Kafka cluster.",
                                    "If you specify the StartingPosition as LATEST, Lambda starts reading from the latest      message in each partition belonging to the topic. Because there can be some delay after trigger configuration      before Lambda starts reading the messages, Lambda doesn't read any messages produced during this window.",
                                    "Lambda processes records from one or more Kafka topic partitions that you specify and sends a JSON payload to      your function. A single Lambda payload can contain messages from multiple partitions. When more records are available,       Lambda continues processing records in batches, based on the      BatchSize value that you specify in a CreateEventSourceMapping request, until your function      catches up with the topic.",
                                    "If your function returns an error for any of the messages in a batch, Lambda retries the whole batch of      messages until processing succeeds or the messages expire. You can send records that fail all retry attempts      to an on-failure destination for later processing.",
                                    "NoteWhile Lambda functions typically have a maximum timeout limit of 15 minutes,\n      event source mappings for Amazon MSK, self-managed Apache Kafka, Amazon DocumentDB, and Amazon MQ for ActiveMQ and RabbitMQ only support functions with\n      maximum timeout limits of 14 minutes. This constraint ensures that the event source mapping can properly\n      handle function errors and retries."
                                ]
                            },
                            {
                                "sub_header": "Polling and stream starting positions",
                                "content": [
                                    "Be aware that stream polling during event source mapping creation and updates is eventually consistent.",
                                    "This behavior means that if you specify LATEST as the starting position for the stream, the event source mapping could     miss events during creation or updates. To ensure that no events are missed, specify the stream starting position as TRIM_HORIZON     or AT_TIMESTAMP."
                                ]
                            },
                            {
                                "sub_header": "Message throughput scaling behavior for self-managed Apache Kafka event source mappings",
                                "content": [
                                    "You can choose between two modes of message throughput scaling behavior for your Amazon MSK      event source mapping:",
                                    {
                                        "sub_header": "Default (on-demand) mode",
                                        "content": [
                                            "When you initially create an self-managed Apache Kafka event source, Lambda allocates a default number of event        pollers to process all partitions in the Kafka topic. Lambda automatically scales up or down the        number of event pollers based on message load.",
                                            "In one-minute intervals, Lambda evaluates the consumer offset lag of all the partitions in the        topic. If the offset lag is too high, the partition is receiving messages faster than Lambda can        process them. If necessary, Lambda adds or removes event pollers from the topic. This autoscaling        process of adding or removing event pollers occurs within three minutes of evaluation.",
                                            "If your target Lambda function is throttled, Lambda reduces the number of event pollers. This        action reduces the workload on the function by reducing the number of messages that event        pollers can retrieve and send to the function.",
                                            "To monitor the throughput of your Kafka topic, you can view the Apache Kafka consumer metrics,        such as consumer_lag and consumer_offset."
                                        ]
                                    },
                                    {
                                        "sub_header": "Configuring provisioned mode",
                                        "content": [
                                            "For workloads where you need to fine-tune the throughput of your event source mapping,        you can use provisioned mode. In provisioned mode, you define minimum and maximum limits        for the amount of provisioned event pollers. These provisioned event pollers are dedicated        to your event source mapping, and can handle unexpected message spikes instantly when they        occur. We recommend that you use provisioned mode for Kafka workloads that have strict        performance requirements.",
                                            "In Lambda, an event poller is a compute unit capable of handling up to 5 MBps of throughput.    For reference, suppose your event source produces an average payload of 1MB, and the average function duration is 1 sec.    If the payload doesn’t undergo any transformation (such as filtering), a single poller can support 5 MBps throughput,    and 5 concurrent Lambda invocations. Using provisioned mode incurs additional costs. For pricing estimates,    see AWS Lambda pricing.",
                                            "In provisioned mode, the range of accepted values for the minimum number of event pollers                (MinimumPollers) is between 1 and 200, inclusive. The range of                accepted values for the maximum number of event pollers (MaximumPollers)                is between 1 and 2,000, inclusive. MaximumPollers must be greater than                or equal to MinimumPollers. In addition, to maintain ordered                processing within partitions, Lambda caps the MaximumPollers to the                number of partitions in the topic.",
                                            "For more details about choosing appropriate values for minimum and maximum event pollers,        see Best practices and considerations when using provisioned mode.",
                                            "You can configure provisioned mode for your self-managed Apache Kafka event source mapping using the console        or the Lambda API.",
                                            "  3.Configuration Choose , then choose Triggers.",
                                            "  4.Edit Choose the self-managed Apache Kafka event source mapping that you want to configure provisioned mode for,\n            then choose .",
                                            "  5.Event source mapping configuration Under , choose \n            Configure provisioned mode.",
                                            "  6.Minimum event pollers For , enter a value between 1 and 200.\n                                If you don't specify a value, Lambda chooses a default value of 1.",
                                            "  7.Maximum event pollers For , enter a value between 1 and 2,000.\n                                This value must be greater than or equal to your value for Minimum event\n                                pollers. If you don't specify a value, Lambda chooses a default value of 200.",
                                            "  8.Save Choose .",
                                            "You can configure provisioned mode programmatically using the ProvisionedPollerConfig object                in your                 EventSourceMappingConfiguration. For example, the following UpdateEventSourceMapping CLI                command configures a MinimumPollers value of 5, and a                MaximumPollers value of 100.",
                                            "aws lambda update-event-source-mapping \\    --uuid a1b2c3d4-5678-90ab-cdef-EXAMPLE11111 \\    --provisioned-poller-config '{\"MinimumPollers\": 5, \"MaximumPollers\": 100}'",
                                            "After configuring provisioned mode, you can observe the usage of event pollers for your workload by monitoring    the ProvisionedPollers metric. For more information, see Event source mapping metrics.",
                                            "To disable provisioned mode and return to default (on-demand) mode,                you can use the following UpdateEventSourceMapping CLI                command:",
                                            "aws lambda update-event-source-mapping \\    --uuid a1b2c3d4-5678-90ab-cdef-EXAMPLE11111 \\    --provisioned-poller-config '{}'"
                                        ]
                                    },
                                    {
                                        "sub_header": "Best practices and considerations when using provisioned mode",
                                        "content": [
                                            "The optimal configuration of minimum and maximum event pollers for your event source mapping                depends on your application's performance requirements. We recommend that you start with the                default minimum event pollers to baseline the performance profile. Adjust your configuration                based on observed message processing patterns and your desired performance profile.",
                                            "For workloads with spiky traffic and strict performance needs, increase the minimum event                pollers to handle sudden surges in messages. To determine the minimum event pollers required,                consider your workload's messages per second and average payload size, and use the throughput                capacity of a single event poller (up to 5 MBps) as a reference.",
                                            "To maintain ordered processing within a partition, Lambda limits the maximum event pollers                to the number of partitions in the topic. Additionally, the maximum event pollers your event                source mapping can scale to depends on the function's concurrency settings.",
                                            "When activating provisioned mode, update your network settings to remove AWS PrivateLink VPC                endpoints and associated permissions."
                                        ]
                                    }
                                ]
                            },
                            {
                                "sub_header": "Amazon CloudWatch metrics",
                                "content": [
                                    "Lambda emits the OffsetLag metric while your function processes records. The value of this metric      is the difference in offset between the last record written to the Kafka event source topic and the last record that your function's       consumer group processed. You can use OffsetLag to estimate the latency between when a record is added and when      your consumer group processes it.",
                                    "An increasing trend in OffsetLag can indicate issues with pollers in your function's consumer group. For more information, see      Using CloudWatch metrics with Lambda."
                                ]
                            }
                        ]
                    },
                    {
                        "title": "Event filtering",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/with-kafka-filtering.html",
                        "sections": [
                            "",
                            "You can use event filtering to control which records from a stream or queue Lambda sends to your function.    For general information about how event filtering works, see Control which events Lambda sends to your function.",
                            "This section focuses on event filtering for self-managed Apache Kafka event sources.",
                            "  1.Self-managed Apache Kafka event filtering basics",
                            {
                                "sub_header": "Self-managed Apache Kafka event filtering basics",
                                "content": [
                                    "Suppose a producer is writing messages to a topic in your self-managed Apache Kafka cluster, either in valid JSON format or as plain strings. An example record             would look like the following, with the message converted to a Base64 encoded string in the value field.",
                                    "{    \"mytopic-0\":[        {            \"topic\":\"mytopic\",            \"partition\":0,            \"offset\":15,            \"timestamp\":1545084650987,            \"timestampType\":\"CREATE_TIME\",            \"value\":\"SGVsbG8sIHRoaXMgaXMgYSB0ZXN0Lg==\",            \"headers\":[]        }    ]}",
                                    "Suppose your Apache Kafka producer is writing messages to your topic in the following JSON format.",
                                    "{    \"device_ID\": \"AB1234\",    \"session\":{        \"start_time\": \"yyyy-mm-ddThh:mm:ss\",        \"duration\": 162    }}",
                                    "You can use the value key to filter records. Suppose you wanted to filter only those records where device_ID             begins with the letters AB. The FilterCriteria object would be as follows.",
                                    "{    \"Filters\": [        {            \"Pattern\": \"{ \\\"value\\\" : { \\\"device_ID\\\" : [ { \\\"prefix\\\": \\\"AB\\\" } ] } }\"        }    ]}",
                                    "For added clarity, here is the value of the filter's Pattern expanded in plain JSON.",
                                    "{    \"value\": {        \"device_ID\": [ { \"prefix\": \"AB\" } ]      }}",
                                    "You can add your filter using the console, AWS CLI or an AWS SAM template.",
                                    "ConsoleTo add this filter using the console, follow the instructions in Attaching filter criteria to an event source mapping (console) and enter the following                         string for the Filter criteria.{ \"value\" : { \"device_ID\" : [ { \"prefix\":  \"AB\" } ] } }AWS CLITo create a new event source mapping with these filter criteria using the AWS Command Line Interface (AWS CLI), run the following                        command.aws lambda create-event-source-mapping \\    --function-name my-function \\    --event-source-arn arn:aws:kafka:us-east-2:123456789012:cluster/my-cluster/b-8ac7cc01-5898-482d-be2f-a6b596050ea8 \\    --filter-criteria '{\"Filters\": [{\"Pattern\": \"{ \\\"value\\\" : { \\\"device_ID\\\" : [ { \\\"prefix\\\":  \\\"AB\\\" } ] } }\"}]}'To add these filter criteria to an existing event source mapping, run the following command.aws lambda update-event-source-mapping \\    --uuid \"a1b2c3d4-5678-90ab-cdef-11111EXAMPLE\" \\    --filter-criteria '{\"Filters\": [{\"Pattern\": \"{ \\\"value\\\" : { \\\"device_ID\\\" : [ { \\\"prefix\\\":  \\\"AB\\\" } ] } }\"}]}'AWS SAMTo add this filter using AWS SAM, add the following snippet to the YAML template for your event source.FilterCriteria:  Filters:    - Pattern: '{ \"value\" : { \"device_ID\" : [ { \"prefix\":  \"AB\" } ] } }'anchoranchoranchorConsoleAWS CLIAWS SAMTo add this filter using the console, follow the instructions in Attaching filter criteria to an event source mapping (console) and enter the following                         string for the Filter criteria.{ \"value\" : { \"device_ID\" : [ { \"prefix\":  \"AB\" } ] } }",
                                    "With self-managed Apache Kafka, you can also filter records where the message is a plain string. Suppose you want to ignore those messages where the string is             \"error\". The FilterCriteria object would look as follows.",
                                    "{    \"Filters\": [        {            \"Pattern\": \"{ \\\"value\\\" : [ { \\\"anything-but\\\": [ \\\"error\\\" ] } ] }\"        }    ]}",
                                    "For added clarity, here is the value of the filter's Pattern expanded in plain JSON.",
                                    "{    \"value\": [        {        \"anything-but\": [ \"error\" ]        }    ]}",
                                    "You can add your filter using the console, AWS CLI or an AWS SAM template.",
                                    "ConsoleTo add this filter using the console, follow the instructions in Attaching filter criteria to an event source mapping (console) and enter the following                         string for the Filter criteria.{ \"value\" : [ { \"anything-but\": [ \"error\" ] } ] }AWS CLITo create a new event source mapping with these filter criteria using the AWS Command Line Interface (AWS CLI), run the following                        command.aws lambda create-event-source-mapping \\    --function-name my-function \\    --event-source-arn arn:aws:kafka:us-east-2:123456789012:cluster/my-cluster/b-8ac7cc01-5898-482d-be2f-a6b596050ea8 \\    --filter-criteria '{\"Filters\": [{\"Pattern\": \"{ \\\"value\\\" : [ { \\\"anything-but\\\": [ \\\"error\\\" ] } ] }\"}]}'To add these filter criteria to an existing event source mapping, run the following command.aws lambda update-event-source-mapping \\    --uuid \"a1b2c3d4-5678-90ab-cdef-11111EXAMPLE\" \\    --filter-criteria '{\"Filters\": [{\"Pattern\": \"{ \\\"value\\\" : [ { \\\"anything-but\\\": [ \\\"error\\\" ] } ] }\"}]}'AWS SAMTo add this filter using AWS SAM, add the following snippet to the YAML template for your event source.FilterCriteria:  Filters:    - Pattern: '{ \"value\" : [ { \"anything-but\": [ \"error\" ] } ] }'anchoranchoranchorConsoleAWS CLIAWS SAMTo add this filter using the console, follow the instructions in Attaching filter criteria to an event source mapping (console) and enter the following                         string for the Filter criteria.{ \"value\" : [ { \"anything-but\": [ \"error\" ] } ] }",
                                    "Self-managed Apache Kafka messages must be UTF-8 encoded strings, either plain strings or in JSON format. That's because Lambda decodes Kafka byte arrays into UTF-8 before             applying filter criteria. If your messages use another encoding, such as UTF-16 or ASCII, or if the message format doesn't match the             FilterCriteria format, Lambda processes metadata filters only. The following table summarizes the specific behavior:",
                                    "\n\nIncoming message format\nFilter pattern format for message properties\nResulting action\n\n\n\n\nPlain string\n\n\nPlain string\n\n\nLambda filters based on your filter criteria.\n\n\n\n\nPlain string\n\n\nNo filter pattern for data properties\n\n\nLambda filters (on the other metadata properties only) based on your filter criteria.\n\n\n\n\nPlain string\n\n\nValid JSON\n\n\nLambda filters (on the other metadata properties only) based on your filter criteria.\n\n\n\n\nValid JSON\n\n\nPlain string\n\n\nLambda filters (on the other metadata properties only) based on your filter criteria.\n\n\n\n\nValid JSON\n\n\nNo filter pattern for data properties\n\n\nLambda filters (on the other metadata properties only) based on your filter criteria.\n\n\n\n\nValid JSON\n\n\nValid JSON\n\n\nLambda filters based on your filter criteria.\n\n\n\n\nNon-UTF-8 encoded string\n\n\nJSON, plain string, or no pattern\n\n\nLambda filters (on the other metadata properties only) based on your filter criteria.\n\n\n"
                                ]
                            }
                        ]
                    },
                    {
                        "title": "On-failure destinations",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/with-kafka-on-failure.html",
                        "sections": [
                            "",
                            "To retain records of failed event source mapping invocations, add a destination to your function's event source mapping. Each record sent to the destination is a JSON document containing metadata about the failed invocation. For Amazon S3 destinations, Lambda also sends the entire invocation record along with the metadata. You can configure any Amazon SNS topic, Amazon SQS queue, or S3 bucket as a destination.",
                            "With Amazon S3 destinations, you can use the Amazon S3 Event Notifications feature to receive notifications when objects are uploaded to your destination S3 bucket. You can also configure S3 Event Notifications to invoke another Lambda function to perform automated processing on failed batches.",
                            "Your execution role must have permissions for the destination:",
                            "  1.For SQS destinations:  sqs:SendMessage",
                            "  2.For SNS destinations:  sns:Publish",
                            "  3.For S3 bucket destinations:   s3:PutObject and s3:ListBucket",
                            "You must deploy a VPC endpoint for your on-failure destination service inside your Apache Kafka cluster VPC.",
                            "Additionally, if you configured a KMS key on your destination, Lambda needs the following        permissions depending on the destination type:",
                            {
                                "sub_header": "Configuring on-failure destinations for an self-managed Apache Kafka event source mapping",
                                "content": [
                                    "To configure an on-failure destination using the console, follow these steps:",
                                    "Open the Functions page of the Lambda console.\nChoose a function.\n\nUnder Function overview, choose Add destination.\n\nFor Source, choose Event source mapping invocation.\n\nFor Event source mapping, choose an event source that's configured\n              for this function.\n\nFor Condition, select On failure. For event\n              source mapping invocations, this is the only accepted condition.\n\nFor Destination type, choose the destination type that Lambda sends\n              invocation records to.\n\nFor Destination, choose a resource.\n\nChoose Save.\n",
                                    "You can also configure an on-failure destination using the AWS CLI. For example, the following          create-event-source-mapping command adds an event source mapping with an SQS on-failure destination to          MyFunction:",
                                    "aws lambda create-event-source-mapping \\--function-name \"MyFunction\" \\--event-source-arn arn:aws:kafka:us-east-1:123456789012:cluster/vpc-2priv-2pub/751d2973-a626-431c-9d4e-d7975eb44dd7-2 \\--destination-config '{\"OnFailure\": {\"Destination\": \"arn:aws:sqs:us-east-1:123456789012:dest-queue\"}}'",
                                    "The following update-event-source-mapping command adds an S3 on-failure destination to the event source associated with the input uuid:",
                                    "aws lambda update-event-source-mapping \\--uuid f89f8514-cdd9-4602-9e1f-01a5b77d449b \\--destination-config '{\"OnFailure\": {\"Destination\": \"arn:aws:s3:::dest-bucket\"}}'",
                                    "To remove a destination, supply an empty string as the argument to the          destination-config parameter:",
                                    "aws lambda update-event-source-mapping \\--uuid f89f8514-cdd9-4602-9e1f-01a5b77d449b \\--destination-config '{\"OnFailure\": {\"Destination\": \"\"}}'",
                                    {
                                        "sub_header": "Security best practices for Amazon S3 destinations",
                                        "content": [
                                            "Deleting an S3 bucket that's configured as a destination without removing the destination from your function's configuration can create a security risk. If another       user knows your destination bucket's name, they can recreate the bucket in their AWS account. Records of failed invocations will be sent to their bucket, potentially       exposing data from your function.",
                                            {
                                                "code_example": "s3:PutObject"
                                            },
                                            "The following example shows an IAM policy that limits your function's s3:PutObject permissions to buckets in your account. This policy also gives Lambda        the s3:ListBucket permission it needs to use an S3 bucket as a destination.",
                                            "{    \"Version\": \"2012-10-17\",    \"Statement\": [        {            \"Sid\": \"S3BucketResourceAccountWrite\",            \"Effect\": \"Allow\",            \"Action\": [                \"s3:PutObject\",                \"s3:ListBucket\"            ],            \"Resource\": \"arn:aws:s3:::*/*\",            \"Condition\": {                \"StringEquals\": {                    \"s3:ResourceAccount\": \"111122223333\"                }            }        }    ]}",
                                            "To add a permissions policy to your funcion's execution role using the AWS Management Console or AWS CLI, refer to the instructions in the following procedures:",
                                            "ConsoleTo add a permissions policy to a function's execution role (console)Open the Functions page of the Lambda console.Select the Lambda function whose execution role you want to modify.In the Configuration tab, select Permissions.In the Execution role tab, select your function's Role name to open the role's IAM console page.Add a permissions policy to the role by doing the following:In the Permissions policies pane, choose Add permissions and select Create inline policy.In Policy editor, select JSON.Paste the policy you want to add into the editor (replacing the existing JSON), and then choose Next.Under Policy details, enter a Policy name.Choose Create policy.AWS CLITo add a permissions policy to a function's execution role (CLI)Create a JSON policy document with the required permissions and save it in a local directory.Use the IAM put-role-policy CLI command to add the permissions to your function's execution role. Run the following command from the                 directory you saved your JSON policy document in and replace the role name, policy name, and policy document with your own values.aws iam put-role-policy \\--role-name my_lambda_role \\--policy-name LambdaS3DestinationPolicy \\--policy-document file://my_policy.jsonanchoranchorConsoleAWS CLITo add a permissions policy to a function's execution role (console)Open the Functions page of the Lambda console.Select the Lambda function whose execution role you want to modify.In the Configuration tab, select Permissions.In the Execution role tab, select your function's Role name to open the role's IAM console page.Add a permissions policy to the role by doing the following:In the Permissions policies pane, choose Add permissions and select Create inline policy.In Policy editor, select JSON.Paste the policy you want to add into the editor (replacing the existing JSON), and then choose Next.Under Policy details, enter a Policy name.Choose Create policy."
                                        ]
                                    },
                                    {
                                        "sub_header": "SNS and SQS example invocation record",
                                        "content": [
                                            "The following example shows what Lambda sends to an SNS topic or SQS queue destination for a          failed Kafka event source invocation. Each of the keys under recordsInfo contains          both the Kafka topic and partition, separated by a hyphen. For example, for the key          \"Topic-0\", Topic is the Kafka topic, and 0 is the          partition. For each topic and partition, you can use the offsets and timestamp data to find          the original invocation records.",
                                            "{    \"requestContext\": {        \"requestId\": \"316aa6d0-8154-xmpl-9af7-85d5f4a6bc81\",        \"functionArn\": \"arn:aws:lambda:us-east-1:123456789012:function:myfunction\",        \"condition\": \"RetryAttemptsExhausted\" | \"MaximumPayloadSizeExceeded\",        \"approximateInvokeCount\": 1    },    \"responseContext\": { // null if record is MaximumPayloadSizeExceeded        \"statusCode\": 200,        \"executedVersion\": \"$LATEST\",        \"functionError\": \"Unhandled\"    },    \"version\": \"1.0\",    \"timestamp\": \"2019-11-14T00:38:06.021Z\",    \"KafkaBatchInfo\": {        \"batchSize\": 500,        \"eventSourceArn\": \"arn:aws:kafka:us-east-1:123456789012:cluster/vpc-2priv-2pub/751d2973-a626-431c-9d4e-d7975eb44dd7-2\",        \"bootstrapServers\": \"...\",        \"payloadSize\": 2039086, // In bytes        \"recordsInfo\": {            \"Topic-0\": {                \"firstRecordOffset\": \"49601189658422359378836298521827638475320189012309704722\",                \"lastRecordOffset\": \"49601189658422359378836298522902373528957594348623495186\",                \"firstRecordTimestamp\": \"2019-11-14T00:38:04.835Z\",                \"lastRecordTimestamp\": \"2019-11-14T00:38:05.580Z\",            },            \"Topic-1\": {                \"firstRecordOffset\": \"49601189658422359378836298521827638475320189012309704722\",                \"lastRecordOffset\": \"49601189658422359378836298522902373528957594348623495186\",                \"firstRecordTimestamp\": \"2019-11-14T00:38:04.835Z\",                \"lastRecordTimestamp\": \"2019-11-14T00:38:05.580Z\",            }        }    }}"
                                        ]
                                    },
                                    {
                                        "sub_header": "S3 destination example invocation record",
                                        "content": [
                                            "For S3 destinations, Lambda sends the entire invocation record along with the metadata          to the destination. The following example shows that Lambda sends to an S3 bucket destination          for a failed Kafka event source invocation. In addition to all of the fields from the previous          example for SQS and SNS destinations, the payload field contains the original          invocation record as an escaped JSON string.",
                                            "{    \"requestContext\": {        \"requestId\": \"316aa6d0-8154-xmpl-9af7-85d5f4a6bc81\",        \"functionArn\": \"arn:aws:lambda:us-east-1:123456789012:function:myfunction\",        \"condition\": \"RetryAttemptsExhausted\" | \"MaximumPayloadSizeExceeded\",        \"approximateInvokeCount\": 1    },    \"responseContext\": { // null if record is MaximumPayloadSizeExceeded        \"statusCode\": 200,        \"executedVersion\": \"$LATEST\",        \"functionError\": \"Unhandled\"    },    \"version\": \"1.0\",    \"timestamp\": \"2019-11-14T00:38:06.021Z\",    \"KafkaBatchInfo\": {        \"batchSize\": 500,        \"eventSourceArn\": \"arn:aws:kafka:us-east-1:123456789012:cluster/vpc-2priv-2pub/751d2973-a626-431c-9d4e-d7975eb44dd7-2\",        \"bootstrapServers\": \"...\",        \"payloadSize\": 2039086, // In bytes        \"recordsInfo\": {            \"Topic-0\": {                \"firstRecordOffset\": \"49601189658422359378836298521827638475320189012309704722\",                \"lastRecordOffset\": \"49601189658422359378836298522902373528957594348623495186\",                \"firstRecordTimestamp\": \"2019-11-14T00:38:04.835Z\",                \"lastRecordTimestamp\": \"2019-11-14T00:38:05.580Z\",            },            \"Topic-1\": {                \"firstRecordOffset\": \"49601189658422359378836298521827638475320189012309704722\",                \"lastRecordOffset\": \"49601189658422359378836298522902373528957594348623495186\",                \"firstRecordTimestamp\": \"2019-11-14T00:38:04.835Z\",                \"lastRecordTimestamp\": \"2019-11-14T00:38:05.580Z\",            }        }    },    \"payload\": \"<Whole Event>\" // Only available in S3}",
                                            "TipWe recommend enabling S3 versioning on your destination bucket."
                                        ]
                                    }
                                ]
                            }
                        ]
                    },
                    {
                        "title": "Troubleshooting",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/with-kafka-troubleshoot.html",
                        "sections": [
                            "",
                            "The following topics provide troubleshooting advice for errors and issues that you might encounter when using    self-managed Apache Kafka with Lambda. If you find an issue that is not listed here, you can use the    Feedback button on this page to report it.",
                            "For more help with troubleshooting, visit the AWS Knowledge Center.",
                            {
                                "sub_header": "Authentication and authorization errors",
                                "content": [
                                    "If any of the permissions required to consume data from the Kafka cluster are missing, Lambda displays one of      the following error messages in the event source mapping under LastProcessingResult.",
                                    "  1.Cluster failed to authorize Lambda",
                                    "  2.SASL authentication failed",
                                    "  3.Server failed to authenticate Lambda",
                                    "  4.Lambda failed to authenticate server",
                                    "  5.Provided certificate or private key is invalid",
                                    {
                                        "sub_header": "Cluster failed to authorize Lambda",
                                        "content": [
                                            "For SASL/SCRAM or mTLS, this error indicates that the provided user doesn't have all of the following        required Kafka access control list (ACL) permissions:",
                                            "When you create Kafka ACLs with the required kafka-cluster permissions, specify the topic and        group as resources. The topic name must match the topic in the event source mapping. The group name must match        the event source mapping's UUID.",
                                            "After you add the required permissions to the execution role, it might take several minutes for the changes        to take effect."
                                        ]
                                    },
                                    {
                                        "sub_header": "SASL authentication failed",
                                        "content": [
                                            "For SASL/SCRAM or SASL/PLAIN, this error indicates that the provided sign-in credentials aren't        valid."
                                        ]
                                    },
                                    {
                                        "sub_header": "Server failed to authenticate Lambda",
                                        "content": [
                                            "This error indicates that the Kafka broker failed to authenticate Lambda. This can occur for any of the        following reasons:"
                                        ]
                                    },
                                    {
                                        "sub_header": "Lambda failed to authenticate server",
                                        "content": [
                                            "This error indicates that Lambda failed to authenticate the Kafka broker. This can occur for any of the        following reasons:"
                                        ]
                                    },
                                    {
                                        "sub_header": "Provided certificate or private key is invalid",
                                        "content": [
                                            "This error indicates that the Kafka consumer couldn't use the provided certificate or private key. Make sure        that the certificate and key use PEM format, and that the private key encryption uses a PBES1 algorithm."
                                        ]
                                    }
                                ]
                            },
                            {
                                "sub_header": "Event source mapping errors",
                                "content": [
                                    "When you add your Apache Kafka cluster as an event source for your Lambda function, if your function encounters an error, your Kafka consumer stops processing records. Consumers of a topic partition are those that subscribe to, read, and process your records. Your other Kafka consumers can continue processing records, provided they don't encounter the same error.",
                                    "To determine the cause of a stopped consumer, check the StateTransitionReason field in the response of EventSourceMapping. The following list describes the event source errors that you can receive:",
                                    {
                                        "code_example": "ESM_CONFIG_NOT_VALID"
                                    },
                                    "NoteIf your Lambda event records exceed the allowed size limit of 6 MB, they can go\n        unprocessed."
                                ]
                            }
                        ]
                    }
                ],
                "sections": [
                    "",
                    "NoteIf you want to send data to a target other than a Lambda function or enrich the data before sending it, see \n    Amazon EventBridge Pipes.",
                    "Lambda supports Apache Kafka as an event source. Apache Kafka is a an    open-source event streaming platform that supports workloads such as data pipelines and streaming analytics.",
                    "You can use the AWS managed Kafka service Amazon Managed Streaming for Apache Kafka (Amazon MSK), or a self-managed Kafka cluster. For details    about using Lambda with Amazon MSK, see Using Lambda with Amazon MSK.",
                    "This topic describes how to use Lambda with a self-managed Kafka cluster. In AWS terminology, a self-managed    cluster includes non-AWS hosted Kafka clusters. For example, you can host your Kafka cluster with a cloud provider    such as Confluent Cloud.",
                    "Apache Kafka as an event source operates similarly to using Amazon Simple Queue Service (Amazon SQS) or Amazon Kinesis. Lambda internally polls for    new messages from the event source and then synchronously invokes the target Lambda function. Lambda reads the    messages in batches and provides these to your function as an event payload. The maximum batch size is configurable    (the default is 100 messages). For more information, see Batching behavior.",
                    "To optimize the throughput of your self-managed Apache Kafka event source mapping, configure provisioned mode. In provisioned    mode, you can define the minimum and maximum number of event pollers allocated to your event source mapping.    This can improve the ability of your event source mapping to handle unexpected message spikes. For more    information, see provisioned mode.",
                    "WarningLambda event source mappings process each event at least once, and duplicate processing of records can occur. To avoid potential issues \nrelated to duplicate events, we strongly recommend that you make your function code idempotent. To learn more, see How do I make my Lambda function idempotent \nin the AWS Knowledge Center.",
                    "For Kafka-based event sources, Lambda supports processing control parameters, such as batching windows      and batch size. For more information, see Batching behavior.",
                    "For an example of how to use self-managed Kafka as an event source, see Using self-hosted Apache Kafka as an      event source for AWS Lambda on the AWS Compute Blog.",
                    "  1.Example event",
                    "  2.Configuring self-managed Apache Kafka event sources for Lambda",
                    "  3.Processing self-managed Apache Kafka messages with Lambda",
                    "  4.Using event filtering with a self-managed Apache Kafka event source",
                    "  5.Capturing discarded batches for a self-managed Apache Kafka event source",
                    "  6.Troubleshooting self-managed Apache Kafka event source mapping errors",
                    {
                        "sub_header": "Example event",
                        "content": [
                            "Lambda sends the batch of messages in the event parameter when it invokes your Lambda function. The event payload    contains an array of messages. Each array item contains details of the Kafka topic and Kafka partition identifier,    together with a timestamp and a base64-encoded message.",
                            "{   \"eventSource\": \"SelfManagedKafka\",   \"bootstrapServers\":\"b-2.demo-cluster-1.a1bcde.c1.kafka.us-east-1.amazonaws.com:9092,b-1.demo-cluster-1.a1bcde.c1.kafka.us-east-1.amazonaws.com:9092\",   \"records\":{      \"mytopic-0\":[         {            \"topic\":\"mytopic\",            \"partition\":0,            \"offset\":15,            \"timestamp\":1545084650987,            \"timestampType\":\"CREATE_TIME\",            \"key\":\"abcDEFghiJKLmnoPQRstuVWXyz1234==\",            \"value\":\"SGVsbG8sIHRoaXMgaXMgYSB0ZXN0Lg==\",            \"headers\":[               {                  \"headerKey\":[                     104,                     101,                     97,                     100,                     101,                     114,                     86,                     97,                     108,                     117,                     101                  ]               }            ]         }      ]   }}"
                        ]
                    }
                ]
            },
            {
                "title": "API Gateway",
                "href": "https://docs.aws.amazon.com/lambda/latest/dg/services-apigateway.html",
                "contents": [
                    {
                        "title": "Tutorial",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/services-apigateway-tutorial.html",
                        "sections": [
                            "",
                            "In this tutorial, you create a REST API through which you invoke a Lambda function using an HTTP request.   Your Lambda function will perform create, read, update, and delete (CRUD) operations on a DynamoDB table.   This function is provided here for demonstration, but you will learn to configure an API Gateway REST API that can   invoke any Lambda function.",
                            "\n\n",
                            "Using API Gateway provides users with a secure HTTP endpoint to invoke your Lambda function and can help manage   large volumes of calls to your function by throttling traffic and automatically validating and authorizing API   calls. API Gateway also provides flexible security controls using AWS Identity and Access Management (IAM) and Amazon Cognito. This is useful for use cases where   advance authorization is required for calls to your application.",
                            "To complete this tutorial, you will go through the following stages:",
                            "\n\nCreate and configure a Lambda function in Python or Node.js to perform operations on a DynamoDB table.\n\nCreate a REST API in API Gateway to connect to your Lambda function.\n\nCreate a DynamoDB table and test it with your Lambda function in the console.\n\nDeploy your API and test the full setup using curl in a terminal.\n",
                            "By completing these stages, you will learn how to use API Gateway to create an HTTP endpoint that can securely invoke   a Lambda function at any scale. You will also learn how to deploy your API, and how to test it in the console and by sending   an HTTP request using a terminal.",
                            "  1.Prerequisites",
                            "  2.Create a permissions policy",
                            "  3.Create an execution role",
                            "  4.Create the function",
                            "  5.Invoke the function using the AWS CLI",
                            "  6.Create a REST API using API Gateway",
                            "  7.Create a resource on your REST API",
                            "  8.Create an HTTP POST method",
                            "  9.Create a DynamoDB table",
                            "  10.Test the integration of API Gateway, Lambda, and DynamoDB",
                            "  11.Deploy the API",
                            "  12.Use curl to invoke your function using HTTP requests",
                            "  13.Clean up your resources (optional)",
                            {
                                "sub_header": "Prerequisites",
                                "content": [
                                    {
                                        "code_example": "zip"
                                    }
                                ]
                            },
                            {
                                "sub_header": "Create a permissions policy",
                                "content": [
                                    "\n\n",
                                    "Before you can create an execution role for your Lambda function, you first       need to create a permissions policy to give your function permission to access the required AWS resources. For this tutorial,       the policy allows Lambda to perform CRUD operations on a DynamoDB table and write to Amazon CloudWatch Logs.",
                                    {
                                        "code_example": "{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Sid\": \"Stmt1428341300017\",\n      \"Action\": [\n        \"dynamodb:DeleteItem\",\n        \"dynamodb:GetItem\",\n        \"dynamodb:PutItem\",\n        \"dynamodb:Query\",\n        \"dynamodb:Scan\",\n        \"dynamodb:UpdateItem\"\n      ],\n      \"Effect\": \"Allow\",\n      \"Resource\": \"*\"\n    },\n    {\n      \"Sid\": \"\",\n      \"Resource\": \"*\",\n      \"Action\": [\n        \"logs:CreateLogGroup\",\n        \"logs:CreateLogStream\",\n        \"logs:PutLogEvents\"\n      ],\n      \"Effect\": \"Allow\"\n    }\n  ]\n}"
                                    }
                                ]
                            },
                            {
                                "sub_header": "Create an execution role",
                                "content": [
                                    "\n\n",
                                    "An execution role is an AWS Identity and Access Management (IAM) role that grants a Lambda function permission to access AWS services and resources. To enable your function to perform operations on a DynamoDB table, you attach the permissions policy you created in the previous step.",
                                    {
                                        "code_example": "lambda-apigateway-policy"
                                    },
                                    "Later in the tutorial, you need the Amazon Resource Name (ARN) of the role you just created. On the Roles     page of the IAM console, choose the name of your role (lambda-apigateway-role) and copy the Role ARN displayed     on the Summary page."
                                ]
                            },
                            {
                                "sub_header": "Create the function",
                                "content": [
                                    "\n\n",
                                    "The following code example receives an event input from API Gateway specifying an operation to perform on the DynamoDB table       you will create and some payload data. If the parameters the function receives are valid, it performs the requested operation on the table.",
                                    "Node.jsExample index.mjsNote the region setting. This must match the AWS Region where you deploy the function and create the DynamoDB table.import { DynamoDBDocumentClient, PutCommand, GetCommand,          UpdateCommand, DeleteCommand} from \"@aws-sdk/lib-dynamodb\";import { DynamoDBClient } from \"@aws-sdk/client-dynamodb\";const ddbClient = new DynamoDBClient({ region: \"us-east-2\" });const ddbDocClient = DynamoDBDocumentClient.from(ddbClient);// Define the name of the DDB table to perform the CRUD operations onconst tablename = \"lambda-apigateway\";/** * Provide an event that contains the following keys: * *   - operation: one of 'create,' 'read,' 'update,' 'delete,' or 'echo' *   - payload: a JSON object containing the parameters for the table item *     to perform the operation on */export const handler = async (event, context) => {        const operation = event.operation;        if (operation == 'echo'){          return(event.payload);     }         else {         event.payload.TableName = tablename;        let response;                switch (operation) {          case 'create':               response = await ddbDocClient.send(new PutCommand(event.payload));               break;          case 'read':               response = await ddbDocClient.send(new GetCommand(event.payload));               break;          case 'update':               response = ddbDocClient.send(new UpdateCommand(event.payload));               break;          case 'delete':               response = ddbDocClient.send(new DeleteCommand(event.payload));               break;          default:            response = 'Unknown operation: ${operation}';          }        console.log(response);        return response;    }};NoteIn this example, the name of the DynamoDB table is defined as a variable in your function code. In a real application,               best practice is to pass this parameter as an environment variable and to avoid hardcoding the table name. For more information see               Using AWS Lambda environment variables.To create the functionSave the code example as a file named index.mjs and, if necessary, edit the AWS region specified in the code.                 The region specified in the code must be the same as the region in which you create your DynamoDB table later in the tutorial.Create a deployment package using the following zip command.zip function.zip index.mjsCreate a Lambda function using the create-function AWS CLI command. For the                  role parameter, enter the execution role's Amazon Resource Name (ARN) that you copied                earlier.aws lambda create-function \\--function-name LambdaFunctionOverHttps \\--zip-file fileb://function.zip \\--handler index.handler \\--runtime nodejs22.x \\--role arn:aws:iam::123456789012:role/service-role/lambda-apigateway-rolePython 3Example LambdaFunctionOverHttps.pyimport boto3# Define the DynamoDB table that Lambda will connect totable_name = \"lambda-apigateway\"# Create the DynamoDB resourcedynamo = boto3.resource('dynamodb').Table(table_name)# Define some functions to perform the CRUD operationsdef create(payload):    return dynamo.put_item(Item=payload['Item'])def read(payload):    return dynamo.get_item(Key=payload['Key'])def update(payload):    return dynamo.update_item(**{k: payload[k] for k in ['Key', 'UpdateExpression',     'ExpressionAttributeNames', 'ExpressionAttributeValues'] if k in payload})def delete(payload):    return dynamo.delete_item(Key=payload['Key'])def echo(payload):    return payloadoperations = {    'create': create,    'read': read,    'update': update,    'delete': delete,    'echo': echo,}def lambda_handler(event, context):    '''Provide an event that contains the following keys:      - operation: one of the operations in the operations dict below      - payload: a JSON object containing parameters to pass to the         operation being performed    '''        operation = event['operation']    payload = event['payload']        if operation in operations:        return operations[operation](payload)            else:        raise ValueError(f'Unrecognized operation \"{operation}\"')NoteIn this example, the name of the DynamoDB table is defined as a variable in your function code. In a real application,               best practice is to pass this parameter as an environment variable and to avoid hardcoding the table name. For more information see               Using AWS Lambda environment variables.To create the functionSave the code example as a file named LambdaFunctionOverHttps.py.Create a deployment package using the following zip command.zip function.zip LambdaFunctionOverHttps.pyCreate a Lambda function using the create-function AWS CLI command. For the                  role parameter, enter the execution role's Amazon Resource Name (ARN) that you copied                earlier.aws lambda create-function \\--function-name LambdaFunctionOverHttps \\--zip-file fileb://function.zip \\--handler LambdaFunctionOverHttps.lambda_handler \\--runtime python3.12 \\--role arn:aws:iam::123456789012:role/service-role/lambda-apigateway-roleanchoranchorNode.jsPython 3Example index.mjsNote the region setting. This must match the AWS Region where you deploy the function and create the DynamoDB table.import { DynamoDBDocumentClient, PutCommand, GetCommand,          UpdateCommand, DeleteCommand} from \"@aws-sdk/lib-dynamodb\";import { DynamoDBClient } from \"@aws-sdk/client-dynamodb\";const ddbClient = new DynamoDBClient({ region: \"us-east-2\" });const ddbDocClient = DynamoDBDocumentClient.from(ddbClient);// Define the name of the DDB table to perform the CRUD operations onconst tablename = \"lambda-apigateway\";/** * Provide an event that contains the following keys: * *   - operation: one of 'create,' 'read,' 'update,' 'delete,' or 'echo' *   - payload: a JSON object containing the parameters for the table item *     to perform the operation on */export const handler = async (event, context) => {        const operation = event.operation;        if (operation == 'echo'){          return(event.payload);     }         else {         event.payload.TableName = tablename;        let response;                switch (operation) {          case 'create':               response = await ddbDocClient.send(new PutCommand(event.payload));               break;          case 'read':               response = await ddbDocClient.send(new GetCommand(event.payload));               break;          case 'update':               response = ddbDocClient.send(new UpdateCommand(event.payload));               break;          case 'delete':               response = ddbDocClient.send(new DeleteCommand(event.payload));               break;          default:            response = 'Unknown operation: ${operation}';          }        console.log(response);        return response;    }};NoteIn this example, the name of the DynamoDB table is defined as a variable in your function code. In a real application,               best practice is to pass this parameter as an environment variable and to avoid hardcoding the table name. For more information see               Using AWS Lambda environment variables.To create the functionSave the code example as a file named index.mjs and, if necessary, edit the AWS region specified in the code.                 The region specified in the code must be the same as the region in which you create your DynamoDB table later in the tutorial.Create a deployment package using the following zip command.zip function.zip index.mjsCreate a Lambda function using the create-function AWS CLI command. For the                  role parameter, enter the execution role's Amazon Resource Name (ARN) that you copied                earlier.aws lambda create-function \\--function-name LambdaFunctionOverHttps \\--zip-file fileb://function.zip \\--handler index.handler \\--runtime nodejs22.x \\--role arn:aws:iam::123456789012:role/service-role/lambda-apigateway-role"
                                ]
                            },
                            {
                                "sub_header": "Invoke the function using the AWS CLI",
                                "content": [
                                    "\n\n",
                                    "Before integrating your function with API Gateway, confirm that you have deployed the function successfully. Create       a test event containing the parameters your API Gateway API will send to Lambda and use the AWS CLI invoke command       to run your function.",
                                    {
                                        "code_example": "input.txt"
                                    }
                                ]
                            },
                            {
                                "sub_header": "Create a REST API using API Gateway",
                                "content": [
                                    "\n\n",
                                    "In this step, you create the API Gateway REST API you will use to invoke your Lambda function.",
                                    {
                                        "code_example": "DynamoDBOperations"
                                    }
                                ]
                            },
                            {
                                "sub_header": "Create a resource on your REST API",
                                "content": [
                                    "\n\n",
                                    "To add an HTTP method to your API, you first need to create a resource for that method to operate on. Here you create     the resource to manage your DynamoDB table.",
                                    {
                                        "code_example": "DynamoDBManager"
                                    }
                                ]
                            },
                            {
                                "sub_header": "Create an HTTP POST method",
                                "content": [
                                    "\n\n",
                                    "In this step, you create a method (POST) for your DynamoDBManager resource. You link     this POST method to your Lambda function so that when the method receives an HTTP request, API Gateway invokes     your Lambda function.",
                                    {
                                        "code_example": "POST"
                                    },
                                    {
                                        "code_example": "/DynamoDBManager"
                                    }
                                ]
                            },
                            {
                                "sub_header": "Create a DynamoDB table",
                                "content": [
                                    "\n\n",
                                    "Create an empty DynamoDB table that your Lambda function will perform CRUD operations on.",
                                    {
                                        "code_example": "lambda-apigateway"
                                    }
                                ]
                            },
                            {
                                "sub_header": "Test the integration of API Gateway, Lambda, and DynamoDB",
                                "content": [
                                    "\n\n",
                                    "You're now ready to test the integration of your API Gateway API method with your Lambda function and your DynamoDB table. Using the       API Gateway console, you send requests directly to your POST method using the console's test function.       In this step, you first use a create operation to add a new item to your DynamoDB table, then you       use an update operation to modify the item.",
                                    {
                                        "code_example": "DynamoDBOperations"
                                    },
                                    {
                                        "code_example": "{\n    \"operation\": \"update\",\n    \"payload\": {\n        \"Key\": {\n            \"id\": \"1234ABCD\"\n        },\n        \"UpdateExpression\": \"SET #num = :newNum\",\n        \"ExpressionAttributeNames\": {\n            \"#num\": \"number\"\n        },\n        \"ExpressionAttributeValues\": {\n            \":newNum\": 10\n        }\n    }\n}"
                                    }
                                ]
                            },
                            {
                                "sub_header": "Deploy the API",
                                "content": [
                                    "\n\n",
                                    "For a client to call the API, you must create a deployment and an associated stage. A stage represents a snapshot     of your API including its methods and integrations.",
                                    {
                                        "code_example": "DynamoDBOperations"
                                    }
                                ]
                            },
                            {
                                "sub_header": "Use curl to invoke your function using HTTP requests",
                                "content": [
                                    "\n\n",
                                    "You can now invoke your Lambda function by issuing an HTTP request to your API. In this step, you will create a     new item in your DynamoDB table and then perform read, update, and delete operations on that item.",
                                    {
                                        "code_example": "curl"
                                    },
                                    {
                                        "code_example": "curl"
                                    },
                                    {
                                        "code_example": "curl"
                                    },
                                    {
                                        "code_example": "curl"
                                    }
                                ]
                            },
                            {
                                "sub_header": "Clean up your resources (optional)",
                                "content": [
                                    "You can now delete the resources that you created for this tutorial, unless you want to retain them. By deleting AWS resources that you're no longer using, you prevent unnecessary charges to your AWS account.",
                                    {
                                        "code_example": "delete"
                                    },
                                    "To delete the execution role\nOpen the Roles page of the IAM console.\n\nSelect the execution role that you created.\n\nChoose Delete.\n\nEnter the name of the role in the text input field and choose Delete.\n",
                                    "To delete the API\nOpen the APIs page of the API Gateway console.\n\nSelect the API you created.\n\nChoose Actions, Delete.\n\nChoose Delete.\n",
                                    {
                                        "code_example": "delete"
                                    }
                                ]
                            }
                        ]
                    },
                    {
                        "title": "Errors",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/services-apigateway-errors.html",
                        "sections": [
                            "",
                            "API Gateway treats all invocation and function errors as internal errors. If the Lambda API rejects the invocation        request, API Gateway returns a 500 error code. If the function runs but returns an error, or returns a response in the        wrong format, API Gateway returns a 502. In both cases, the body of the response from API Gateway is {\"message\":            \"Internal server error\"}.",
                            "NoteAPI Gateway does not retry any Lambda invocations. If Lambda returns an error, API Gateway returns an error response to\n            the client.",
                            "The following example shows an X-Ray trace map for a request that resulted in a function error and a 502 from        API Gateway. The client receives the generic error message.",
                            "\n\n",
                            "To customize the error response, you must catch errors in your code and format a response in the required        format.",
                            {
                                "code_example": "var formatError = function(error){\n  var response = {\n    \"statusCode\": error.statusCode,\n    \"headers\": {\n      \"Content-Type\": \"text/plain\",\n      \"x-amzn-ErrorType\": error.code\n    },\n    \"isBase64Encoded\": false,\n    \"body\": error.code + \": \" + error.message\n  }\n  return response\n}"
                            },
                            "API Gateway converts this response into an HTTP error with a custom status code and body. In the trace map, the        function node is green because it handled the error.",
                            "\n\n"
                        ]
                    },
                    {
                        "title": "Select an HTTP invoke method for Lambda",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/apig-http-invoke-decision.html",
                        "sections": [
                            "",
                            "Many common use cases for Lambda involve invoking your function using an HTTP request. For example, you might want to invoke a function     directly from a web browser, or to use a tool like curl or Postman.",
                            "The following sections explain what your choices are for invoking Lambda through HTTP and provide information to help you make the right decision for your particular use case.",
                            {
                                "sub_header": "What are your choices when selecting an HTTP invoke method?",
                                "content": [
                                    "Lambda offers two main methods to invoke a function using an HTTP request - function URLs and       API Gateway. The key differences between these two options are as follows:",
                                    "  1.Lambda function URLs  provide a simple, direct HTTP endpoint for a Lambda function. They are optimized for \n        simplicity and cost-effectiveness and provide the fastest path to expose a Lambda function via HTTP.",
                                    "  2.API Gateway  is a more advanced service for building fully-featured APIs.  is optimized for building and managing \n          productions APIs at scale and provides comprehensive tools for security, monitoring, and traffic management."
                                ]
                            },
                            {
                                "sub_header": "Recommendations if you already know your requirements",
                                "content": [
                                    "If you're already clear on your requirements, here are our basic recommendations:",
                                    "We recommend function URLs for simple applications or prototyping where       you only need basic authentication methods and request/response handling and where you want to keep costs and complexity to a minimum.",
                                    "API Gateway is a better choice for production applications at scale or for cases where you need more advanced features       like OpenAPI Description support, a choice of authentication options, custom domain names, or rich request/response       handling including throttling, caching, and request/response transformation."
                                ]
                            },
                            {
                                "sub_header": "What to consider when selecting a method to invoke your Lambda function",
                                "content": [
                                    "When selecting between function URLs and API Gateway, you need to consider the following factors:",
                                    "By understanding these factors, you can select the option that best balances your security, complexity, and cost requirements.",
                                    "The following information summarizes the main differences between the two options.",
                                    "  1.Function URLs  provide basic authentication options through AWS Identity and Access Management (IAM). You can configure your endpoints to be \n              either public (no authentication) or to require IAM authentication. With IAM authentication, you can use standard AWS credentials or IAM roles to \n              control access. While straightforward to set up, this approach provides limited options compared with other authenticaton methods.",
                                    "  2.API Gateway  provides access to a more comprehensive range of authentication options. As well as IAM authentication, \n              you can use Lambda authorizers \n              (custom authentication logic), Amazon Cognito user pools, and OAuth2.0 \n              flows. This flexibility allows you to implement complex authentication schemes, including third-party authentication providers, token-based authentication, and multi-factor \n              authentication.",
                                    "  3.Function URLs  provide basic authentication options through AWS Identity and Access Management (IAM). You can configure your endpoints to be \n              either public (no authentication) or to require IAM authentication. With IAM authentication, you can use standard AWS credentials or IAM roles to \n              control access. While straightforward to set up, this approach provides limited options compared with other authenticaton methods.",
                                    "  4.API Gateway  provides access to a more comprehensive range of authentication options. As well as IAM authentication, \n              you can use Lambda authorizers \n              (custom authentication logic), Amazon Cognito user pools, and OAuth2.0 \n              flows. This flexibility allows you to implement complex authentication schemes, including third-party authentication providers, token-based authentication, and multi-factor \n              authentication.",
                                    "  5.Function URLs  provide basic HTTP request and response handling. They support standard HTTP methods and \n              include built-in cross-origin resource sharing (CORS) support. While they can handle JSON payloads and query parameters naturally, they don't offer request \n              transformation or validation capabilities. Response handling is similarly straightforward – the client receives the response from your Lambda function exactly as \n              Lambda returns it.",
                                    "  6.API Gateway  provides sophisticated request and response handling capabilities. You can define request validators, transform \n              requests and responses using mapping templates, set up request/response headers, and implement response caching.  also supports binary payloads and custom domain \n              names and can modify responses before they reach the client. You can set up models for request/response validation and transformation using JSON Schema.",
                                    "  7.Function URLs  provide basic HTTP request and response handling. They support standard HTTP methods and \n              include built-in cross-origin resource sharing (CORS) support. While they can handle JSON payloads and query parameters naturally, they don't offer request \n              transformation or validation capabilities. Response handling is similarly straightforward – the client receives the response from your Lambda function exactly as \n              Lambda returns it.",
                                    "  8.API Gateway  provides sophisticated request and response handling capabilities. You can define request validators, transform \n              requests and responses using mapping templates, set up request/response headers, and implement response caching.  also supports binary payloads and custom domain \n              names and can modify responses before they reach the client. You can set up models for request/response validation and transformation using JSON Schema.",
                                    "  9.Function URLs  scale directly with your Lambda function's concurrency limits and handle traffic spikes by scaling your function \n            up to its maximum configured concurrency limit. Once that limit is reached, Lambda responds to additional requests with HTTP 429 responses. There's no built-in queuing \n            mechanism, so handling scaling is entirely dependent on your Lambda function's configuration. By default, Lambda functions have a limit of 1,000 concurrent executions \n            per AWS Region.",
                                    "  10.API Gateway  provides additional scaling capabilities on top of Lambda's own scaling. It includes built-in request queuing and throttling \n            controls, allowing you to manage traffic spikes more gracefully.  can handle up to 10,000 requests per second per region by default, with a burst capacity of \n            5,000 requests per second. It also provides tools to throttle requests at different levels (API, stage, or method) to protect your backend.",
                                    "  11.Function URLs  scale directly with your Lambda function's concurrency limits and handle traffic spikes by scaling your function \n            up to its maximum configured concurrency limit. Once that limit is reached, Lambda responds to additional requests with HTTP 429 responses. There's no built-in queuing \n            mechanism, so handling scaling is entirely dependent on your Lambda function's configuration. By default, Lambda functions have a limit of 1,000 concurrent executions \n            per AWS Region.",
                                    "  12.API Gateway  provides additional scaling capabilities on top of Lambda's own scaling. It includes built-in request queuing and throttling \n            controls, allowing you to manage traffic spikes more gracefully.  can handle up to 10,000 requests per second per region by default, with a burst capacity of \n            5,000 requests per second. It also provides tools to throttle requests at different levels (API, stage, or method) to protect your backend.",
                                    "  13.Function URLs  offer basic monitoring through Amazon CloudWatch metrics, including request count, latency, and error rates. You get \n              access to standard Lambda metrics and logs, which show the raw requests coming into your function. While this provides essential operational visibility, the metrics \n              are focused mainly on function execution.",
                                    "  14.API Gateway  provides comprehensive monitoring capabilities including detailed metrics, logging, and tracing options. You can \n              monitor API calls, latency, error rates, and cache hit/miss rates through CloudWatch.  also integrates with AWS X-Ray for distributed tracing and provides \n              customizable logging formats.",
                                    "  15.Function URLs  offer basic monitoring through Amazon CloudWatch metrics, including request count, latency, and error rates. You get \n              access to standard Lambda metrics and logs, which show the raw requests coming into your function. While this provides essential operational visibility, the metrics \n              are focused mainly on function execution.",
                                    "  16.API Gateway  provides comprehensive monitoring capabilities including detailed metrics, logging, and tracing options. You can \n              monitor API calls, latency, error rates, and cache hit/miss rates through CloudWatch.  also integrates with AWS X-Ray for distributed tracing and provides \n              customizable logging formats.",
                                    "  17.Function URLs  follow the standard Lambda pricing model – you only pay for function invocations and compute time. There are \n              no additional charges for the URL endpoint itself. This makes it a cost-effective choice for simple APIs or low-traffic applications if you don't need the additional \n              features of API Gateway.",
                                    "  18.API Gateway  offers a free tier that includes one million API calls \n              received for REST APIs and one million API calls received for HTTP APIs. After this,  charges for API calls, data transfer, and caching (if enabled). Refer to the  \n              pricing page to understand the costs for your own use case.",
                                    "  19.Function URLs  follow the standard Lambda pricing model – you only pay for function invocations and compute time. There are \n              no additional charges for the URL endpoint itself. This makes it a cost-effective choice for simple APIs or low-traffic applications if you don't need the additional \n              features of API Gateway.",
                                    "  20.API Gateway  offers a free tier that includes one million API calls \n              received for REST APIs and one million API calls received for HTTP APIs. After this,  charges for API calls, data transfer, and caching (if enabled). Refer to the  \n              pricing page to understand the costs for your own use case.",
                                    "  21.Function URLs  are designed for simplicity and direct Lambda integration. They support both HTTP and HTTPS endpoints, offer \n              built-in CORS support, and provide dual-stack (IPv4 and IPv6) endpoints. While they lack advanced features, they excel in scenarios where you need a quick, \n              straightforward way to expose Lambda functions via HTTP.",
                                    "  22.API Gateway  includes numerous additional features such as API versioning, stage management, API keys for usage plans, API documentation \n              through Swagger/OpenAPI, WebSocket APIs, private APIs within a VPC, and WAF integration for additional security. It also supports canary deployments, mock integrations \n              for testing, and integration with other AWS services beyond Lambda.",
                                    "  23.Function URLs  are designed for simplicity and direct Lambda integration. They support both HTTP and HTTPS endpoints, offer \n              built-in CORS support, and provide dual-stack (IPv4 and IPv6) endpoints. While they lack advanced features, they excel in scenarios where you need a quick, \n              straightforward way to expose Lambda functions via HTTP.",
                                    "  24.API Gateway  includes numerous additional features such as API versioning, stage management, API keys for usage plans, API documentation \n              through Swagger/OpenAPI, WebSocket APIs, private APIs within a VPC, and WAF integration for additional security. It also supports canary deployments, mock integrations \n              for testing, and integration with other AWS services beyond Lambda."
                                ]
                            },
                            {
                                "sub_header": "Select a method to invoke your Lambda function",
                                "content": [
                                    "Now that you've read about the criteria for selecting between Lambda function URLs and API Gateway and the key differences between them, you can select the option     that best meets your needs and use the following resources to help you get started using it.",
                                    "Function URLsGet started with function URLs with the following resourcesFollow the tutorial Creating a Lambda function with a function URLLearn more about function URLs in the Creating and managing Lambda function URLs chapter of this guideTry the in-console guided tutorial Create a simple web app by doing the following:Open the functions page of the Lambda console.Open the help panel by choosing the icon in the top right corner of the screen.Select Tutorials.In Create a simple web app, choose Start tutorial.API GatewayGet started with Lambda and API Gateway with the following resourcesFollow the tutorial Using Lambda with API Gateway to create a REST API integrated with a backend               Lambda function.Learn more about the different kinds of API offered by API Gateway in the following sections of the Amazon API Gateway Developer Guide:API Gateway REST APIsAPI Gateway HTTP APIsAPI Gateway WebSocket APIsTry one or more of the examples in the Tutorials and workshops                 section of the Amazon API Gateway Developer Guide.anchoranchorFunction URLsAPI GatewayGet started with function URLs with the following resourcesFollow the tutorial Creating a Lambda function with a function URLLearn more about function URLs in the Creating and managing Lambda function URLs chapter of this guideTry the in-console guided tutorial Create a simple web app by doing the following:Open the functions page of the Lambda console.Open the help panel by choosing the icon in the top right corner of the screen.Select Tutorials.In Create a simple web app, choose Start tutorial."
                                ]
                            }
                        ]
                    }
                ],
                "sections": [
                    "",
                    "You can create a web API with an HTTP endpoint for your Lambda function by using Amazon API Gateway. API Gateway provides tools    for creating and documenting web APIs that route HTTP requests to Lambda functions. You can secure access to your API    with authentication and authorization controls. Your APIs can serve traffic over the internet or can be accessible    only within your VPC.",
                    "Resources in your API define one or more methods, such as GET or POST. Methods have an integration that routes    requests to a Lambda function or another integration type. You can define each resource and method individually, or    use special resource and method types to match all requests that fit a pattern. A proxy      resource catches all paths beneath a resource. The ANY method catches all HTTP    methods.",
                    "  1.Choosing an API type",
                    "  2.Adding an endpoint to your Lambda function",
                    "  3.Proxy integration",
                    "  4.Event format",
                    "  5.Response format",
                    "  6.Permissions",
                    "  7.Sample application",
                    "  8.Tutorial: Using Lambda with API Gateway",
                    "  9.Handling Lambda errors with an API Gateway API",
                    "  10.Select a method to invoke your Lambda function using an HTTP request",
                    {
                        "sub_header": "Choosing an API type",
                        "content": [
                            "API Gateway supports three types of APIs that invoke Lambda functions:",
                            "HTTP APIs and REST APIs are both RESTful APIs that process HTTP requests and return responses. HTTP APIs are      newer and are built with the API Gateway version 2 API. The following features are new for HTTP APIs:",
                            {
                                "code_example": "$default"
                            },
                            "REST APIs are the classic RESTful APIs that API Gateway has supported since launch. REST APIs currently have more      customization, integration, and management features.",
                            "  1.Integration types  – REST APIs support custom Lambda integrations.\n          With a custom integration, you can send just the body of the request to the function, or apply a transform\n          template to the request body before sending it to the function.",
                            "  2.Access control  – REST APIs support more options for authentication\n          and authorization.",
                            "  3.Monitoring and tracing  – REST APIs support AWS X-Ray tracing and\n          additional logging options.",
                            "For a detailed comparison, see Choose between HTTP APIs and REST APIs in the API Gateway Developer Guide.",
                            "WebSocket APIs also use the API Gateway version 2 API and support a similar feature set. Use a WebSocket API for      applications that benefit from a persistent connection between the client and API. WebSocket APIs provide      full-duplex communication, which means that both the client and the API can send messages continuously without      waiting for a response.",
                            "HTTP APIs support a simplified event format (version 2.0). For an example of an event from an HTTP      API, see Create AWS Lambda proxy integrations for HTTP APIs in API Gateway.",
                            "For more information, see Create AWS Lambda proxy integrations for HTTP APIs in API Gateway."
                        ]
                    },
                    {
                        "sub_header": "Adding an endpoint to your Lambda function",
                        "content": [
                            "To add a public endpoint to your Lambda functionOpen the Functions page of the Lambda console.\nChoose a function.\n\nUnder Function overview, choose Add trigger.\n\nSelect API Gateway.\n\nChoose Create an API or Use an existing API.\n\nNew API: For API type, choose HTTP API. For more information, see Choosing an API type.\n\nExisting API: Select the API from the dropdown list or enter the API ID (for example, r3pmxmplak).\n\n\nFor Security, choose Open.\n\nChoose Add.\n"
                        ]
                    },
                    {
                        "sub_header": "Proxy integration",
                        "content": [
                            "API Gateway APIs are comprised of stages, resources, methods, and integrations. The stage and resource determine the      path of the endpoint:",
                            {
                                "code_example": "/prod/"
                            },
                            "A Lambda integration maps a path and HTTP method combination to a Lambda function. You can configure API Gateway to pass      the body of the HTTP request as-is (custom integration), or to encapsulate the request body in a document that      includes all of the request information including headers, resource, path, and method.",
                            "For more information, see Lambda proxy integrations in API Gateway."
                        ]
                    },
                    {
                        "sub_header": "Event format",
                        "content": [
                            "Amazon API Gateway invokes your function synchronously with an event that contains      a JSON representation of the HTTP request. For a custom integration, the event is the body of the request. For a      proxy integration, the event has a defined structure. For an example of a proxy event from an API Gateway REST      API, see Input format of a Lambda function for proxy integration in the API Gateway Developer Guide."
                        ]
                    },
                    {
                        "sub_header": "Response format",
                        "content": [
                            "API Gateway waits for a response from your function and relays the result to the caller. For a custom integration, you      define an integration response and a method response to convert the output from the function to an HTTP response.      For a proxy integration, the function must respond with a representation of the response in a specific      format.",
                            "The following example shows a response object from a Node.js function. The response object represents a      successful HTTP response that contains a JSON document.",
                            {
                                "code_example": "var response = {\n      \"statusCode\": 200,\n      \"headers\": {\n        \"Content-Type\": \"application/json\"\n      },\n      \"isBase64Encoded\": false,\n      \"multiValueHeaders\": { \n        \"X-Custom-Header\": [\"My value\", \"My other value\"],\n      },\n      \"body\": \"{\\n  \\\"TotalCodeSize\\\": 104330022,\\n  \\\"FunctionCount\\\": 26\\n}\"\n    }"
                            },
                            "The Lambda runtime serializes the response object into JSON and sends it to the API. The API parses the response      and uses it to create an HTTP response, which it then sends to the client that made the original request.",
                            {
                                "code_example": "< HTTP/1.1 200 OK\n  < Content-Type: application/json\n  < Content-Length: 55\n  < Connection: keep-alive\n  < x-amzn-RequestId: 32998fea-xmpl-4268-8c72-16138d629356\n  < X-Custom-Header: My value\n  < X-Custom-Header: My other value\n  < X-Amzn-Trace-Id: Root=1-5e6aa925-ccecxmplbae116148e52f036\n  <\n  {\n    \"TotalCodeSize\": 104330022,\n    \"FunctionCount\": 26\n  }"
                            }
                        ]
                    },
                    {
                        "sub_header": "Permissions",
                        "content": [
                            "Amazon API Gateway gets permission to invoke your function from the function's resource-based policy. You can grant invoke permission to an      entire API, or grant limited access to a stage, resource, or method.",
                            "When you add an API to your function by using the Lambda console, using the API Gateway console, or in an AWS SAM      template, the function's resource-based policy is updated automatically. The following is an example function policy.",
                            {
                                "code_example": "{\n  \"Version\": \"2012-10-17\",\n  \"Id\": \"default\",\n  \"Statement\": [\n    {\n      \"Sid\": \"nodejs-apig-functiongetEndpointPermissionProd-BWDBXMPLXE2F\",\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"Service\": \"apigateway.amazonaws.com\"\n      },\n      \"Action\": \"lambda:InvokeFunction\",\n      \"Resource\": \"arn:aws:lambda:us-east-2:111122223333:function:nodejs-apig-function-1G3MXMPLXVXYI\",\n      \"Condition\": {\n        \"StringEquals\": {\n          \"aws:SourceAccount\": \"111122223333\"\n        },\n        \"ArnLike\": {\n          \"aws:SourceArn\": \"arn:aws:execute-api:us-east-2:111122223333:ktyvxmpls1/*/GET/\"\n        }\n      }\n    }\n  ]\n}"
                            },
                            "You can manage function policy permissions manually with the following API operations:",
                            "To grant invocation permission to an existing API, use the add-permission command. Example:",
                            "aws lambda add-permission \\  --function-name my-function \\  --statement-id apigateway-get --action lambda:InvokeFunction \\  --principal apigateway.amazonaws.com \\  --source-arn \"arn:aws:execute-api:us-east-2:123456789012:mnh1xmpli7/default/GET/\"",
                            "You should see the following output:",
                            "{    \"Statement\": \"{\\\"Sid\\\":\\\"apigateway-test-2\\\",\\\"Effect\\\":\\\"Allow\\\",\\\"Principal\\\":{\\\"Service\\\":\\\"apigateway.amazonaws.com\\\"},\\\"Action\\\":\\\"lambda:InvokeFunction\\\",\\\"Resource\\\":\\\"arn:aws:lambda:us-east-2:123456789012:function:my-function\\\",\\\"Condition\\\":{\\\"ArnLike\\\":{\\\"AWS:SourceArn\\\":\\\"arn:aws:execute-api:us-east-2:123456789012:mnh1xmpli7/default/GET\\\"}}}\"}",
                            "NoteIf your function and API are in different AWS Regions, the Region identifier in the source ARN must match the\n        Region of the function, not the Region of the API. When API Gateway invokes a function, it uses a resource ARN that is\n        based on the ARN of the API, but modified to match the function's Region.",
                            "The source ARN in this example grants permission to an integration on the GET method of the root resource in      the default stage of an API, with ID mnh1xmpli7. You can use an asterisk in the source ARN to grant      permissions to multiple stages, methods, or resources.",
                            {
                                "code_example": "mnh1xmpli7/*/GET/*"
                            },
                            "For details on viewing the policy and removing statements, see Working with resource-based IAM policies in Lambda."
                        ]
                    },
                    {
                        "sub_header": "Sample application",
                        "content": [
                            "The API Gateway with Node.js sample app includes a function with an AWS SAM      template that creates a REST API that has AWS X-Ray tracing enabled. It also includes scripts for deploying,      invoking the function, testing the API, and cleanup."
                        ]
                    }
                ]
            },
            {
                "title": "Infrastructure Composer",
                "href": "https://docs.aws.amazon.com/lambda/latest/dg/services-appcomposer.html",
                "sections": [
                    "",
                    "AWS Infrastructure Composer is a visual builder for desiging modern applications on AWS. You design your application architecture by dragging, grouping, and   connecting AWS services in a visual canvas. Infrastructure Composer creates infrastructure as code (IaC) templates from your design that you can deploy using     AWS SAM or     AWS CloudFormation.",
                    {
                        "sub_header": "Exporting a Lambda function to Infrastructure Composer",
                        "content": [
                            "You can get started using Infrastructure Composer by creating a new project based on the configuration of an existing Lambda function using the Lambda console.      To export your function's configuration and code to Infrastructure Composer to create a new project, do the following:",
                            "\nOpen the Functions page of the Lambda console.\n\nSelect the function you want to use as a basis for your Infrastructure Composer project.\n\nIn the Function overview pane, choose Export to Infrastructure Composer.\nTo export your function's configuration and code to Infrastructure Composer, Lambda creates an Amazon S3 bucket in your account to temporarily store this data.\n\nIn the dialog box, choose Confirm and create project to accept the default name for this bucket and export your function's \n         configuration and code to Infrastructure Composer.\n\n(Optional) To choose another name for the Amazon S3 bucket that Lambda creates, enter a new name and choose Confirm and create project. \n         Amazon S3 bucket names must be globally unique and follow the bucket naming rules.\n\nTo save your project and function files in Infrastructure Composer, activate local sync mode.\n",
                            "NoteIf you've used the Export to Application Composer feature before and created an Amazon S3 bucket using the default name, \n       Lambda can re-use this bucket if it still exists. Accept the default bucket name in the dialog box to re-use the existing bucket.",
                            {
                                "sub_header": "Amazon S3 transfer bucket configuration",
                                "content": [
                                    "The Amazon S3 bucket that Lambda creates to transfer your function's configuration automatically encrypts objects using the AES 256 encryption        standard. Lambda also configures the bucket to use the bucket owner condition        to ensure that only your AWS account is able to add objects to the bucket.",
                                    "Lambda configures the bucket to automatically delete objects 10 days after they are uploaded. However, Lambda doesn't        automaticaly delete the bucket itself. To delete the bucket from your AWS account, follow the instructions in Deleting a bucket.        The default bucket name uses the prefix lambdasam, a 10-digit alphanumeric string, and the AWS Region you created your function in:",
                                    "lambdasam-06f22da95b-us-east-1",
                                    "To avoid additional charges being added to your AWS account, we recommend that you delete the Amazon S3 bucket as soon as you have finished exporting        your function to Infrastructure Composer.",
                                    "Standard Amazon S3 pricing applies."
                                ]
                            },
                            {
                                "sub_header": "Required permissions",
                                "content": [
                                    "To use the Lambda integration with Infrastructure Composer feature, you need certain permissions to download an AWS SAM template and to write your      function's configuration to Amazon S3.",
                                    "To download an AWS SAM template, you must have permission to use the following API actions:",
                                    "You can grant permission to use all of these actions by adding the AWSLambda_ReadOnlyAccess        AWS managed policy to your IAM user role.",
                                    "For Lambda to write your function's configuration to Amazon S3, you must have permission to use the following API actions:",
                                    "If you are unable to export your function's configuration to Infrastructure Composer, check that your account has the required permissions for these        operations. If you have the required permissions, but still cannot export your function's configuration, check for any        resource-based policies that might limit access to Amazon S3."
                                ]
                            }
                        ]
                    },
                    {
                        "sub_header": "Other resources",
                        "content": [
                            "For a more detailed tutorial on how to design a serverless application in Infrastructure Composer based on an existing Lambda function, see       Using Lambda with infrastructure as code (IaC).",
                            "To use Infrastructure Composer and AWS SAM to design and deploy a complete serverless application using Lambda, you can also       follow the AWS Infrastructure Composer tutorial in the       AWS Serverless Patterns Workshop."
                        ]
                    }
                ]
            },
            {
                "title": "CloudFormation",
                "href": "https://docs.aws.amazon.com/lambda/latest/dg/services-cloudformation.html",
                "sections": [
                    "",
                    "In an AWS CloudFormation template, you can specify a Lambda function as the target of a custom resource. Use custom    resources to process parameters, retrieve configuration values, or call other AWS services during stack lifecycle    events.",
                    "The following example invokes a function that's defined elsewhere in the template.",
                    {
                        "code_example": "Resources:\n  primerinvoke:\n    Type: AWS::CloudFormation::CustomResource\n    Version: \"1.0\"\n    Properties:\n      ServiceToken: !GetAtt primer.Arn\n      FunctionName: !Ref randomerror"
                    },
                    "The service token is the Amazon Resource Name (ARN) of the function that AWS CloudFormation invokes when you create, update,    or delete the stack. You can also include additional properties like FunctionName, which AWS CloudFormation passes    to your function as is.",
                    "AWS CloudFormation invokes your Lambda function asynchronously with an event that    includes a callback URL.",
                    {
                        "code_example": "{\n    \"RequestType\": \"Create\",\n    \"ServiceToken\": \"arn:aws:lambda:us-east-1:123456789012:function:lambda-error-processor-primer-14ROR2T3JKU66\",\n    \"ResponseURL\": \"https://cloudformation-custom-resource-response-useast1.s3-us-east-1.amazonaws.com/arn%3Aaws%3Acloudformation%3Aus-east-1%3A123456789012%3Astack/lambda-error-processor/1134083a-2608-1e91-9897-022501a2c456%7Cprimerinvoke%7C5d478078-13e9-baf0-464a-7ef285ecc786?AWSAccessKeyId=AKIAIOSFODNN7EXAMPLE&Expires=1555451971&Signature=28UijZePE5I4dvukKQqM%2F9Rf1o4%3D\",\n    \"StackId\": \"arn:aws:cloudformation:us-east-1:123456789012:stack/lambda-error-processor/1134083a-2608-1e91-9897-022501a2c456\",\n    \"RequestId\": \"5d478078-13e9-baf0-464a-7ef285ecc786\",\n    \"LogicalResourceId\": \"primerinvoke\",\n    \"ResourceType\": \"AWS::CloudFormation::CustomResource\",\n    \"ResourceProperties\": {\n        \"ServiceToken\": \"arn:aws:lambda:us-east-1:123456789012:function:lambda-error-processor-primer-14ROR2T3JKU66\",\n        \"FunctionName\": \"lambda-error-processor-randomerror-ZWUC391MQAJK\"\n    }\n}"
                    },
                    "The function is responsible for returning a response to the callback URL that indicates success or failure. For    the full response syntax, see Custom resource response      objects.",
                    {
                        "code_example": "{\n    \"Status\": \"SUCCESS\",\n    \"PhysicalResourceId\": \"2019/04/18/[$LATEST]b3d1bfc65f19ec610654e4d9b9de47a0\",\n    \"StackId\": \"arn:aws:cloudformation:us-east-1:123456789012:stack/lambda-error-processor/1134083a-2608-1e91-9897-022501a2c456\",\n    \"RequestId\": \"5d478078-13e9-baf0-464a-7ef285ecc786\",\n    \"LogicalResourceId\": \"primerinvoke\"\n}"
                    },
                    "AWS CloudFormation provides a library called cfn-response that handles sending the response. If you define your    function within a template, you can require the library by name. AWS CloudFormation then adds the library to the deployment    package that it creates for the function.",
                    "If your function that a Custom Resource uses has an Elastic Network Interface attached to it,     add the following resources to the VPC policy where region is the Region the function is in without the dashes. For example,     us-east-1 is useast1. This will allow the Custom Resource to respond to the callback URL that sends a signal back to the AWS CloudFormation stack.",
                    "arn:aws:s3:::cloudformation-custom-resource-response-region\",\"arn:aws:s3:::cloudformation-custom-resource-response-region/*\",",
                    "The following example function invokes a second function. If the call succeeds, the function sends a success    response to AWS CloudFormation, and the stack update continues. The template uses the AWS::Serverless::Function resource type provided by    AWS Serverless Application Model.",
                    {
                        "code_example": "Transform: 'AWS::Serverless-2016-10-31'\nResources:\n  primer:\n    Type: AWS::Serverless::Function\n    Properties:\n      Handler: index.handler\n      Runtime: nodejs16.x\n      InlineCode: |\n        var aws = require('aws-sdk');\n        var response = require('cfn-response');\n        exports.handler = function(event, context) {\n            // For Delete requests, immediately send a SUCCESS response.\n            if (event.RequestType == \"Delete\") {\n                response.send(event, context, \"SUCCESS\");\n                return;\n            }\n            var responseStatus = \"FAILED\";\n            var responseData = {};\n            var functionName = event.ResourceProperties.FunctionName\n            var lambda = new aws.Lambda();\n            lambda.invoke({ FunctionName: functionName }, function(err, invokeResult) {\n                if (err) {\n                    responseData = {Error: \"Invoke call failed\"};\n                    console.log(responseData.Error + \":\\n\", err);\n                }\n                else responseStatus = \"SUCCESS\";\n                response.send(event, context, responseStatus, responseData);\n            });\n        };\n      Description: Invoke a function to create a log stream.\n      MemorySize: 128\n      Timeout: 8\n      Role: !GetAtt role.Arn\n      Tracing: Active"
                    },
                    "If the function that the custom resource invokes isn't defined in a template, you can get the source code for      cfn-response from cfn-response module in the AWS CloudFormation User Guide.",
                    "For more information about custom resources, see Custom      resources in the AWS CloudFormation User Guide."
                ]
            },
            {
                "title": "Amazon DocumentDB",
                "href": "https://docs.aws.amazon.com/lambda/latest/dg/with-documentdb.html",
                "contents": [
                    {
                        "title": "Tutorial",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/with-documentdb-tutorial.html",
                        "sections": [
                            "",
                            "    In this tutorial, you create a basic Lambda function that consumes events from an Amazon DocumentDB (with MongoDB compatibility) change stream.    To complete this tutorial, you will go through the following stages:  ",
                            "  1.Prerequisites",
                            "  2.Create the AWS Cloud9 environment",
                            "  3.Create the Amazon EC2 security group",
                            "  4.Create the Amazon DocumentDB cluster",
                            "  5.Create the secret in Secrets Manager",
                            "  6.Install the mongo shell",
                            "  7.Connect to the Amazon DocumentDB cluster",
                            "  8.Activate change streams",
                            "  9.Create interface VPC endpoints",
                            "  10.Create the execution role",
                            "  11.Create the Lambda function",
                            "  12.Create the Lambda event source mapping",
                            "  13.Test your function - manual invoke",
                            "  14.Test your function - insert a record",
                            "  15.Test your function - update a record",
                            "  16.Test your function - delete a record",
                            "  17.Clean up your resources",
                            {
                                "sub_header": "Prerequisites",
                                "content": [
                                    {
                                        "code_example": "zip"
                                    }
                                ]
                            },
                            {
                                "sub_header": "Create the AWS Cloud9 environment",
                                "content": [
                                    "\n\n",
                                    "      Before creating the Lambda function, you need to create and configure your Amazon DocumentDB cluster. The steps to set up      your cluster in this tutorial is based on the procedure in      Get Started with Amazon DocumentDB.    ",
                                    "NoteIf you already have an Amazon DocumentDB cluster set up, ensure that you activate change streams and\n      create the necessary interface VPC endpoints. Then, you can skip directly to the\n      function creation steps.",
                                    "      First, create an AWS Cloud9 environment. You’ll use this environment throughout this tutorial to connect to and query your Amazon DocumentDB cluster.    ",
                                    {
                                        "code_example": "DocumentDBCloud9Environment"
                                    }
                                ]
                            },
                            {
                                "sub_header": "Create the Amazon EC2 security group",
                                "content": [
                                    "\n\n",
                                    "      Next, create an Amazon EC2 security group      with rules that allow traffic between your Amazon DocumentDB cluster and your AWS Cloud9 environment.    ",
                                    {
                                        "code_example": "DocDBTutorial"
                                    }
                                ]
                            },
                            {
                                "sub_header": "Create the Amazon DocumentDB cluster",
                                "content": [
                                    "\n\n",
                                    "      In this step, you’ll create an Amazon DocumentDB cluster using the security group from the previous step.    ",
                                    {
                                        "code_example": "default (VPC)"
                                    }
                                ]
                            },
                            {
                                "sub_header": "Create the secret in Secrets Manager",
                                "content": [
                                    "\n\n",
                                    "      To access your Amazon DocumentDB cluster manually, you must provide username and password credentials. For Lambda to access your cluster,      you must provide a Secrets Manager secret that contains these same access credentials when setting up your event source mapping.      In this step, you’ll create this secret.    ",
                                    {
                                        "code_example": "DocumentDBSecret"
                                    },
                                    "      Note down the Secret ARNof your secret. You’ll need it in a later step.    "
                                ]
                            },
                            {
                                "sub_header": "Install the mongo shell",
                                "content": [
                                    "\n\n",
                                    "      In this step, you’ll install the mongo shell in your AWS Cloud9 environment. The mongo shell is a command-line utility      that you use to connect to and query your Amazon DocumentDB cluster.    ",
                                    {
                                        "code_example": "DocumentDBCloud9Environment"
                                    }
                                ]
                            },
                            {
                                "sub_header": "Connect to the Amazon DocumentDB cluster",
                                "content": [
                                    "\n\n",
                                    "      You’re now ready to connect to your Amazon DocumentDB cluster using the mongo shell.    ",
                                    {
                                        "code_example": "<insertYourPassword>"
                                    },
                                    "After entering this command, if the command prompt becomes rs0:PRIMARY>, then you’re connected to your Amazon DocumentDB cluster."
                                ]
                            },
                            {
                                "sub_header": "Activate change streams",
                                "content": [
                                    "\n\n",
                                    "For this tutorial, you’ll track changes to the products collection of the docdbdemo database      in your Amazon DocumentDB cluster. You do this by activating change streams. First, create the docdbdemo database and test it by inserting a record.",
                                    {
                                        "code_example": "docdbdemo"
                                    },
                                    "      Next, activate change streams on the products collection of the docdbdemo database using the following command:    ",
                                    "db.adminCommand({modifyChangeStreams: 1,    database: \"docdbdemo\",    collection: \"products\",     enable: true});",
                                    "      You should see output that looks like this:    ",
                                    "{ \"ok\" : 1, \"operationTime\" : Timestamp(1680126165, 1) }"
                                ]
                            },
                            {
                                "sub_header": "Create interface VPC endpoints",
                                "content": [
                                    "\n\n",
                                    "      Next, create interface VPC endpoints      to ensure that Lambda and Secrets Manager (used later to store our cluster access credentials) can connect to your default VPC.     ",
                                    {
                                        "code_example": "lambda-default-vpc"
                                    },
                                    "      This completes the cluster setup portion of this tutorial.    "
                                ]
                            },
                            {
                                "sub_header": "Create the execution role",
                                "content": [
                                    "\n\n",
                                    "      In the next set of steps, you’ll create your Lambda function. First, you need to create the execution role that gives      your function permission to access your cluster. You do this by creating an IAM policy first, then attaching this      policy to an IAM role.    ",
                                    {
                                        "code_example": "{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"LambdaESMNetworkingAccess\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"ec2:CreateNetworkInterface\",\n                \"ec2:DescribeNetworkInterfaces\",\n                \"ec2:DescribeVpcs\",\n                \"ec2:DeleteNetworkInterface\",\n                \"ec2:DescribeSubnets\",\n                \"ec2:DescribeSecurityGroups\",\n                \"kms:Decrypt\"\n            ],\n            \"Resource\": \"*\"\n        },\n        {\n            \"Sid\": \"LambdaDocDBESMAccess\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"rds:DescribeDBClusters\",\n                \"rds:DescribeDBClusterParameters\",\n                \"rds:DescribeDBSubnetGroups\"\n            ],\n            \"Resource\": \"*\"\n        },\n        {\n            \"Sid\": \"LambdaDocDBESMGetSecretValueAccess\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"secretsmanager:GetSecretValue\"\n            ],\n            \"Resource\": \"arn:aws:secretsmanager:us-east-1:123456789012:secret:DocumentDBSecret\"\n        }\n    ]\n}"
                                    },
                                    {
                                        "code_example": "AWSDocumentDBLambdaPolicy"
                                    }
                                ]
                            },
                            {
                                "sub_header": "Create the Lambda function",
                                "content": [
                                    "\n\n",
                                    "      The following example code receives an Amazon DocumentDB event input and processes the message that it contains.    ",
                                    ".NETAWS SDK for .NETNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Consuming a Amazon DocumentDB event with Lambda using .NET.using Amazon.Lambda.Core;using System.Text.Json;using System;using System.Collections.Generic;using System.Text.Json.Serialization;//Assembly attribute to enable the Lambda function's JSON input to be converted into a .NET class.[assembly: LambdaSerializer(typeof(Amazon.Lambda.Serialization.SystemTextJson.DefaultLambdaJsonSerializer))]namespace LambdaDocDb;public class Function{         /// <summary>    /// Lambda function entry point to process Amazon DocumentDB events.    /// </summary>    /// <param name=\"event\">The Amazon DocumentDB event.</param>    /// <param name=\"context\">The Lambda context object.</param>    /// <returns>A string to indicate successful processing.</returns>    public string FunctionHandler(Event evnt, ILambdaContext context)    {                foreach (var record in evnt.Events)        {            ProcessDocumentDBEvent(record, context);        }        return \"OK\";    }     private void ProcessDocumentDBEvent(DocumentDBEventRecord record, ILambdaContext context)    {                var eventData = record.Event;        var operationType = eventData.OperationType;        var databaseName = eventData.Ns.Db;        var collectionName = eventData.Ns.Coll;        var fullDocument = JsonSerializer.Serialize(eventData.FullDocument, new JsonSerializerOptions { WriteIndented = true });        context.Logger.LogLine($\"Operation type: {operationType}\");        context.Logger.LogLine($\"Database: {databaseName}\");        context.Logger.LogLine($\"Collection: {collectionName}\");        context.Logger.LogLine($\"Full document:\\n{fullDocument}\");    }    public class Event    {        [JsonPropertyName(\"eventSourceArn\")]        public string EventSourceArn { get; set; }        [JsonPropertyName(\"events\")]        public List<DocumentDBEventRecord> Events { get; set; }        [JsonPropertyName(\"eventSource\")]        public string EventSource { get; set; }    }    public class DocumentDBEventRecord    {        [JsonPropertyName(\"event\")]        public EventData Event { get; set; }    }    public class EventData    {        [JsonPropertyName(\"_id\")]        public IdData Id { get; set; }        [JsonPropertyName(\"clusterTime\")]        public ClusterTime ClusterTime { get; set; }        [JsonPropertyName(\"documentKey\")]        public DocumentKey DocumentKey { get; set; }        [JsonPropertyName(\"fullDocument\")]        public Dictionary<string, object> FullDocument { get; set; }        [JsonPropertyName(\"ns\")]        public Namespace Ns { get; set; }        [JsonPropertyName(\"operationType\")]        public string OperationType { get; set; }    }    public class IdData    {        [JsonPropertyName(\"_data\")]        public string Data { get; set; }    }    public class ClusterTime    {        [JsonPropertyName(\"$timestamp\")]        public Timestamp Timestamp { get; set; }    }    public class Timestamp    {        [JsonPropertyName(\"t\")]        public long T { get; set; }        [JsonPropertyName(\"i\")]        public int I { get; set; }    }    public class DocumentKey    {        [JsonPropertyName(\"_id\")]        public Id Id { get; set; }    }    public class Id    {        [JsonPropertyName(\"$oid\")]        public string Oid { get; set; }    }    public class Namespace    {        [JsonPropertyName(\"db\")]        public string Db { get; set; }        [JsonPropertyName(\"coll\")]        public string Coll { get; set; }    }}GoSDK for Go V2Note        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Consuming a Amazon DocumentDB event with Lambda using Go.package mainimport (\t\"context\"\t\"encoding/json\"\t\"fmt\"\t\"github.com/aws/aws-lambda-go/lambda\")type Event struct {\tEvents []Record `json:\"events\"`}type Record struct {\tEvent struct {\t\tOperationType string `json:\"operationType\"`\t\tNS            struct {\t\t\tDB   string `json:\"db\"`\t\t\tColl string `json:\"coll\"`\t\t} `json:\"ns\"`\t\tFullDocument interface{} `json:\"fullDocument\"`\t} `json:\"event\"`}func main() {\tlambda.Start(handler)}func handler(ctx context.Context, event Event) (string, error) {\tfmt.Println(\"Loading function\")\tfor _, record := range event.Events {\t\tlogDocumentDBEvent(record)\t}\treturn \"OK\", nil}func logDocumentDBEvent(record Record) {\tfmt.Printf(\"Operation type: %s\\n\", record.Event.OperationType)\tfmt.Printf(\"db: %s\\n\", record.Event.NS.DB)\tfmt.Printf(\"collection: %s\\n\", record.Event.NS.Coll)\tdocBytes, _ := json.MarshalIndent(record.Event.FullDocument, \"\", \"  \")\tfmt.Printf(\"Full document: %s\\n\", string(docBytes))}JavaScriptSDK for JavaScript (v3)Note        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Consuming a Amazon DocumentDB event with Lambda using JavaScript.console.log('Loading function');exports.handler = async (event, context) => {    event.events.forEach(record => {        logDocumentDBEvent(record);    });    return 'OK';};const logDocumentDBEvent = (record) => {    console.log('Operation type: ' + record.event.operationType);    console.log('db: ' + record.event.ns.db);    console.log('collection: ' + record.event.ns.coll);    console.log('Full document:', JSON.stringify(record.event.fullDocument, null, 2));};Consuming a Amazon DocumentDB event with Lambda using TypeScriptimport { DocumentDBEventRecord, DocumentDBEventSubscriptionContext } from 'aws-lambda';console.log('Loading function');export const handler = async (  event: DocumentDBEventSubscriptionContext,  context: any): Promise<string> => {  event.events.forEach((record: DocumentDBEventRecord) => {    logDocumentDBEvent(record);  });  return 'OK';};const logDocumentDBEvent = (record: DocumentDBEventRecord): void => {  console.log('Operation type: ' + record.event.operationType);  console.log('db: ' + record.event.ns.db);  console.log('collection: ' + record.event.ns.coll);  console.log('Full document:', JSON.stringify(record.event.fullDocument, null, 2));};PHPSDK for PHPNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Consuming a Amazon DocumentDB event with Lambda using PHP.<?phprequire __DIR__.'/vendor/autoload.php';use Bref\\Context\\Context;use Bref\\Event\\Handler;class DocumentDBEventHandler implements Handler{    public function handle($event, Context $context): string    {        $events = $event['events'] ?? [];        foreach ($events as $record) {            $this->logDocumentDBEvent($record['event']);        }        return 'OK';    }    private function logDocumentDBEvent($event): void    {        // Extract information from the event record        $operationType = $event['operationType'] ?? 'Unknown';        $db = $event['ns']['db'] ?? 'Unknown';        $collection = $event['ns']['coll'] ?? 'Unknown';        $fullDocument = $event['fullDocument'] ?? [];        // Log the event details        echo \"Operation type: $operationType\\n\";        echo \"Database: $db\\n\";        echo \"Collection: $collection\\n\";        echo \"Full document: \" . json_encode($fullDocument, JSON_PRETTY_PRINT) . \"\\n\";    }}return new DocumentDBEventHandler();PythonSDK for Python (Boto3)Note        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Consuming a Amazon DocumentDB event with Lambda using Python.import jsondef lambda_handler(event, context):    for record in event.get('events', []):        log_document_db_event(record)    return 'OK'def log_document_db_event(record):    event_data = record.get('event', {})    operation_type = event_data.get('operationType', 'Unknown')    db = event_data.get('ns', {}).get('db', 'Unknown')    collection = event_data.get('ns', {}).get('coll', 'Unknown')    full_document = event_data.get('fullDocument', {})    print(f\"Operation type: {operation_type}\")    print(f\"db: {db}\")    print(f\"collection: {collection}\")    print(\"Full document:\", json.dumps(full_document, indent=2))RubySDK for RubyNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Consuming a Amazon DocumentDB event with Lambda using Ruby.require 'json'def lambda_handler(event:, context:)  event['events'].each do |record|    log_document_db_event(record)  end  'OK'enddef log_document_db_event(record)  event_data = record['event'] || {}  operation_type = event_data['operationType'] || 'Unknown'  db = event_data.dig('ns', 'db') || 'Unknown'  collection = event_data.dig('ns', 'coll') || 'Unknown'  full_document = event_data['fullDocument'] || {}  puts \"Operation type: #{operation_type}\"  puts \"db: #{db}\"  puts \"collection: #{collection}\"  puts \"Full document: #{JSON.pretty_generate(full_document)}\"endRustSDK for RustNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Consuming a Amazon DocumentDB event with Lambda using Rust.use lambda_runtime::{service_fn, tracing, Error, LambdaEvent};use aws_lambda_events::{    event::documentdb::{DocumentDbEvent, DocumentDbInnerEvent},   };// Built with the following dependencies://lambda_runtime = \"0.11.1\"//serde_json = \"1.0\"//tokio = { version = \"1\", features = [\"macros\"] }//tracing = { version = \"0.1\", features = [\"log\"] }//tracing-subscriber = { version = \"0.3\", default-features = false, features = [\"fmt\"] }//aws_lambda_events = \"0.15.0\"async fn function_handler(event: LambdaEvent<DocumentDbEvent>) ->Result<(), Error> {        tracing::info!(\"Event Source ARN: {:?}\", event.payload.event_source_arn);    tracing::info!(\"Event Source: {:?}\", event.payload.event_source);      let records = &event.payload.events;       if records.is_empty() {        tracing::info!(\"No records found. Exiting.\");        return Ok(());    }    for record in records{        log_document_db_event(record);    }    tracing::info!(\"Document db records processed\");    // Prepare the response    Ok(())}fn log_document_db_event(record: &DocumentDbInnerEvent)-> Result<(), Error>{    tracing::info!(\"Change Event: {:?}\", record.event);        Ok(())}#[tokio::main]async fn main() -> Result<(), Error> {    tracing_subscriber::fmt()    .with_max_level(tracing::Level::INFO)    .with_target(false)    .without_time()    .init();    let func = service_fn(function_handler);    lambda_runtime::run(func).await?;    Ok(())    }anchoranchoranchoranchoranchoranchoranchor.NETGoJavaScriptPHPPythonRubyRustAWS SDK for .NETNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Consuming a Amazon DocumentDB event with Lambda using .NET.using Amazon.Lambda.Core;using System.Text.Json;using System;using System.Collections.Generic;using System.Text.Json.Serialization;//Assembly attribute to enable the Lambda function's JSON input to be converted into a .NET class.[assembly: LambdaSerializer(typeof(Amazon.Lambda.Serialization.SystemTextJson.DefaultLambdaJsonSerializer))]namespace LambdaDocDb;public class Function{         /// <summary>    /// Lambda function entry point to process Amazon DocumentDB events.    /// </summary>    /// <param name=\"event\">The Amazon DocumentDB event.</param>    /// <param name=\"context\">The Lambda context object.</param>    /// <returns>A string to indicate successful processing.</returns>    public string FunctionHandler(Event evnt, ILambdaContext context)    {                foreach (var record in evnt.Events)        {            ProcessDocumentDBEvent(record, context);        }        return \"OK\";    }     private void ProcessDocumentDBEvent(DocumentDBEventRecord record, ILambdaContext context)    {                var eventData = record.Event;        var operationType = eventData.OperationType;        var databaseName = eventData.Ns.Db;        var collectionName = eventData.Ns.Coll;        var fullDocument = JsonSerializer.Serialize(eventData.FullDocument, new JsonSerializerOptions { WriteIndented = true });        context.Logger.LogLine($\"Operation type: {operationType}\");        context.Logger.LogLine($\"Database: {databaseName}\");        context.Logger.LogLine($\"Collection: {collectionName}\");        context.Logger.LogLine($\"Full document:\\n{fullDocument}\");    }    public class Event    {        [JsonPropertyName(\"eventSourceArn\")]        public string EventSourceArn { get; set; }        [JsonPropertyName(\"events\")]        public List<DocumentDBEventRecord> Events { get; set; }        [JsonPropertyName(\"eventSource\")]        public string EventSource { get; set; }    }    public class DocumentDBEventRecord    {        [JsonPropertyName(\"event\")]        public EventData Event { get; set; }    }    public class EventData    {        [JsonPropertyName(\"_id\")]        public IdData Id { get; set; }        [JsonPropertyName(\"clusterTime\")]        public ClusterTime ClusterTime { get; set; }        [JsonPropertyName(\"documentKey\")]        public DocumentKey DocumentKey { get; set; }        [JsonPropertyName(\"fullDocument\")]        public Dictionary<string, object> FullDocument { get; set; }        [JsonPropertyName(\"ns\")]        public Namespace Ns { get; set; }        [JsonPropertyName(\"operationType\")]        public string OperationType { get; set; }    }    public class IdData    {        [JsonPropertyName(\"_data\")]        public string Data { get; set; }    }    public class ClusterTime    {        [JsonPropertyName(\"$timestamp\")]        public Timestamp Timestamp { get; set; }    }    public class Timestamp    {        [JsonPropertyName(\"t\")]        public long T { get; set; }        [JsonPropertyName(\"i\")]        public int I { get; set; }    }    public class DocumentKey    {        [JsonPropertyName(\"_id\")]        public Id Id { get; set; }    }    public class Id    {        [JsonPropertyName(\"$oid\")]        public string Oid { get; set; }    }    public class Namespace    {        [JsonPropertyName(\"db\")]        public string Db { get; set; }        [JsonPropertyName(\"coll\")]        public string Coll { get; set; }    }}",
                                    {
                                        "code_example": "index.js"
                                    }
                                ]
                            },
                            {
                                "sub_header": "Create the Lambda event source mapping",
                                "content": [
                                    "\n\n",
                                    "      Create the event source mapping that associates your Amazon DocumentDB change stream with your Lambda function.      After you create this event source mapping, AWS Lambda immediately starts polling the stream.    ",
                                    {
                                        "code_example": "ProcessDocumentDBRecords"
                                    }
                                ]
                            },
                            {
                                "sub_header": "Test your function - manual invoke",
                                "content": [
                                    "\n\n",
                                    "      To test that you created your function and event source mapping correctly, invoke your function using the invoke command.      To do this, first copy the following event JSON into a file called input.txt:    ",
                                    "{  \"eventSourceArn\": \"arn:aws:rds:us-east-1:123456789012:cluster:canaryclusterb2a659a2-qo5tcmqkcl03\",  \"events\": [    {      \"event\": {        \"_id\": {          \"_data\": \"0163eeb6e7000000090100000009000041e1\"        },        \"clusterTime\": {          \"$timestamp\": {            \"t\": 1676588775,            \"i\": 9          }        },        \"documentKey\": {          \"_id\": {            \"$oid\": \"63eeb6e7d418cd98afb1c1d7\"          }        },        \"fullDocument\": {          \"_id\": {            \"$oid\": \"63eeb6e7d418cd98afb1c1d7\"          },          \"anyField\": \"sampleValue\"        },        \"ns\": {          \"db\": \"docdbdemo\",          \"coll\": \"products\"        },        \"operationType\": \"insert\"      }    }  ],  \"eventSource\": \"aws:docdb\"}",
                                    "      Then, use the following command to invoke your function with this event:    ",
                                    "aws lambda invoke \\  --function-name ProcessDocumentDBRecords \\  --cli-binary-format raw-in-base64-out \\  --region us-east-1 \\  --payload file://input.txt out.txt",
                                    "      You should see a response that looks like the following:    ",
                                    "{   \"StatusCode\": 200,   \"ExecutedVersion\": \"$LATEST\"}",
                                    "      You can verify that your function successfully processed the event by checking CloudWatch Logs.    ",
                                    "To verify manual invocation via CloudWatch Logs\nOpen the Functions page in the Lambda console.\n\nChoose the Monitor tab, then choose View CloudWatch logs.\n          This takes you to the specific log group associated with your function in the CloudWatch console.\n\nChoose the most recent log stream. Within the log messages, you should see the event JSON.\n"
                                ]
                            },
                            {
                                "sub_header": "Test your function - insert a record",
                                "content": [
                                    "\n\n",
                                    "      Test your end-to-end setup by interacting directly with your Amazon DocumentDB database. In the next set of steps,      you’ll insert a record, update it, then delete it.    ",
                                    {
                                        "code_example": "docdbdemo"
                                    }
                                ]
                            },
                            {
                                "sub_header": "Test your function - update a record",
                                "content": [
                                    "\n\n",
                                    "      Next, update the record you just inserted with the following command:    ",
                                    "db.products.update(    { \"name\": \"Pencil\" },    { $set: { \"price\": 0.50 }})",
                                    "      Verify that your function successfully processed this event by checking CloudWatch Logs.    "
                                ]
                            },
                            {
                                "sub_header": "Test your function - delete a record",
                                "content": [
                                    "\n\n",
                                    "Finally, delete the record you just updated with the following command:",
                                    "db.products.remove( { \"name\": \"Pencil\" } )",
                                    "Verify that your function successfully processed this event by checking CloudWatch Logs."
                                ]
                            },
                            {
                                "sub_header": "Clean up your resources",
                                "content": [
                                    "      You can now delete the resources that you created for this tutorial, unless you want to retain them. By deleting AWS      resources that you're no longer using, you prevent unnecessary charges to your AWS account.    ",
                                    {
                                        "code_example": "delete"
                                    },
                                    "To delete the execution role\nOpen the Roles page of the IAM console.\n\nSelect the execution role that you created.\n\nChoose Delete.\n\nEnter the name of the role in the text input field and choose Delete.\n",
                                    {
                                        "code_example": "delete"
                                    },
                                    {
                                        "code_example": "delete"
                                    },
                                    "To delete the secret in Secrets Manager\nOpen the Secrets Manager console.\n\nChoose the secret you created for this tutorial.\n\nChoose Actions, Delete secret.\n\nChoose Schedule deletion.\n",
                                    "To delete the Amazon EC2 security group\nOpen the EC2 console. Under\n      Network and Security, choose Security groups.\n\nSelect the security group you created for this tutorial.\n\nChoose Actions, Delete security groups.\n\nChoose Delete.\n",
                                    {
                                        "code_example": "delete"
                                    }
                                ]
                            }
                        ]
                    }
                ],
                "sections": [
                    "",
                    "You can use a Lambda function to process events in an Amazon DocumentDB (with MongoDB compatibility) change stream by configuring an    Amazon DocumentDB cluster as an event source. Then, you can automate event-driven workloads by invoking your Lambda function    each time that data changes with your Amazon DocumentDB cluster.",
                    "NoteLambda supports version 4.0 and 5.0 of Amazon DocumentDB only. Lambda doesn't support version 3.6.Also, for event source mappings, Lambda supports instance-based clusters and\n      regional clusters only. Lambda doesn't support\n      \n        elastic clusters or\n      \n        global clusters. This limitation doesn't apply when using Lambda as a client to connect to Amazon DocumentDB. Lambda can connect to \n    all cluster types to perform CRUD operations.",
                    "Lambda processes events from Amazon DocumentDB change streams sequentially in the order in which they arrive.    Because of this, your function can handle only one concurrent invocation from Amazon DocumentDB at a time.    To monitor your function, you can track its concurrency metrics.",
                    "WarningLambda event source mappings process each event at least once, and duplicate processing of records can occur. To avoid potential issues \nrelated to duplicate events, we strongly recommend that you make your function code idempotent. To learn more, see How do I make my Lambda function idempotent \nin the AWS Knowledge Center.",
                    "  1.Example Amazon DocumentDB event",
                    "  2.Prerequisites and permissions",
                    "  3.Configure network security",
                    "  4.Creating an Amazon DocumentDB event source mapping (console)",
                    "  5.Creating an Amazon DocumentDB event source mapping (SDK or CLI)",
                    "  6.Polling and stream starting positions",
                    "  7.Monitoring your Amazon DocumentDB event source",
                    "  8.Tutorial: Using AWS Lambda with Amazon DocumentDB Streams",
                    {
                        "sub_header": "Example Amazon DocumentDB event",
                        "content": [
                            "{    \"eventSourceArn\": \"arn:aws:rds:us-east-1:123456789012:cluster:canaryclusterb2a659a2-qo5tcmqkcl03\",    \"events\": [        {            \"event\": {                \"_id\": {                    \"_data\": \"0163eeb6e7000000090100000009000041e1\"                },                \"clusterTime\": {                    \"$timestamp\": {                        \"t\": 1676588775,                        \"i\": 9                    }                },                \"documentKey\": {                    \"_id\": {                        \"$oid\": \"63eeb6e7d418cd98afb1c1d7\"                    }                },                \"fullDocument\": {                    \"_id\": {                        \"$oid\": \"63eeb6e7d418cd98afb1c1d7\"                    },                    \"anyField\": \"sampleValue\"                },                \"ns\": {                    \"db\": \"test_database\",                    \"coll\": \"test_collection\"                },                \"operationType\": \"insert\"            }        }    ],    \"eventSource\": \"aws:docdb\"}",
                            "For more information about the events in this example and their shapes, see Change Events on the MongoDB      Documentation website."
                        ]
                    },
                    {
                        "sub_header": "Prerequisites and permissions",
                        "content": [
                            "Before you can use Amazon DocumentDB as an event source for your Lambda function, note the following prerequisites. You      must:",
                            {
                                "code_example": "source-access-configurations"
                            },
                            "NoteWhile Lambda functions typically have a maximum timeout limit of 15 minutes,\n      event source mappings for Amazon MSK, self-managed Apache Kafka, Amazon DocumentDB, and Amazon MQ for ActiveMQ and RabbitMQ only support functions with\n      maximum timeout limits of 14 minutes. This constraint ensures that the event source mapping can properly\n      handle function errors and retries."
                        ]
                    },
                    {
                        "sub_header": "Configure network security",
                        "content": [
                            "To give Lambda full access to Amazon DocumentDB through your event source mapping, either your cluster must use a public endpoint             (public IP address), or you must provide access to the Amazon VPC you created the cluster in.",
                            "When you use Amazon DocumentDB with Lambda, create AWS PrivateLink VPC endpoints that provide your function            access to the resources in your Amazon VPC.",
                            "NoteAWS PrivateLink VPC endpoints are required for functions with event source mappings that use the default (on-demand) mode\n                for event pollers. If your event source mapping uses \n                provisioned mode, you don't need to configure AWS PrivateLink VPC endpoints.",
                            "Create an endpoint to provide access to the following resources:",
                            "Alternatively, configure a NAT gateway on each public subnet in the Amazon VPC. For more information,             see Enable internet access for VPC-connected Lambda functions.",
                            "When you create an event source mapping for Amazon DocumentDB, Lambda checks whether Elastic Network Interfaces (ENIs)             are already present for the subnets and security groups configured for your Amazon VPC. If Lambda finds existing ENIs, it             attempts to re-use them. Otherwise, Lambda creates new ENIs to connect to the event source and invoke your function.",
                            "NoteLambda functions always run inside VPCs owned by the Lambda service. Your function's VPC configuration\n                does not affect the event source mapping. Only the networking configuration of the event source's determines \n                how Lambda connects to your event source.",
                            "Configure the security groups for the Amazon VPC containing your cluster. By default,            Amazon DocumentDB uses the following ports: 27017.",
                            {
                                "code_example": "443"
                            },
                            "If your cluster uses authentication, you can also restrict the endpoint policy for the Secrets Manager endpoint.             To call the Secrets Manager API, Lambda uses your function role, not the Lambda service principal.",
                            {
                                "code_example": "{\n      \"Statement\": [\n          {\n              \"Action\": \"secretsmanager:GetSecretValue\",\n              \"Effect\": \"Allow\",\n              \"Principal\": {\n                  \"AWS\": [\n                      \"arn:aws::iam::123456789012:role/my-role\"\n                  ]\n              },\n              \"Resource\": \"arn:aws::secretsmanager:us-west-2:123456789012:secret:my-secret\"\n          }\n      ]\n  }"
                            },
                            "When you use Amazon VPC endpoints, AWS routes your API calls to invoke your function using the endpoint's Elastic Network Interface (ENI).            The Lambda service principal needs to call lambda:InvokeFunction on any roles and functions that use those ENIs.",
                            "By default, Amazon VPC endpoints have open IAM policies that allow broad access to resources. Best practice is to restrict these            policies to perform the needed actions using that endpoint. To ensure that your event source mapping is able to invoke your Lambda            function, the VPC endpoint policy must allow the Lambda service principal to call sts:AssumeRole and            lambda:InvokeFunction. Restricting your VPC endpoint policies to allow only API calls originating within your organization            prevents the event source mapping from functioning properly, so \"Resource\": \"*\" is required in these policies.",
                            "The following example VPC endpoint policies show how to grant the required access to the Lambda service principal for the            AWS STS and Lambda endpoints.",
                            {
                                "code_example": "{\n      \"Statement\": [\n          {\n              \"Action\": \"sts:AssumeRole\",\n              \"Effect\": \"Allow\",\n              \"Principal\": {\n                  \"Service\": [\n                      \"lambda.amazonaws.com\"\n                  ]\n              },\n              \"Resource\": \"*\"\n          }\n      ]\n    }"
                            },
                            {
                                "code_example": "{\n      \"Statement\": [\n          {\n              \"Action\": \"lambda:InvokeFunction\",\n              \"Effect\": \"Allow\",\n              \"Principal\": {\n                  \"Service\": [\n                      \"lambda.amazonaws.com\"\n                  ]\n              },\n              \"Resource\": \"*\"\n          }\n      ]\n  }"
                            }
                        ]
                    },
                    {
                        "sub_header": "Creating an Amazon DocumentDB event source mapping (console)",
                        "content": [
                            "For a Lambda function to read from an Amazon DocumentDB cluster's change stream, create an event source mapping. This section describes how to do this from      the Lambda console. For AWS SDK and AWS CLI instructions, see Creating an Amazon DocumentDB event source mapping (SDK or CLI).",
                            "To create an Amazon DocumentDB event source mapping (console)Open the Functions page of the Lambda console.\nChoose the name of a function.\n\nUnder Function overview, choose Add\n          trigger.\n\nUnder Trigger configuration, in the dropdown list, choose\n          DocumentDB.\n\nConfigure the required options, and then choose Add.\n",
                            "Lambda supports the following options for Amazon DocumentDB event sources:",
                            {
                                "code_example": "current_time - log_retention_duration"
                            }
                        ]
                    },
                    {
                        "sub_header": "Creating an Amazon DocumentDB event source mapping (SDK or CLI)",
                        "content": [
                            "To create or manage an Amazon DocumentDB event source mapping with an AWS SDK, you can use the following API      operations:",
                            "To create the event source mapping with the AWS CLI, use the create-event-source-mapping command. The following example uses this command to map a      function named my-function to an Amazon DocumentDB change stream. The event source is specified by an Amazon      Resource Name (ARN), with a batch size of 500, starting from the timestamp in Unix time. The command also      specifies the Secrets Manager key that Lambda uses to connect to Amazon DocumentDB. Additionally, it includes        document-db-event-source-config parameters that specify the database and the collection to read      from.",
                            "aws lambda create-event-source-mapping --function-name my-function \\    --event-source-arn arn:aws:rds:us-west-2:123456789012:cluster:privatecluster7de2-epzcyvu4pjoy    --batch-size 500 \\    --starting-position AT_TIMESTAMP \\    --starting-position-timestamp 1541139109 \\    --source-access-configurations '[{\"Type\":\"BASIC_AUTH\",\"URI\":\"arn:aws:secretsmanager:us-east-1:123456789012:secret:DocDBSecret-BAtjxi\"}]' \\    --document-db-event-source-config '{\"DatabaseName\":\"test_database\", \"CollectionName\": \"test_collection\"}' \\",
                            "You should see output that looks like this:",
                            "{    \"UUID\": \"2b733gdc-8ac3-cdf5-af3a-1827b3b11284\",    \"BatchSize\": 500,    \"DocumentDBEventSourceConfig\": {        \"CollectionName\": \"test_collection\",        \"DatabaseName\": \"test_database\",        \"FullDocument\": \"Default\"    },    \"MaximumBatchingWindowInSeconds\": 0,    \"EventSourceArn\": \"arn:aws:rds:us-west-2:123456789012:cluster:privatecluster7de2-epzcyvu4pjoy\",    \"FunctionArn\": \"arn:aws:lambda:us-west-2:123456789012:function:my-function\",    \"LastModified\": 1541348195.412,    \"LastProcessingResult\": \"No records processed\",    \"State\": \"Creating\",    \"StateTransitionReason\": \"User action\"}",
                            "After creation, you can use the update-event-source-mapping command to update the settings for your Amazon DocumentDB event      source. The following example updates the batch size to 1,000 and the batch window to 10 seconds. For this      command, you need the UUID of your event source mapping, which you can retrieve using the        list-event-source-mapping command or the Lambda console.",
                            "aws lambda update-event-source-mapping --function-name my-function \\    --uuid f89f8514-cdd9-4602-9e1f-01a5b77d449b \\    --batch-size 1000 \\    --batch-window 10",
                            "You should see this output that looks like this:",
                            "{    \"UUID\": \"2b733gdc-8ac3-cdf5-af3a-1827b3b11284\",    \"BatchSize\": 500,    \"DocumentDBEventSourceConfig\": {        \"CollectionName\": \"test_collection\",        \"DatabaseName\": \"test_database\",        \"FullDocument\": \"Default\"    },    \"MaximumBatchingWindowInSeconds\": 0,    \"EventSourceArn\": \"arn:aws:rds:us-west-2:123456789012:cluster:privatecluster7de2-epzcyvu4pjoy\",    \"FunctionArn\": \"arn:aws:lambda:us-west-2:123456789012:function:my-function\",    \"LastModified\": 1541359182.919,    \"LastProcessingResult\": \"OK\",    \"State\": \"Updating\",    \"StateTransitionReason\": \"User action\"}",
                            "Lambda updates settings asynchronously, so you may not see these changes in the output until the process      completes. To view the current settings of your event source mapping, use the get-event-source-mapping command.",
                            "aws lambda get-event-source-mapping --uuid f89f8514-cdd9-4602-9e1f-01a5b77d449b",
                            "You should see this output that looks like this:",
                            "{    \"UUID\": \"2b733gdc-8ac3-cdf5-af3a-1827b3b11284\",    \"DocumentDBEventSourceConfig\": {        \"CollectionName\": \"test_collection\",        \"DatabaseName\": \"test_database\",        \"FullDocument\": \"Default\"    },    \"BatchSize\": 1000,    \"MaximumBatchingWindowInSeconds\": 10,    \"EventSourceArn\": \"arn:aws:rds:us-west-2:123456789012:cluster:privatecluster7de2-epzcyvu4pjoy\",    \"FunctionArn\": \"arn:aws:lambda:us-west-2:123456789012:function:my-function\",    \"LastModified\": 1541359182.919,    \"LastProcessingResult\": \"OK\",    \"State\": \"Enabled\",    \"StateTransitionReason\": \"User action\"}",
                            "To delete your Amazon DocumentDB event source mapping, use the delete-event-source-mapping command.",
                            "aws lambda delete-event-source-mapping \\    --uuid 2b733gdc-8ac3-cdf5-af3a-1827b3b11284"
                        ]
                    },
                    {
                        "sub_header": "Polling and stream starting positions",
                        "content": [
                            "Be aware that stream polling during event source mapping creation and updates is eventually consistent.",
                            "This behavior means that if you specify LATEST as the starting position for the stream, the event source mapping could     miss events during creation or updates. To ensure that no events are missed, specify the stream starting position as TRIM_HORIZON     or AT_TIMESTAMP."
                        ]
                    },
                    {
                        "sub_header": "Monitoring your Amazon DocumentDB event source",
                        "content": [
                            "To help you monitor your Amazon DocumentDB event source, Lambda emits the IteratorAge metric when your      function finishes processing a batch of records. Iterator age is the difference between the      timestamp of the most recent event and the current timestamp. Essentially, the IteratorAge metric      indicates how old the last processed record in the batch is. If your function is currently processing new events,      then you can use the iterator age to estimate the latency between when a record is added and when your function      processes it. An increasing trend in IteratorAge can indicate issues with your function.      For more information, see Using CloudWatch metrics with Lambda.",
                            "Amazon DocumentDB change streams aren't optimized to handle large time gaps between events. If your Amazon DocumentDB event source doesn't      receive any events for an extended period of time, Lambda may disable the event source mapping. The length of      this time period can vary from a few weeks to a few months depending on cluster size and other workloads.",
                            "Lambda supports payloads of up to 6 MB. However, Amazon DocumentDB change stream events can be up to 16 MB in size. If      your change stream tries to send Lambda a change stream event larger than 6 MB, then Lambda drops the message and      emits the OversizedRecordCount metric. Lambda emits all metrics on a best-effort basis."
                        ]
                    }
                ]
            },
            {
                "title": "DynamoDB",
                "href": "https://docs.aws.amazon.com/lambda/latest/dg/with-ddb.html",
                "contents": [
                    {
                        "title": "Create mapping",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/services-dynamodb-eventsourcemapping.html",
                        "sections": [
                            "",
                            "Create an event source mapping to tell Lambda to send records from your stream to a Lambda function. You can        create multiple event source mappings to process the same data with multiple Lambda functions, or to process items        from multiple streams with a single function.",
                            "To configure your function to read from DynamoDB Streams, attach the AWSLambdaDynamoDBExecutionRole AWS managed policy to your execution role and then create a DynamoDB        trigger.",
                            {
                                "code_example": "AWSLambdaDynamoDBExecutionRole"
                            },
                            "Lambda supports the following options for DynamoDB event sources:",
                            "  1.DynamoDB table  – The  to read records from.",
                            "  2.Batch size  – The number of records to send to the function in each batch, up\n                to 10,000. Lambda passes all of the records in the batch to the function in a single call, as long as the total\n                size of the events doesn't exceed the payload limit for\n                synchronous invocation (6 MB).",
                            "  3.Batch window  – Specify the maximum amount of time to gather records before\n          invoking the function, in seconds.",
                            "  4.Starting position  – Process only new records, or all existing records.",
                            "  5.Latest  – Process new records that are added to the stream.",
                            "  6.Trim horizon  – Process all records in the stream.",
                            "  7.On-failure destination  – A standard SQS queue or standard SNS topic\n  for records that can't be processed. When Lambda discards a batch of records that's too old or has exhausted\n  all retries, Lambda sends details about the batch to the queue or topic.",
                            "  8.Retry attempts  – The maximum number of times that\n  Lambda retries when the function returns an error. This doesn't apply to service errors or throttles where the\n  batch didn't reach the function.",
                            "  9.Maximum age of record  – The maximum age of a record that\n  Lambda sends to your function.",
                            "  10.Split batch on error  – When the function returns an error,\n  split the batch into two before retrying. Your original batch size setting remains unchanged.",
                            "  11.Concurrent batches per shard  – Concurrently process multiple batches from the same shard.",
                            "  12.Enabled  – Set to true to enable the event source mapping. Set to false to stop\n                processing records. Lambda keeps track of the last record processed and resumes processing from that point when\n                the mapping is reenabled.",
                            "NoteYou are not charged for GetRecords API calls invoked by Lambda as part of DynamoDB triggers.",
                            "To manage the event source configuration later, choose the trigger in the designer."
                        ]
                    },
                    {
                        "title": "Batch item failures",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/services-ddb-batchfailurereporting.html",
                        "sections": [
                            "",
                            "When consuming and processing streaming data from an event source, by default Lambda checkpoints to the highest    sequence number of a batch only when the batch is a complete success. Lambda treats all other results as a complete    failure and retries processing the batch up to the retry limit. To allow for partial successes while processing    batches from a stream, turn on ReportBatchItemFailures. Allowing partial successes can help to reduce    the number of retries on a record, though it doesn’t entirely prevent the possibility of retries in a successful record.",
                            "To turn on ReportBatchItemFailures, include the enum value    ReportBatchItemFailures in the FunctionResponseTypes list. This list indicates    which response types are enabled for your function. You can configure this list when you create or update an event source mapping.",
                            {
                                "sub_header": "Report syntax",
                                "content": [
                                    "When configuring reporting on batch item failures, the StreamsEventResponse class is returned with a      list of batch item failures. You can use a StreamsEventResponse object to return the sequence number      of the first failed record in the batch. You can also create your own custom class using the correct response      syntax. The following JSON structure shows the required response syntax:",
                                    "{   \"batchItemFailures\": [         {            \"itemIdentifier\": \"<SequenceNumber>\"        }    ]}",
                                    {
                                        "code_example": "batchItemFailures"
                                    }
                                ]
                            },
                            {
                                "sub_header": "Success and failure conditions",
                                "content": [
                                    "Lambda treats a batch as a complete success if you return any of the following:",
                                    {
                                        "code_example": "batchItemFailure"
                                    },
                                    "Lambda treats a batch as a complete failure if you return any of the following:",
                                    {
                                        "code_example": "itemIdentifier"
                                    },
                                    "Lambda retries failures based on your retry strategy."
                                ]
                            },
                            {
                                "sub_header": "Bisecting a batch",
                                "content": [
                                    "If your invocation fails and BisectBatchOnFunctionError is turned on, the batch is bisected      regardless of your ReportBatchItemFailures setting.",
                                    "When a partial batch success response is received and both BisectBatchOnFunctionError and        ReportBatchItemFailures are turned on, the batch is bisected at the returned sequence number and      Lambda retries only the remaining records.",
                                    "Here are some examples of function code that return the list of failed message IDs in the batch:",
                                    ".NETAWS SDK for .NETNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Reporting DynamoDB batch item failures with Lambda using .NET.// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.// SPDX-License-Identifier: Apache-2.0using System.Text.Json;using System.Text;using Amazon.Lambda.Core;using Amazon.Lambda.DynamoDBEvents;// Assembly attribute to enable the Lambda function's JSON input to be converted into a .NET class.[assembly: LambdaSerializer(typeof(Amazon.Lambda.Serialization.SystemTextJson.DefaultLambdaJsonSerializer))]namespace AWSLambda_DDB;public class Function{    public StreamsEventResponse FunctionHandler(DynamoDBEvent dynamoEvent, ILambdaContext context)    {        context.Logger.LogInformation($\"Beginning to process {dynamoEvent.Records.Count} records...\");        List<StreamsEventResponse.BatchItemFailure> batchItemFailures = new List<StreamsEventResponse.BatchItemFailure>();        StreamsEventResponse streamsEventResponse = new StreamsEventResponse();        foreach (var record in dynamoEvent.Records)        {            try            {                var sequenceNumber = record.Dynamodb.SequenceNumber;                context.Logger.LogInformation(sequenceNumber);            }            catch (Exception ex)            {                context.Logger.LogError(ex.Message);                batchItemFailures.Add(new StreamsEventResponse.BatchItemFailure() { ItemIdentifier = record.Dynamodb.SequenceNumber });            }        }        if (batchItemFailures.Count > 0)        {            streamsEventResponse.BatchItemFailures = batchItemFailures;        }        context.Logger.LogInformation(\"Stream processing complete.\");        return streamsEventResponse;    }}GoSDK for Go V2Note        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Reporting DynamoDB batch item failures with Lambda using Go.// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.// SPDX-License-Identifier: Apache-2.0package mainimport (\t\"context\"\t\"github.com/aws/aws-lambda-go/events\"\t\"github.com/aws/aws-lambda-go/lambda\")type BatchItemFailure struct {\tItemIdentifier string `json:\"ItemIdentifier\"`}type BatchResult struct {\tBatchItemFailures []BatchItemFailure `json:\"BatchItemFailures\"`}func HandleRequest(ctx context.Context, event events.DynamoDBEvent) (*BatchResult, error) {\tvar batchItemFailures []BatchItemFailure\tcurRecordSequenceNumber := \"\"\tfor _, record := range event.Records {\t\t// Process your record\t\tcurRecordSequenceNumber = record.Change.SequenceNumber\t}\tif curRecordSequenceNumber != \"\" {\t\tbatchItemFailures = append(batchItemFailures, BatchItemFailure{ItemIdentifier: curRecordSequenceNumber})\t}\t\tbatchResult := BatchResult{\t\tBatchItemFailures: batchItemFailures,\t}\treturn &batchResult, nil}func main() {\tlambda.Start(HandleRequest)}JavaSDK for Java 2.xNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Reporting DynamoDB batch item failures with Lambda using Java.// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.// SPDX-License-Identifier: Apache-2.0import com.amazonaws.services.lambda.runtime.Context;import com.amazonaws.services.lambda.runtime.RequestHandler;import com.amazonaws.services.lambda.runtime.events.DynamodbEvent;import com.amazonaws.services.lambda.runtime.events.StreamsEventResponse;import com.amazonaws.services.lambda.runtime.events.models.dynamodb.StreamRecord;import java.io.Serializable;import java.util.ArrayList;import java.util.List;public class ProcessDynamodbRecords implements RequestHandler<DynamodbEvent, Serializable> {    @Override    public StreamsEventResponse handleRequest(DynamodbEvent input, Context context) {        List<StreamsEventResponse.BatchItemFailure> batchItemFailures = new ArrayList<>();        String curRecordSequenceNumber = \"\";        for (DynamodbEvent.DynamodbStreamRecord dynamodbStreamRecord : input.getRecords()) {          try {                //Process your record                StreamRecord dynamodbRecord = dynamodbStreamRecord.getDynamodb();                curRecordSequenceNumber = dynamodbRecord.getSequenceNumber();                            } catch (Exception e) {                /* Since we are working with streams, we can return the failed item immediately.                   Lambda will immediately begin to retry processing from this failed item onwards. */                batchItemFailures.add(new StreamsEventResponse.BatchItemFailure(curRecordSequenceNumber));                return new StreamsEventResponse(batchItemFailures);            }        }              return new StreamsEventResponse();       }}JavaScriptSDK for JavaScript (v3)Note        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Reporting DynamoDB batch item failures with Lambda using JavaScript.export const handler = async (event) => {  const records = event.Records;  let curRecordSequenceNumber = \"\";  for (const record of records) {    try {      // Process your record      curRecordSequenceNumber = record.dynamodb.SequenceNumber;    } catch (e) {      // Return failed record's sequence number      return { batchItemFailures: [{ itemIdentifier: curRecordSequenceNumber }] };    }  }  return { batchItemFailures: [] };};Reporting DynamoDB batch item failures with Lambda using TypeScript.import {  DynamoDBBatchResponse,  DynamoDBBatchItemFailure,  DynamoDBStreamEvent,} from \"aws-lambda\";export const handler = async (  event: DynamoDBStreamEvent): Promise<DynamoDBBatchResponse> => {  const batchItemFailures: DynamoDBBatchItemFailure[] = [];  let curRecordSequenceNumber;  for (const record of event.Records) {    curRecordSequenceNumber = record.dynamodb?.SequenceNumber;    if (curRecordSequenceNumber) {      batchItemFailures.push({        itemIdentifier: curRecordSequenceNumber,      });    }  }  return { batchItemFailures: batchItemFailures };};PHPSDK for PHPNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Reporting DynamoDB batch item failures with Lambda using PHP.<?php# using bref/bref and bref/logger for simplicityuse Bref\\Context\\Context;use Bref\\Event\\DynamoDb\\DynamoDbEvent;use Bref\\Event\\Handler as StdHandler;use Bref\\Logger\\StderrLogger;require __DIR__ . '/vendor/autoload.php';class Handler implements StdHandler{    private StderrLogger $logger;    public function __construct(StderrLogger $logger)    {        $this->logger = $logger;    }    /**     * @throws JsonException     * @throws \\Bref\\Event\\InvalidLambdaEvent     */    public function handle(mixed $event, Context $context): array    {        $dynamoDbEvent = new DynamoDbEvent($event);        $this->logger->info(\"Processing records\");        $records = $dynamoDbEvent->getRecords();        $failedRecords = [];        foreach ($records as $record) {            try {                $data = $record->getData();                $this->logger->info(json_encode($data));                // TODO: Do interesting work based on the new data            } catch (Exception $e) {                $this->logger->error($e->getMessage());                // failed processing the record                $failedRecords[] = $record->getSequenceNumber();            }        }        $totalRecords = count($records);        $this->logger->info(\"Successfully processed $totalRecords records\");        // change format for the response        $failures = array_map(            fn(string $sequenceNumber) => ['itemIdentifier' => $sequenceNumber],            $failedRecords        );        return [            'batchItemFailures' => $failures        ];    }}$logger = new StderrLogger();return new Handler($logger);PythonSDK for Python (Boto3)Note        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Reporting DynamoDB batch item failures with Lambda using Python.# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.# SPDX-License-Identifier: Apache-2.0def handler(event, context):    records = event.get(\"Records\")    curRecordSequenceNumber = \"\"        for record in records:        try:            # Process your record            curRecordSequenceNumber = record[\"dynamodb\"][\"SequenceNumber\"]        except Exception as e:            # Return failed record's sequence number            return {\"batchItemFailures\":[{\"itemIdentifier\": curRecordSequenceNumber}]}    return {\"batchItemFailures\":[]}RubySDK for RubyNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Reporting DynamoDB batch item failures with Lambda using Ruby.def lambda_handler(event:, context:)    records = event[\"Records\"]    cur_record_sequence_number = \"\"      records.each do |record|      begin        # Process your record        cur_record_sequence_number = record[\"dynamodb\"][\"SequenceNumber\"]      rescue StandardError => e        # Return failed record's sequence number        return {\"batchItemFailures\" => [{\"itemIdentifier\" => cur_record_sequence_number}]}      end    end      {\"batchItemFailures\" => []}  endRustSDK for RustNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Reporting DynamoDB batch item failures with Lambda using Rust.use aws_lambda_events::{    event::dynamodb::{Event, EventRecord, StreamRecord},    streams::{DynamoDbBatchItemFailure, DynamoDbEventResponse},};use lambda_runtime::{run, service_fn, Error, LambdaEvent};/// Process the stream recordfn process_record(record: &EventRecord) -> Result<(), Error> {    let stream_record: &StreamRecord = &record.change;    // process your stream record here...    tracing::info!(\"Data: {:?}\", stream_record);    Ok(())}/// Main Lambda handler here...async fn function_handler(event: LambdaEvent<Event>) -> Result<DynamoDbEventResponse, Error> {    let mut response = DynamoDbEventResponse {        batch_item_failures: vec![],    };    let records = &event.payload.records;    if records.is_empty() {        tracing::info!(\"No records found. Exiting.\");        return Ok(response);    }    for record in records {        tracing::info!(\"EventId: {}\", record.event_id);        // Couldn't find a sequence number        if record.change.sequence_number.is_none() {            response.batch_item_failures.push(DynamoDbBatchItemFailure {                item_identifier: Some(\"\".to_string()),            });            return Ok(response);        }        // Process your record here...        if process_record(record).is_err() {            response.batch_item_failures.push(DynamoDbBatchItemFailure {                item_identifier: record.change.sequence_number.clone(),            });            /* Since we are working with streams, we can return the failed item immediately.            Lambda will immediately begin to retry processing from this failed item onwards. */            return Ok(response);        }    }    tracing::info!(\"Successfully processed {} record(s)\", records.len());    Ok(response)}#[tokio::main]async fn main() -> Result<(), Error> {    tracing_subscriber::fmt()        .with_max_level(tracing::Level::INFO)        // disable printing the name of the module in every log line.        .with_target(false)        // disabling time is handy because CloudWatch will add the ingestion time.        .without_time()        .init();    run(service_fn(function_handler)).await}anchoranchoranchoranchoranchoranchoranchoranchor.NETGoJavaJavaScriptPHPPythonRubyRustAWS SDK for .NETNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Reporting DynamoDB batch item failures with Lambda using .NET.// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.// SPDX-License-Identifier: Apache-2.0using System.Text.Json;using System.Text;using Amazon.Lambda.Core;using Amazon.Lambda.DynamoDBEvents;// Assembly attribute to enable the Lambda function's JSON input to be converted into a .NET class.[assembly: LambdaSerializer(typeof(Amazon.Lambda.Serialization.SystemTextJson.DefaultLambdaJsonSerializer))]namespace AWSLambda_DDB;public class Function{    public StreamsEventResponse FunctionHandler(DynamoDBEvent dynamoEvent, ILambdaContext context)    {        context.Logger.LogInformation($\"Beginning to process {dynamoEvent.Records.Count} records...\");        List<StreamsEventResponse.BatchItemFailure> batchItemFailures = new List<StreamsEventResponse.BatchItemFailure>();        StreamsEventResponse streamsEventResponse = new StreamsEventResponse();        foreach (var record in dynamoEvent.Records)        {            try            {                var sequenceNumber = record.Dynamodb.SequenceNumber;                context.Logger.LogInformation(sequenceNumber);            }            catch (Exception ex)            {                context.Logger.LogError(ex.Message);                batchItemFailures.Add(new StreamsEventResponse.BatchItemFailure() { ItemIdentifier = record.Dynamodb.SequenceNumber });            }        }        if (batchItemFailures.Count > 0)        {            streamsEventResponse.BatchItemFailures = batchItemFailures;        }        context.Logger.LogInformation(\"Stream processing complete.\");        return streamsEventResponse;    }}"
                                ]
                            }
                        ]
                    },
                    {
                        "title": "Error handling",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/services-dynamodb-errors.html",
                        "sections": [
                            "",
                            "Error handling for DynamoDB event source mappings depends on whether the error occurs before the function is invoked or during function invocation:",
                            "  1.Before invocation:  If a Lambda event source mapping is unable to invoke the function due to throttling or other issues, it retries until the records expire or exceed the maximum age configured on the event source mapping (MaximumRecordAgeInSeconds).",
                            "  2.During invocation:  If the function is invoked but returns an error, Lambda retries until the records expire, exceed the maximum age (MaximumRecordAgeInSeconds), or reach the configured retry quota (MaximumRetryAttempts). For function errors, you can also configure BisectBatchOnFunctionError, which splits a failed batch into two smaller batches, isolating bad records and avoiding timeouts. Splitting batches doesn't consume the retry quota.",
                            "If the error handling measures fail, Lambda discards the records and continues processing  batches from the stream. With the default settings, this means that a bad record can block processing on the affected  shard for up to one day. To avoid this, configure your function's event source mapping with a reasonable  number of retries and a maximum record age that fits your use case.",
                            {
                                "sub_header": "Configuring destinations for failed invocations",
                                "content": [
                                    "To retain records of failed event source mapping invocations, add a destination to your function's event source mapping. Each record sent to the destination is a JSON document containing metadata about the failed invocation. For Amazon S3 destinations, Lambda also sends the entire invocation record along with the metadata. You can configure any Amazon SNS topic, Amazon SQS queue, or S3 bucket as a destination.",
                                    "With Amazon S3 destinations, you can use the Amazon S3 Event Notifications feature to receive notifications when objects are uploaded to your destination S3 bucket. You can also configure S3 Event Notifications to invoke another Lambda function to perform automated processing on failed batches.",
                                    "Your execution role must have permissions for the destination:",
                                    "  1.For SQS destinations:  sqs:SendMessage",
                                    "  2.For SNS destinations:  sns:Publish",
                                    "  3.For S3 bucket destinations:   s3:PutObject and s3:ListBucket",
                                    "If you've enabled encryption with your own KMS key for an S3 destination, your function's execution role must also have permission to call             kms:GenerateDataKey.            If the KMS key and S3 bucket destination are in a different account from your Lambda function            and execution role, configure the KMS key to trust the execution role to allow            kms:GenerateDataKey.",
                                    "To configure an on-failure destination using the console, follow these steps:",
                                    "Open the Functions page of the Lambda console.\nChoose a function.\n\nUnder Function overview, choose Add destination.\n\nFor Source, choose Event source mapping invocation.\n\nFor Event source mapping, choose an event source that's configured\n              for this function.\n\nFor Condition, select On failure. For event\n              source mapping invocations, this is the only accepted condition.\n\nFor Destination type, choose the destination type that Lambda sends\n              invocation records to.\n\nFor Destination, choose a resource.\n\nChoose Save.\n",
                                    "You can also configure an on-failure destination using the AWS Command Line Interface (AWS CLI). For example, the following          create-event-source-mapping command adds an event source mapping with an SQS on-failure destination to          MyFunction:",
                                    "aws lambda create-event-source-mapping \\--function-name \"MyFunction\" \\--event-source-arn arn:aws:dynamodb:us-east-2:123456789012:table/my-table/stream/2024-06-10T19:26:16.525 \\--destination-config '{\"OnFailure\": {\"Destination\": \"arn:aws:sqs:us-east-1:123456789012:dest-queue\"}}'",
                                    "The following update-event-source-mapping command updates an event source mapping to send failed invocation records to an SNS destination after two retry attempts, or if the records are more than an hour old.",
                                    "aws lambda update-event-source-mapping \\--uuid f89f8514-cdd9-4602-9e1f-01a5b77d449b \\--maximum-retry-attempts 2 \\--maximum-record-age-in-seconds 3600 \\--destination-config '{\"OnFailure\": {\"Destination\": \"arn:aws:sns:us-east-1:123456789012:dest-topic\"}}'",
                                    "Updated settings are applied asynchronously and aren't reflected in the output until the process completes. Use    the get-event-source-mapping command to view the current status.",
                                    "To remove a destination, supply an empty string as the argument to the          destination-config parameter:",
                                    "aws lambda update-event-source-mapping \\--uuid f89f8514-cdd9-4602-9e1f-01a5b77d449b \\--destination-config '{\"OnFailure\": {\"Destination\": \"\"}}'",
                                    {
                                        "sub_header": "Security best practices for Amazon S3 destinations",
                                        "content": [
                                            "Deleting an S3 bucket that's configured as a destination without removing the destination from your function's configuration can create a security risk. If another       user knows your destination bucket's name, they can recreate the bucket in their AWS account. Records of failed invocations will be sent to their bucket, potentially       exposing data from your function.",
                                            {
                                                "code_example": "s3:PutObject"
                                            },
                                            "The following example shows an IAM policy that limits your function's s3:PutObject permissions to buckets in your account. This policy also gives Lambda        the s3:ListBucket permission it needs to use an S3 bucket as a destination.",
                                            "{    \"Version\": \"2012-10-17\",    \"Statement\": [        {            \"Sid\": \"S3BucketResourceAccountWrite\",            \"Effect\": \"Allow\",            \"Action\": [                \"s3:PutObject\",                \"s3:ListBucket\"            ],            \"Resource\": \"arn:aws:s3:::*/*\",            \"Condition\": {                \"StringEquals\": {                    \"s3:ResourceAccount\": \"111122223333\"                }            }        }    ]}",
                                            "To add a permissions policy to your funcion's execution role using the AWS Management Console or AWS CLI, refer to the instructions in the following procedures:",
                                            "ConsoleTo add a permissions policy to a function's execution role (console)Open the Functions page of the Lambda console.Select the Lambda function whose execution role you want to modify.In the Configuration tab, select Permissions.In the Execution role tab, select your function's Role name to open the role's IAM console page.Add a permissions policy to the role by doing the following:In the Permissions policies pane, choose Add permissions and select Create inline policy.In Policy editor, select JSON.Paste the policy you want to add into the editor (replacing the existing JSON), and then choose Next.Under Policy details, enter a Policy name.Choose Create policy.AWS CLITo add a permissions policy to a function's execution role (CLI)Create a JSON policy document with the required permissions and save it in a local directory.Use the IAM put-role-policy CLI command to add the permissions to your function's execution role. Run the following command from the                 directory you saved your JSON policy document in and replace the role name, policy name, and policy document with your own values.aws iam put-role-policy \\--role-name my_lambda_role \\--policy-name LambdaS3DestinationPolicy \\--policy-document file://my_policy.jsonanchoranchorConsoleAWS CLITo add a permissions policy to a function's execution role (console)Open the Functions page of the Lambda console.Select the Lambda function whose execution role you want to modify.In the Configuration tab, select Permissions.In the Execution role tab, select your function's Role name to open the role's IAM console page.Add a permissions policy to the role by doing the following:In the Permissions policies pane, choose Add permissions and select Create inline policy.In Policy editor, select JSON.Paste the policy you want to add into the editor (replacing the existing JSON), and then choose Next.Under Policy details, enter a Policy name.Choose Create policy."
                                        ]
                                    },
                                    {
                                        "sub_header": "Example Amazon SNS and Amazon SQS invocation record",
                                        "content": [
                                            "The following example shows an invocation record Lambda sends to an SQS or SNS destination for a DynamoDB stream.",
                                            "{    \"requestContext\": {        \"requestId\": \"316aa6d0-8154-xmpl-9af7-85d5f4a6bc81\",        \"functionArn\": \"arn:aws:lambda:us-east-2:123456789012:function:myfunction\",        \"condition\": \"RetryAttemptsExhausted\",        \"approximateInvokeCount\": 1    },    \"responseContext\": {        \"statusCode\": 200,        \"executedVersion\": \"$LATEST\",        \"functionError\": \"Unhandled\"    },    \"version\": \"1.0\",    \"timestamp\": \"2019-11-14T00:13:49.717Z\",    \"DDBStreamBatchInfo\": {        \"shardId\": \"shardId-00000001573689847184-864758bb\",        \"startSequenceNumber\": \"800000000003126276362\",        \"endSequenceNumber\": \"800000000003126276362\",        \"approximateArrivalOfFirstRecord\": \"2019-11-14T00:13:19Z\",        \"approximateArrivalOfLastRecord\": \"2019-11-14T00:13:19Z\",        \"batchSize\": 1,        \"streamArn\": \"arn:aws:dynamodb:us-east-2:123456789012:table/mytable/stream/2019-11-14T00:04:06.388\"    }}",
                                            "You can use this information to retrieve the affected records from the stream for  troubleshooting. The actual records aren't included, so you must process this record and retrieve them from the  stream before they expire and are lost."
                                        ]
                                    },
                                    {
                                        "sub_header": "Example Amazon S3 invocation record",
                                        "content": [
                                            "The following example shows an invocation record Lambda sends to an S3 bucket for a DynamoDB stream. In addition to all of the fields from the previous example for SQS and SNS destinations, the payload field       contains the original invocation record as an escaped JSON string.",
                                            "{    \"requestContext\": {        \"requestId\": \"316aa6d0-8154-xmpl-9af7-85d5f4a6bc81\",        \"functionArn\": \"arn:aws:lambda:us-east-2:123456789012:function:myfunction\",        \"condition\": \"RetryAttemptsExhausted\",        \"approximateInvokeCount\": 1    },    \"responseContext\": {        \"statusCode\": 200,        \"executedVersion\": \"$LATEST\",        \"functionError\": \"Unhandled\"    },    \"version\": \"1.0\",    \"timestamp\": \"2019-11-14T00:13:49.717Z\",    \"DDBStreamBatchInfo\": {        \"shardId\": \"shardId-00000001573689847184-864758bb\",        \"startSequenceNumber\": \"800000000003126276362\",        \"endSequenceNumber\": \"800000000003126276362\",        \"approximateArrivalOfFirstRecord\": \"2019-11-14T00:13:19Z\",        \"approximateArrivalOfLastRecord\": \"2019-11-14T00:13:19Z\",        \"batchSize\": 1,        \"streamArn\": \"arn:aws:dynamodb:us-east-2:123456789012:table/mytable/stream/2019-11-14T00:04:06.388\"    },    \"payload\": \"<Whole Event>\" // Only available in S3}",
                                            "The S3 object containing the invocation record uses the following naming convention:",
                                            "aws/lambda/<ESM-UUID>/<shardID>/YYYY/MM/DD/YYYY-MM-DDTHH.MM.SS-<Random UUID>"
                                        ]
                                    }
                                ]
                            }
                        ]
                    },
                    {
                        "title": "Stateful processing",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/services-ddb-windows.html",
                        "sections": [
                            "",
                            "Lambda functions can run continuous stream processing applications. A stream represents unbounded data that flows    continuously through your application. To analyze information from this continuously updating input, you can bound    the included records using a window defined in terms of time.",
                            "Tumbling windows are distinct time windows that open and close at regular intervals. By default, Lambda invocations    are stateless—you cannot use them for processing data across multiple continuous invocations without an external database.    However, with tumbling windows, you can maintain your state across invocations. This state contains the aggregate result    of the messages previously processed for the current window. Your state can be a maximum of 1 MB per shard. If it exceeds    that size, Lambda terminates the window early.",
                            "Each record in a stream belongs to a specific window. Lambda will process each record at least once, but doesn't guarantee that each record will be processed only once. In rare cases, such as error handling, some records might be processed more than once. Records are always processed in order the first time. If records are processed more than once, they might be processed out of order.",
                            {
                                "sub_header": "Aggregation and processing",
                                "content": [
                                    "Your user managed function is invoked both for aggregation and for processing the final results of that      aggregation. Lambda aggregates all records received in the window. You can receive these records in multiple      batches, each as a separate invocation. Each invocation receives a state. Thus, when using tumbling windows,      your Lambda function response must contain a state property. If the response does not contain a      state property, Lambda considers this a failed invocation. To satisfy this condition, your function      can return a TimeWindowEventResponse object, which has the following JSON shape:",
                                    {
                                        "code_example": "TimeWindowEventResponse"
                                    },
                                    {
                                        "code_example": "Map<String, String>"
                                    },
                                    "At the end of the window, the flag isFinalInvokeForWindow is set to true to indicate      that this is the final state and that it’s ready for processing. After processing, the window completes and your      final invocation completes, and then the state is dropped.",
                                    "At the end of your window, Lambda uses final processing for actions on the aggregation results. Your final      processing is synchronously invoked. After successful invocation, your function checkpoints the sequence number      and stream processing continues. If invocation is unsuccessful, your Lambda function suspends further processing      until a successful invocation.",
                                    {
                                        "code_example": "\n{\n   \"Records\":[\n      {\n         \"eventID\":\"1\",\n         \"eventName\":\"INSERT\",\n         \"eventVersion\":\"1.0\",\n         \"eventSource\":\"aws:dynamodb\",\n         \"awsRegion\":\"us-east-1\",\n         \"dynamodb\":{\n            \"Keys\":{\n               \"Id\":{\n                  \"N\":\"101\"\n               }\n            },\n            \"NewImage\":{\n               \"Message\":{\n                  \"S\":\"New item!\"\n               },\n               \"Id\":{\n                  \"N\":\"101\"\n               }\n            },\n            \"SequenceNumber\":\"111\",\n            \"SizeBytes\":26,\n            \"StreamViewType\":\"NEW_AND_OLD_IMAGES\"\n         },\n         \"eventSourceARN\":\"stream-ARN\"\n      },\n      {\n         \"eventID\":\"2\",\n         \"eventName\":\"MODIFY\",\n         \"eventVersion\":\"1.0\",\n         \"eventSource\":\"aws:dynamodb\",\n         \"awsRegion\":\"us-east-1\",\n         \"dynamodb\":{\n            \"Keys\":{\n               \"Id\":{\n                  \"N\":\"101\"\n               }\n            },\n            \"NewImage\":{\n               \"Message\":{\n                  \"S\":\"This item has changed\"\n               },\n               \"Id\":{\n                  \"N\":\"101\"\n               }\n            },\n            \"OldImage\":{\n               \"Message\":{\n                  \"S\":\"New item!\"\n               },\n               \"Id\":{\n                  \"N\":\"101\"\n               }\n            },\n            \"SequenceNumber\":\"222\",\n            \"SizeBytes\":59,\n            \"StreamViewType\":\"NEW_AND_OLD_IMAGES\"\n         },\n         \"eventSourceARN\":\"stream-ARN\"\n      },\n      {\n         \"eventID\":\"3\",\n         \"eventName\":\"REMOVE\",\n         \"eventVersion\":\"1.0\",\n         \"eventSource\":\"aws:dynamodb\",\n         \"awsRegion\":\"us-east-1\",\n         \"dynamodb\":{\n            \"Keys\":{\n               \"Id\":{\n                  \"N\":\"101\"\n               }\n            },\n            \"OldImage\":{\n               \"Message\":{\n                  \"S\":\"This item has changed\"\n               },\n               \"Id\":{\n                  \"N\":\"101\"\n               }\n            },\n            \"SequenceNumber\":\"333\",\n            \"SizeBytes\":38,\n            \"StreamViewType\":\"NEW_AND_OLD_IMAGES\"\n         },\n         \"eventSourceARN\":\"stream-ARN\"\n      }\n   ],\n    \"window\": {\n        \"start\": \"2020-07-30T17:00:00Z\",\n        \"end\": \"2020-07-30T17:05:00Z\"\n    },\n    \"state\": {\n        \"1\": \"state1\"\n    },\n    \"shardId\": \"shard123456789\",\n    \"eventSourceARN\": \"stream-ARN\",\n    \"isFinalInvokeForWindow\": false,\n    \"isWindowTerminatedEarly\": false\n}\n"
                                    }
                                ]
                            },
                            {
                                "sub_header": "Configuration",
                                "content": [
                                    "You can configure tumbling windows when you create or update an event source mapping. To configure a tumbling window, specify the window in seconds (TumblingWindowInSeconds). The following        example AWS Command Line Interface (AWS CLI) command creates a streaming event source mapping that has a tumbling window of 120        seconds. The Lambda function defined for aggregation and processing is named        tumbling-window-example-function.",
                                    "aws lambda create-event-source-mapping \\--event-source-arn arn:aws:dynamodb:us-east-2:123456789012:table/my-table/stream/2024-06-10T19:26:16.525 \\--function-name tumbling-window-example-function \\--starting-position TRIM_HORIZON \\--tumbling-window-in-seconds 120",
                                    "Lambda determines tumbling window boundaries based on the time when records were inserted into the stream. All      records have an approximate timestamp available that Lambda uses in boundary determinations.",
                                    "Tumbling window aggregations do not support resharding. When the shard ends, Lambda considers the window      closed, and the child shards start their own window in a fresh state.",
                                    "Tumbling windows fully support the existing retry policies maxRetryAttempts and        maxRecordAge.",
                                    {
                                        "code_example": "def lambda_handler(event, context):\n    print('Incoming event: ', event)\n    print('Incoming state: ', event['state'])\n\n#Check if this is the end of the window to either aggregate or process.\n    if event['isFinalInvokeForWindow']:\n        # logic to handle final state of the window\n        print('Destination invoke')\n    else:\n        print('Aggregate invoke')\n\n#Check for early terminations\n    if event['isWindowTerminatedEarly']:\n        print('Window terminated early')\n\n    #Aggregation logic\n    state = event['state']\n    for record in event['Records']:\n        state[record['dynamodb']['NewImage']['Id']] = state.get(record['dynamodb']['NewImage']['Id'], 0) + 1\n\n    print('Returning state: ', state)\n    return {'state': state}"
                                    }
                                ]
                            }
                        ]
                    },
                    {
                        "title": "Parameters",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/services-ddb-params.html",
                        "sections": [
                            "",
                            "All Lambda event source types share the same CreateEventSourceMapping and UpdateEventSourceMapping      API operations. However, only some of the parameters apply to DynamoDB Streams.",
                            {
                                "code_example": "ReportBatchItemFailures"
                            }
                        ]
                    },
                    {
                        "title": "Event filtering",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/with-ddb-filtering.html",
                        "sections": [
                            "",
                            "You can use event filtering to control which records from a stream or queue Lambda sends to your function.    For general information about how event filtering works, see Control which events Lambda sends to your function.",
                            "This section focuses on event filtering for DynamoDB event sources.",
                            "  1.DynamoDB event",
                            "  2.Filtering with table attributes",
                            "  3.Filtering with Boolean expressions",
                            "  4.Using the Exists operator",
                            "  5.JSON format for DynamoDB filtering",
                            {
                                "sub_header": "DynamoDB event",
                                "content": [
                                    "Suppose you have a DynamoDB table with the primary key CustomerName and attributes AccountManager and       PaymentTerms. The following shows an example record from your DynamoDB table’s stream.",
                                    "{      \"eventID\": \"1\",      \"eventVersion\": \"1.0\",      \"dynamodb\": {          \"ApproximateCreationDateTime\": \"1678831218.0\",          \"Keys\": {              \"CustomerName\": {                  \"S\": \"AnyCompany Industries\"              },              \"NewImage\": {                  \"AccountManager\": {                      \"S\": \"Pat Candella\"                  },                  \"PaymentTerms\": {                      \"S\": \"60 days\"                  },                  \"CustomerName\": {                      \"S\": \"AnyCompany Industries\"                  }              },              \"SequenceNumber\": \"111\",              \"SizeBytes\": 26,              \"StreamViewType\": \"NEW_IMAGE\"          }      }  }",
                                    "To filter based on the key and attribute values in your DynamoDB table, use the dynamodb key in the record.       The following sections provide examples for different filter types.",
                                    {
                                        "sub_header": "Filtering with table keys",
                                        "content": [
                                            "Suppose you want your function to process only those records where the primary key CustomerName is “AnyCompany Industries.” The       FilterCriteria object would be as follows.",
                                            "{     \"Filters\": [          {              \"Pattern\": \"{ \\\"dynamodb\\\" : { \\\"Keys\\\" : { \\\"CustomerName\\\" : { \\\"S\\\" : [ \\\"AnyCompany Industries\\\" ] } } } }\"          }      ] }",
                                            "For added clarity, here is the value of the filter's Pattern expanded in plain JSON. ",
                                            "{     \"dynamodb\": {          \"Keys\": {              \"CustomerName\": {                  \"S\": [ \"AnyCompany Industries\" ]                  }              }          } }",
                                            "You can add your filter using the console, AWS CLI or an AWS SAM template.",
                                            "ConsoleTo add this filter using the console, follow the instructions in Attaching filter criteria to an event source mapping (console) and enter the following             string for the Filter criteria.{ \"dynamodb\" : { \"Keys\" : { \"CustomerName\" : { \"S\" : [ \"AnyCompany Industries\" ] } } } }AWS CLITo create a new event source mapping with these filter criteria using the AWS Command Line Interface (AWS CLI), run the following            command.aws lambda create-event-source-mapping \\    --function-name my-function \\    --event-source-arn arn:aws:dynamodb:us-east-2:123456789012:table/my-table \\    --filter-criteria '{\"Filters\": [{\"Pattern\": \"{ \\\"dynamodb\\\" : { \\\"Keys\\\" : { \\\"CustomerName\\\" : { \\\"S\\\" : [ \\\"AnyCompany Industries\\\" ] } } } }\"}]}'To add these filter criteria to an existing event source mapping, run the following command.aws lambda update-event-source-mapping \\    --uuid \"a1b2c3d4-5678-90ab-cdef-11111EXAMPLE\" \\    --filter-criteria '{\"Filters\": [{\"Pattern\": \"{ \\\"dynamodb\\\" : { \\\"Keys\\\" : { \\\"CustomerName\\\" : { \\\"S\\\" : [ \\\"AnyCompany Industries\\\" ] } } } }\"}]}'AWS SAMTo add this filter using AWS SAM, add the following snippet to the YAML template for your event source.FilterCriteria:   Filters:     - Pattern: '{ \"dynamodb\" : { \"Keys\" : { \"CustomerName\" : { \"S\" : [ \"AnyCompany Industries\" ] } } } }'anchoranchoranchorConsoleAWS CLIAWS SAMTo add this filter using the console, follow the instructions in Attaching filter criteria to an event source mapping (console) and enter the following             string for the Filter criteria.{ \"dynamodb\" : { \"Keys\" : { \"CustomerName\" : { \"S\" : [ \"AnyCompany Industries\" ] } } } }"
                                        ]
                                    }
                                ]
                            },
                            {
                                "sub_header": "Filtering with table attributes",
                                "content": [
                                    "With DynamoDB, you can also use the NewImage and OldImage keys to filter for attribute values. Suppose you want       to filter records where the AccountManager attribute in the latest table image is “Pat Candella” or \"Shirley Rodriguez.\" The       FilterCriteria object would be as follows.",
                                    "{    \"Filters\": [        {            \"Pattern\": \"{ \\\"dynamodb\\\" : { \\\"NewImage\\\" : { \\\"AccountManager\\\" : { \\\"S\\\" : [ \\\"Pat Candella\\\", \\\"Shirley Rodriguez\\\" ] } } } }\"        }    ]}",
                                    "For added clarity, here is the value of the filter's Pattern expanded in plain JSON.",
                                    "{    \"dynamodb\": {        \"NewImage\": {            \"AccountManager\": {                \"S\": [ \"Pat Candella\", \"Shirley Rodriguez\" ]            }        }    }}",
                                    "You can add your filter using the console, AWS CLI or an AWS SAM template.",
                                    "ConsoleTo add this filter using the console, follow the instructions in Attaching filter criteria to an event source mapping (console) and enter the following             string for the Filter criteria.{ \"dynamodb\" : { \"NewImage\" : { \"AccountManager\" : { \"S\" : [ \"Pat Candella\", \"Shirley Rodriguez\" ] } } } }AWS CLITo create a new event source mapping with these filter criteria using the AWS Command Line Interface (AWS CLI), run the following            command.aws lambda create-event-source-mapping \\    --function-name my-function \\    --event-source-arn arn:aws:dynamodb:us-east-2:123456789012:table/my-table \\    --filter-criteria '{\"Filters\": [{\"Pattern\": \"{ \\\"dynamodb\\\" : { \\\"NewImage\\\" : { \\\"AccountManager\\\" : { \\\"S\\\" : [ \\\"Pat Candella\\\", \\\"Shirley Rodriguez\\\" ] } } } }\"}]}'To add these filter criteria to an existing event source mapping, run the following command.aws lambda update-event-source-mapping \\    --uuid \"a1b2c3d4-5678-90ab-cdef-11111EXAMPLE\" \\    --filter-criteria '{\"Filters\": [{\"Pattern\": \"{ \\\"dynamodb\\\" : { \\\"NewImage\\\" : { \\\"AccountManager\\\" : { \\\"S\\\" : [ \\\"Pat Candella\\\", \\\"Shirley Rodriguez\\\" ] } } } }\"}]}'AWS SAMTo add this filter using AWS SAM, add the following snippet to the YAML template for your event source.FilterCriteria:  Filters:    - Pattern: '{ \"dynamodb\" : { \"NewImage\" : { \"AccountManager\" : { \"S\" : [ \"Pat Candella\", \"Shirley Rodriguez\" ] } } } }'anchoranchoranchorConsoleAWS CLIAWS SAMTo add this filter using the console, follow the instructions in Attaching filter criteria to an event source mapping (console) and enter the following             string for the Filter criteria.{ \"dynamodb\" : { \"NewImage\" : { \"AccountManager\" : { \"S\" : [ \"Pat Candella\", \"Shirley Rodriguez\" ] } } } }"
                                ]
                            },
                            {
                                "sub_header": "Filtering with Boolean expressions",
                                "content": [
                                    "You can also create filters using Boolean AND expressions. These expressions can include both your table's key and attribute parameters.     Suppose you want to filter records where the NewImage value of AccountManager is \"Pat Candella\" and the     OldImage value is \"Terry Whitlock\". The FilterCriteria object would be as follows.",
                                    "{    \"Filters\": [        {            \"Pattern\": \"{ \\\"dynamodb\\\" : { \\\"NewImage\\\" : { \\\"AccountManager\\\" : { \\\"S\\\" : [ \\\"Pat Candella\\\" ] } } } , \\\"dynamodb\\\" : { \\\"OldImage\\\" : { \\\"AccountManager\\\" : { \\\"S\\\" : [ \\\"Terry Whitlock\\\" ] } } } }\"        }    ]}",
                                    "For added clarity, here is the value of the filter's Pattern expanded in plain JSON.",
                                    "{     \"dynamodb\" : {         \"NewImage\" : {             \"AccountManager\" : {                 \"S\" : [                     \"Pat Candella\"                 ]             }         }     },     \"dynamodb\": {         \"OldImage\": {             \"AccountManager\": {                 \"S\": [                     \"Terry Whitlock\"                 ]             }         }     } }",
                                    "You can add your filter using the console, AWS CLI or an AWS SAM template.",
                                    "ConsoleTo add this filter using the console, follow the instructions in Attaching filter criteria to an event source mapping (console) and enter the following             string for the Filter criteria.{ \"dynamodb\" : { \"NewImage\" : { \"AccountManager\" : { \"S\" : [ \"Pat Candella\" ] } } } , \"dynamodb\" : { \"OldImage\" : { \"AccountManager\" : { \"S\" : [ \"Terry Whitlock\" ] } } } }AWS CLITo create a new event source mapping with these filter criteria using the AWS Command Line Interface (AWS CLI), run the following            command.aws lambda create-event-source-mapping \\    --function-name my-function \\    --event-source-arn arn:aws:dynamodb:us-east-2:123456789012:table/my-table \\    --filter-criteria '{\"Filters\": [{\"Pattern\": \"{ \\\"dynamodb\\\" : { \\\"NewImage\\\" : { \\\"AccountManager\\\" : { \\\"S\\\" : [ \\\"Pat Candella\\\" ] } } } , \\\"dynamodb\\\" : { \\\"OldImage\\\" : { \\\"AccountManager\\\" : { \\\"S\\\" : [ \\\"Terry Whitlock\\\" ] } } } } \"}]}'To add these filter criteria to an existing event source mapping, run the following command.aws lambda update-event-source-mapping \\    --uuid \"a1b2c3d4-5678-90ab-cdef-11111EXAMPLE\" \\    --filter-criteria '{\"Filters\": [{\"Pattern\": \"{ \\\"dynamodb\\\" : { \\\"NewImage\\\" : { \\\"AccountManager\\\" : { \\\"S\\\" : [ \\\"Pat Candella\\\" ] } } } , \\\"dynamodb\\\" : { \\\"OldImage\\\" : { \\\"AccountManager\\\" : { \\\"S\\\" : [ \\\"Terry Whitlock\\\" ] } } } } \"}]}'AWS SAMTo add this filter using AWS SAM, add the following snippet to the YAML template for your event source.FilterCriteria:  Filters:    - Pattern: '{ \"dynamodb\" : { \"NewImage\" : { \"AccountManager\" : { \"S\" : [ \"Pat Candella\" ] } } } , \"dynamodb\" : { \"OldImage\" : { \"AccountManager\" : { \"S\" : [ \"Terry Whitlock\" ] } } } }'anchoranchoranchorConsoleAWS CLIAWS SAMTo add this filter using the console, follow the instructions in Attaching filter criteria to an event source mapping (console) and enter the following             string for the Filter criteria.{ \"dynamodb\" : { \"NewImage\" : { \"AccountManager\" : { \"S\" : [ \"Pat Candella\" ] } } } , \"dynamodb\" : { \"OldImage\" : { \"AccountManager\" : { \"S\" : [ \"Terry Whitlock\" ] } } } }",
                                    "NoteDynamoDB event filtering doesn’t support the use of numeric operators (numeric equals and numeric range). Even if items in your \n        table are stored as numbers, these parameters are converted to strings in the JSON record object."
                                ]
                            },
                            {
                                "sub_header": "Using the Exists operator",
                                "content": [
                                    "Because of the way that JSON event objects from DynamoDB are structured, using the Exists operator requires special care.       The Exists operator only works on leaf nodes in the event JSON, so if your filter pattern uses Exists to test for an intermediate       node, it won't work. Consider the following DynamoDB table item:",
                                    "{  \"UserID\": {\"S\": \"12345\"},  \"Name\": {\"S\": \"John Doe\"},  \"Organizations\": {\"L\": [      {\"S\":\"Sales\"},      {\"S\":\"Marketing\"},      {\"S\":\"Support\"}    ]  }}",
                                    "You might want to create a filter pattern like the following that would test for events containing \"Organizations\":",
                                    "{ \"dynamodb\" : { \"NewImage\" : { \"Organizations\" : [ { \"exists\": true } ] } } }",
                                    "However, this filter pattern would never return a match because \"Organizations\" is not a leaf node. The following         example shows how to properly use the Exists operator to construct the desired filter pattern:",
                                    "{ \"dynamodb\" : { \"NewImage\" : {\"Organizations\": {\"L\": {\"S\": [ {\"exists\": true } ] } } } } }"
                                ]
                            },
                            {
                                "sub_header": "JSON format for DynamoDB filtering",
                                "content": [
                                    "To properly filter events from DynamoDB sources, both the data field and your filter criteria for the data field (dynamodb)       must be in valid JSON format. If either field isn't in a valid JSON format, Lambda drops the message or throws an exception. The following       table summarizes the specific behavior: ",
                                    "\n\nIncoming data format\nFilter pattern format for data properties\nResulting action\n\n\n\n\nValid JSON\n\n\nValid JSON\n\n\nLambda filters based on your filter criteria.\n\n\n\n\nValid JSON\n\n\nNo filter pattern for data properties\n\n\nLambda filters (on the other metadata properties only) based on your filter criteria.\n\n\n\n\nValid JSON\n\n\nNon-JSON\n\n\nLambda throws an exception at the time of the event source mapping creation or update. The filter pattern\n                for data properties must be in a valid JSON format.\n\n\n\n\nNon-JSON\n\n\nValid JSON\n\n\nLambda drops the record.\n\n\n\n\nNon-JSON\n\n\nNo filter pattern for data properties\n\n\nLambda filters (on the other metadata properties only) based on your filter criteria.\n\n\n\n\nNon-JSON\n\n\nNon-JSON\n\n\nLambda throws an exception at the time of the event source mapping creation or update. The filter pattern\n                for data properties must be in a valid JSON format.\n\n\n"
                                ]
                            }
                        ]
                    },
                    {
                        "title": "Tutorial",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/with-ddb-example.html",
                        "sections": [
                            "",
                            " In this tutorial, you create a Lambda function to consume events from an Amazon DynamoDB stream.",
                            {
                                "sub_header": "Prerequisites",
                                "content": [
                                    "This tutorial assumes that you have some knowledge of basic Lambda operations and the Lambda console. If you      haven't already, follow the instructions in Create a Lambda function with the console to create your first Lambda function.",
                                    "To complete the following steps, you need the AWS CLI version 2. Commands and the expected output are listed in separate blocks:",
                                    "aws --version",
                                    "You should see the following output:",
                                    "aws-cli/2.13.27 Python/3.11.6 Linux/4.14.328-248.540.amzn2.x86_64 exe/x86_64.amzn.2",
                                    "For long commands, an escape character (\\) is used to split a command over multiple lines.",
                                    "On Linux and macOS, use your preferred shell and package manager.",
                                    {
                                        "code_example": "zip"
                                    }
                                ]
                            },
                            {
                                "sub_header": "Create the execution role",
                                "content": [
                                    "Create the execution role that gives your function      permission to access AWS resources.",
                                    {
                                        "code_example": "lambda-dynamodb-role"
                                    },
                                    "The AWSLambdaDynamoDBExecutionRole has the permissions that the function needs to read      items from DynamoDB and write logs to CloudWatch Logs."
                                ]
                            },
                            {
                                "sub_header": "Create the function",
                                "content": [
                                    "Create a Lambda function that processes your DynamoDB events. The function code writes some of      the incoming event data to CloudWatch Logs.",
                                    ".NETAWS SDK for .NETNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Consuming a DynamoDB event with Lambda using .NET.// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.// SPDX-License-Identifier: Apache-2.0using System.Text.Json;using System.Text;using Amazon.Lambda.Core;using Amazon.Lambda.DynamoDBEvents;// Assembly attribute to enable the Lambda function's JSON input to be converted into a .NET class.[assembly: LambdaSerializer(typeof(Amazon.Lambda.Serialization.SystemTextJson.DefaultLambdaJsonSerializer))]namespace AWSLambda_DDB;public class Function{    public void FunctionHandler(DynamoDBEvent dynamoEvent, ILambdaContext context)    {        context.Logger.LogInformation($\"Beginning to process {dynamoEvent.Records.Count} records...\");        foreach (var record in dynamoEvent.Records)        {            context.Logger.LogInformation($\"Event ID: {record.EventID}\");            context.Logger.LogInformation($\"Event Name: {record.EventName}\");            context.Logger.LogInformation(JsonSerializer.Serialize(record));        }        context.Logger.LogInformation(\"Stream processing complete.\");    }}GoSDK for Go V2Note        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Consuming a DynamoDB event with Lambda using Go.// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.// SPDX-License-Identifier: Apache-2.0package mainimport (\t\"context\"\t\"github.com/aws/aws-lambda-go/lambda\"\t\"github.com/aws/aws-lambda-go/events\"\t\"fmt\")func HandleRequest(ctx context.Context, event events.DynamoDBEvent) (*string, error) {\tif len(event.Records) == 0 {\t\treturn nil, fmt.Errorf(\"received empty event\")\t}\tfor _, record := range event.Records {\t \tLogDynamoDBRecord(record)\t}\tmessage := fmt.Sprintf(\"Records processed: %d\", len(event.Records))\treturn &message, nil}func main() {\tlambda.Start(HandleRequest)}func LogDynamoDBRecord(record events.DynamoDBEventRecord){\tfmt.Println(record.EventID)\tfmt.Println(record.EventName)\tfmt.Printf(\"%+v\\n\", record.Change)}JavaSDK for Java 2.xNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Consuming a DynamoDB event with Lambda using Java.import com.amazonaws.services.lambda.runtime.Context;import com.amazonaws.services.lambda.runtime.RequestHandler;import com.amazonaws.services.lambda.runtime.events.DynamodbEvent;import com.amazonaws.services.lambda.runtime.events.DynamodbEvent.DynamodbStreamRecord;import com.google.gson.Gson;import com.google.gson.GsonBuilder;public class example implements RequestHandler<DynamodbEvent, Void> {    private static final Gson GSON = new GsonBuilder().setPrettyPrinting().create();    @Override    public Void handleRequest(DynamodbEvent event, Context context) {        System.out.println(GSON.toJson(event));        event.getRecords().forEach(this::logDynamoDBRecord);        return null;    }    private void logDynamoDBRecord(DynamodbStreamRecord record) {        System.out.println(record.getEventID());        System.out.println(record.getEventName());        System.out.println(\"DynamoDB Record: \" + GSON.toJson(record.getDynamodb()));    }}JavaScriptSDK for JavaScript (v3)Note        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Consuming a DynamoDB event with Lambda using JavaScript.// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.// SPDX-License-Identifier: Apache-2.0exports.handler = async (event, context) => {    console.log(JSON.stringify(event, null, 2));    event.Records.forEach(record => {        logDynamoDBRecord(record);    });};const logDynamoDBRecord = (record) => {    console.log(record.eventID);    console.log(record.eventName);    console.log(`DynamoDB Record: ${JSON.stringify(record.dynamodb)}`);};Consuming a DynamoDB event with Lambda using TypeScript.export const handler = async (event, context) => {    console.log(JSON.stringify(event, null, 2));    event.Records.forEach(record => {        logDynamoDBRecord(record);    });}const logDynamoDBRecord = (record) => {    console.log(record.eventID);    console.log(record.eventName);    console.log(`DynamoDB Record: ${JSON.stringify(record.dynamodb)}`);};PHPSDK for PHPNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Consuming a DynamoDB event with Lambda using PHP.<?php# using bref/bref and bref/logger for simplicityuse Bref\\Context\\Context;use Bref\\Event\\DynamoDb\\DynamoDbEvent;use Bref\\Event\\DynamoDb\\DynamoDbHandler;use Bref\\Logger\\StderrLogger;require __DIR__ . '/vendor/autoload.php';class Handler extends DynamoDbHandler{    private StderrLogger $logger;    public function __construct(StderrLogger $logger)    {        $this->logger = $logger;    }    /**     * @throws JsonException     * @throws \\Bref\\Event\\InvalidLambdaEvent     */    public function handleDynamoDb(DynamoDbEvent $event, Context $context): void    {        $this->logger->info(\"Processing DynamoDb table items\");        $records = $event->getRecords();        foreach ($records as $record) {            $eventName = $record->getEventName();            $keys = $record->getKeys();            $old = $record->getOldImage();            $new = $record->getNewImage();                        $this->logger->info(\"Event Name:\".$eventName.\"\\n\");            $this->logger->info(\"Keys:\". json_encode($keys).\"\\n\");            $this->logger->info(\"Old Image:\". json_encode($old).\"\\n\");            $this->logger->info(\"New Image:\". json_encode($new));                        // TODO: Do interesting work based on the new data            // Any exception thrown will be logged and the invocation will be marked as failed        }        $totalRecords = count($records);        $this->logger->info(\"Successfully processed $totalRecords items\");    }}$logger = new StderrLogger();return new Handler($logger);PythonSDK for Python (Boto3)Note        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Consuming a DynamoDB event with Lambda using Python.import jsondef lambda_handler(event, context):    print(json.dumps(event, indent=2))    for record in event['Records']:        log_dynamodb_record(record)def log_dynamodb_record(record):    print(record['eventID'])    print(record['eventName'])    print(f\"DynamoDB Record: {json.dumps(record['dynamodb'])}\")RubySDK for RubyNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Consuming a DynamoDB event with Lambda using Ruby.def lambda_handler(event:, context:)    return 'received empty event' if event['Records'].empty?      event['Records'].each do |record|      log_dynamodb_record(record)    end      \"Records processed: #{event['Records'].length}\"  end    def log_dynamodb_record(record)    puts record['eventID']    puts record['eventName']    puts \"DynamoDB Record: #{JSON.generate(record['dynamodb'])}\"  end  RustSDK for RustNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Consuming a DynamoDB event with Lambda using Rust.use lambda_runtime::{service_fn, tracing, Error, LambdaEvent};use aws_lambda_events::{    event::dynamodb::{Event, EventRecord},   };// Built with the following dependencies://lambda_runtime = \"0.11.1\"//serde_json = \"1.0\"//tokio = { version = \"1\", features = [\"macros\"] }//tracing = { version = \"0.1\", features = [\"log\"] }//tracing-subscriber = { version = \"0.3\", default-features = false, features = [\"fmt\"] }//aws_lambda_events = \"0.15.0\"async fn function_handler(event: LambdaEvent<Event>) ->Result<(), Error> {        let records = &event.payload.records;    tracing::info!(\"event payload: {:?}\",records);    if records.is_empty() {        tracing::info!(\"No records found. Exiting.\");        return Ok(());    }    for record in records{        log_dynamo_dbrecord(record);    }    tracing::info!(\"Dynamo db records processed\");    // Prepare the response    Ok(())}fn log_dynamo_dbrecord(record: &EventRecord)-> Result<(), Error>{    tracing::info!(\"EventId: {}\", record.event_id);    tracing::info!(\"EventName: {}\", record.event_name);    tracing::info!(\"DynamoDB Record: {:?}\", record.change );    Ok(())}#[tokio::main]async fn main() -> Result<(), Error> {    tracing_subscriber::fmt()    .with_max_level(tracing::Level::INFO)    .with_target(false)    .without_time()    .init();    let func = service_fn(function_handler);    lambda_runtime::run(func).await?;    Ok(())    }anchoranchoranchoranchoranchoranchoranchoranchor.NETGoJavaJavaScriptPHPPythonRubyRustAWS SDK for .NETNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Consuming a DynamoDB event with Lambda using .NET.// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.// SPDX-License-Identifier: Apache-2.0using System.Text.Json;using System.Text;using Amazon.Lambda.Core;using Amazon.Lambda.DynamoDBEvents;// Assembly attribute to enable the Lambda function's JSON input to be converted into a .NET class.[assembly: LambdaSerializer(typeof(Amazon.Lambda.Serialization.SystemTextJson.DefaultLambdaJsonSerializer))]namespace AWSLambda_DDB;public class Function{    public void FunctionHandler(DynamoDBEvent dynamoEvent, ILambdaContext context)    {        context.Logger.LogInformation($\"Beginning to process {dynamoEvent.Records.Count} records...\");        foreach (var record in dynamoEvent.Records)        {            context.Logger.LogInformation($\"Event ID: {record.EventID}\");            context.Logger.LogInformation($\"Event Name: {record.EventName}\");            context.Logger.LogInformation(JsonSerializer.Serialize(record));        }        context.Logger.LogInformation(\"Stream processing complete.\");    }}",
                                    {
                                        "code_example": "example.js"
                                    }
                                ]
                            },
                            {
                                "sub_header": "Test the Lambda function",
                                "content": [
                                    "In this step, you invoke your Lambda function manually using the invoke AWS Lambda CLI command and      the following sample DynamoDB event. Copy the following into a file named input.txt.",
                                    {
                                        "code_example": "{\n   \"Records\":[\n      {\n         \"eventID\":\"1\",\n         \"eventName\":\"INSERT\",\n         \"eventVersion\":\"1.0\",\n         \"eventSource\":\"aws:dynamodb\",\n         \"awsRegion\":\"us-east-1\",\n         \"dynamodb\":{\n            \"Keys\":{\n               \"Id\":{\n                  \"N\":\"101\"\n               }\n            },\n            \"NewImage\":{\n               \"Message\":{\n                  \"S\":\"New item!\"\n               },\n               \"Id\":{\n                  \"N\":\"101\"\n               }\n            },\n            \"SequenceNumber\":\"111\",\n            \"SizeBytes\":26,\n            \"StreamViewType\":\"NEW_AND_OLD_IMAGES\"\n         },\n         \"eventSourceARN\":\"stream-ARN\"\n      },\n      {\n         \"eventID\":\"2\",\n         \"eventName\":\"MODIFY\",\n         \"eventVersion\":\"1.0\",\n         \"eventSource\":\"aws:dynamodb\",\n         \"awsRegion\":\"us-east-1\",\n         \"dynamodb\":{\n            \"Keys\":{\n               \"Id\":{\n                  \"N\":\"101\"\n               }\n            },\n            \"NewImage\":{\n               \"Message\":{\n                  \"S\":\"This item has changed\"\n               },\n               \"Id\":{\n                  \"N\":\"101\"\n               }\n            },\n            \"OldImage\":{\n               \"Message\":{\n                  \"S\":\"New item!\"\n               },\n               \"Id\":{\n                  \"N\":\"101\"\n               }\n            },\n            \"SequenceNumber\":\"222\",\n            \"SizeBytes\":59,\n            \"StreamViewType\":\"NEW_AND_OLD_IMAGES\"\n         },\n         \"eventSourceARN\":\"stream-ARN\"\n      },\n      {\n         \"eventID\":\"3\",\n         \"eventName\":\"REMOVE\",\n         \"eventVersion\":\"1.0\",\n         \"eventSource\":\"aws:dynamodb\",\n         \"awsRegion\":\"us-east-1\",\n         \"dynamodb\":{\n            \"Keys\":{\n               \"Id\":{\n                  \"N\":\"101\"\n               }\n            },\n            \"OldImage\":{\n               \"Message\":{\n                  \"S\":\"This item has changed\"\n               },\n               \"Id\":{\n                  \"N\":\"101\"\n               }\n            },\n            \"SequenceNumber\":\"333\",\n            \"SizeBytes\":38,\n            \"StreamViewType\":\"NEW_AND_OLD_IMAGES\"\n         },\n         \"eventSourceARN\":\"stream-ARN\"\n      }\n   ]\n}"
                                    },
                                    "Run the following invoke command. ",
                                    "aws lambda invoke --function-name ProcessDynamoDBRecords \\    --cli-binary-format raw-in-base64-out \\    --payload file://input.txt outputfile.txt",
                                    "The cli-binary-format option is required if you're using AWS CLI version 2. To make this the default setting, run aws configure set cli-binary-format raw-in-base64-out. For more information, see AWS CLI supported global command line options in the AWS Command Line Interface User Guide for Version 2.",
                                    "The function returns the string message in the response body. ",
                                    "Verify the output in the outputfile.txt file."
                                ]
                            },
                            {
                                "sub_header": "Create a DynamoDB table with a stream enabled",
                                "content": [
                                    "Create an Amazon DynamoDB table with a stream enabled.",
                                    {
                                        "code_example": "lambda-dynamodb-stream"
                                    },
                                    "To enable streams\nOpen the DynamoDB console.\n\nChoose Tables.\n\nChoose the lambda-dynamodb-stream table.\n\nUnder Exports and streams, choose DynamoDB stream details.\n\nChoose Turn on.\n\nFor View type, choose Key attributes only.\n\nChoose Turn on stream.\n",
                                    "Write down the stream ARN. You need this in the next step when you associate the stream with your Lambda      function. For more information on enabling streams, see Capturing table        activity with DynamoDB Streams."
                                ]
                            },
                            {
                                "sub_header": "Add an event source in AWS Lambda",
                                "content": [
                                    "Create an event source mapping in AWS Lambda. This event source mapping associates the DynamoDB stream with      your Lambda function. After you create this event source mapping, AWS Lambda starts polling the stream.",
                                    "Run the following AWS CLI create-event-source-mapping command. After the command runs, note      down the UUID. You'll need this UUID to refer to the event source mapping in any commands, for example, when      deleting the event source mapping.",
                                    "aws lambda create-event-source-mapping --function-name ProcessDynamoDBRecords \\    --batch-size 100 --starting-position LATEST --event-source DynamoDB-stream-arn",
                                    " This creates a mapping between the specified DynamoDB stream and the Lambda function. You can associate a DynamoDB      stream with multiple Lambda functions, and associate the same Lambda function with multiple streams. However, the      Lambda functions will share the read throughput for the stream they share. ",
                                    "You can get the list of event source mappings by running the following command.",
                                    "aws lambda list-event-source-mappings",
                                    "The list returns all of the event source mappings you created, and for each mapping it shows the        LastProcessingResult, among other things. This field is used to provide an informative message if      there are any problems. Values such as No records processed (indicates that AWS Lambda has not started      polling or that there are no records in the stream) and OK (indicates AWS Lambda successfully read      records from the stream and invoked your Lambda function) indicate that there are no issues. If there are issues,      you receive an error message.",
                                    "If you have a lot of event source mappings, use the function name parameter to narrow down the results.",
                                    "aws lambda list-event-source-mappings --function-name ProcessDynamoDBRecords"
                                ]
                            },
                            {
                                "sub_header": "Test the setup",
                                "content": [
                                    "Test the end-to-end experience. As you perform table updates, DynamoDB writes event records to the stream. As      AWS Lambda polls the stream, it detects new records in the stream and invokes your Lambda function on your behalf      by passing events to the function. ",
                                    "\n\nIn the DynamoDB console, add, update, and delete items to the table. DynamoDB writes records of these actions to\n          the stream.\n\nAWS Lambda polls the stream and when it detects updates to the stream, it invokes your Lambda function by\n          passing in the event data it finds in the stream.\n\nYour function runs and creates logs in Amazon CloudWatch. You can verify the logs reported in the Amazon CloudWatch\n          console.\n"
                                ]
                            },
                            {
                                "sub_header": "Clean up your resources",
                                "content": [
                                    "You can now delete the resources that you created for this tutorial, unless you want to retain them. By deleting AWS resources that you're no longer using, you prevent unnecessary charges to your AWS account.",
                                    {
                                        "code_example": "delete"
                                    },
                                    "To delete the execution role\nOpen the Roles page of the IAM console.\n\nSelect the execution role that you created.\n\nChoose Delete.\n\nEnter the name of the role in the text input field and choose Delete.\n",
                                    {
                                        "code_example": "delete"
                                    }
                                ]
                            }
                        ]
                    }
                ],
                "sections": [
                    "",
                    "NoteIf you want to send data to a target other than a Lambda function or enrich the data before sending it, see \n    Amazon EventBridge Pipes.",
                    "You can use an AWS Lambda function to process records in an Amazon DynamoDB      stream. With DynamoDB Streams, you can trigger a Lambda function to perform additional work each time a DynamoDB table is    updated.",
                    "  1.Polling and batching streams",
                    "  2.Polling and stream starting positions",
                    "  3.Simultaneous readers of a shard in DynamoDB Streams",
                    "  4.Example event",
                    "  5.Process DynamoDB records with Lambda",
                    "  6.Configuring partial batch response with DynamoDB and Lambda",
                    "  7.Retain discarded records for a DynamoDB event source in Lambda",
                    "  8.Implementing stateful DynamoDB stream processing in Lambda",
                    "  9.Lambda parameters for Amazon DynamoDB event source mappings",
                    "  10.Using event filtering with a DynamoDB event source",
                    "  11.Tutorial: Using AWS Lambda with Amazon DynamoDB streams",
                    {
                        "sub_header": "Polling and batching streams",
                        "content": [
                            "Lambda polls shards in your DynamoDB stream for records at a base rate of 4 times per second. When records are      available, Lambda invokes your function and waits for the result. If processing succeeds, Lambda resumes polling until      it receives more records.",
                            "By default, Lambda invokes your function as soon as records are available. If the batch      that Lambda reads from the event source has only one record in it, Lambda sends only one record to the function. To avoid invoking the function      with a small number of records, you can tell the event source to buffer records for up to 5 minutes by configuring a        batching window. Before invoking the function, Lambda continues to read records from the event source      until it has gathered a full batch, the batching window expires, or the batch reaches the payload limit of 6 MB. For more information,      see Batching behavior.",
                            "WarningLambda event source mappings process each event at least once, and duplicate processing of records can occur. To avoid potential issues \nrelated to duplicate events, we strongly recommend that you make your function code idempotent. To learn more, see How do I make my Lambda function idempotent \nin the AWS Knowledge Center.",
                            "Lambda doesn't wait for any configured extensions to complete      before sending the next batch for processing. In other words, your extensions may continue to run as Lambda      processes the next batch of records. This can cause throttling issues if you breach any of your account's       concurrency settings or limits. To detect whether this is a      potential issue, monitor your functions and check whether you're seeing higher      concurrency metrics than expected for your event      source mapping. Due to short times in between invokes, Lambda may briefly report higher concurrency usage      than the number of shards. This can be true even for Lambda functions without extensions.",
                            "Configure the       ParallelizationFactor setting to process one shard of a DynamoDB stream with more than one Lambda invocation simultaneously.       You can specify the number of concurrent batches that Lambda polls from a shard via a parallelization factor from 1      (default) to 10. For example, when you set ParallelizationFactor to 2, you can have 200 concurrent      Lambda invocations at maximum to process 100 DynamoDB stream shards (though in practice, you may see different values      for the ConcurrentExecutions metric). This helps scale up the processing throughput when the data volume      is volatile and the IteratorAge is high. When you increase the number of concurrent batches per shard,      Lambda still ensures in-order processing at the item (partition and sort key) level."
                        ]
                    },
                    {
                        "sub_header": "Polling and stream starting positions",
                        "content": [
                            "Be aware that stream polling during event source mapping creation and updates is eventually consistent.",
                            "This behavior means that if you specify LATEST as the starting position for the stream, the event source mapping could     miss events during creation or updates. To ensure that no events are missed, specify the stream starting position as TRIM_HORIZON."
                        ]
                    },
                    {
                        "sub_header": "Simultaneous readers of a shard in DynamoDB Streams",
                        "content": [
                            "For single-Region tables that are not global tables, you can design for up to two Lambda functions to read from the same DynamoDB Streams shard at the same time. Exceeding this limit can result in request throttling.      For global tables, we recommend you limit the number of simultaneous functions to one to avoid request throttling."
                        ]
                    },
                    {
                        "sub_header": "Example event",
                        "content": [
                            {
                                "code_example": "{\n  \"Records\": [\n    {\n      \"eventID\": \"1\",\n      \"eventVersion\": \"1.0\",\n      \"dynamodb\": {\n        \"Keys\": {\n          \"Id\": {\n            \"N\": \"101\"\n          }\n        },\n        \"NewImage\": {\n          \"Message\": {\n            \"S\": \"New item!\"\n          },\n          \"Id\": {\n            \"N\": \"101\"\n          }\n        },\n        \"StreamViewType\": \"NEW_AND_OLD_IMAGES\",\n        \"SequenceNumber\": \"111\",\n        \"SizeBytes\": 26\n      },\n      \"awsRegion\": \"us-west-2\",\n      \"eventName\": \"INSERT\",\n      \"eventSourceARN\": \"arn:aws:dynamodb:us-east-2:123456789012:table/my-table/stream/2024-06-10T19:26:16.525\",\n      \"eventSource\": \"aws:dynamodb\"\n    },\n    {\n      \"eventID\": \"2\",\n      \"eventVersion\": \"1.0\",\n      \"dynamodb\": {\n        \"OldImage\": {\n          \"Message\": {\n            \"S\": \"New item!\"\n          },\n          \"Id\": {\n            \"N\": \"101\"\n          }\n        },\n        \"SequenceNumber\": \"222\",\n        \"Keys\": {\n          \"Id\": {\n            \"N\": \"101\"\n          }\n        },\n        \"SizeBytes\": 59,\n        \"NewImage\": {\n          \"Message\": {\n            \"S\": \"This item has changed\"\n          },\n          \"Id\": {\n            \"N\": \"101\"\n          }\n        },\n        \"StreamViewType\": \"NEW_AND_OLD_IMAGES\"\n      },\n      \"awsRegion\": \"us-west-2\",\n      \"eventName\": \"MODIFY\",\n      \"eventSourceARN\": \"arn:aws:dynamodb:us-east-2:123456789012:table/my-table/stream/2024-06-10T19:26:16.525\",\n      \"eventSource\": \"aws:dynamodb\"\n    }\n  ]}"
                            }
                        ]
                    }
                ]
            },
            {
                "title": "EC2",
                "href": "https://docs.aws.amazon.com/lambda/latest/dg/services-ec2.html",
                "sections": [
                    "",
                    "You can use AWS Lambda to process lifecycle events from Amazon Elastic Compute Cloud and manage Amazon EC2 resources. Amazon EC2 sends events    to Amazon EventBridge (CloudWatch Events) for    lifecycle events    such as when an instance changes state, when an Amazon Elastic Block Store volume snapshot completes, or when a spot instance    is scheduled to be terminated. You configure EventBridge (CloudWatch Events) to forward those events to a Lambda function for processing.",
                    "EventBridge (CloudWatch Events) invokes your Lambda function asynchronously with the event document from Amazon EC2.",
                    {
                        "code_example": "{\n    \"version\": \"0\",\n    \"id\": \"b6ba298a-7732-2226-xmpl-976312c1a050\",\n    \"detail-type\": \"EC2 Instance State-change Notification\",\n    \"source\": \"aws.ec2\",\n    \"account\": \"111122223333\",\n    \"time\": \"2019-10-02T17:59:30Z\",\n    \"region\": \"us-east-1\",\n    \"resources\": [\n        \"arn:aws:ec2:us-east-1:111122223333:instance/i-0c314xmplcd5b8173\"\n    ],\n    \"detail\": {\n        \"instance-id\": \"i-0c314xmplcd5b8173\",\n        \"state\": \"running\"\n    }\n}\n"
                    },
                    "For details on configuring events, see Invoke a Lambda function on a schedule. For an example function that processes Amazon EBS snapshot notifications, see    EventBridge Scheduler for Amazon EBS.",
                    "You can also use the AWS SDK to manage instances and other resources with the Amazon EC2 API.         ",
                    {
                        "sub_header": "Granting permissions to EventBridge (CloudWatch Events)",
                        "content": [
                            "To process lifecycle events from Amazon EC2, EventBridge (CloudWatch Events) needs permission to invoke your function. This permission comes      from the function's resource-based policy. If you use the      EventBridge (CloudWatch Events) console to configure an event trigger, the console updates the resource-based policy on your behalf.      Otherwise, add a statement like the following:",
                            {
                                "code_example": "{\n  \"Sid\": \"ec2-events\",\n  \"Effect\": \"Allow\",\n  \"Principal\": {\n    \"Service\": \"events.amazonaws.com\"\n  },\n  \"Action\": \"lambda:InvokeFunction\",\n  \"Resource\": \"arn:aws:lambda:us-east-1:12456789012:function:my-function\",\n  \"Condition\": {\n    \"ArnLike\": {\n      \"AWS:SourceArn\": \"arn:aws:events:us-east-1:12456789012:rule/*\"\n    }\n  }\n}"
                            },
                            "To add a statement, use the add-permission AWS CLI command.",
                            "aws lambda add-permission --action lambda:InvokeFunction --statement-id ec2-events \\--principal events.amazonaws.com --function-name my-function --source-arn 'arn:aws:events:us-east-1:12456789012:rule/*'",
                            "If your function uses the AWS SDK to manage Amazon EC2 resources, add Amazon EC2 permissions to the function's execution role."
                        ]
                    }
                ]
            },
            {
                "title": "Elastic Load Balancing (Application Load Balancer)",
                "href": "https://docs.aws.amazon.com/lambda/latest/dg/services-alb.html",
                "sections": [
                    "",
                    "You can use a Lambda function to process requests from an Application Load Balancer. Elastic Load Balancing supports Lambda functions as a target for    an Application Load Balancer. Use load balancer rules to route HTTP requests to a function, based on path or header values. Process the    request and return an HTTP response from your Lambda function.",
                    "Elastic Load Balancing invokes your Lambda function synchronously with an event that contains the request body and    metadata.",
                    {
                        "code_example": "{\n    \"requestContext\": {\n        \"elb\": {\n            \"targetGroupArn\": \"arn:aws:elasticloadbalancing:us-east-1:123456789012:targetgroup/lambda-279XGJDqGZ5rsrHC2Fjr/49e9d65c45c6791a\"\n        }\n    },\n    \"httpMethod\": \"GET\",\n    \"path\": \"/lambda\",\n    \"queryStringParameters\": {\n        \"query\": \"1234ABCD\"\n    },\n    \"headers\": {\n        \"accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8\",\n        \"accept-encoding\": \"gzip\",\n        \"accept-language\": \"en-US,en;q=0.9\",\n        \"connection\": \"keep-alive\",\n        \"host\": \"lambda-alb-123578498.us-east-1.elb.amazonaws.com\",\n        \"upgrade-insecure-requests\": \"1\",\n        \"user-agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36\",\n        \"x-amzn-trace-id\": \"Root=1-5c536348-3d683b8b04734faae651f476\",\n        \"x-forwarded-for\": \"72.12.164.125\",\n        \"x-forwarded-port\": \"80\",\n        \"x-forwarded-proto\": \"http\",\n        \"x-imforwards\": \"20\"\n    },\n    \"body\": \"\",\n    \"isBase64Encoded\": False\n}"
                    },
                    "Your function processes the event and returns a response document to the load balancer in JSON. Elastic Load Balancing converts    the document to an HTTP success or error response and returns it to the user.",
                    {
                        "code_example": "{\n    \"statusCode\": 200,\n    \"statusDescription\": \"200 OK\",\n    \"isBase64Encoded\": False,\n    \"headers\": {\n        \"Content-Type\": \"text/html\"\n    },\n    \"body\": \"<h1>Hello from Lambda!</h1>\"\n}"
                    },
                    "To configure an Application Load Balancer as a function trigger, grant Elastic Load Balancing permission to run the function, create a target    group that routes requests to the function, and add a rule to the load balancer that sends requests to the target    group.",
                    "Use the add-permission command to add a permission statement to your function's resource-based    policy.",
                    "aws lambda add-permission --function-name alb-function \\--statement-id load-balancer --action \"lambda:InvokeFunction\" \\--principal elasticloadbalancing.amazonaws.com",
                    "You should see the following output:",
                    "{    \"Statement\": \"{\\\"Sid\\\":\\\"load-balancer\\\",\\\"Effect\\\":\\\"Allow\\\",\\\"Principal\\\":{\\\"Service\\\":\\\"elasticloadbalancing.amazonaws.com\\\"},\\\"Action\\\":\\\"lambda:InvokeFunction\\\",\\\"Resource\\\":\\\"arn:aws:lambda:us-west-2:123456789012:function:alb-function\\\"}\"}",
                    "For instructions on configuring the Application Load Balancer listener and target group, see Lambda functions as a target in the      User Guide for Application Load Balancers."
                ]
            },
            {
                "title": "Invoke using an EventBridge Scheduler",
                "href": "https://docs.aws.amazon.com/lambda/latest/dg/with-eventbridge-scheduler.html",
                "sections": [
                    "",
                    "Amazon EventBridge Scheduler is a serverless scheduler that allows you to create, run, and manage tasks    from one central, managed service. With EventBridge Scheduler, you can create schedules using cron and rate expressions for recurring patterns, or configure one-time invocations. You can set    up flexible time windows for delivery, define retry limits, and set the maximum retention time for unprocessed events.",
                    "When you set up EventBridge Scheduler with Lambda, EventBridge Scheduler invokes your Lambda function asynchronously.      This page explains how to use EventBridge Scheduler to invoke a Lambda function on a schedule.",
                    {
                        "sub_header": "Set up the execution role",
                        "content": [
                            " When you create a new schedule, EventBridge Scheduler must have permission to invoke its target API operation on your behalf. You grant these permissions to EventBridge Scheduler        using an execution role. The permission policy you attach to your schedule's execution role defines the required permissions.        These permissions depend on the target API you want EventBridge Scheduler to invoke.",
                            "        When you use the EventBridge Scheduler console to create a schedule, as in the following procedure, EventBridge Scheduler automatically sets up an execution role based on your selected target.        If you want to create a schedule using one of the EventBridge Scheduler SDKs, the AWS CLI, or AWS CloudFormation, you must have an existing execution role that grants the permissions        EventBridge Scheduler requires to invoke a target. For more information about manually setting up an execution role for your schedule, see Setting up an execution role        in the EventBridge Scheduler User Guide.    "
                        ]
                    },
                    {
                        "sub_header": "Create a schedule",
                        "content": [
                            {
                                "code_example": "MyTestSchedule"
                            },
                            "To confirm that EventBridge Scheduler invoked the function, check the function's Amazon CloudWatch logs."
                        ]
                    },
                    {
                        "sub_header": "Related resources",
                        "content": [
                            "        For more information about EventBridge Scheduler, see the following:    "
                        ]
                    }
                ]
            },
            {
                "title": "IoT",
                "href": "https://docs.aws.amazon.com/lambda/latest/dg/services-iot.html",
                "sections": [
                    "",
                    "AWS IoT provides secure communication between internet-connected devices (such as sensors) and the AWS Cloud.    This makes it possible for you to collect, store, and analyze telemetry data from multiple devices.",
                    "You can create AWS IoT rules for your devices to interact with AWS services. The AWS IoT Rules Engine provides a SQL-based language to select data    from message payloads and send the data to other services, such as Amazon S3, Amazon DynamoDB, and AWS Lambda. You define a rule    to invoke a Lambda function when you want to invoke another AWS service or a third-party service. ",
                    "When an incoming IoT message triggers the rule, AWS IoT invokes your Lambda function asynchronously and passes data from the IoT message to the function. ",
                    "The following example shows a moisture reading from a greenhouse sensor. The row and pos values identify the location of the sensor. This example    event is based on the greenhouse type in the AWS IoT Rules tutorials. ",
                    {
                        "code_example": "\n{\n    \"row\" : \"10\",\n    \"pos\" : \"23\",\n    \"moisture\" : \"75\"\n}"
                    },
                    "For asynchronous invocation, Lambda queues the message and retries  if your function returns an error. Configure your function with a destination to retain  events that your function could not process.",
                    "You need to grant permission for the AWS IoT service to invoke your Lambda function. Use the      add-permission command to add a permission statement to your function's resource-based policy.",
                    "aws lambda add-permission --function-name my-function \\--statement-id iot-events --action \"lambda:InvokeFunction\" --principal iot.amazonaws.com",
                    "You should see the following output:",
                    "{    \"Statement\": \"{\\\"Sid\\\":\\\"iot-events\\\",\\\"Effect\\\":\\\"Allow\\\",\\\"Principal\\\":{\\\"Service\\\":\\\"iot.amazonaws.com\\\"},\\\"Action\\\":\\\"lambda:InvokeFunction\\\",\\\"Resource\\\":\\\"arn:aws:lambda:us-east-1:123456789012:function:my-function\\\"}\"} ",
                    "For more information about how to use Lambda with AWS IoT, see Creating an AWS Lambda rule.  "
                ]
            },
            {
                "title": "Kinesis Data Streams",
                "href": "https://docs.aws.amazon.com/lambda/latest/dg/with-kinesis.html",
                "contents": [
                    {
                        "title": "Create mapping",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/services-kinesis-create.html",
                        "sections": [
                            "",
                            "To process Amazon Kinesis Data Streams records with Lambda, create a consumer for your stream and then create a Lambda event source mapping.",
                            {
                                "sub_header": "Configuring your data stream and function",
                                "content": [
                                    "Your Lambda function is a consumer application for your data stream. It processes one batch of records at a      time from each shard. You can map a Lambda function to a shared-throughput consumer (standard iterator), or to a dedicated-throughput consumer with enhanced fan-out.",
                                    "  1.Standard iterator:  Lambda polls each shard in your Kinesis stream for records at a base rate of once per\n          second. When more records are available, Lambda keeps processing batches until the function catches up with the\n          stream. The event source mapping shares read throughput with other consumers of the shard.",
                                    "  2.Enhanced fan-out:  To minimize latency and maximize read throughput, create a data stream consumer with enhanced fan-out. Enhanced fan-out consumers get a dedicated connection to each shard that doesn't impact other applications reading from the stream. Stream consumers use HTTP/2 to reduce latency by pushing records to Lambda over a long-lived\n          connection and by compressing request headers. You can create a stream consumer with the Kinesis RegisterStreamConsumer API.",
                                    "aws kinesis register-stream-consumer \\--consumer-name con1 \\--stream-arn arn:aws:kinesis:us-east-2:123456789012:stream/lambda-stream",
                                    "You should see the following output:",
                                    "{    \"Consumer\": {        \"ConsumerName\": \"con1\",        \"ConsumerARN\": \"arn:aws:kinesis:us-east-2:123456789012:stream/lambda-stream/consumer/con1:1540591608\",        \"ConsumerStatus\": \"CREATING\",        \"ConsumerCreationTimestamp\": 1540591608.0    }}",
                                    "To increase the speed at which your function processes records, add shards to your data stream. Lambda      processes records in each shard in order. It stops processing additional records in a shard if your function      returns an error. With more shards, there are more batches being processed at once, which lowers the impact of      errors on concurrency.",
                                    "If your function can't scale up to handle the total number of concurrent batches, request a quota increase or reserve concurrency for your function."
                                ]
                            },
                            {
                                "sub_header": "Create an event source mapping to invoke a Lambda function",
                                "content": [
                                    "To invoke your Lambda function with records from your data stream, create an event source mapping.       You can create multiple event source mappings to process the same data with multiple Lambda functions, or to process items       from multiple data streams with a single function. When processing items from multiple streams, each batch contains records       from only a single shard or stream.",
                                    "You can configure event source mappings to process records from a stream in a different AWS account.       To learn more, see Creating a cross-account event source mapping.",
                                    "Before you create an event source mapping, you need to give your Lambda function permission to read from a Kinesis data stream.       Lambda needs the following permissions to manage resources related to your Kinesis data stream:",
                                    "The AWS managed policy AWSLambdaKinesisExecutionRole       includes these permissions. Add this managed policy to your function as described in the following procedure.",
                                    "AWS Management ConsoleTo add Kinesis permissions to your functionOpen the Functions page of the Lambda console           and select your function.In the Configuration tab, select Permissions.In the Execution role pane, under Role name, choose the link to           your function’s execution role. This link opens the page for that role in the IAM console.In the Permissions policies pane, choose Add permissions, then           select Attach policies.In the search field, enter AWSLambdaKinesisExecutionRole.Select the checkbox next to the policy and choose Add permission.AWS CLITo add Kinesis permissions to your functionRun the following CLI command to add the AWSLambdaKinesisExecutionRole policy to your function’s execution role:aws iam attach-role-policy \\--role-name MyFunctionRole \\--policy-arn arn:aws:iam::aws:policy/service-role/AWSLambdaKinesisExecutionRoleAWS SAMTo add Kinesis permissions to your functionIn your function’s definition, add the Policies property as shown in the following example:Resources:  MyFunction:    Type: AWS::Serverless::Function    Properties:      CodeUri: ./my-function/      Handler: index.handler      Runtime: nodejs22.x      Policies:        - AWSLambdaKinesisExecutionRoleanchoranchoranchorAWS Management ConsoleAWS CLIAWS SAMTo add Kinesis permissions to your functionOpen the Functions page of the Lambda console           and select your function.In the Configuration tab, select Permissions.In the Execution role pane, under Role name, choose the link to           your function’s execution role. This link opens the page for that role in the IAM console.In the Permissions policies pane, choose Add permissions, then           select Attach policies.In the search field, enter AWSLambdaKinesisExecutionRole.Select the checkbox next to the policy and choose Add permission.",
                                    "After configuring the required permissions, create the event source mapping.",
                                    "AWS Management ConsoleTo create the Kinesis event source mappingOpen the Functions page of the Lambda console                 and select your function.In the Function overview pane, choose Add trigger.Under Trigger configuration, for the source, select Kinesis.Select the Kinesis stream you want to create the event source mapping for and, optionally, a consumer of your stream.(Optional) edit the Batch size, Starting position, and Batch window                 for your event source mapping.Choose Add.When creating your event source mapping from the console, your IAM role must have the            kinesis:ListStreams and            kinesis:ListStreamConsumers permissions.AWS CLITo create the Kinesis event source mappingRun the following CLI command to create a Kinesis event source mapping. Choose your own batch size and starting                 position according to your use case.aws lambda create-event-source-mapping \\--function-name MyFunction \\--event-source-arn arn:aws:kinesis:us-east-2:123456789012:stream/lambda-stream \\--starting-position LATEST \\--batch-size 100To specify a batching window, add the --maximum-batching-window-in-seconds option. For more information about using this and other parameters, see create-event-source-mapping             in the AWS CLI Command Reference.AWS SAMTo create the Kinesis event source mappingIn your function’s definition, add the KinesisEvent property as shown in the following example:Resources:  MyFunction:    Type: AWS::Serverless::Function    Properties:      CodeUri: ./my-function/      Handler: index.handler      Runtime: nodejs22.x      Policies:        - AWSLambdaKinesisExecutionRole      Events:        KinesisEvent:          Type: Kinesis          Properties:            Stream: !GetAtt MyKinesisStream.Arn            StartingPosition: LATEST            BatchSize: 100  MyKinesisStream:    Type: AWS::Kinesis::Stream    Properties:      ShardCount: 1To learn more about creating an event source mapping for Kinesis Data Streams in AWS SAM, see Kinesis             in the AWS Serverless Application Model Developer Guide.anchoranchoranchorAWS Management ConsoleAWS CLIAWS SAMTo create the Kinesis event source mappingOpen the Functions page of the Lambda console                 and select your function.In the Function overview pane, choose Add trigger.Under Trigger configuration, for the source, select Kinesis.Select the Kinesis stream you want to create the event source mapping for and, optionally, a consumer of your stream.(Optional) edit the Batch size, Starting position, and Batch window                 for your event source mapping.Choose Add.When creating your event source mapping from the console, your IAM role must have the            kinesis:ListStreams and            kinesis:ListStreamConsumers permissions."
                                ]
                            },
                            {
                                "sub_header": "Polling and stream starting position",
                                "content": [
                                    "Be aware that stream polling during event source mapping creation and updates is eventually consistent.",
                                    "This behavior means that if you specify LATEST as the starting position for the stream, the event source mapping could     miss events during creation or updates. To ensure that no events are missed, specify the stream starting position as TRIM_HORIZON     or AT_TIMESTAMP."
                                ]
                            },
                            {
                                "sub_header": "Creating a cross-account event source mapping",
                                "content": [
                                    "Amazon Kinesis Data Streams supports resource-based policies.       Because of this, you can process data ingested into a stream in one AWS account with a Lambda function in another account.",
                                    "To create an event source mapping for your Lambda function using a Kinesis stream in a different AWS account, you must       configure the stream using a resource-based policy to give your Lambda function permission to read items. To learn how to       configure your stream to allow cross-account access, see Sharing access with cross-account AWS Lambda functions       in the Amazon Kinesis Streams Developer guide.",
                                    "Once you’ve configured your stream with a resource-based policy that gives your Lambda function the required       permissions, create the event source mapping using any of the methods described in the previous section.",
                                    "If you choose to create your event source mapping using the Lambda console, paste the ARN of your stream directly       into the input field. If you want to specify a consumer for your stream, pasting the ARN of the       consumer automatically populates the stream field."
                                ]
                            }
                        ]
                    },
                    {
                        "title": "Batch item failures",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/services-kinesis-batchfailurereporting.html",
                        "sections": [
                            "",
                            "When consuming and processing streaming data from an event source, by default Lambda checkpoints to the highest    sequence number of a batch only when the batch is a complete success. Lambda treats all other results as a complete    failure and retries processing the batch up to the retry limit. To allow for partial successes while processing    batches from a stream, turn on ReportBatchItemFailures. Allowing partial successes can help to reduce    the number of retries on a record, though it doesn’t entirely prevent the possibility of retries in a successful record.",
                            "To turn on ReportBatchItemFailures, include the enum value    ReportBatchItemFailures in the FunctionResponseTypes list. This list indicates    which response types are enabled for your function. You can configure this list when you create or update an event source mapping.",
                            {
                                "sub_header": "Report syntax",
                                "content": [
                                    "When configuring reporting on batch item failures, the StreamsEventResponse class is returned with a      list of batch item failures. You can use a StreamsEventResponse object to return the sequence number      of the first failed record in the batch. You can also create your own custom class using the correct response      syntax. The following JSON structure shows the required response syntax:",
                                    "{   \"batchItemFailures\": [         {            \"itemIdentifier\": \"<SequenceNumber>\"        }    ]}",
                                    {
                                        "code_example": "batchItemFailures"
                                    }
                                ]
                            },
                            {
                                "sub_header": "Success and failure conditions",
                                "content": [
                                    "Lambda treats a batch as a complete success if you return any of the following:",
                                    {
                                        "code_example": "batchItemFailure"
                                    },
                                    "Lambda treats a batch as a complete failure if you return any of the following:",
                                    {
                                        "code_example": "itemIdentifier"
                                    },
                                    "Lambda retries failures based on your retry strategy."
                                ]
                            },
                            {
                                "sub_header": "Bisecting a batch",
                                "content": [
                                    "If your invocation fails and BisectBatchOnFunctionError is turned on, the batch is bisected      regardless of your ReportBatchItemFailures setting.",
                                    "When a partial batch success response is received and both BisectBatchOnFunctionError and        ReportBatchItemFailures are turned on, the batch is bisected at the returned sequence number and      Lambda retries only the remaining records.",
                                    "Here are some examples of function code that return the list of failed message IDs in the batch:",
                                    ".NETAWS SDK for .NETNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Reporting Kinesis batch item failures with Lambda using .NET.// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.// SPDX-License-Identifier: Apache-2.0﻿using System.Text;using System.Text.Json.Serialization;using Amazon.Lambda.Core;using Amazon.Lambda.KinesisEvents;using AWS.Lambda.Powertools.Logging;// Assembly attribute to enable the Lambda function's JSON input to be converted into a .NET class.[assembly: LambdaSerializer(typeof(Amazon.Lambda.Serialization.SystemTextJson.DefaultLambdaJsonSerializer))]namespace KinesisIntegration;public class Function{    // Powertools Logger requires an environment variables against your function    // POWERTOOLS_SERVICE_NAME    [Logging(LogEvent = true)]    public async Task<StreamsEventResponse> FunctionHandler(KinesisEvent evnt, ILambdaContext context)    {        if (evnt.Records.Count == 0)        {            Logger.LogInformation(\"Empty Kinesis Event received\");            return new StreamsEventResponse();        }        foreach (var record in evnt.Records)        {            try            {                Logger.LogInformation($\"Processed Event with EventId: {record.EventId}\");                string data = await GetRecordDataAsync(record.Kinesis, context);                Logger.LogInformation($\"Data: {data}\");                // TODO: Do interesting work based on the new data            }            catch (Exception ex)            {                Logger.LogError($\"An error occurred {ex.Message}\");                /* Since we are working with streams, we can return the failed item immediately.                   Lambda will immediately begin to retry processing from this failed item onwards. */                return new StreamsEventResponse                {                    BatchItemFailures = new List<StreamsEventResponse.BatchItemFailure>                    {                        new StreamsEventResponse.BatchItemFailure { ItemIdentifier = record.Kinesis.SequenceNumber }                    }                };            }        }        Logger.LogInformation($\"Successfully processed {evnt.Records.Count} records.\");        return new StreamsEventResponse();    }    private async Task<string> GetRecordDataAsync(KinesisEvent.Record record, ILambdaContext context)    {        byte[] bytes = record.Data.ToArray();        string data = Encoding.UTF8.GetString(bytes);        await Task.CompletedTask; //Placeholder for actual async work        return data;    }}public class StreamsEventResponse{    [JsonPropertyName(\"batchItemFailures\")]    public IList<BatchItemFailure> BatchItemFailures { get; set; }    public class BatchItemFailure    {        [JsonPropertyName(\"itemIdentifier\")]        public string ItemIdentifier { get; set; }    }}GoSDK for Go V2Note        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Reporting Kinesis batch item failures with Lambda using Go.// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.// SPDX-License-Identifier: Apache-2.0package mainimport (\t\"context\"\t\"fmt\"\t\"github.com/aws/aws-lambda-go/events\"\t\"github.com/aws/aws-lambda-go/lambda\")func handler(ctx context.Context, kinesisEvent events.KinesisEvent) (map[string]interface{}, error) {\tbatchItemFailures := []map[string]interface{}{}\tfor _, record := range kinesisEvent.Records {\t\tcurRecordSequenceNumber := \"\"\t\t// Process your record\t\tif /* Your record processing condition here */ {\t\t\tcurRecordSequenceNumber = record.Kinesis.SequenceNumber\t\t}\t\t// Add a condition to check if the record processing failed\t\tif curRecordSequenceNumber != \"\" {\t\t\tbatchItemFailures = append(batchItemFailures, map[string]interface{}{\"itemIdentifier\": curRecordSequenceNumber})\t\t}\t}\tkinesisBatchResponse := map[string]interface{}{\t\t\"batchItemFailures\": batchItemFailures,\t}\treturn kinesisBatchResponse, nil}func main() {\tlambda.Start(handler)}JavaSDK for Java 2.xNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Reporting Kinesis batch item failures with Lambda using Java.// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.// SPDX-License-Identifier: Apache-2.0import com.amazonaws.services.lambda.runtime.Context;import com.amazonaws.services.lambda.runtime.RequestHandler;import com.amazonaws.services.lambda.runtime.events.KinesisEvent;import com.amazonaws.services.lambda.runtime.events.StreamsEventResponse;import java.io.Serializable;import java.util.ArrayList;import java.util.List;public class ProcessKinesisRecords implements RequestHandler<KinesisEvent, StreamsEventResponse> {    @Override    public StreamsEventResponse handleRequest(KinesisEvent input, Context context) {        List<StreamsEventResponse.BatchItemFailure> batchItemFailures = new ArrayList<>();        String curRecordSequenceNumber = \"\";        for (KinesisEvent.KinesisEventRecord kinesisEventRecord : input.getRecords()) {            try {                //Process your record                KinesisEvent.Record kinesisRecord = kinesisEventRecord.getKinesis();                curRecordSequenceNumber = kinesisRecord.getSequenceNumber();            } catch (Exception e) {                /* Since we are working with streams, we can return the failed item immediately.                   Lambda will immediately begin to retry processing from this failed item onwards. */                batchItemFailures.add(new StreamsEventResponse.BatchItemFailure(curRecordSequenceNumber));                return new StreamsEventResponse(batchItemFailures);            }        }              return new StreamsEventResponse(batchItemFailures);       }}JavaScriptSDK for JavaScript (v3)Note        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Reporting Kinesis batch item failures with Lambda using Javascript.// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.// SPDX-License-Identifier: Apache-2.0exports.handler = async (event, context) => {  for (const record of event.Records) {    try {      console.log(`Processed Kinesis Event - EventID: ${record.eventID}`);      const recordData = await getRecordDataAsync(record.kinesis);      console.log(`Record Data: ${recordData}`);      // TODO: Do interesting work based on the new data    } catch (err) {      console.error(`An error occurred ${err}`);      /* Since we are working with streams, we can return the failed item immediately.            Lambda will immediately begin to retry processing from this failed item onwards. */      return {        batchItemFailures: [{ itemIdentifier: record.kinesis.sequenceNumber }],      };    }  }  console.log(`Successfully processed ${event.Records.length} records.`);  return { batchItemFailures: [] };};async function getRecordDataAsync(payload) {  var data = Buffer.from(payload.data, \"base64\").toString(\"utf-8\");  await Promise.resolve(1); //Placeholder for actual async work  return data;}Reporting Kinesis batch item failures with Lambda using TypeScript.// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.// SPDX-License-Identifier: Apache-2.0import {  KinesisStreamEvent,  Context,  KinesisStreamHandler,  KinesisStreamRecordPayload,  KinesisStreamBatchResponse,} from \"aws-lambda\";import { Buffer } from \"buffer\";import { Logger } from \"@aws-lambda-powertools/logger\";const logger = new Logger({  logLevel: \"INFO\",  serviceName: \"kinesis-stream-handler-sample\",});export const functionHandler: KinesisStreamHandler = async (  event: KinesisStreamEvent,  context: Context): Promise<KinesisStreamBatchResponse> => {  for (const record of event.Records) {    try {      logger.info(`Processed Kinesis Event - EventID: ${record.eventID}`);      const recordData = await getRecordDataAsync(record.kinesis);      logger.info(`Record Data: ${recordData}`);      // TODO: Do interesting work based on the new data    } catch (err) {      logger.error(`An error occurred ${err}`);      /* Since we are working with streams, we can return the failed item immediately.            Lambda will immediately begin to retry processing from this failed item onwards. */      return {        batchItemFailures: [{ itemIdentifier: record.kinesis.sequenceNumber }],      };    }  }  logger.info(`Successfully processed ${event.Records.length} records.`);  return { batchItemFailures: [] };};async function getRecordDataAsync(  payload: KinesisStreamRecordPayload): Promise<string> {  var data = Buffer.from(payload.data, \"base64\").toString(\"utf-8\");  await Promise.resolve(1); //Placeholder for actual async work  return data;}PHPSDK for PHPNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Reporting Kinesis batch item failures with Lambda using PHP.// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.// SPDX-License-Identifier: Apache-2.0<?php# using bref/bref and bref/logger for simplicityuse Bref\\Context\\Context;use Bref\\Event\\Kinesis\\KinesisEvent;use Bref\\Event\\Handler as StdHandler;use Bref\\Logger\\StderrLogger;require __DIR__ . '/vendor/autoload.php';class Handler implements StdHandler{    private StderrLogger $logger;    public function __construct(StderrLogger $logger)    {        $this->logger = $logger;    }    /**     * @throws JsonException     * @throws \\Bref\\Event\\InvalidLambdaEvent     */    public function handle(mixed $event, Context $context): array    {        $kinesisEvent = new KinesisEvent($event);        $this->logger->info(\"Processing records\");        $records = $kinesisEvent->getRecords();        $failedRecords = [];        foreach ($records as $record) {            try {                $data = $record->getData();                $this->logger->info(json_encode($data));                // TODO: Do interesting work based on the new data            } catch (Exception $e) {                $this->logger->error($e->getMessage());                // failed processing the record                $failedRecords[] = $record->getSequenceNumber();            }        }        $totalRecords = count($records);        $this->logger->info(\"Successfully processed $totalRecords records\");        // change format for the response        $failures = array_map(            fn(string $sequenceNumber) => ['itemIdentifier' => $sequenceNumber],            $failedRecords        );        return [            'batchItemFailures' => $failures        ];    }}$logger = new StderrLogger();return new Handler($logger);PythonSDK for Python (Boto3)Note        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Reporting Kinesis batch item failures with Lambda using Python.# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.# SPDX-License-Identifier: Apache-2.0def handler(event, context):    records = event.get(\"Records\")    curRecordSequenceNumber = \"\"        for record in records:        try:            # Process your record            curRecordSequenceNumber = record[\"kinesis\"][\"sequenceNumber\"]        except Exception as e:            # Return failed record's sequence number            return {\"batchItemFailures\":[{\"itemIdentifier\": curRecordSequenceNumber}]}    return {\"batchItemFailures\":[]}RubySDK for RubyNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Reporting Kinesis batch item failures with Lambda using Ruby.# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.# SPDX-License-Identifier: Apache-2.0require 'aws-sdk'def lambda_handler(event:, context:)  batch_item_failures = []  event['Records'].each do |record|    begin      puts \"Processed Kinesis Event - EventID: #{record['eventID']}\"      record_data = get_record_data_async(record['kinesis'])      puts \"Record Data: #{record_data}\"      # TODO: Do interesting work based on the new data    rescue StandardError => err      puts \"An error occurred #{err}\"      # Since we are working with streams, we can return the failed item immediately.      # Lambda will immediately begin to retry processing from this failed item onwards.      return { batchItemFailures: [{ itemIdentifier: record['kinesis']['sequenceNumber'] }] }    end  end  puts \"Successfully processed #{event['Records'].length} records.\"  { batchItemFailures: batch_item_failures }enddef get_record_data_async(payload)  data = Base64.decode64(payload['data']).force_encoding('utf-8')  # Placeholder for actual async work  sleep(1)  dataendRustSDK for RustNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Reporting Kinesis batch item failures with Lambda using Rust.// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.// SPDX-License-Identifier: Apache-2.0use aws_lambda_events::{    event::kinesis::KinesisEvent,    kinesis::KinesisEventRecord,    streams::{KinesisBatchItemFailure, KinesisEventResponse},};use lambda_runtime::{run, service_fn, Error, LambdaEvent};async fn function_handler(event: LambdaEvent<KinesisEvent>) -> Result<KinesisEventResponse, Error> {    let mut response = KinesisEventResponse {        batch_item_failures: vec![],    };    if event.payload.records.is_empty() {        tracing::info!(\"No records found. Exiting.\");        return Ok(response);    }    for record in &event.payload.records {        tracing::info!(            \"EventId: {}\",            record.event_id.as_deref().unwrap_or_default()        );        let record_processing_result = process_record(record);        if record_processing_result.is_err() {            response.batch_item_failures.push(KinesisBatchItemFailure {                item_identifier: record.kinesis.sequence_number.clone(),            });            /* Since we are working with streams, we can return the failed item immediately.            Lambda will immediately begin to retry processing from this failed item onwards. */            return Ok(response);        }    }    tracing::info!(        \"Successfully processed {} records\",        event.payload.records.len()    );    Ok(response)}fn process_record(record: &KinesisEventRecord) -> Result<(), Error> {    let record_data = std::str::from_utf8(record.kinesis.data.as_slice());    if let Some(err) = record_data.err() {        tracing::error!(\"Error: {}\", err);        return Err(Error::from(err));    }    let record_data = record_data.unwrap_or_default();    // do something interesting with the data    tracing::info!(\"Data: {}\", record_data);    Ok(())}#[tokio::main]async fn main() -> Result<(), Error> {    tracing_subscriber::fmt()        .with_max_level(tracing::Level::INFO)        // disable printing the name of the module in every log line.        .with_target(false)        // disabling time is handy because CloudWatch will add the ingestion time.        .without_time()        .init();    run(service_fn(function_handler)).await}anchoranchoranchoranchoranchoranchoranchoranchor.NETGoJavaJavaScriptPHPPythonRubyRustAWS SDK for .NETNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Reporting Kinesis batch item failures with Lambda using .NET.// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.// SPDX-License-Identifier: Apache-2.0﻿using System.Text;using System.Text.Json.Serialization;using Amazon.Lambda.Core;using Amazon.Lambda.KinesisEvents;using AWS.Lambda.Powertools.Logging;// Assembly attribute to enable the Lambda function's JSON input to be converted into a .NET class.[assembly: LambdaSerializer(typeof(Amazon.Lambda.Serialization.SystemTextJson.DefaultLambdaJsonSerializer))]namespace KinesisIntegration;public class Function{    // Powertools Logger requires an environment variables against your function    // POWERTOOLS_SERVICE_NAME    [Logging(LogEvent = true)]    public async Task<StreamsEventResponse> FunctionHandler(KinesisEvent evnt, ILambdaContext context)    {        if (evnt.Records.Count == 0)        {            Logger.LogInformation(\"Empty Kinesis Event received\");            return new StreamsEventResponse();        }        foreach (var record in evnt.Records)        {            try            {                Logger.LogInformation($\"Processed Event with EventId: {record.EventId}\");                string data = await GetRecordDataAsync(record.Kinesis, context);                Logger.LogInformation($\"Data: {data}\");                // TODO: Do interesting work based on the new data            }            catch (Exception ex)            {                Logger.LogError($\"An error occurred {ex.Message}\");                /* Since we are working with streams, we can return the failed item immediately.                   Lambda will immediately begin to retry processing from this failed item onwards. */                return new StreamsEventResponse                {                    BatchItemFailures = new List<StreamsEventResponse.BatchItemFailure>                    {                        new StreamsEventResponse.BatchItemFailure { ItemIdentifier = record.Kinesis.SequenceNumber }                    }                };            }        }        Logger.LogInformation($\"Successfully processed {evnt.Records.Count} records.\");        return new StreamsEventResponse();    }    private async Task<string> GetRecordDataAsync(KinesisEvent.Record record, ILambdaContext context)    {        byte[] bytes = record.Data.ToArray();        string data = Encoding.UTF8.GetString(bytes);        await Task.CompletedTask; //Placeholder for actual async work        return data;    }}public class StreamsEventResponse{    [JsonPropertyName(\"batchItemFailures\")]    public IList<BatchItemFailure> BatchItemFailures { get; set; }    public class BatchItemFailure    {        [JsonPropertyName(\"itemIdentifier\")]        public string ItemIdentifier { get; set; }    }}"
                                ]
                            }
                        ]
                    },
                    {
                        "title": "Error handling",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/kinesis-on-failure-destination.html",
                        "sections": [
                            "",
                            "Error handling for Kinesis event source mappings depends on whether the error occurs before the function is invoked or during function invocation:",
                            "  1.Before invocation:  If a Lambda event source mapping is unable to invoke the function due to throttling or other issues, it retries until the records expire or exceed the maximum age configured on the event source mapping (MaximumRecordAgeInSeconds).",
                            "  2.During invocation:  If the function is invoked but returns an error, Lambda retries until the records expire, exceed the maximum age (MaximumRecordAgeInSeconds), or reach the configured retry quota (MaximumRetryAttempts). For function errors, you can also configure BisectBatchOnFunctionError, which splits a failed batch into two smaller batches, isolating bad records and avoiding timeouts. Splitting batches doesn't consume the retry quota.",
                            "If the error handling measures fail, Lambda discards the records and continues processing  batches from the stream. With the default settings, this means that a bad record can block processing on the affected  shard for up to one week. To avoid this, configure your function's event source mapping with a reasonable  number of retries and a maximum record age that fits your use case.",
                            {
                                "sub_header": "Configuring destinations for failed invocations",
                                "content": [
                                    "To retain records of failed event source mapping invocations, add a destination to your function's event source mapping. Each record sent to the destination is a JSON document containing metadata about the failed invocation. For Amazon S3 destinations, Lambda also sends the entire invocation record along with the metadata. You can configure any Amazon SNS topic, Amazon SQS queue, or S3 bucket as a destination.",
                                    "With Amazon S3 destinations, you can use the Amazon S3 Event Notifications feature to receive notifications when objects are uploaded to your destination S3 bucket. You can also configure S3 Event Notifications to invoke another Lambda function to perform automated processing on failed batches.",
                                    "Your execution role must have permissions for the destination:",
                                    "  1.For SQS destinations:  sqs:SendMessage",
                                    "  2.For SNS destinations:  sns:Publish",
                                    "  3.For S3 bucket destinations:   s3:PutObject and s3:ListBucket",
                                    "If you've enabled encryption with your own KMS key for an S3 destination, your function's execution role must also have permission to call             kms:GenerateDataKey.            If the KMS key and S3 bucket destination are in a different account from your Lambda function            and execution role, configure the KMS key to trust the execution role to allow            kms:GenerateDataKey.",
                                    "To configure an on-failure destination using the console, follow these steps:",
                                    "Open the Functions page of the Lambda console.\nChoose a function.\n\nUnder Function overview, choose Add destination.\n\nFor Source, choose Event source mapping invocation.\n\nFor Event source mapping, choose an event source that's configured\n              for this function.\n\nFor Condition, select On failure. For event\n              source mapping invocations, this is the only accepted condition.\n\nFor Destination type, choose the destination type that Lambda sends\n              invocation records to.\n\nFor Destination, choose a resource.\n\nChoose Save.\n",
                                    "You can also configure an on-failure destination using the AWS Command Line Interface (AWS CLI). For example, the following          create-event-source-mapping command adds an event source mapping with an SQS on-failure destination to          MyFunction:",
                                    "aws lambda create-event-source-mapping \\--function-name \"MyFunction\" \\--event-source-arn arn:aws:kinesis:us-east-2:123456789012:stream/lambda-stream \\--destination-config '{\"OnFailure\": {\"Destination\": \"arn:aws:sqs:us-east-1:123456789012:dest-queue\"}}'",
                                    "The following update-event-source-mapping command updates an event source mapping to send failed invocation records to an SNS destination after two retry attempts, or if the records are more than an hour old.",
                                    "aws lambda update-event-source-mapping \\--uuid f89f8514-cdd9-4602-9e1f-01a5b77d449b \\--maximum-retry-attempts 2 \\--maximum-record-age-in-seconds 3600 \\--destination-config '{\"OnFailure\": {\"Destination\": \"arn:aws:sns:us-east-1:123456789012:dest-topic\"}}'",
                                    "Updated settings are applied asynchronously and aren't reflected in the output until the process completes. Use    the get-event-source-mapping command to view the current status.",
                                    "To remove a destination, supply an empty string as the argument to the          destination-config parameter:",
                                    "aws lambda update-event-source-mapping \\--uuid f89f8514-cdd9-4602-9e1f-01a5b77d449b \\--destination-config '{\"OnFailure\": {\"Destination\": \"\"}}'",
                                    {
                                        "sub_header": "Security best practices for Amazon S3 destinations",
                                        "content": [
                                            "Deleting an S3 bucket that's configured as a destination without removing the destination from your function's configuration can create a security risk. If another       user knows your destination bucket's name, they can recreate the bucket in their AWS account. Records of failed invocations will be sent to their bucket, potentially       exposing data from your function.",
                                            {
                                                "code_example": "s3:PutObject"
                                            },
                                            "The following example shows an IAM policy that limits your function's s3:PutObject permissions to buckets in your account. This policy also gives Lambda        the s3:ListBucket permission it needs to use an S3 bucket as a destination.",
                                            "{    \"Version\": \"2012-10-17\",    \"Statement\": [        {            \"Sid\": \"S3BucketResourceAccountWrite\",            \"Effect\": \"Allow\",            \"Action\": [                \"s3:PutObject\",                \"s3:ListBucket\"            ],            \"Resource\": \"arn:aws:s3:::*/*\",            \"Condition\": {                \"StringEquals\": {                    \"s3:ResourceAccount\": \"111122223333\"                }            }        }    ]}",
                                            "To add a permissions policy to your funcion's execution role using the AWS Management Console or AWS CLI, refer to the instructions in the following procedures:",
                                            "ConsoleTo add a permissions policy to a function's execution role (console)Open the Functions page of the Lambda console.Select the Lambda function whose execution role you want to modify.In the Configuration tab, select Permissions.In the Execution role tab, select your function's Role name to open the role's IAM console page.Add a permissions policy to the role by doing the following:In the Permissions policies pane, choose Add permissions and select Create inline policy.In Policy editor, select JSON.Paste the policy you want to add into the editor (replacing the existing JSON), and then choose Next.Under Policy details, enter a Policy name.Choose Create policy.AWS CLITo add a permissions policy to a function's execution role (CLI)Create a JSON policy document with the required permissions and save it in a local directory.Use the IAM put-role-policy CLI command to add the permissions to your function's execution role. Run the following command from the                 directory you saved your JSON policy document in and replace the role name, policy name, and policy document with your own values.aws iam put-role-policy \\--role-name my_lambda_role \\--policy-name LambdaS3DestinationPolicy \\--policy-document file://my_policy.jsonanchoranchorConsoleAWS CLITo add a permissions policy to a function's execution role (console)Open the Functions page of the Lambda console.Select the Lambda function whose execution role you want to modify.In the Configuration tab, select Permissions.In the Execution role tab, select your function's Role name to open the role's IAM console page.Add a permissions policy to the role by doing the following:In the Permissions policies pane, choose Add permissions and select Create inline policy.In Policy editor, select JSON.Paste the policy you want to add into the editor (replacing the existing JSON), and then choose Next.Under Policy details, enter a Policy name.Choose Create policy."
                                        ]
                                    },
                                    {
                                        "sub_header": "Example Amazon SNS and Amazon SQS invocation record",
                                        "content": [
                                            "The following example shows what Lambda sends to an SQS queue or SNS topic for a failed Kinesis      event source invocation. Because Lambda sends only the metadata for these destination types, use the      streamArn, shardId, startSequenceNumber, and      endSequenceNumber fields to obtain the full original record. All of the fields shown in the       KinesisBatchInfo property will always be present.",
                                            "{    \"requestContext\": {        \"requestId\": \"c9b8fa9f-5a7f-xmpl-af9c-0c604cde93a5\",        \"functionArn\": \"arn:aws:lambda:us-east-2:123456789012:function:myfunction\",        \"condition\": \"RetryAttemptsExhausted\",        \"approximateInvokeCount\": 1    },    \"responseContext\": {        \"statusCode\": 200,        \"executedVersion\": \"$LATEST\",        \"functionError\": \"Unhandled\"    },    \"version\": \"1.0\",    \"timestamp\": \"2019-11-14T00:38:06.021Z\",    \"KinesisBatchInfo\": {        \"shardId\": \"shardId-000000000001\",        \"startSequenceNumber\": \"49601189658422359378836298521827638475320189012309704722\",        \"endSequenceNumber\": \"49601189658422359378836298522902373528957594348623495186\",        \"approximateArrivalOfFirstRecord\": \"2019-11-14T00:38:04.835Z\",        \"approximateArrivalOfLastRecord\": \"2019-11-14T00:38:05.580Z\",        \"batchSize\": 500,        \"streamArn\": \"arn:aws:kinesis:us-east-2:123456789012:stream/mystream\"    }}",
                                            "You can use this information to retrieve the affected records from the stream for  troubleshooting. The actual records aren't included, so you must process this record and retrieve them from the  stream before they expire and are lost."
                                        ]
                                    },
                                    {
                                        "sub_header": "Example Amazon S3 invocation record",
                                        "content": [
                                            "The following example shows what Lambda sends to an Amazon S3 bucket for a failed Kinesis      event source invocation. In addition to all of the fields from the previous example for SQS and SNS destinations, the payload field       contains the original invocation record as an escaped JSON string.",
                                            "{    \"requestContext\": {        \"requestId\": \"c9b8fa9f-5a7f-xmpl-af9c-0c604cde93a5\",        \"functionArn\": \"arn:aws:lambda:us-east-2:123456789012:function:myfunction\",        \"condition\": \"RetryAttemptsExhausted\",        \"approximateInvokeCount\": 1    },    \"responseContext\": {        \"statusCode\": 200,        \"executedVersion\": \"$LATEST\",        \"functionError\": \"Unhandled\"    },    \"version\": \"1.0\",    \"timestamp\": \"2019-11-14T00:38:06.021Z\",    \"KinesisBatchInfo\": {        \"shardId\": \"shardId-000000000001\",        \"startSequenceNumber\": \"49601189658422359378836298521827638475320189012309704722\",        \"endSequenceNumber\": \"49601189658422359378836298522902373528957594348623495186\",        \"approximateArrivalOfFirstRecord\": \"2019-11-14T00:38:04.835Z\",        \"approximateArrivalOfLastRecord\": \"2019-11-14T00:38:05.580Z\",        \"batchSize\": 500,        \"streamArn\": \"arn:aws:kinesis:us-east-2:123456789012:stream/mystream\"    },    \"payload\": \"<Whole Event>\" // Only available in S3}",
                                            "The S3 object containing the invocation record uses the following naming convention:",
                                            "aws/lambda/<ESM-UUID>/<shardID>/YYYY/MM/DD/YYYY-MM-DDTHH.MM.SS-<Random UUID>"
                                        ]
                                    }
                                ]
                            }
                        ]
                    },
                    {
                        "title": "Stateful processing",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/services-kinesis-windows.html",
                        "sections": [
                            "",
                            "Lambda functions can run continuous stream processing applications. A stream represents unbounded data that flows    continuously through your application. To analyze information from this continuously updating input, you can bound    the included records using a window defined in terms of time.",
                            "Tumbling windows are distinct time windows that open and close at regular intervals. By default, Lambda invocations    are stateless—you cannot use them for processing data across multiple continuous invocations without an external database.    However, with tumbling windows, you can maintain your state across invocations. This state contains the aggregate result    of the messages previously processed for the current window. Your state can be a maximum of 1 MB per shard. If it exceeds    that size, Lambda terminates the window early.",
                            "Each record in a stream belongs to a specific window. Lambda will process each record at least once, but doesn't guarantee that each record will be processed only once. In rare cases, such as error handling, some records might be processed more than once. Records are always processed in order the first time. If records are processed more than once, they might be processed out of order.",
                            {
                                "sub_header": "Aggregation and processing",
                                "content": [
                                    "Your user managed function is invoked both for aggregation and for processing the final results of that      aggregation. Lambda aggregates all records received in the window. You can receive these records in multiple      batches, each as a separate invocation. Each invocation receives a state. Thus, when using tumbling windows,      your Lambda function response must contain a state property. If the response does not contain a      state property, Lambda considers this a failed invocation. To satisfy this condition, your function      can return a TimeWindowEventResponse object, which has the following JSON shape:",
                                    {
                                        "code_example": "TimeWindowEventResponse"
                                    },
                                    {
                                        "code_example": "Map<String, String>"
                                    },
                                    "At the end of the window, the flag isFinalInvokeForWindow is set to true to indicate      that this is the final state and that it’s ready for processing. After processing, the window completes and your      final invocation completes, and then the state is dropped.",
                                    "At the end of your window, Lambda uses final processing for actions on the aggregation results. Your final      processing is synchronously invoked. After successful invocation, your function checkpoints the sequence number      and stream processing continues. If invocation is unsuccessful, your Lambda function suspends further processing      until a successful invocation.",
                                    {
                                        "code_example": "\n{\n    \"Records\": [\n        {\n            \"kinesis\": {\n                \"kinesisSchemaVersion\": \"1.0\",\n                \"partitionKey\": \"1\",\n                \"sequenceNumber\": \"49590338271490256608559692538361571095921575989136588898\",\n                \"data\": \"SGVsbG8sIHRoaXMgaXMgYSB0ZXN0Lg==\",\n                \"approximateArrivalTimestamp\": 1607497475.000\n            },\n            \"eventSource\": \"aws:kinesis\",\n            \"eventVersion\": \"1.0\",\n            \"eventID\": \"shardId-000000000006:49590338271490256608559692538361571095921575989136588898\",\n            \"eventName\": \"aws:kinesis:record\",\n            \"invokeIdentityArn\": \"arn:aws:iam::123456789012:role/lambda-kinesis-role\",\n            \"awsRegion\": \"us-east-1\",\n            \"eventSourceARN\": \"arn:aws:kinesis:us-east-1:123456789012:stream/lambda-stream\"\n        }\n    ],\n    \"window\": {\n        \"start\": \"2020-12-09T07:04:00Z\",\n        \"end\": \"2020-12-09T07:06:00Z\"\n    },\n    \"state\": {\n        \"1\": 282,\n        \"2\": 715\n    },\n    \"shardId\": \"shardId-000000000006\",\n    \"eventSourceARN\": \"arn:aws:kinesis:us-east-1:123456789012:stream/lambda-stream\",\n    \"isFinalInvokeForWindow\": false,\n    \"isWindowTerminatedEarly\": false\n}\n"
                                    }
                                ]
                            },
                            {
                                "sub_header": "Configuration",
                                "content": [
                                    "You can configure tumbling windows when you create or update an event source mapping. To configure a tumbling window, specify the window in seconds (TumblingWindowInSeconds). The following            example AWS Command Line Interface (AWS CLI) command creates a streaming event source mapping that has a tumbling window of 120            seconds. The Lambda function defined for aggregation and processing is named            tumbling-window-example-function.",
                                    "aws lambda create-event-source-mapping \\--event-source-arn arn:aws:kinesis:us-east-1:123456789012:stream/lambda-stream \\--function-name tumbling-window-example-function \\--starting-position TRIM_HORIZON \\--tumbling-window-in-seconds 120",
                                    "Lambda determines tumbling window boundaries based on the time when records were inserted into the stream. All            records have an approximate timestamp available that Lambda uses in boundary determinations.",
                                    "Tumbling window aggregations do not support resharding. When a shard ends, Lambda considers the current window to be closed, and any child shards will start their own window in a fresh state. When no new records are being added to the current window, Lambda waits for up to 2 minutes before assuming that the window is over. This helps ensure that the function reads all records in the current window, even if the records are added intermittently.",
                                    "Tumbling windows fully support the existing retry policies maxRetryAttempts and            maxRecordAge.",
                                    {
                                        "code_example": "def lambda_handler(event, context):\n    print('Incoming event: ', event)\n    print('Incoming state: ', event['state'])\n\n#Check if this is the end of the window to either aggregate or process.\n    if event['isFinalInvokeForWindow']:\n        # logic to handle final state of the window\n        print('Destination invoke')\n    else:\n        print('Aggregate invoke')\n\n#Check for early terminations\n    if event['isWindowTerminatedEarly']:\n        print('Window terminated early')\n\n    #Aggregation logic\n    state = event['state']\n    for record in event['Records']:\n        state[record['kinesis']['partitionKey']] = state.get(record['kinesis']['partitionKey'], 0) + 1\n\n    print('Returning state: ', state)\n    return {'state': state}"
                                    }
                                ]
                            }
                        ]
                    },
                    {
                        "title": "Parameters",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/services-kinesis-parameters.html",
                        "sections": [
                            "",
                            "All Lambda event source mappings share the same CreateEventSourceMapping and UpdateEventSourceMapping        API operations. However, only some of the parameters apply to Kinesis.",
                            {
                                "code_example": "ReportBatchItemFailures"
                            }
                        ]
                    },
                    {
                        "title": "Event filtering",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/with-kinesis-filtering.html",
                        "sections": [
                            "",
                            "You can use event filtering to control which records from a stream or queue Lambda sends to your function.    For general information about how event filtering works, see Control which events Lambda sends to your function.",
                            "This section focuses on event filtering for Kinesis event sources.",
                            "  1.Kinesis event filtering basics",
                            "  2.Filtering Kinesis aggregated records",
                            {
                                "sub_header": "Kinesis event filtering basics",
                                "content": [
                                    "Suppose a producer is putting JSON formatted data into your Kinesis data stream. An example record would look like the following, with the             JSON data converted to a Base64 encoded string in the data field.",
                                    "{    \"kinesis\": {        \"kinesisSchemaVersion\": \"1.0\",        \"partitionKey\": \"1\",        \"sequenceNumber\": \"49590338271490256608559692538361571095921575989136588898\",        \"data\": \"eyJSZWNvcmROdW1iZXIiOiAiMDAwMSIsICJUaW1lU3RhbXAiOiAieXl5eS1tbS1kZFRoaDptbTpzcyIsICJSZXF1ZXN0Q29kZSI6ICJBQUFBIn0=\",        \"approximateArrivalTimestamp\": 1545084650.987        },    \"eventSource\": \"aws:kinesis\",    \"eventVersion\": \"1.0\",    \"eventID\": \"shardId-000000000006:49590338271490256608559692538361571095921575989136588898\",    \"eventName\": \"aws:kinesis:record\",    \"invokeIdentityArn\": \"arn:aws:iam::123456789012:role/lambda-role\",    \"awsRegion\": \"us-east-2\",    \"eventSourceARN\": \"arn:aws:kinesis:us-east-2:123456789012:stream/lambda-stream\"}",
                                    "As long as the data the producer puts into the stream is valid JSON, you can use event filtering to filter records using the data             key. Suppose a producer is putting records into your Kinesis stream in the following JSON format.",
                                    "{    \"record\": 12345,    \"order\": {        \"type\": \"buy\",        \"stock\": \"ANYCO\",        \"quantity\": 1000        }}",
                                    "To filter only those records where the order type is “buy,” the FilterCriteria object would be as follows.",
                                    "{    \"Filters\": [        {            \"Pattern\": \"{ \\\"data\\\" : { \\\"order\\\" : { \\\"type\\\" : [ \\\"buy\\\" ] } } }\"        }    ]}",
                                    "For added clarity, here is the value of the filter's Pattern expanded in plain JSON. ",
                                    "{    \"data\": {        \"order\": {            \"type\": [ \"buy\" ]            }      }}",
                                    "You can add your filter using the console, AWS CLI or an AWS SAM template.",
                                    "ConsoleTo add this filter using the console, follow the instructions in Attaching filter criteria to an event source mapping (console) and enter the following                         string for the Filter criteria.{ \"data\" : { \"order\" : { \"type\" : [ \"buy\" ] } } }AWS CLITo create a new event source mapping with these filter criteria using the AWS Command Line Interface (AWS CLI), run the following                        command.aws lambda create-event-source-mapping \\    --function-name my-function \\    --event-source-arn arn:aws:kinesis:us-east-2:123456789012:stream/my-stream \\    --filter-criteria '{\"Filters\": [{\"Pattern\": \"{ \\\"data\\\" : { \\\"order\\\" : { \\\"type\\\" : [ \\\"buy\\\" ] } } }\"}]}'To add these filter criteria to an existing event source mapping, run the following command.aws lambda update-event-source-mapping \\    --uuid \"a1b2c3d4-5678-90ab-cdef-11111EXAMPLE\" \\    --filter-criteria '{\"Filters\": [{\"Pattern\": \"{ \\\"data\\\" : { \\\"order\\\" : { \\\"type\\\" : [ \\\"buy\\\" ] } } }\"}]}'AWS SAMTo add this filter using AWS SAM, add the following snippet to the YAML template for your event source.FilterCriteria:  Filters:    - Pattern: '{ \"data\" : { \"order\" : { \"type\" : [ \"buy\" ] } } }'anchoranchoranchorConsoleAWS CLIAWS SAMTo add this filter using the console, follow the instructions in Attaching filter criteria to an event source mapping (console) and enter the following                         string for the Filter criteria.{ \"data\" : { \"order\" : { \"type\" : [ \"buy\" ] } } }",
                                    "To properly filter events from Kinesis sources, both the data field and your filter criteria for the data field must be in valid JSON format.             If either field isn't in a valid JSON format, Lambda drops the message or throws an exception. The following table summarizes the specific behavior: ",
                                    "\n\nIncoming data format\nFilter pattern format for data properties\nResulting action\n\n\n\n\nValid JSON\n\n\nValid JSON\n\n\nLambda filters based on your filter criteria.\n\n\n\n\nValid JSON\n\n\nNo filter pattern for data properties\n\n\nLambda filters (on the other metadata properties only) based on your filter criteria.\n\n\n\n\nValid JSON\n\n\nNon-JSON\n\n\nLambda throws an exception at the time of the event source mapping creation or update. The filter pattern\n                                for data properties must be in a valid JSON format.\n\n\n\n\nNon-JSON\n\n\nValid JSON\n\n\nLambda drops the record.\n\n\n\n\nNon-JSON\n\n\nNo filter pattern for data properties\n\n\nLambda filters (on the other metadata properties only) based on your filter criteria.\n\n\n\n\nNon-JSON\n\n\nNon-JSON\n\n\nLambda throws an exception at the time of the event source mapping creation or update. The filter pattern\n                                for data properties must be in a valid JSON format.\n\n\n"
                                ]
                            },
                            {
                                "sub_header": "Filtering Kinesis aggregated records",
                                "content": [
                                    "With Kinesis, you can aggregate multiple records into a single Kinesis Data Streams record to increase your data throughput. Lambda can only apply             filter criteria to aggregated records when you use Kinesis enhanced fan-out.             Filtering aggregated records with standard Kinesis isn't supported. When using enhanced fan-out, you configure a Kinesis dedicated-throughput consumer             to act as the trigger for your Lambda function. Lambda then filters the aggregated records and passes only those records that meet your filter criteria.",
                                    "To learn more about Kinesis record aggregation, refer to the Aggregation             section on the Kinesis Producer Library (KPL) Key Concepts page. To Learn more about using Lambda with Kinesis enhanced fan-out, see             Increasing real-time stream processing performance with Amazon Kinesis Data Streams enhanced fan-out and AWS Lambda             on the AWS compute blog."
                                ]
                            }
                        ]
                    },
                    {
                        "title": "Tutorial",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/with-kinesis-example.html",
                        "sections": [
                            "",
                            "In this tutorial, you create a Lambda function to consume events from a Amazon Kinesis data stream. ",
                            "\n\nCustom app writes records to the stream.\n\nAWS Lambda polls the stream and, when it detects new records in the stream, invokes your Lambda\n        function.\n\nAWS Lambda runs the Lambda function by assuming the execution role you specified at the time you created\n        the Lambda function.\n",
                            {
                                "sub_header": "Prerequisites",
                                "content": [
                                    "This tutorial assumes that you have some knowledge of basic Lambda operations and the Lambda console. If you      haven't already, follow the instructions in Create a Lambda function with the console to create your first Lambda function.",
                                    "To complete the following steps, you need the AWS CLI version 2. Commands and the expected output are listed in separate blocks:",
                                    "aws --version",
                                    "You should see the following output:",
                                    "aws-cli/2.13.27 Python/3.11.6 Linux/4.14.328-248.540.amzn2.x86_64 exe/x86_64.amzn.2",
                                    "For long commands, an escape character (\\) is used to split a command over multiple lines.",
                                    "On Linux and macOS, use your preferred shell and package manager.",
                                    {
                                        "code_example": "zip"
                                    }
                                ]
                            },
                            {
                                "sub_header": "Create the execution role",
                                "content": [
                                    "Create the execution role that gives your function      permission to access AWS resources.",
                                    {
                                        "code_example": "lambda-kinesis-role"
                                    },
                                    "The AWSLambdaKinesisExecutionRole policy has the permissions that the function needs to      read items from Kinesis and write logs to CloudWatch Logs."
                                ]
                            },
                            {
                                "sub_header": "Create the function",
                                "content": [
                                    "Create a Lambda function that processes your Kinesis messages. The function code logs the event ID      and event data of the Kinesis record to CloudWatch Logs.",
                                    "This tutorial uses the Node.js 18.x runtime, but we've also provided example code in other runtime      languages. You can select the tab in the following box to see code for the runtime you're interested in.      The JavaScript code you'll use in this step is in the first example shown in the      JavaScript tab.",
                                    ".NETAWS SDK for .NETNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Consuming a Kinesis event with Lambda using .NET.// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.// SPDX-License-Identifier: Apache-2.0﻿using System.Text;using Amazon.Lambda.Core;using Amazon.Lambda.KinesisEvents;using AWS.Lambda.Powertools.Logging;// Assembly attribute to enable the Lambda function's JSON input to be converted into a .NET class.[assembly: LambdaSerializer(typeof(Amazon.Lambda.Serialization.SystemTextJson.DefaultLambdaJsonSerializer))]namespace KinesisIntegrationSampleCode;public class Function{    // Powertools Logger requires an environment variables against your function    // POWERTOOLS_SERVICE_NAME    [Logging(LogEvent = true)]    public async Task FunctionHandler(KinesisEvent evnt, ILambdaContext context)    {        if (evnt.Records.Count == 0)        {            Logger.LogInformation(\"Empty Kinesis Event received\");            return;        }        foreach (var record in evnt.Records)        {            try            {                Logger.LogInformation($\"Processed Event with EventId: {record.EventId}\");                string data = await GetRecordDataAsync(record.Kinesis, context);                Logger.LogInformation($\"Data: {data}\");                // TODO: Do interesting work based on the new data            }            catch (Exception ex)            {                Logger.LogError($\"An error occurred {ex.Message}\");                throw;            }        }        Logger.LogInformation($\"Successfully processed {evnt.Records.Count} records.\");    }    private async Task<string> GetRecordDataAsync(KinesisEvent.Record record, ILambdaContext context)    {        byte[] bytes = record.Data.ToArray();        string data = Encoding.UTF8.GetString(bytes);        await Task.CompletedTask; //Placeholder for actual async work        return data;    }}GoSDK for Go V2Note        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Consuming a Kinesis event with Lambda using Go.// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.// SPDX-License-Identifier: Apache-2.0package mainimport (\t\"context\"\t\"log\"\t\"github.com/aws/aws-lambda-go/events\"\t\"github.com/aws/aws-lambda-go/lambda\")func handler(ctx context.Context, kinesisEvent events.KinesisEvent) error {\tif len(kinesisEvent.Records) == 0 {\t\tlog.Printf(\"empty Kinesis event received\")\t\treturn nil\t}\tfor _, record := range kinesisEvent.Records {\t\tlog.Printf(\"processed Kinesis event with EventId: %v\", record.EventID)\t\trecordDataBytes := record.Kinesis.Data\t\trecordDataText := string(recordDataBytes)\t\tlog.Printf(\"record data: %v\", recordDataText)\t\t// TODO: Do interesting work based on the new data\t}\tlog.Printf(\"successfully processed %v records\", len(kinesisEvent.Records))\treturn nil}func main() {\tlambda.Start(handler)}JavaSDK for Java 2.xNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Consuming a Kinesis event with Lambda using Java.// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.// SPDX-License-Identifier: Apache-2.0package example;import com.amazonaws.services.lambda.runtime.Context;import com.amazonaws.services.lambda.runtime.LambdaLogger;import com.amazonaws.services.lambda.runtime.RequestHandler;import com.amazonaws.services.lambda.runtime.events.KinesisEvent;public class Handler implements RequestHandler<KinesisEvent, Void> {    @Override    public Void handleRequest(final KinesisEvent event, final Context context) {        LambdaLogger logger = context.getLogger();        if (event.getRecords().isEmpty()) {            logger.log(\"Empty Kinesis Event received\");            return null;        }        for (KinesisEvent.KinesisEventRecord record : event.getRecords()) {            try {                logger.log(\"Processed Event with EventId: \"+record.getEventID());                String data = new String(record.getKinesis().getData().array());                logger.log(\"Data:\"+ data);                // TODO: Do interesting work based on the new data            }            catch (Exception ex) {                logger.log(\"An error occurred:\"+ex.getMessage());                throw ex;            }        }        logger.log(\"Successfully processed:\"+event.getRecords().size()+\" records\");        return null;    }}JavaScriptSDK for JavaScript (v3)Note        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Consuming a Kinesis event with Lambda using JavaScript.// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.// SPDX-License-Identifier: Apache-2.0exports.handler = async (event, context) => {  for (const record of event.Records) {    try {      console.log(`Processed Kinesis Event - EventID: ${record.eventID}`);      const recordData = await getRecordDataAsync(record.kinesis);      console.log(`Record Data: ${recordData}`);      // TODO: Do interesting work based on the new data    } catch (err) {      console.error(`An error occurred ${err}`);      throw err;    }  }  console.log(`Successfully processed ${event.Records.length} records.`);};async function getRecordDataAsync(payload) {  var data = Buffer.from(payload.data, \"base64\").toString(\"utf-8\");  await Promise.resolve(1); //Placeholder for actual async work  return data;}Consuming a Kinesis event with Lambda using TypeScript.// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.// SPDX-License-Identifier: Apache-2.0import {  KinesisStreamEvent,  Context,  KinesisStreamHandler,  KinesisStreamRecordPayload,} from \"aws-lambda\";import { Buffer } from \"buffer\";import { Logger } from \"@aws-lambda-powertools/logger\";const logger = new Logger({  logLevel: \"INFO\",  serviceName: \"kinesis-stream-handler-sample\",});export const functionHandler: KinesisStreamHandler = async (  event: KinesisStreamEvent,  context: Context): Promise<void> => {  for (const record of event.Records) {    try {      logger.info(`Processed Kinesis Event - EventID: ${record.eventID}`);      const recordData = await getRecordDataAsync(record.kinesis);      logger.info(`Record Data: ${recordData}`);      // TODO: Do interesting work based on the new data    } catch (err) {      logger.error(`An error occurred ${err}`);      throw err;    }    logger.info(`Successfully processed ${event.Records.length} records.`);  }};async function getRecordDataAsync(  payload: KinesisStreamRecordPayload): Promise<string> {  var data = Buffer.from(payload.data, \"base64\").toString(\"utf-8\");  await Promise.resolve(1); //Placeholder for actual async work  return data;}PHPSDK for PHPNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Consuming an Kinesis event with Lambda using PHP.// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.// SPDX-License-Identifier: Apache-2.0<?php# using bref/bref and bref/logger for simplicityuse Bref\\Context\\Context;use Bref\\Event\\Kinesis\\KinesisEvent;use Bref\\Event\\Kinesis\\KinesisHandler;use Bref\\Logger\\StderrLogger;require __DIR__ . '/vendor/autoload.php';class Handler extends KinesisHandler{    private StderrLogger $logger;    public function __construct(StderrLogger $logger)    {        $this->logger = $logger;    }    /**     * @throws JsonException     * @throws \\Bref\\Event\\InvalidLambdaEvent     */    public function handleKinesis(KinesisEvent $event, Context $context): void    {        $this->logger->info(\"Processing records\");        $records = $event->getRecords();        foreach ($records as $record) {            $data = $record->getData();            $this->logger->info(json_encode($data));            // TODO: Do interesting work based on the new data            // Any exception thrown will be logged and the invocation will be marked as failed        }        $totalRecords = count($records);        $this->logger->info(\"Successfully processed $totalRecords records\");    }}$logger = new StderrLogger();return new Handler($logger);PythonSDK for Python (Boto3)Note        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Consuming a Kinesis event with Lambda using Python.# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.# SPDX-License-Identifier: Apache-2.0import base64def lambda_handler(event, context):    for record in event['Records']:        try:            print(f\"Processed Kinesis Event - EventID: {record['eventID']}\")            record_data = base64.b64decode(record['kinesis']['data']).decode('utf-8')            print(f\"Record Data: {record_data}\")            # TODO: Do interesting work based on the new data        except Exception as e:            print(f\"An error occurred {e}\")            raise e    print(f\"Successfully processed {len(event['Records'])} records.\")RubySDK for RubyNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Consuming an Kinesis event with Lambda using Ruby.# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.# SPDX-License-Identifier: Apache-2.0require 'aws-sdk'def lambda_handler(event:, context:)  event['Records'].each do |record|    begin      puts \"Processed Kinesis Event - EventID: #{record['eventID']}\"      record_data = get_record_data_async(record['kinesis'])      puts \"Record Data: #{record_data}\"      # TODO: Do interesting work based on the new data    rescue => err      $stderr.puts \"An error occurred #{err}\"      raise err    end  end  puts \"Successfully processed #{event['Records'].length} records.\"enddef get_record_data_async(payload)  data = Base64.decode64(payload['data']).force_encoding('UTF-8')  # Placeholder for actual async work  # You can use Ruby's asynchronous programming tools like async/await or fibers here.  return dataendRustSDK for RustNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Consuming an Kinesis event with Lambda using Rust.// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.// SPDX-License-Identifier: Apache-2.0use aws_lambda_events::event::kinesis::KinesisEvent;use lambda_runtime::{run, service_fn, Error, LambdaEvent};async fn function_handler(event: LambdaEvent<KinesisEvent>) -> Result<(), Error> {    if event.payload.records.is_empty() {        tracing::info!(\"No records found. Exiting.\");        return Ok(());    }    event.payload.records.iter().for_each(|record| {        tracing::info!(\"EventId: {}\",record.event_id.as_deref().unwrap_or_default());        let record_data = std::str::from_utf8(&record.kinesis.data);        match record_data {            Ok(data) => {                // log the record data                tracing::info!(\"Data: {}\", data);            }            Err(e) => {                tracing::error!(\"Error: {}\", e);            }        }    });    tracing::info!(        \"Successfully processed {} records\",        event.payload.records.len()    );    Ok(())}#[tokio::main]async fn main() -> Result<(), Error> {    tracing_subscriber::fmt()        .with_max_level(tracing::Level::INFO)        // disable printing the name of the module in every log line.        .with_target(false)        // disabling time is handy because CloudWatch will add the ingestion time.        .without_time()        .init();    run(service_fn(function_handler)).await}anchoranchoranchoranchoranchoranchoranchoranchor.NETGoJavaJavaScriptPHPPythonRubyRustAWS SDK for .NETNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Consuming a Kinesis event with Lambda using .NET.// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.// SPDX-License-Identifier: Apache-2.0﻿using System.Text;using Amazon.Lambda.Core;using Amazon.Lambda.KinesisEvents;using AWS.Lambda.Powertools.Logging;// Assembly attribute to enable the Lambda function's JSON input to be converted into a .NET class.[assembly: LambdaSerializer(typeof(Amazon.Lambda.Serialization.SystemTextJson.DefaultLambdaJsonSerializer))]namespace KinesisIntegrationSampleCode;public class Function{    // Powertools Logger requires an environment variables against your function    // POWERTOOLS_SERVICE_NAME    [Logging(LogEvent = true)]    public async Task FunctionHandler(KinesisEvent evnt, ILambdaContext context)    {        if (evnt.Records.Count == 0)        {            Logger.LogInformation(\"Empty Kinesis Event received\");            return;        }        foreach (var record in evnt.Records)        {            try            {                Logger.LogInformation($\"Processed Event with EventId: {record.EventId}\");                string data = await GetRecordDataAsync(record.Kinesis, context);                Logger.LogInformation($\"Data: {data}\");                // TODO: Do interesting work based on the new data            }            catch (Exception ex)            {                Logger.LogError($\"An error occurred {ex.Message}\");                throw;            }        }        Logger.LogInformation($\"Successfully processed {evnt.Records.Count} records.\");    }    private async Task<string> GetRecordDataAsync(KinesisEvent.Record record, ILambdaContext context)    {        byte[] bytes = record.Data.ToArray();        string data = Encoding.UTF8.GetString(bytes);        await Task.CompletedTask; //Placeholder for actual async work        return data;    }}",
                                    {
                                        "code_example": "mkdir kinesis-tutorial\ncd kinesis-tutorial"
                                    }
                                ]
                            },
                            {
                                "sub_header": "Test the\n        Lambda function",
                                "content": [
                                    "Invoke your Lambda function manually using the invoke AWS Lambda CLI command and a sample Kinesis      event.",
                                    {
                                        "code_example": "input.txt"
                                    }
                                ]
                            },
                            {
                                "sub_header": "Create a Kinesis stream",
                                "content": [
                                    "Use the create-stream  command to create a stream.",
                                    "aws kinesis create-stream --stream-name lambda-stream --shard-count 1",
                                    "Run the following describe-stream command to get the stream ARN.",
                                    "aws kinesis describe-stream --stream-name lambda-stream",
                                    "You should see the following output:",
                                    "{    \"StreamDescription\": {        \"Shards\": [            {                \"ShardId\": \"shardId-000000000000\",                \"HashKeyRange\": {                    \"StartingHashKey\": \"0\",                    \"EndingHashKey\": \"340282366920746074317682119384634633455\"                },                \"SequenceNumberRange\": {                    \"StartingSequenceNumber\": \"49591073947768692513481539594623130411957558361251844610\"                }            }        ],        \"StreamARN\": \"arn:aws:kinesis:us-east-1:111122223333:stream/lambda-stream\",        \"StreamName\": \"lambda-stream\",        \"StreamStatus\": \"ACTIVE\",        \"RetentionPeriodHours\": 24,        \"EnhancedMonitoring\": [            {                \"ShardLevelMetrics\": []            }        ],        \"EncryptionType\": \"NONE\",        \"KeyId\": null,        \"StreamCreationTimestamp\": 1544828156.0    }}",
                                    "You use the stream ARN in the next step to associate the stream with your Lambda function."
                                ]
                            },
                            {
                                "sub_header": "Add an event source in\n        AWS Lambda",
                                "content": [
                                    "Run the following AWS CLI add-event-source command.",
                                    "aws lambda create-event-source-mapping --function-name ProcessKinesisRecords \\--event-source  arn:aws:kinesis:us-east-1:111122223333:stream/lambda-stream \\--batch-size 100 --starting-position LATEST",
                                    "Note the mapping ID for later use. You can get a list of event source mappings by running the        list-event-source-mappings command.",
                                    "aws lambda list-event-source-mappings --function-name ProcessKinesisRecords \\--event-source arn:aws:kinesis:us-east-1:111122223333:stream/lambda-stream",
                                    "In the response, you can verify the status value is enabled. Event source mappings can be      disabled to pause polling temporarily without losing any records."
                                ]
                            },
                            {
                                "sub_header": "Test the setup",
                                "content": [
                                    "To test the event source mapping, add event records to your Kinesis stream. The --data value is a      string that the CLI encodes to base64 prior to sending it to Kinesis. You can run the same command more than once to      add multiple records to the stream.",
                                    "aws kinesis put-record --stream-name lambda-stream --partition-key 1 \\--data \"Hello, this is a test.\"",
                                    "Lambda uses the execution role to read records from the stream. Then it invokes your Lambda function, passing in      batches of records. The function decodes data from each record and logs it, sending the output to CloudWatch Logs. View the      logs in the CloudWatch console."
                                ]
                            },
                            {
                                "sub_header": "Clean up your resources",
                                "content": [
                                    "You can now delete the resources that you created for this tutorial, unless you want to retain them. By deleting AWS resources that you're no longer using, you prevent unnecessary charges to your AWS account.",
                                    "To delete the execution role\nOpen the Roles page of the IAM console.\n\nSelect the execution role that you created.\n\nChoose Delete.\n\nEnter the name of the role in the text input field and choose Delete.\n",
                                    {
                                        "code_example": "delete"
                                    },
                                    {
                                        "code_example": "delete"
                                    }
                                ]
                            }
                        ]
                    }
                ],
                "sections": [
                    "",
                    "You can use a Lambda function to process records in an Amazon Kinesis data stream. You can map a Lambda function to a Kinesis Data Streams shared-throughput consumer (standard iterator), or to a    dedicated-throughput consumer with enhanced fan-out. For standard iterators, Lambda polls each shard in your Kinesis stream for records using HTTP protocol. The event source mapping shares read throughput with other consumers of the shard.",
                    " For details about Kinesis data streams, see Reading Data from      Amazon Kinesis Data Streams.",
                    "NoteKinesis charges for each shard and, for enhanced fan-out, data read from the stream. For pricing details, see\n      Amazon Kinesis pricing.",
                    "  1. Polling and batching streams",
                    "  2. Example event",
                    "  3.Process Amazon Kinesis Data Streams records with Lambda",
                    "  4.Configuring partial batch response with Kinesis Data Streams and Lambda",
                    "  5.Retain discarded batch records for a Kinesis Data Streams event source in Lambda",
                    "  6.Implementing stateful Kinesis Data Streams processing in Lambda",
                    "  7.Lambda parameters for Amazon Kinesis Data Streams event source mappings",
                    "  8.Using event filtering with a Kinesis event source",
                    "  9.Tutorial: Using Lambda with Kinesis Data Streams",
                    {
                        "sub_header": " Polling and batching streams",
                        "content": [
                            "Lambda reads records from the data stream and invokes your function synchronously with an event that contains stream records. Lambda reads records in batches and invokes your    function to process records from the batch. Each batch contains records from a single shard/data stream.",
                            "For standard Kinesis data streams, Lambda polls shards in your stream for records at a rate of once per second for each shard.       For Kinesis enhanced fan-out,     Lambda uses an HTTP/2 connection to listen for records being pushed from Kinesis. When records are available, Lambda invokes your     function and waits for the result.",
                            "By default, Lambda invokes your function as soon as records are available. If the batch      that Lambda reads from the event source has only one record in it, Lambda sends only one record to the function. To avoid invoking the function      with a small number of records, you can tell the event source to buffer records for up to 5 minutes by configuring a        batching window. Before invoking the function, Lambda continues to read records from the event source      until it has gathered a full batch, the batching window expires, or the batch reaches the payload limit of 6 MB. For more information,      see Batching behavior.",
                            "WarningLambda event source mappings process each event at least once, and duplicate processing of records can occur. To avoid potential issues \nrelated to duplicate events, we strongly recommend that you make your function code idempotent. To learn more, see How do I make my Lambda function idempotent \nin the AWS Knowledge Center.",
                            "Lambda doesn't wait for any configured extensions to complete      before sending the next batch for processing. In other words, your extensions may continue to run as Lambda      processes the next batch of records. This can cause throttling issues if you breach any of your account's       concurrency settings or limits. To detect whether this is a      potential issue, monitor your functions and check whether you're seeing higher      concurrency metrics than expected for your event      source mapping. Due to short times in between invokes, Lambda may briefly report higher concurrency usage      than the number of shards. This can be true even for Lambda functions without extensions.",
                            "Configure the ParallelizationFactor setting to process one shard of a Kinesis data stream with more than one Lambda invocation simultaneously.       You can specify the number of concurrent batches that Lambda polls from a shard via a parallelization factor from 1 (default) to 10. For example, when you set ParallelizationFactor       to 2, you can have 200 concurrent Lambda invocations at maximum to process 100 Kinesis data shards (though in practice, you may see different values for the ConcurrentExecutions metric).      This helps scale up the processing throughput when the data volume is volatile and       the IteratorAge is high. When you increase the number of concurrent batches per shard, Lambda still ensures in-order processing at the partition-key level.",
                            "You can also use ParallelizationFactor with Kinesis aggregation. The behavior of the event source mapping      depends on whether you're using enhanced fan-out:",
                            "  1.Without enhanced fan-out : All of the events inside an aggregated event must have the same\n          partition key. The partition key must also match that of the aggregated event. If the events inside the aggregated event have\n          different partition keys, Lambda cannot guarantee in-order processing of the events by partition key.",
                            "  2.With enhanced fan-out : First, Lambda decodes the aggregated event into its individual events.\n          The aggregated event can have a different partition key than events it contains. However, events that don't correspond to\n          the partition key are dropped and lost.\n          Lambda doesn't process these events, and doesn't send them to a configured failure destination."
                        ]
                    },
                    {
                        "sub_header": " Example event",
                        "content": [
                            {
                                "code_example": "{\n    \"Records\": [\n        {\n            \"kinesis\": {\n                \"kinesisSchemaVersion\": \"1.0\",\n                \"partitionKey\": \"1\",\n                \"sequenceNumber\": \"49590338271490256608559692538361571095921575989136588898\",\n                \"data\": \"SGVsbG8sIHRoaXMgaXMgYSB0ZXN0Lg==\",\n                \"approximateArrivalTimestamp\": 1545084650.987\n            },\n            \"eventSource\": \"aws:kinesis\",\n            \"eventVersion\": \"1.0\",\n            \"eventID\": \"shardId-000000000006:49590338271490256608559692538361571095921575989136588898\",\n            \"eventName\": \"aws:kinesis:record\",\n            \"invokeIdentityArn\": \"arn:aws:iam::123456789012:role/lambda-role\",\n            \"awsRegion\": \"us-east-2\",\n            \"eventSourceARN\": \"arn:aws:kinesis:us-east-2:123456789012:stream/lambda-stream\"\n        },\n        {\n            \"kinesis\": {\n                \"kinesisSchemaVersion\": \"1.0\",\n                \"partitionKey\": \"1\",\n                \"sequenceNumber\": \"49590338271490256608559692540925702759324208523137515618\",\n                \"data\": \"VGhpcyBpcyBvbmx5IGEgdGVzdC4=\",\n                \"approximateArrivalTimestamp\": 1545084711.166\n            },\n            \"eventSource\": \"aws:kinesis\",\n            \"eventVersion\": \"1.0\",\n            \"eventID\": \"shardId-000000000006:49590338271490256608559692540925702759324208523137515618\",\n            \"eventName\": \"aws:kinesis:record\",\n            \"invokeIdentityArn\": \"arn:aws:iam::123456789012:role/lambda-role\",\n            \"awsRegion\": \"us-east-2\",\n            \"eventSourceARN\": \"arn:aws:kinesis:us-east-2:123456789012:stream/lambda-stream\"\n        }\n    ]\n}"
                            }
                        ]
                    }
                ]
            },
            {
                "title": "MQ",
                "href": "https://docs.aws.amazon.com/lambda/latest/dg/with-mq.html",
                "contents": [
                    {
                        "title": "Configure event source",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/process-mq-messages-with-lambda.html",
                        "sections": [
                            "",
                            "  1.Configure network security",
                            "  2.Create the event source mapping",
                            {
                                "sub_header": "Configure network security",
                                "content": [
                                    "To give Lambda full access to Amazon MQ through your event source mapping, either your broker must use a public endpoint             (public IP address), or you must provide access to the Amazon VPC you created the broker in.",
                                    "When you use Amazon MQ with Lambda, create AWS PrivateLink VPC endpoints that provide your function            access to the resources in your Amazon VPC.",
                                    "NoteAWS PrivateLink VPC endpoints are required for functions with event source mappings that use the default (on-demand) mode\n                for event pollers. If your event source mapping uses \n                provisioned mode, you don't need to configure AWS PrivateLink VPC endpoints.",
                                    "Create an endpoint to provide access to the following resources:",
                                    "Alternatively, configure a NAT gateway on each public subnet in the Amazon VPC. For more information,             see Enable internet access for VPC-connected Lambda functions.",
                                    "When you create an event source mapping for Amazon MQ, Lambda checks whether Elastic Network Interfaces (ENIs)             are already present for the subnets and security groups configured for your Amazon VPC. If Lambda finds existing ENIs, it             attempts to re-use them. Otherwise, Lambda creates new ENIs to connect to the event source and invoke your function.",
                                    "NoteLambda functions always run inside VPCs owned by the Lambda service. Your function's VPC configuration\n                does not affect the event source mapping. Only the networking configuration of the event source's determines \n                how Lambda connects to your event source.",
                                    "Configure the security groups for the Amazon VPC containing your broker. By default,            Amazon MQ uses the following ports: 61617 (Amazon MQ for ActiveMQ), and 5671 (Amazon MQ for RabbitMQ).",
                                    {
                                        "code_example": "443"
                                    },
                                    "If your broker uses authentication, you can also restrict the endpoint policy for the Secrets Manager endpoint.             To call the Secrets Manager API, Lambda uses your function role, not the Lambda service principal.",
                                    {
                                        "code_example": "{\n      \"Statement\": [\n          {\n              \"Action\": \"secretsmanager:GetSecretValue\",\n              \"Effect\": \"Allow\",\n              \"Principal\": {\n                  \"AWS\": [\n                      \"arn:aws::iam::123456789012:role/my-role\"\n                  ]\n              },\n              \"Resource\": \"arn:aws::secretsmanager:us-west-2:123456789012:secret:my-secret\"\n          }\n      ]\n  }"
                                    },
                                    "When you use Amazon VPC endpoints, AWS routes your API calls to invoke your function using the endpoint's Elastic Network Interface (ENI).            The Lambda service principal needs to call lambda:InvokeFunction on any roles and functions that use those ENIs.",
                                    "By default, Amazon VPC endpoints have open IAM policies that allow broad access to resources. Best practice is to restrict these            policies to perform the needed actions using that endpoint. To ensure that your event source mapping is able to invoke your Lambda            function, the VPC endpoint policy must allow the Lambda service principal to call sts:AssumeRole and            lambda:InvokeFunction. Restricting your VPC endpoint policies to allow only API calls originating within your organization            prevents the event source mapping from functioning properly, so \"Resource\": \"*\" is required in these policies.",
                                    "The following example VPC endpoint policies show how to grant the required access to the Lambda service principal for the            AWS STS and Lambda endpoints.",
                                    {
                                        "code_example": "{\n      \"Statement\": [\n          {\n              \"Action\": \"sts:AssumeRole\",\n              \"Effect\": \"Allow\",\n              \"Principal\": {\n                  \"Service\": [\n                      \"lambda.amazonaws.com\"\n                  ]\n              },\n              \"Resource\": \"*\"\n          }\n      ]\n    }"
                                    },
                                    {
                                        "code_example": "{\n      \"Statement\": [\n          {\n              \"Action\": \"lambda:InvokeFunction\",\n              \"Effect\": \"Allow\",\n              \"Principal\": {\n                  \"Service\": [\n                      \"lambda.amazonaws.com\"\n                  ]\n              },\n              \"Resource\": \"*\"\n          }\n      ]\n  }"
                                    }
                                ]
                            },
                            {
                                "sub_header": "Create the event source mapping",
                                "content": [
                                    "Create        an event source mapping to tell Lambda to        send records from an Amazon MQ broker to a Lambda function. You can create multiple event source mappings to process        the same data with multiple functions, or to process items from multiple sources with a single function.",
                                    "To configure your function to read from Amazon MQ, add the required permissions and create an          MQ trigger in the Lambda console.",
                                    "To read records from an Amazon MQ broker, your Lambda function needs the following permissions. You grant Lambda permission to interact with your Amazon MQ broker        and its underlying resouces by adding permission statements to your function execution role:",
                                    {
                                        "code_example": "kms:Decrypt"
                                    },
                                    {
                                        "code_example": "{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n      {\n        \"Effect\": \"Allow\",\n        \"Action\": [\n          \"mq:DescribeBroker\",\n          \"secretsmanager:GetSecretValue\",\n          \"ec2:CreateNetworkInterface\",\n          \"ec2:DeleteNetworkInterface\",\n          \"ec2:DescribeNetworkInterfaces\", \n          \"ec2:DescribeSecurityGroups\",\n          \"ec2:DescribeSubnets\",\n          \"ec2:DescribeVpcs\",\n          \"logs:CreateLogGroup\",\n          \"logs:CreateLogStream\", \n          \"logs:PutLogEvents\"\t\t\n        ],\n        \"Resource\": \"*\"\n      }\n    ]\n  }"
                                    },
                                    "Lambda supports the following options for Amazon MQ event sources:",
                                    "  1.MQ broker  – Select an Amazon .",
                                    "  2.Batch size  – Set the maximum number of messages to retrieve in a single\n            batch.",
                                    "  3.Queue name  – Enter the Amazon MQ queue to consume.",
                                    "  4.Source access configuration  – Enter virtual host information and the Secrets Manager\n            secret that stores your broker credentials.",
                                    "  5.Enable trigger  – Disable the trigger to stop processing records.",
                                    "To enable or disable the trigger (or delete it), choose the MQ trigger in the designer. To reconfigure the trigger, use the event source mapping API        operations."
                                ]
                            }
                        ]
                    },
                    {
                        "title": "Parameters",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/services-mq-params.html",
                        "sections": [
                            "",
                            "All Lambda event source types share the same CreateEventSourceMapping and UpdateEventSourceMapping        API operations. However, only some of the parameters apply to Amazon MQ and RabbitMQ.",
                            "\n\nParameter\nRequired\nDefault\nNotes\n\n\n\n\nBatchSize\n\n\nN\n\n\n100\n\n\nMaximum: 10,000\n\n\n\n\nEnabled\n\n\nN\n\n\ntrue\n\nnone\n\n\n\nFunctionName\n\n\nY\n\nN/A \nnone\n\n\n\nFilterCriteria\n\n\nN\n\n\nN/A \n\n\nControl which events Lambda sends to your function\n\n\n\n\nMaximumBatchingWindowInSeconds\n\n\nN\n\n\n500 ms\n\n\nBatching behavior\n\n\n\n\nQueues\n\n\nN\n\nN/A\n\nThe name of the Amazon MQ broker destination queue to consume.\n\n\n\n\nSourceAccessConfigurations\n\n\nN\n\nN/A \n\nFor ActiveMQ, BASIC_AUTH credentials. For RabbitMQ, can contain both BASIC_AUTH credentials and VIRTUAL_HOST information.\n\n\n"
                        ]
                    },
                    {
                        "title": "Event filtering",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/with-mq-filtering.html",
                        "sections": [
                            "",
                            "You can use event filtering to control which records from a stream or queue Lambda sends to your function.    For general information about how event filtering works, see Control which events Lambda sends to your function.",
                            "This section focuses on event filtering for Amazon MQ event sources.",
                            "  1.Amazon MQ event filtering basics",
                            {
                                "sub_header": "Amazon MQ event filtering basics",
                                "content": [
                                    "Suppose your Amazon MQ message queue contains messages either in valid JSON format or as plain strings. An example record would look like the             following, with the data converted to a Base64 encoded string in the data field.",
                                    "ActiveMQ{     \"messageID\": \"ID:b-9bcfa592-423a-4942-879d-eb284b418fc8-1.mq.us-east-2.amazonaws.com-37557-1234520418293-4:1:1:1:1\",     \"messageType\": \"jms/text-message\",    \"deliveryMode\": 1,    \"replyTo\": null,    \"type\": null,    \"expiration\": \"60000\",    \"priority\": 1,    \"correlationId\": \"myJMSCoID\",    \"redelivered\": false,    \"destination\": {       \"physicalName\": \"testQueue\"     },    \"data\":\"QUJDOkFBQUE=\",    \"timestamp\": 1598827811958,    \"brokerInTime\": 1598827811958,     \"brokerOutTime\": 1598827811959,     \"properties\": {      \"index\": \"1\",      \"doAlarm\": \"false\",      \"myCustomProperty\": \"value\"    }}RabbitMQ{    \"basicProperties\": {        \"contentType\": \"text/plain\",        \"contentEncoding\": null,        \"headers\": {            \"header1\": {                \"bytes\": [                  118,                  97,                  108,                  117,                  101,                  49                ]            },            \"header2\": {                \"bytes\": [                  118,                  97,                  108,                  117,                  101,                  50                ]            },            \"numberInHeader\": 10        },        \"deliveryMode\": 1,        \"priority\": 34,        \"correlationId\": null,        \"replyTo\": null,        \"expiration\": \"60000\",        \"messageId\": null,        \"timestamp\": \"Jan 1, 1970, 12:33:41 AM\",        \"type\": null,        \"userId\": \"AIDACKCEVSQ6C2EXAMPLE\",        \"appId\": null,        \"clusterId\": null,        \"bodySize\": 80        },    \"redelivered\": false,    \"data\": \"eyJ0aW1lb3V0IjowLCJkYXRhIjoiQ1pybWYwR3c4T3Y0YnFMUXhENEUifQ==\"}anchoranchorActiveMQRabbitMQ{     \"messageID\": \"ID:b-9bcfa592-423a-4942-879d-eb284b418fc8-1.mq.us-east-2.amazonaws.com-37557-1234520418293-4:1:1:1:1\",     \"messageType\": \"jms/text-message\",    \"deliveryMode\": 1,    \"replyTo\": null,    \"type\": null,    \"expiration\": \"60000\",    \"priority\": 1,    \"correlationId\": \"myJMSCoID\",    \"redelivered\": false,    \"destination\": {       \"physicalName\": \"testQueue\"     },    \"data\":\"QUJDOkFBQUE=\",    \"timestamp\": 1598827811958,    \"brokerInTime\": 1598827811958,     \"brokerOutTime\": 1598827811959,     \"properties\": {      \"index\": \"1\",      \"doAlarm\": \"false\",      \"myCustomProperty\": \"value\"    }}",
                                    "For both Active MQ and Rabbit MQ brokers, you can use event filtering to filter records using the data key. Suppose your             Amazon MQ queue contains messages in the following JSON format.",
                                    "{    \"timeout\": 0,    \"IPAddress\": \"203.0.113.254\"}",
                                    "To filter only those records where the timeout field is greater than 0, the FilterCriteria object would be             as follows.",
                                    "{    \"Filters\": [        {            \"Pattern\": \"{ \\\"data\\\" : { \\\"timeout\\\" : [ { \\\"numeric\\\": [ \\\">\\\", 0] } } ] } }\"        }    ]}",
                                    "For added clarity, here is the value of the filter's Pattern expanded in plain JSON.",
                                    "{    \"data\": {        \"timeout\": [ { \"numeric\": [ \">\", 0 ] } ]        }}",
                                    "You can add your filter using the console, AWS CLI or an AWS SAM template.",
                                    "Consoleto add this filter using the console, follow the instructions in Attaching filter criteria to an event source mapping (console) and enter the following                         string for the Filter criteria.{ \"data\" : { \"timeout\" : [ { \"numeric\": [ \">\", 0 ] } ] } }AWS CLITo create a new event source mapping with these filter criteria using the AWS Command Line Interface (AWS CLI), run the following                        command.aws lambda create-event-source-mapping \\    --function-name my-function \\    --event-source-arn arn:aws:mq:us-east-2:123456789012:broker:my-broker:b-8ac7cc01-5898-482d-be2f-a6b596050ea8 \\    --filter-criteria '{\"Filters\": [{\"Pattern\": \"{ \\\"data\\\" : { \\\"timeout\\\" : [ { \\\"numeric\\\": [ \\\">\\\", 0 ] } ] } }\"}]}'To add these filter criteria to an existing event source mapping, run the following command.aws lambda update-event-source-mapping \\    --uuid \"a1b2c3d4-5678-90ab-cdef-11111EXAMPLE\" \\    --filter-criteria '{\"Filters\": [{\"Pattern\": \"{ \\\"data\\\" : { \\\"timeout\\\" : [ { \\\"numeric\\\": [ \\\">\\\", 0 ] } ] } }\"}]}'To add these filter criteria to an existing event source mapping, run the following command.aws lambda update-event-source-mapping \\    --uuid \"a1b2c3d4-5678-90ab-cdef-11111EXAMPLE\" \\    --filter-criteria '{\"Filters\": [{\"Pattern\": \"{ \\\"data\\\" : { \\\"timeout\\\" : [ { \\\"numeric\\\": [ \\\">\\\", 0 ] } ] } }\"}]}'AWS SAMTo add this filter using AWS SAM, add the following snippet to the YAML template for your event source.FilterCriteria:  Filters:    - Pattern: '{ \"data\" : { \"timeout\" : [ { \"numeric\": [ \">\", 0 ] } ] } }'anchoranchoranchorConsoleAWS CLIAWS SAMto add this filter using the console, follow the instructions in Attaching filter criteria to an event source mapping (console) and enter the following                         string for the Filter criteria.{ \"data\" : { \"timeout\" : [ { \"numeric\": [ \">\", 0 ] } ] } }",
                                    "With Amazon MQ, you can also filter records where the message is a plain string. Suppose you want to process only records where the             message begins with \"Result: \". The FilterCriteria object would look as follows.",
                                    "{    \"Filters\": [        {            \"Pattern\": \"{ \\\"data\\\" : [ { \\\"prefix\\\": \\\"Result: \\\" } ] }\"        }    ]}",
                                    "For added clarity, here is the value of the filter's Pattern expanded in plain JSON.",
                                    "{    \"data\": [        {        \"prefix\": \"Result: \"        }    ]}",
                                    "You can add your filter using the console, AWS CLI or an AWS SAM template.",
                                    "ConsoleTo add this filter using the console, follow the instructions in Attaching filter criteria to an event source mapping (console) and enter the following                         string for the Filter criteria.{ \"data\" : [ { \"prefix\": \"Result: \" } ] }AWS CLITo create a new event source mapping with these filter criteria using the AWS Command Line Interface (AWS CLI), run the following                        command.aws lambda create-event-source-mapping \\    --function-name my-function \\    --event-source-arn arn:aws:mq:us-east-2:123456789012:broker:my-broker:b-8ac7cc01-5898-482d-be2f-a6b596050ea8 \\    --filter-criteria '{\"Filters\": [{\"Pattern\": \"{ \\\"data\\\" : [ { \\\"prefix\\\": \\\"Result: \\\" } ] }\"}]}'To add these filter criteria to an existing event source mapping, run the following command.aws lambda update-event-source-mapping \\    --uuid \"a1b2c3d4-5678-90ab-cdef-11111EXAMPLE\" \\    --filter-criteria '{\"Filters\": [{\"Pattern\": \"{ \\\"data\\\" : [ { \\\"prefix\\\": \\\"Result: \\\" } ] }\"}]}'AWS SAMTo add this filter using AWS SAM, add the following snippet to the YAML template for your event source.FilterCriteria:  Filters:    - Pattern: '{ \"data\" : [ { \"prefix\": \"Result \" } ] }'anchoranchoranchorConsoleAWS CLIAWS SAMTo add this filter using the console, follow the instructions in Attaching filter criteria to an event source mapping (console) and enter the following                         string for the Filter criteria.{ \"data\" : [ { \"prefix\": \"Result: \" } ] }",
                                    "Amazon MQ messages must be UTF-8 encoded strings, either plain strings or in JSON format. That's because Lambda decodes Amazon MQ byte arrays into UTF-8 before             applying filter criteria. If your messages use another encoding, such as UTF-16 or ASCII, or if the message format doesn't match the             FilterCriteria format, Lambda processes metadata filters only. The following table summarizes the specific behavior:",
                                    "\n\nIncoming message format\nFilter pattern format for message properties\nResulting action\n\n\n\n\nPlain string\n\n\nPlain string\n\n\nLambda filters based on your filter criteria.\n\n\n\n\nPlain string\n\n\nNo filter pattern for data properties\n\n\nLambda filters (on the other metadata properties only) based on your filter criteria.\n\n\n\n\nPlain string\n\n\nValid JSON\n\n\nLambda filters (on the other metadata properties only) based on your filter criteria.\n\n\n\n\nValid JSON\n\n\nPlain string\n\n\nLambda filters (on the other metadata properties only) based on your filter criteria.\n\n\n\n\nValid JSON\n\n\nNo filter pattern for data properties\n\n\nLambda filters (on the other metadata properties only) based on your filter criteria.\n\n\n\n\nValid JSON\n\n\nValid JSON\n\n\nLambda filters based on your filter criteria.\n\n\n\n\nNon-UTF-8 encoded string\n\n\nJSON, plain string, or no pattern\n\n\nLambda filters (on the other metadata properties only) based on your filter criteria.\n\n\n"
                                ]
                            }
                        ]
                    },
                    {
                        "title": "Troubleshoot",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/services-mq-errors.html",
                        "sections": [
                            "",
                            "When a Lambda function encounters an unrecoverable error, your Amazon MQ consumer stops processing records. Any        other consumers can continue processing, provided that they do not encounter the same error. To determine the        potential cause of a stopped consumer, check the StateTransitionReason field in the return details of        your EventSourceMapping for one of the following codes:",
                            {
                                "code_example": "ESM_CONFIG_NOT_VALID"
                            },
                            "Records also go unprocessed if Lambda drops        them due to their size. The size limit for Lambda records is 6 MB. To        redeliver messages upon function error, you can use a dead-letter queue (DLQ). For more information, see Message Redelivery and            DLQ Handling on the Apache ActiveMQ website and Reliability Guide on the RabbitMQ        website.",
                            {
                                "code_example": "maximumRedeliveries"
                            }
                        ]
                    }
                ],
                "sections": [
                    "",
                    "NoteIf you want to send data to a target other than a Lambda function or enrich the data before sending it, see \n    Amazon EventBridge Pipes.",
                    "Amazon MQ is a managed message broker service for Apache ActiveMQ    and RabbitMQ. A message broker enables software    applications and components to communicate using various programming languages, operating systems, and formal    messaging protocols through either topic or queue event destinations.",
                    "Amazon MQ can also manage Amazon Elastic Compute Cloud (Amazon EC2) instances on your behalf by installing ActiveMQ or RabbitMQ brokers and by providing    different network topologies and other infrastructure needs.",
                    "You can use a Lambda function to process records from your Amazon MQ message broker. Lambda invokes your function    through an event source mapping, a Lambda resource that reads    messages from your broker and invokes the function synchronously.",
                    "WarningLambda event source mappings process each event at least once, and duplicate processing of records can occur. To avoid potential issues \nrelated to duplicate events, we strongly recommend that you make your function code idempotent. To learn more, see How do I make my Lambda function idempotent \nin the AWS Knowledge Center.",
                    "The Amazon MQ event source mapping has the following configuration restrictions:",
                    {
                        "code_example": "TextMessage"
                    },
                    "Lambda automatically supports the latest versions of ActiveMQ and RabbitMQ that Amazon MQ supports. For the latest    supported versions, see Amazon MQ release notes in the      Amazon MQ Developer Guide.",
                    "NoteBy default, Amazon MQ has a weekly maintenance window for brokers. During that\n      window of time, brokers are unavailable. For brokers without standby, Lambda cannot process any messages during that window.",
                    "  1.Understanding the Lambda consumer group for Amazon MQ",
                    "  2.Configuring Amazon MQ event source for Lambda",
                    "  3.Event source mapping parameters",
                    "  4.Filter events from an Amazon MQ event source",
                    "  5.Troubleshoot Amazon MQ event source mapping errors",
                    {
                        "sub_header": "Understanding the Lambda consumer group for Amazon MQ",
                        "content": [
                            "To interact with Amazon MQ, Lambda creates a consumer group which can read from your Amazon MQ brokers. The consumer      group is created with the same ID as the event source mapping UUID.",
                            "For Amazon MQ event sources, Lambda batches records together and sends them to your function in a single payload.      To control behavior, you can configure the batching window and batch size. Lambda pulls messages until it processes      the payload size maximum of 6 MB, the batching window expires, or the number of records reaches the full batch      size. For more information, see Batching behavior.",
                            "The consumer group retrieves the messages as a BLOB of bytes, base64-encodes them into a single JSON payload, and then invokes your function. If your function returns an error for any of the messages in a batch, Lambda retries the      whole batch of messages until processing succeeds or the messages expire.",
                            "NoteWhile Lambda functions typically have a maximum timeout limit of 15 minutes,\n      event source mappings for Amazon MSK, self-managed Apache Kafka, Amazon DocumentDB, and Amazon MQ for ActiveMQ and RabbitMQ only support functions with\n      maximum timeout limits of 14 minutes. This constraint ensures that the event source mapping can properly\n      handle function errors and retries.",
                            "You can monitor a given function's concurrency usage using the ConcurrentExecutions metric in      Amazon CloudWatch. For more information about concurrency, see Configuring reserved concurrency for a function.",
                            {
                                "code_example": "{\n   \"eventSource\": \"aws:mq\",\n   \"eventSourceArn\": \"arn:aws:mq:us-east-2:111122223333:broker:test:b-9bcfa592-423a-4942-879d-eb284b418fc8\",\n   \"messages\": [\n      { \n        \"messageID\": \"ID:b-9bcfa592-423a-4942-879d-eb284b418fc8-1.mq.us-east-2.amazonaws.com-37557-1234520418293-4:1:1:1:1\", \n        \"messageType\": \"jms/text-message\",\n        \"deliveryMode\": 1,\n        \"replyTo\": null,\n        \"type\": null,\n        \"expiration\": \"60000\",\n        \"priority\": 1,\n        \"correlationId\": \"myJMSCoID\",\n        \"redelivered\": false,\n        \"destination\": { \n          \"physicalName\": \"testQueue\" \n        },\n        \"data\":\"QUJDOkFBQUE=\",\n        \"timestamp\": 1598827811958,\n        \"brokerInTime\": 1598827811958, \n        \"brokerOutTime\": 1598827811959, \n        \"properties\": {\n          \"index\": \"1\",\n          \"doAlarm\": \"false\",\n          \"myCustomProperty\": \"value\"\n        }\n      },\n      { \n        \"messageID\": \"ID:b-9bcfa592-423a-4942-879d-eb284b418fc8-1.mq.us-east-2.amazonaws.com-37557-1234520418293-4:1:1:1:1\",\n        \"messageType\": \"jms/bytes-message\",\n        \"deliveryMode\": 1,\n        \"replyTo\": null,\n        \"type\": null,\n        \"expiration\": \"60000\",\n        \"priority\": 2,\n        \"correlationId\": \"myJMSCoID1\",\n        \"redelivered\": false,\n        \"destination\": { \n          \"physicalName\": \"testQueue\" \n        },\n        \"data\":\"LQaGQ82S48k=\",\n        \"timestamp\": 1598827811958,\n        \"brokerInTime\": 1598827811958, \n        \"brokerOutTime\": 1598827811959, \n        \"properties\": {\n          \"index\": \"1\",\n          \"doAlarm\": \"false\",\n          \"myCustomProperty\": \"value\"\n        }\n      }\n   ]\n}"
                            }
                        ]
                    }
                ]
            },
            {
                "title": "MSK",
                "href": "https://docs.aws.amazon.com/lambda/latest/dg/with-msk.html",
                "contents": [
                    {
                        "title": "Configure event source",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/with-msk-configure.html",
                        "sections": [
                            "",
                            "Before you create an event source mapping for your Amazon MSK cluster, you need to ensure that        your cluster and the VPC it resides in are correctly configured. You also need to make sure        that your Lambda function's execution role has         the necessary IAM permissions.",
                            "Follow the instructions in the following sections to configure your Amazon MSK cluster, VPC, and Lambda        function. To learn how to create the event source mapping, see        Adding Amazon MSK as an event source.",
                            "  1.MSK cluster authentication",
                            "  2.Managing API access and permissions",
                            "  3.Authentication and authorization errors",
                            "  4.Configure network security",
                            {
                                "sub_header": "MSK cluster authentication",
                                "content": [
                                    "Lambda needs permission to access the Amazon MSK cluster, retrieve records, and perform other tasks. Amazon MSK supports      several options for controlling client access to the MSK cluster.",
                                    "  1.Unauthenticated access",
                                    "  2.SASL/SCRAM authentication",
                                    "  3.IAM role-based authentication",
                                    "  4.Mutual TLS authentication",
                                    "  5.Configuring the mTLS secret",
                                    "  6.How Lambda chooses a bootstrap broker",
                                    {
                                        "sub_header": "Unauthenticated access",
                                        "content": [
                                            "If no clients access the cluster over the internet, you can use unauthenticated access."
                                        ]
                                    },
                                    {
                                        "sub_header": "SASL/SCRAM authentication",
                                        "content": [
                                            "Amazon MSK supports Simple Authentication and Security Layer/Salted Challenge Response Authentication Mechanism        (SASL/SCRAM) authentication with Transport Layer Security (TLS) encryption. For Lambda to connect to the cluster,        you store the authentication credentials (user name and password) in an AWS Secrets Manager secret.",
                                            "For more information about using Secrets Manager, see User name and password authentication with AWS Secrets Manager        in the Amazon Managed Streaming for Apache Kafka Developer Guide.",
                                            "Amazon MSK doesn't support SASL/PLAIN authentication."
                                        ]
                                    },
                                    {
                                        "sub_header": "IAM role-based authentication",
                                        "content": [
                                            "You can use IAM to authenticate the identity of clients that connect to the MSK cluster. If IAM auth is        active on your MSK cluster, and you don't provide a secret for auth, Lambda automatically defaults to using IAM auth.        To create and deploy user or role-based policies, use the IAM console or API. For more information, see IAM access control in        the Amazon Managed Streaming for Apache Kafka Developer Guide.",
                                            "To allow Lambda to connect to the MSK cluster, read records, and perform other required actions, add the        following permissions to your function's execution role.",
                                            "{    \"Version\": \"2012-10-17\",    \"Statement\": [        {            \"Effect\": \"Allow\",            \"Action\": [                \"kafka-cluster:Connect\",                \"kafka-cluster:DescribeGroup\",                \"kafka-cluster:AlterGroup\",                \"kafka-cluster:DescribeTopic\",                \"kafka-cluster:ReadData\",                \"kafka-cluster:DescribeClusterDynamicConfiguration\"            ],            \"Resource\": [                \"arn:aws:kafka:region:account-id:cluster/cluster-name/cluster-uuid\",                \"arn:aws:kafka:region:account-id:topic/cluster-name/cluster-uuid/topic-name\",                \"arn:aws:kafka:region:account-id:group/cluster-name/cluster-uuid/consumer-group-id\"            ]        }    ]}       ",
                                            "You can scope these permissions to a specific cluster, topic, and group. For more information, see the          Amazon MSK Kafka          actions in the Amazon Managed Streaming for Apache Kafka Developer Guide."
                                        ]
                                    },
                                    {
                                        "sub_header": "Mutual TLS authentication",
                                        "content": [
                                            "Mutual TLS (mTLS) provides two-way authentication between the client and server. The client sends a        certificate to the server for the server to verify the client, and the server sends a certificate to the client        for the client to verify the server. ",
                                            "For Amazon MSK, Lambda acts as the client. You configure a client certificate (as a secret in Secrets Manager) to        authenticate Lambda with the brokers in your MSK cluster. The client certificate must be signed by a CA in the        server's trust store. The MSK cluster sends a server certificate to Lambda to authenticate the brokers with        Lambda. The server certificate must be signed by a certificate authority (CA) that's in the AWS trust store. ",
                                            "For instructions on how to generate a client certificate, see         Introducing mutual TLS authentication for Amazon MSK as an event source.",
                                            "Amazon MSK doesn't support self-signed server certificates, because all brokers in Amazon MSK use public certificates signed by          Amazon Trust Services CAs, which Lambda trusts by        default.",
                                            "For more information about mTLS for Amazon MSK, see Mutual TLS Authentication in the          Amazon Managed Streaming for Apache Kafka Developer Guide."
                                        ]
                                    },
                                    {
                                        "sub_header": "Configuring the mTLS secret",
                                        "content": [
                                            "The CLIENT_CERTIFICATE_TLS_AUTH secret requires a certificate field and a private key field. For an        encrypted private key, the secret requires a private key password. Both the certificate and private key must be        in PEM format.",
                                            "NoteLambda supports the PBES1 (but not\n          PBES2) private key encryption algorithms.",
                                            "The certificate field must contain a list of certificates, beginning with the client certificate, followed        by any intermediate certificates, and ending with the root certificate. Each certificate must start on a new        line with the following structure:",
                                            "-----BEGIN CERTIFICATE-----          <certificate contents>-----END CERTIFICATE-----      ",
                                            "Secrets Manager supports secrets up to 65,536 bytes, which is enough space for long certificate chains.",
                                            "The private key must be in PKCS #8        format, with the following structure:",
                                            "-----BEGIN PRIVATE KEY-----           <private key contents>-----END PRIVATE KEY-----            ",
                                            "For an encrypted private key, use the following structure:",
                                            "-----BEGIN ENCRYPTED PRIVATE KEY-----            <private key contents>-----END ENCRYPTED PRIVATE KEY-----           ",
                                            "The following example shows the contents of a secret for mTLS authentication using an encrypted private key.        For an encrypted private key, you include the private key password in the secret.",
                                            "{ \"privateKeyPassword\": \"testpassword\", \"certificate\": \"-----BEGIN CERTIFICATE-----MIIE5DCCAsygAwIBAgIRAPJdwaFaNRrytHBto0j5BA0wDQYJKoZIhvcNAQELBQAw...j0Lh4/+1HfgyE2KlmII36dg4IMzNjAFEBZiCRoPimO40s1cRqtFHXoal0QQbIlxkcmUuiAii9R0=-----END CERTIFICATE----------BEGIN CERTIFICATE-----MIIFgjCCA2qgAwIBAgIQdjNZd6uFf9hbNC5RdfmHrzANBgkqhkiG9w0BAQsFADBb...rQoiowbbk5wXCheYSANQIfTZ6weQTgiCHCCbuuMKNVS95FkXm0vqVD/YpXKwA/noc8PH3PSoAaRwMMgOSA2ALJvbRz8mpg==-----END CERTIFICATE-----\", \"privateKey\": \"-----BEGIN ENCRYPTED PRIVATE KEY-----MIIFKzBVBgkqhkiG9w0BBQ0wSDAnBgkqhkiG9w0BBQwwGgQUiAFcK5hT/X7Kjmgp...QrSekqF+kWzmB6nAfSzgO9IaoAaytLvNgGTckWeUkWn/V0Ck+LdGUXzAC4RxZnoQzp2mwJn2NYB7AZ7+imp0azDZb+8YG2aUCiyqb6PnnA==-----END ENCRYPTED PRIVATE KEY-----\"} "
                                        ]
                                    },
                                    {
                                        "sub_header": "How Lambda chooses a bootstrap broker",
                                        "content": [
                                            "Lambda chooses a         bootstrap broker based on the authentication methods available on your cluster, and whether you provide a secret        for authentication. If you provide a secret for mTLS or SASL/SCRAM, Lambda automatically chooses that auth method.        If you don't provide a secret, Lambda selects the strongest auth method that's active on your cluster. The following is        the order of priority in which Lambda selects a broker, from strongest to weakest auth:",
                                            "NoteIf Lambda can't connect to the most secure broker type, Lambda doesn't attempt to connect to a different (weaker)\n        broker type. If you want Lambda to choose a weaker broker type, deactivate all stronger auth methods on your cluster."
                                        ]
                                    }
                                ]
                            },
                            {
                                "sub_header": "Managing API access and permissions",
                                "content": [
                                    "In addition to accessing the Amazon MSK cluster, your function needs permissions to perform various Amazon MSK API      actions. You add these permissions to the function's execution role. If your users need access to any of the Amazon MSK      API actions, add the required permissions to the identity policy for the user or role.",
                                    "You can add each of the following permissions to your execution role manually. Alternatively, you can attach the      AWS managed policy AWSLambdaMSKExecutionRole to your execution role. The      AWSLambdaMSKExecutionRole policy contains all required API actions and VPC permissions listed below.",
                                    {
                                        "sub_header": "Required Lambda function execution role permissions",
                                        "content": [
                                            "To create and store logs in a log group in Amazon CloudWatch Logs, your Lambda function must have the following        permissions in its execution role:",
                                            "For Lambda to access your Amazon MSK cluster on your behalf, your Lambda function must have the following        permissions in its execution role:",
                                            "You only need to add one of either kafka:DescribeCluster or kafka:DescribeClusterV2. For provisioned MSK        clusters, either permission works. For serverless MSK clusters, you must use kafka:DescribeClusterV2.",
                                            {
                                                "code_example": "kafka:DescribeCluster"
                                            }
                                        ]
                                    },
                                    {
                                        "sub_header": "VPC permissions",
                                        "content": [
                                            "If only users within a VPC can access your Amazon MSK cluster, your Lambda function must have permission to        access your Amazon VPC resources. These resources include your VPC, subnets, security groups, and network        interfaces. To access these resources, your function's execution role must have the following        permissions. These permissions are included in the AWSLambdaMSKExecutionRole AWS managed policy."
                                        ]
                                    },
                                    {
                                        "sub_header": "Optional Lambda function permissions",
                                        "content": [
                                            "Your Lambda function might also need permissions to:",
                                            {
                                                "sub_header": "Secrets Manager and AWS KMS permissions",
                                                "content": [
                                                    "Depending on the type of access control that you're configuring for your Amazon MSK brokers, your Lambda          function might need permission to access your SCRAM secret (if using SASL/SCRAM authentication), or Secrets Manager          secret to decrypt your AWS KMS customer managed key. To access these resources, your function's execution role must have          the following permissions:"
                                                ]
                                            }
                                        ]
                                    },
                                    {
                                        "sub_header": "Adding permissions to your execution role",
                                        "content": [
                                            "Follow these steps to add the AWS managed policy AWSLambdaMSKExecutionRole to your execution role using the IAM console.",
                                            {
                                                "code_example": "AWSLambdaMSKExecutionRole"
                                            }
                                        ]
                                    },
                                    {
                                        "sub_header": "Granting users access with an IAM policy",
                                        "content": [
                                            "By default, users and roles don't have permission to perform Amazon MSK API operations. To grant access to        users in your organization or account, you can add or update an identity-based policy. For more information, see          Amazon MSK          Identity-Based Policy Examples in the Amazon Managed Streaming for Apache Kafka Developer Guide."
                                        ]
                                    }
                                ]
                            },
                            {
                                "sub_header": "Authentication and authorization errors",
                                "content": [
                                    "If any of the permissions required to consume data from the Amazon MSK cluster are missing, Lambda displays one of      the following error messages in the event source mapping under LastProcessingResult.",
                                    "  1.Cluster failed to authorize Lambda",
                                    "  2.SASL authentication failed",
                                    "  3.Server failed to authenticate Lambda",
                                    "  4.Provided certificate or private key is invalid",
                                    {
                                        "sub_header": "Cluster failed to authorize Lambda",
                                        "content": [
                                            "For SASL/SCRAM or mTLS, this error indicates that the provided user doesn't have all of the following        required Kafka access control list (ACL) permissions:",
                                            "For IAM access control, your function's execution role is missing one or more of the permissions required        to access the group or topic. Review the list of required permissions in IAM role-based authentication.",
                                            "When you create either Kafka ACLs or an IAM policy with the required Kafka cluster permissions, specify        the topic and group as resources. The topic name must match the topic in the event source mapping. The group        name must match the event source mapping's UUID.",
                                            "After you add the required permissions to the execution role, it might take several minutes for the changes        to take effect."
                                        ]
                                    },
                                    {
                                        "sub_header": "SASL authentication failed",
                                        "content": [
                                            "For SASL/SCRAM, this error indicates that the provided user name and password aren't valid.",
                                            "For IAM access control, the execution role is missing the kafka-cluster:Connect permission        for the MSK cluster. Add this permission to the role and specify the cluster's Amazon Resource Name (ARN) as a        resource.",
                                            "You might see this error occurring intermittently. The cluster rejects connections after the number of TCP        connections exceeds the Amazon MSK service          quota. Lambda backs off and retries until a connection is successful. After Lambda connects to the        cluster and polls for records, the last processing result changes to OK."
                                        ]
                                    },
                                    {
                                        "sub_header": "Server failed to authenticate Lambda",
                                        "content": [
                                            "This error indicates that the Amazon MSK Kafka brokers failed to authenticate with Lambda. This can occur for        any of the following reasons:"
                                        ]
                                    },
                                    {
                                        "sub_header": "Provided certificate or private key is invalid",
                                        "content": [
                                            "This error indicates that the Amazon MSK consumer couldn't use the provided certificate or private key. Make        sure that the certificate and key use PEM format, and that the private key encryption uses a PBES1        algorithm."
                                        ]
                                    }
                                ]
                            },
                            {
                                "sub_header": "Configure network security",
                                "content": [
                                    "To give Lambda full access to Amazon MSK through your event source mapping, either your cluster must use a public endpoint             (public IP address), or you must provide access to the Amazon VPC you created the cluster in.",
                                    "When you use Amazon MSK with Lambda, create AWS PrivateLink VPC endpoints that provide your function            access to the resources in your Amazon VPC.",
                                    "NoteAWS PrivateLink VPC endpoints are required for functions with event source mappings that use the default (on-demand) mode\n                for event pollers. If your event source mapping uses \n                provisioned mode, you don't need to configure AWS PrivateLink VPC endpoints.",
                                    "Create an endpoint to provide access to the following resources:",
                                    "Alternatively, configure a NAT gateway on each public subnet in the Amazon VPC. For more information,             see Enable internet access for VPC-connected Lambda functions.",
                                    "When you create an event source mapping for Amazon MSK, Lambda checks whether Elastic Network Interfaces (ENIs)             are already present for the subnets and security groups configured for your Amazon VPC. If Lambda finds existing ENIs, it             attempts to re-use them. Otherwise, Lambda creates new ENIs to connect to the event source and invoke your function.",
                                    "NoteLambda functions always run inside VPCs owned by the Lambda service. Your function's VPC configuration\n                does not affect the event source mapping. Only the networking configuration of the event source's determines \n                how Lambda connects to your event source.",
                                    "Configure the security groups for the Amazon VPC containing your cluster. By default,            Amazon MSK uses the following ports: 9092 for plaintext, 9094 for TLS, 9096 for SASL, 9098 for IAM.",
                                    {
                                        "code_example": "443"
                                    },
                                    "If your cluster uses authentication, you can also restrict the endpoint policy for the Secrets Manager endpoint.             To call the Secrets Manager API, Lambda uses your function role, not the Lambda service principal.",
                                    {
                                        "code_example": "{\n      \"Statement\": [\n          {\n              \"Action\": \"secretsmanager:GetSecretValue\",\n              \"Effect\": \"Allow\",\n              \"Principal\": {\n                  \"AWS\": [\n                      \"arn:aws::iam::123456789012:role/my-role\"\n                  ]\n              },\n              \"Resource\": \"arn:aws::secretsmanager:us-west-2:123456789012:secret:my-secret\"\n          }\n      ]\n  }"
                                    },
                                    "When you use Amazon VPC endpoints, AWS routes your API calls to invoke your function using the endpoint's Elastic Network Interface (ENI).            The Lambda service principal needs to call lambda:InvokeFunction on any roles and functions that use those ENIs.",
                                    "By default, Amazon VPC endpoints have open IAM policies that allow broad access to resources. Best practice is to restrict these            policies to perform the needed actions using that endpoint. To ensure that your event source mapping is able to invoke your Lambda            function, the VPC endpoint policy must allow the Lambda service principal to call sts:AssumeRole and            lambda:InvokeFunction. Restricting your VPC endpoint policies to allow only API calls originating within your organization            prevents the event source mapping from functioning properly, so \"Resource\": \"*\" is required in these policies.",
                                    "The following example VPC endpoint policies show how to grant the required access to the Lambda service principal for the            AWS STS and Lambda endpoints.",
                                    {
                                        "code_example": "{\n      \"Statement\": [\n          {\n              \"Action\": \"sts:AssumeRole\",\n              \"Effect\": \"Allow\",\n              \"Principal\": {\n                  \"Service\": [\n                      \"lambda.amazonaws.com\"\n                  ]\n              },\n              \"Resource\": \"*\"\n          }\n      ]\n    }"
                                    },
                                    {
                                        "code_example": "{\n      \"Statement\": [\n          {\n              \"Action\": \"lambda:InvokeFunction\",\n              \"Effect\": \"Allow\",\n              \"Principal\": {\n                  \"Service\": [\n                      \"lambda.amazonaws.com\"\n                  ]\n              },\n              \"Resource\": \"*\"\n          }\n      ]\n  }"
                                    }
                                ]
                            }
                        ]
                    },
                    {
                        "title": "Process messages",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/with-msk-process.html",
                        "sections": [
                            "",
                            "NoteIf you want to send data to a target other than a Lambda function or enrich the data before sending it, see \n    Amazon EventBridge Pipes.",
                            "  1.Adding Amazon MSK as an event source",
                            "  2.Amazon MSK configuration parameters",
                            "  3.Creating cross-account event source mappings",
                            "  4.Using an Amazon MSK cluster as an event source",
                            "  5.Polling and stream starting positions",
                            "  6.Amazon CloudWatch metrics",
                            "  7.Message throughput scaling behavior for Amazon MSK event source mappings",
                            {
                                "sub_header": "Adding Amazon MSK as an event source",
                                "content": [
                                    "To create an event source mapping, add Amazon MSK as a Lambda            function trigger using the Lambda console, an AWS SDK, or the AWS Command Line Interface (AWS CLI). Note that when you add Amazon MSK            as a trigger, Lambda assumes the VPC settings of the Amazon MSK cluster, not the Lambda function's VPC settings.",
                                    "This section describes how to create an event source mapping using the Lambda console and the AWS CLI.",
                                    {
                                        "sub_header": "Prerequisites",
                                        "content": []
                                    },
                                    {
                                        "sub_header": "Customizable consumer group ID",
                                        "content": [
                                            "When setting up Kafka as an event source, you can specify a consumer group ID. This consumer group ID is an    existing identifier for the Kafka consumer group that you want your Lambda function to join. You can use this feature to seamlessly migrate any    ongoing Kafka record processing setups from other consumers to Lambda.",
                                            "If you specify a consumer group ID and there are other active pollers within that consumer group, Kafka distributes messages across      all consumers. In other words, Lambda doesn't receive all message for the Kafka topic. If you want Lambda to handle all messages in the      topic, turn off any other pollers in that consumer group.",
                                            "Additionally, if you specify a consumer group ID, and Kafka finds a valid existing consumer group with the same ID, Lambda ignores the      StartingPosition parameter for your event source mapping. Instead, Lambda begins processing records according to the committed      offset of the consumer group. If you specify a consumer group ID, and Kafka cannot find an existing consumer group, then Lambda configures your      event source with the specified StartingPosition.",
                                            "The consumer group ID that you specify must be unique among all your Kafka event sources. After creating a Kafka event source mapping      with the consumer group ID specified, you cannot update this value."
                                        ]
                                    },
                                    {
                                        "sub_header": "Adding an Amazon MSK trigger (console)",
                                        "content": [
                                            "Follow these steps to add your Amazon MSK cluster and a Kafka topic as a trigger for your Lambda function.",
                                            "To add an Amazon MSK trigger to your Lambda function (console)\nOpen the Functions page of the Lambda\n                        console.\n\nChoose the name of your Lambda function.\n\nUnder Function overview, choose Add trigger.\n\nUnder Trigger configuration, do the following:\n\nChoose the MSK trigger type.\n\nFor MSK cluster, select your cluster.\n\nFor Batch size, enter the maximum number of messages to receive in a single\n                                batch.\n\nFor Batch window, enter the maximum amount of seconds that Lambda spends\n                                gathering records before invoking the function.\n\nFor Topic name, enter the name of a Kafka topic.\n\n(Optional) For Consumer group ID, enter the ID of a Kafka consumer group to join.\n\n(Optional) For Starting position, choose Latest to start\n                                reading the stream from the latest record, Trim horizon to start at the\n                                earliest available record, or At timestamp to specify a timestamp to start\n                                reading from.\n\n(Optional) For Authentication, choose the secret key for authenticating with\n                                the brokers in your MSK cluster.\n\nTo create the trigger in a disabled state for testing (recommended), clear Enable\n                                trigger. Or, to enable the trigger immediately, select Enable\n                                    trigger.\n\n\nTo create the trigger, choose Add.\n"
                                        ]
                                    },
                                    {
                                        "sub_header": "Adding an Amazon MSK trigger (AWS CLI)",
                                        "content": [
                                            "Use the following example AWS CLI commands to create and view an Amazon MSK trigger for your Lambda function.",
                                            {
                                                "sub_header": "Creating a trigger using the AWS CLI",
                                                "content": [
                                                    {
                                                        "code_example": "my-kafka-function"
                                                    },
                                                    {
                                                        "code_example": "SASL_SCRAM_512_AUTH"
                                                    },
                                                    {
                                                        "code_example": "CLIENT_CERTIFICATE_TLS_AUTH"
                                                    },
                                                    "For more information, see the CreateEventSourceMapping API reference documentation."
                                                ]
                                            },
                                            {
                                                "sub_header": "Viewing the status using the AWS CLI",
                                                "content": [
                                                    "The following example uses the get-event-source-mapping AWS CLI command to describe the status of the event source mapping that you created.",
                                                    "aws lambda get-event-source-mapping \\  --uuid 6d9bce8e-836b-442c-8070-74e77903c815"
                                                ]
                                            }
                                        ]
                                    }
                                ]
                            },
                            {
                                "sub_header": "Amazon MSK configuration parameters",
                                "content": [
                                    "All Lambda event source types share the same CreateEventSourceMapping and UpdateEventSourceMapping            API operations. However, only some of the parameters apply to Amazon MSK.",
                                    {
                                        "code_example": "MinimumPollers"
                                    }
                                ]
                            },
                            {
                                "sub_header": "Creating cross-account event source mappings",
                                "content": [
                                    "You can use multi-VPC private connectivity to connect a Lambda function to a provisioned MSK cluster in a different AWS account. Multi-VPC connectivity uses AWS PrivateLink, which keeps all traffic within the AWS network.",
                                    "NoteYou can't create cross-account event source mappings for serverless MSK clusters.",
                                    "To create a cross-account event source mapping, you must first configure multi-VPC connectivity for the MSK cluster. When you create the event source mapping, use the managed VPC connection ARN instead of the cluster ARN, as shown in the following examples. The CreateEventSourceMapping operation also differs depending on which authentication type the MSK cluster uses.",
                                    {
                                        "code_example": "aws lambda create-event-source-mapping \\\n  --event-source-arn arn:aws:kafka:us-east-1:111122223333:vpc-connection/444455556666/my-cluster-name/51jn98b4-0a61-46cc-b0a6-61g9a3d797d5-7 \\\n  --topics AWSKafkaTopic \\\n  --starting-position LATEST \\\n  --function-name my-kafka-function"
                                    },
                                    {
                                        "code_example": "SASL_SCRAM_512_AUTH"
                                    },
                                    {
                                        "code_example": "CLIENT_CERTIFICATE_TLS_AUTH"
                                    }
                                ]
                            },
                            {
                                "sub_header": "Using an Amazon MSK cluster as an event source",
                                "content": [
                                    "When you add your Apache Kafka or Amazon MSK cluster as a trigger for your Lambda function, the cluster is used as an event source.",
                                    "Lambda reads event data from the Kafka topics that you specify as Topics in a            CreateEventSourceMapping request, based on the StartingPosition that you specify. After            successful processing, your Kafka topic is committed to your Kafka cluster.",
                                    "If you specify the StartingPosition as LATEST, Lambda starts reading from the latest            message in each partition belonging to the topic. Because there can be some delay after trigger configuration            before Lambda starts reading the messages, Lambda doesn't read any messages produced during this window.",
                                    "Lambda reads messages sequentially for each Kafka topic partition. A single Lambda payload can contain            messages from multiple partitions. When more records are available, Lambda continues processing records            in batches, based on the BatchSize value that you specify in a CreateEventSourceMapping            request, until your function catches up with the topic.",
                                    "After Lambda processes each batch, it commits the offsets of the messages in that batch. If your            function returns an error for any of the messages in a batch, Lambda retries the whole batch of messages            until processing succeeds or the messages expire. You can send records that fail all retry attempts to            an on-failure destination for later processing.",
                                    "NoteWhile Lambda functions typically have a maximum timeout limit of 15 minutes,\n      event source mappings for Amazon MSK, self-managed Apache Kafka, Amazon DocumentDB, and Amazon MQ for ActiveMQ and RabbitMQ only support functions with\n      maximum timeout limits of 14 minutes. This constraint ensures that the event source mapping can properly\n      handle function errors and retries."
                                ]
                            },
                            {
                                "sub_header": "Polling and stream starting positions",
                                "content": [
                                    "Be aware that stream polling during event source mapping creation and updates is eventually consistent.",
                                    "This behavior means that if you specify LATEST as the starting position for the stream, the event source mapping could     miss events during creation or updates. To ensure that no events are missed, specify the stream starting position as TRIM_HORIZON     or AT_TIMESTAMP."
                                ]
                            },
                            {
                                "sub_header": "Amazon CloudWatch metrics",
                                "content": [
                                    "Lambda emits the OffsetLag metric while your function processes records. The value of this metric      is the difference in offset between the last record written to the Kafka event source topic and the last record that your function's       consumer group processed. You can use OffsetLag to estimate the latency between when a record is added and when      your consumer group processes it.",
                                    "An increasing trend in OffsetLag can indicate issues with pollers in your function's consumer group. For more information, see      Using CloudWatch metrics with Lambda."
                                ]
                            },
                            {
                                "sub_header": "Message throughput scaling behavior for Amazon MSK event source mappings",
                                "content": [
                                    "You can choose between two modes of message throughput scaling behavior for your Amazon MSK            event source mapping:",
                                    {
                                        "sub_header": "Default (on-demand) mode",
                                        "content": [
                                            "When you initially create an Amazon MSK event source, Lambda allocates a default number of event                pollers to process all partitions in the Kafka topic. Lambda automatically scales up or down the                number of event pollers based on message load.",
                                            "In one-minute intervals, Lambda evaluates the offset lag                 of all the partitions in the topic. If the offset lag is too high, the partition is                receiving messages faster than Lambda can process them. If necessary, Lambda adds or removes                event pollers from the topic. This autoscaling process of adding or removing event pollers                occurs within three minutes of evaluation.",
                                            "If your target Lambda function is throttled, Lambda reduces the number of event pollers. This                action reduces the workload on the function by reducing the number of messages that event                pollers can retrieve and send to the function."
                                        ]
                                    },
                                    {
                                        "sub_header": "Configuring provisioned mode",
                                        "content": [
                                            "For workloads where you need to fine-tune the throughput of your event source mapping,                you can use provisioned mode. In provisioned mode, you define minimum and maximum limits                for the amount of provisioned event pollers. These provisioned event pollers are dedicated                to your event source mapping, and can handle unexpected message spikes through responsive                autoscaling. We recommend that you use provisioned mode for Kafka workloads that have strict                performance requirements.",
                                            "In Lambda, an event poller is a compute unit capable of handling up to 5 MBps of throughput.    For reference, suppose your event source produces an average payload of 1MB, and the average function duration is 1 sec.    If the payload doesn’t undergo any transformation (such as filtering), a single poller can support 5 MBps throughput,    and 5 concurrent Lambda invocations. Using provisioned mode incurs additional costs. For pricing estimates,    see AWS Lambda pricing.",
                                            "NoteWhen using provisioned mode, you don't need to create AWS PrivateLink VPC endpoints\n                    or grant the associated permissions as part of your\n                    network configuration.",
                                            "In provisioned mode, the range of accepted values for the minimum number of event pollers                (MinimumPollers) is between 1 and 200, inclusive. The range of                accepted values for the maximum number of event pollers (MaximumPollers)                is between 1 and 2,000, inclusive. MaximumPollers must be greater than                or equal to MinimumPollers. In addition, to maintain ordered                processing within partitions, Lambda caps the MaximumPollers to the                number of partitions in the topic.",
                                            "For more details about choosing appropriate values for minimum and maximum event pollers,                see Best practices and considerations when using provisioned mode.",
                                            "You can configure provisioned mode for your Amazon MSK event source mapping using the console                or the Lambda API.",
                                            "  3.Configuration Choose , then choose Triggers.",
                                            "  4.Edit Choose the Amazon MSK event source mapping that you want to configure provisioned mode for,\n                        then choose .",
                                            "  5.Event source mapping configuration Under , choose \n                        Configure provisioned mode.",
                                            "  6.Minimum event pollers For , enter a value between 1 and 200.\n                                If you don't specify a value, Lambda chooses a default value of 1.",
                                            "  7.Maximum event pollers For , enter a value between 1 and 2,000.\n                                This value must be greater than or equal to your value for Minimum event\n                                pollers. If you don't specify a value, Lambda chooses a default value of 200.",
                                            "  8.Save Choose .",
                                            "You can configure provisioned mode programmatically using the ProvisionedPollerConfig object                in your                 EventSourceMappingConfiguration. For example, the following UpdateEventSourceMapping CLI                command configures a MinimumPollers value of 5, and a                MaximumPollers value of 100.",
                                            "aws lambda update-event-source-mapping \\    --uuid a1b2c3d4-5678-90ab-cdef-EXAMPLE11111 \\    --provisioned-poller-config '{\"MinimumPollers\": 5, \"MaximumPollers\": 100}'",
                                            "After configuring provisioned mode, you can observe the usage of event pollers for your workload by monitoring    the ProvisionedPollers metric. For more information, see Event source mapping metrics.",
                                            "To disable provisioned mode and return to default (on-demand) mode,                you can use the following UpdateEventSourceMapping CLI                command:",
                                            "aws lambda update-event-source-mapping \\    --uuid a1b2c3d4-5678-90ab-cdef-EXAMPLE11111 \\    --provisioned-poller-config '{}'"
                                        ]
                                    },
                                    {
                                        "sub_header": "Best practices and considerations when using provisioned mode",
                                        "content": [
                                            "The optimal configuration of minimum and maximum event pollers for your event source mapping                depends on your application's performance requirements. We recommend that you start with the                default minimum event pollers to baseline the performance profile. Adjust your configuration                based on observed message processing patterns and your desired performance profile.",
                                            "For workloads with spiky traffic and strict performance needs, increase the minimum event                pollers to handle sudden surges in messages. To determine the minimum event pollers required,                consider your workload's messages per second and average payload size, and use the throughput                capacity of a single event poller (up to 5 MBps) as a reference.",
                                            "To maintain ordered processing within a partition, Lambda limits the maximum event pollers                to the number of partitions in the topic. Additionally, the maximum event pollers your event                source mapping can scale to depends on the function's concurrency settings.",
                                            "When activating provisioned mode, update your network settings to remove AWS PrivateLink VPC                endpoints and associated permissions."
                                        ]
                                    }
                                ]
                            }
                        ]
                    },
                    {
                        "title": "Event filtering",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/with-msk-filtering.html",
                        "sections": [
                            "",
                            "You can use event filtering to control which records from a stream or queue Lambda sends to your function.    For general information about how event filtering works, see Control which events Lambda sends to your function.",
                            "This section focuses on event filtering for Amazon MSK event sources.",
                            "  1.Amazon MSK event filtering basics",
                            {
                                "sub_header": "Amazon MSK event filtering basics",
                                "content": [
                                    "Suppose a producer is writing messages to a topic in your Amazon MSK cluster, either in valid JSON format or as plain strings. An example record             would look like the following, with the message converted to a Base64 encoded string in the value field.",
                                    "{    \"mytopic-0\":[        {            \"topic\":\"mytopic\",            \"partition\":0,            \"offset\":15,            \"timestamp\":1545084650987,            \"timestampType\":\"CREATE_TIME\",            \"value\":\"SGVsbG8sIHRoaXMgaXMgYSB0ZXN0Lg==\",            \"headers\":[]        }    ]}",
                                    "Suppose your Apache Kafka producer is writing messages to your topic in the following JSON format.",
                                    "{    \"device_ID\": \"AB1234\",    \"session\":{        \"start_time\": \"yyyy-mm-ddThh:mm:ss\",        \"duration\": 162    }}",
                                    "You can use the value key to filter records. Suppose you wanted to filter only those records where device_ID             begins with the letters AB. The FilterCriteria object would be as follows.",
                                    "{    \"Filters\": [        {            \"Pattern\": \"{ \\\"value\\\" : { \\\"device_ID\\\" : [ { \\\"prefix\\\": \\\"AB\\\" } ] } }\"        }    ]}",
                                    "For added clarity, here is the value of the filter's Pattern expanded in plain JSON.",
                                    "{    \"value\": {        \"device_ID\": [ { \"prefix\": \"AB\" } ]      }}",
                                    "You can add your filter using the console, AWS CLI or an AWS SAM template.",
                                    "ConsoleTo add this filter using the console, follow the instructions in Attaching filter criteria to an event source mapping (console) and enter the following                         string for the Filter criteria.{ \"value\" : { \"device_ID\" : [ { \"prefix\":  \"AB\" } ] } }AWS CLITo create a new event source mapping with these filter criteria using the AWS Command Line Interface (AWS CLI), run the following                        command.aws lambda create-event-source-mapping \\    --function-name my-function \\    --event-source-arn arn:aws:kafka:us-east-2:123456789012:cluster/my-cluster/b-8ac7cc01-5898-482d-be2f-a6b596050ea8 \\    --filter-criteria '{\"Filters\": [{\"Pattern\": \"{ \\\"value\\\" : { \\\"device_ID\\\" : [ { \\\"prefix\\\":  \\\"AB\\\" } ] } }\"}]}'To add these filter criteria to an existing event source mapping, run the following command.aws lambda update-event-source-mapping \\    --uuid \"a1b2c3d4-5678-90ab-cdef-11111EXAMPLE\" \\    --filter-criteria '{\"Filters\": [{\"Pattern\": \"{ \\\"value\\\" : { \\\"device_ID\\\" : [ { \\\"prefix\\\":  \\\"AB\\\" } ] } }\"}]}'AWS SAMTo add this filter using AWS SAM, add the following snippet to the YAML template for your event source.FilterCriteria:  Filters:    - Pattern: '{ \"value\" : { \"device_ID\" : [ { \"prefix\":  \"AB\" } ] } }'anchoranchoranchorConsoleAWS CLIAWS SAMTo add this filter using the console, follow the instructions in Attaching filter criteria to an event source mapping (console) and enter the following                         string for the Filter criteria.{ \"value\" : { \"device_ID\" : [ { \"prefix\":  \"AB\" } ] } }",
                                    "With Amazon MSK, you can also filter records where the message is a plain string. Suppose you want to ignore those messages where the string is             \"error\". The FilterCriteria object would look as follows.",
                                    "{    \"Filters\": [        {            \"Pattern\": \"{ \\\"value\\\" : [ { \\\"anything-but\\\": [ \\\"error\\\" ] } ] }\"        }    ]}",
                                    "For added clarity, here is the value of the filter's Pattern expanded in plain JSON.",
                                    "{    \"value\": [        {        \"anything-but\": [ \"error\" ]        }    ]}",
                                    "You can add your filter using the console, AWS CLI or an AWS SAM template.",
                                    "ConsoleTo add this filter using the console, follow the instructions in Attaching filter criteria to an event source mapping (console) and enter the following                         string for the Filter criteria.{ \"value\" : [ { \"anything-but\": [ \"error\" ] } ] }AWS CLITo create a new event source mapping with these filter criteria using the AWS Command Line Interface (AWS CLI), run the following                        command.aws lambda create-event-source-mapping \\    --function-name my-function \\    --event-source-arn arn:aws:kafka:us-east-2:123456789012:cluster/my-cluster/b-8ac7cc01-5898-482d-be2f-a6b596050ea8 \\    --filter-criteria '{\"Filters\": [{\"Pattern\": \"{ \\\"value\\\" : [ { \\\"anything-but\\\": [ \\\"error\\\" ] } ] }\"}]}'To add these filter criteria to an existing event source mapping, run the following command.aws lambda update-event-source-mapping \\    --uuid \"a1b2c3d4-5678-90ab-cdef-11111EXAMPLE\" \\    --filter-criteria '{\"Filters\": [{\"Pattern\": \"{ \\\"value\\\" : [ { \\\"anything-but\\\": [ \\\"error\\\" ] } ] }\"}]}'AWS SAMTo add this filter using AWS SAM, add the following snippet to the YAML template for your event source.FilterCriteria:  Filters:    - Pattern: '{ \"value\" : [ { \"anything-but\": [ \"error\" ] } ] }'anchoranchoranchorConsoleAWS CLIAWS SAMTo add this filter using the console, follow the instructions in Attaching filter criteria to an event source mapping (console) and enter the following                         string for the Filter criteria.{ \"value\" : [ { \"anything-but\": [ \"error\" ] } ] }",
                                    "Amazon MSK messages must be UTF-8 encoded strings, either plain strings or in JSON format. That's because Lambda decodes Amazon MSK byte arrays into UTF-8 before             applying filter criteria. If your messages use another encoding, such as UTF-16 or ASCII, or if the message format doesn't match the             FilterCriteria format, Lambda processes metadata filters only. The following table summarizes the specific behavior:",
                                    "\n\nIncoming message format\nFilter pattern format for message properties\nResulting action\n\n\n\n\nPlain string\n\n\nPlain string\n\n\nLambda filters based on your filter criteria.\n\n\n\n\nPlain string\n\n\nNo filter pattern for data properties\n\n\nLambda filters (on the other metadata properties only) based on your filter criteria.\n\n\n\n\nPlain string\n\n\nValid JSON\n\n\nLambda filters (on the other metadata properties only) based on your filter criteria.\n\n\n\n\nValid JSON\n\n\nPlain string\n\n\nLambda filters (on the other metadata properties only) based on your filter criteria.\n\n\n\n\nValid JSON\n\n\nNo filter pattern for data properties\n\n\nLambda filters (on the other metadata properties only) based on your filter criteria.\n\n\n\n\nValid JSON\n\n\nValid JSON\n\n\nLambda filters based on your filter criteria.\n\n\n\n\nNon-UTF-8 encoded string\n\n\nJSON, plain string, or no pattern\n\n\nLambda filters (on the other metadata properties only) based on your filter criteria.\n\n\n"
                                ]
                            }
                        ]
                    },
                    {
                        "title": "On-failure destinations",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/with-msk-on-failure.html",
                        "sections": [
                            "",
                            "To retain records of failed event source mapping invocations, add a destination to your function's event source mapping. Each record sent to the destination is a JSON document containing metadata about the failed invocation. For Amazon S3 destinations, Lambda also sends the entire invocation record along with the metadata. You can configure any Amazon SNS topic, Amazon SQS queue, or S3 bucket as a destination.",
                            "With Amazon S3 destinations, you can use the Amazon S3 Event Notifications feature to receive notifications when objects are uploaded to your destination S3 bucket. You can also configure S3 Event Notifications to invoke another Lambda function to perform automated processing on failed batches.",
                            "Your execution role must have permissions for the destination:",
                            "  1.For SQS destinations:  sqs:SendMessage",
                            "  2.For SNS destinations:  sns:Publish",
                            "  3.For S3 bucket destinations:   s3:PutObject and s3:ListBucket",
                            "You must deploy a VPC endpoint for your on-failure destination service inside your Amazon MSK cluster VPC.",
                            "Additionally, if you configured a KMS key on your destination, Lambda needs the following        permissions depending on the destination type:",
                            {
                                "sub_header": "Configuring on-failure destinations for an Amazon MSK event source mapping",
                                "content": [
                                    "To configure an on-failure destination using the console, follow these steps:",
                                    "Open the Functions page of the Lambda console.\nChoose a function.\n\nUnder Function overview, choose Add destination.\n\nFor Source, choose Event source mapping invocation.\n\nFor Event source mapping, choose an event source that's configured\n              for this function.\n\nFor Condition, select On failure. For event\n              source mapping invocations, this is the only accepted condition.\n\nFor Destination type, choose the destination type that Lambda sends\n              invocation records to.\n\nFor Destination, choose a resource.\n\nChoose Save.\n",
                                    "You can also configure an on-failure destination using the AWS CLI. For example, the following          create-event-source-mapping command adds an event source mapping with an SQS on-failure destination to          MyFunction:",
                                    "aws lambda create-event-source-mapping \\--function-name \"MyFunction\" \\--event-source-arn arn:aws:kafka:us-east-1:123456789012:cluster/vpc-2priv-2pub/751d2973-a626-431c-9d4e-d7975eb44dd7-2 \\--destination-config '{\"OnFailure\": {\"Destination\": \"arn:aws:sqs:us-east-1:123456789012:dest-queue\"}}'",
                                    "The following update-event-source-mapping command adds an S3 on-failure destination to the event source associated with the input uuid:",
                                    "aws lambda update-event-source-mapping \\--uuid f89f8514-cdd9-4602-9e1f-01a5b77d449b \\--destination-config '{\"OnFailure\": {\"Destination\": \"arn:aws:s3:::dest-bucket\"}}'",
                                    "To remove a destination, supply an empty string as the argument to the          destination-config parameter:",
                                    "aws lambda update-event-source-mapping \\--uuid f89f8514-cdd9-4602-9e1f-01a5b77d449b \\--destination-config '{\"OnFailure\": {\"Destination\": \"\"}}'",
                                    {
                                        "sub_header": "Security best practices for Amazon S3 destinations",
                                        "content": [
                                            "Deleting an S3 bucket that's configured as a destination without removing the destination from your function's configuration can create a security risk. If another       user knows your destination bucket's name, they can recreate the bucket in their AWS account. Records of failed invocations will be sent to their bucket, potentially       exposing data from your function.",
                                            {
                                                "code_example": "s3:PutObject"
                                            },
                                            "The following example shows an IAM policy that limits your function's s3:PutObject permissions to buckets in your account. This policy also gives Lambda        the s3:ListBucket permission it needs to use an S3 bucket as a destination.",
                                            "{    \"Version\": \"2012-10-17\",    \"Statement\": [        {            \"Sid\": \"S3BucketResourceAccountWrite\",            \"Effect\": \"Allow\",            \"Action\": [                \"s3:PutObject\",                \"s3:ListBucket\"            ],            \"Resource\": \"arn:aws:s3:::*/*\",            \"Condition\": {                \"StringEquals\": {                    \"s3:ResourceAccount\": \"111122223333\"                }            }        }    ]}",
                                            "To add a permissions policy to your funcion's execution role using the AWS Management Console or AWS CLI, refer to the instructions in the following procedures:",
                                            "ConsoleTo add a permissions policy to a function's execution role (console)Open the Functions page of the Lambda console.Select the Lambda function whose execution role you want to modify.In the Configuration tab, select Permissions.In the Execution role tab, select your function's Role name to open the role's IAM console page.Add a permissions policy to the role by doing the following:In the Permissions policies pane, choose Add permissions and select Create inline policy.In Policy editor, select JSON.Paste the policy you want to add into the editor (replacing the existing JSON), and then choose Next.Under Policy details, enter a Policy name.Choose Create policy.AWS CLITo add a permissions policy to a function's execution role (CLI)Create a JSON policy document with the required permissions and save it in a local directory.Use the IAM put-role-policy CLI command to add the permissions to your function's execution role. Run the following command from the                 directory you saved your JSON policy document in and replace the role name, policy name, and policy document with your own values.aws iam put-role-policy \\--role-name my_lambda_role \\--policy-name LambdaS3DestinationPolicy \\--policy-document file://my_policy.jsonanchoranchorConsoleAWS CLITo add a permissions policy to a function's execution role (console)Open the Functions page of the Lambda console.Select the Lambda function whose execution role you want to modify.In the Configuration tab, select Permissions.In the Execution role tab, select your function's Role name to open the role's IAM console page.Add a permissions policy to the role by doing the following:In the Permissions policies pane, choose Add permissions and select Create inline policy.In Policy editor, select JSON.Paste the policy you want to add into the editor (replacing the existing JSON), and then choose Next.Under Policy details, enter a Policy name.Choose Create policy."
                                        ]
                                    },
                                    {
                                        "sub_header": "SNS and SQS example invocation record",
                                        "content": [
                                            "The following example shows what Lambda sends to an SNS topic or SQS queue destination for a          failed Kafka event source invocation. Each of the keys under recordsInfo contains          both the Kafka topic and partition, separated by a hyphen. For example, for the key          \"Topic-0\", Topic is the Kafka topic, and 0 is the          partition. For each topic and partition, you can use the offsets and timestamp data to find          the original invocation records.",
                                            "{    \"requestContext\": {        \"requestId\": \"316aa6d0-8154-xmpl-9af7-85d5f4a6bc81\",        \"functionArn\": \"arn:aws:lambda:us-east-1:123456789012:function:myfunction\",        \"condition\": \"RetryAttemptsExhausted\" | \"MaximumPayloadSizeExceeded\",        \"approximateInvokeCount\": 1    },    \"responseContext\": { // null if record is MaximumPayloadSizeExceeded        \"statusCode\": 200,        \"executedVersion\": \"$LATEST\",        \"functionError\": \"Unhandled\"    },    \"version\": \"1.0\",    \"timestamp\": \"2019-11-14T00:38:06.021Z\",    \"KafkaBatchInfo\": {        \"batchSize\": 500,        \"eventSourceArn\": \"arn:aws:kafka:us-east-1:123456789012:cluster/vpc-2priv-2pub/751d2973-a626-431c-9d4e-d7975eb44dd7-2\",        \"bootstrapServers\": \"...\",        \"payloadSize\": 2039086, // In bytes        \"recordsInfo\": {            \"Topic-0\": {                \"firstRecordOffset\": \"49601189658422359378836298521827638475320189012309704722\",                \"lastRecordOffset\": \"49601189658422359378836298522902373528957594348623495186\",                \"firstRecordTimestamp\": \"2019-11-14T00:38:04.835Z\",                \"lastRecordTimestamp\": \"2019-11-14T00:38:05.580Z\",            },            \"Topic-1\": {                \"firstRecordOffset\": \"49601189658422359378836298521827638475320189012309704722\",                \"lastRecordOffset\": \"49601189658422359378836298522902373528957594348623495186\",                \"firstRecordTimestamp\": \"2019-11-14T00:38:04.835Z\",                \"lastRecordTimestamp\": \"2019-11-14T00:38:05.580Z\",            }        }    }}"
                                        ]
                                    },
                                    {
                                        "sub_header": "S3 destination example invocation record",
                                        "content": [
                                            "For S3 destinations, Lambda sends the entire invocation record along with the metadata          to the destination. The following example shows that Lambda sends to an S3 bucket destination          for a failed Kafka event source invocation. In addition to all of the fields from the previous          example for SQS and SNS destinations, the payload field contains the original          invocation record as an escaped JSON string.",
                                            "{    \"requestContext\": {        \"requestId\": \"316aa6d0-8154-xmpl-9af7-85d5f4a6bc81\",        \"functionArn\": \"arn:aws:lambda:us-east-1:123456789012:function:myfunction\",        \"condition\": \"RetryAttemptsExhausted\" | \"MaximumPayloadSizeExceeded\",        \"approximateInvokeCount\": 1    },    \"responseContext\": { // null if record is MaximumPayloadSizeExceeded        \"statusCode\": 200,        \"executedVersion\": \"$LATEST\",        \"functionError\": \"Unhandled\"    },    \"version\": \"1.0\",    \"timestamp\": \"2019-11-14T00:38:06.021Z\",    \"KafkaBatchInfo\": {        \"batchSize\": 500,        \"eventSourceArn\": \"arn:aws:kafka:us-east-1:123456789012:cluster/vpc-2priv-2pub/751d2973-a626-431c-9d4e-d7975eb44dd7-2\",        \"bootstrapServers\": \"...\",        \"payloadSize\": 2039086, // In bytes        \"recordsInfo\": {            \"Topic-0\": {                \"firstRecordOffset\": \"49601189658422359378836298521827638475320189012309704722\",                \"lastRecordOffset\": \"49601189658422359378836298522902373528957594348623495186\",                \"firstRecordTimestamp\": \"2019-11-14T00:38:04.835Z\",                \"lastRecordTimestamp\": \"2019-11-14T00:38:05.580Z\",            },            \"Topic-1\": {                \"firstRecordOffset\": \"49601189658422359378836298521827638475320189012309704722\",                \"lastRecordOffset\": \"49601189658422359378836298522902373528957594348623495186\",                \"firstRecordTimestamp\": \"2019-11-14T00:38:04.835Z\",                \"lastRecordTimestamp\": \"2019-11-14T00:38:05.580Z\",            }        }    },    \"payload\": \"<Whole Event>\" // Only available in S3}",
                                            "TipWe recommend enabling S3 versioning on your destination bucket."
                                        ]
                                    }
                                ]
                            }
                        ]
                    },
                    {
                        "title": "Tutorial",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/services-msk-tutorial.html",
                        "sections": [
                            "",
                            "In this tutorial, you will perform the following:",
                            "After you are finished with these steps, when events are sent to Amazon MSK, you will be able to set up a Lambda        function to process those events automatically with your own custom Lambda code.",
                            " What can you do with this feature? ",
                            "Example solution: Use an MSK event source mapping to deliver live scores to your        customers.",
                            "Consider the following scenario: Your company hosts a web application where        your customers can view information about live events, such as sports games. Information updates from the game        are provided to your team through a Kafka topic on Amazon MSK. You want to design a solution that consumes updates        from the MSK topic to provide an updated view of the live event to customers inside an application you develop.        You have decided on the following design approach: Your client applications will communicate with a serverless        backend hosted in AWS. Clients will connect over websocket sessions using the Amazon API Gateway WebSocket API.",
                            "In this solution, you need a component that reads MSK events, performs some custom logic to prepare those        events for the application layer and then forwards that information to the API Gateway API. You can implement this        component with AWS Lambda, by providing your custom logic in a Lambda function, then calling it with a        AWS Lambda Amazon MSK event source mapping.",
                            "For more information about implementing solutions using the Amazon API Gateway WebSocket API, see WebSocket API        tutorials in the API Gateway documentation.",
                            {
                                "sub_header": "Prerequisites",
                                "content": [
                                    "An AWS account with the following preconfigured resources:",
                                    "To fulfill these prerequisites, we recommend following Getting started using Amazon MSK in the Amazon MSK            documentation.",
                                    "  3.IAM role-based authentication Ensure  is Enabled\n                            in your cluster security settings. This improves your security by limiting your Lambda\n                            function to only access the Amazon MSK resources needed. This is enabled by default on new Amazon MSK\n                            clusters.",
                                    "  4.Public access Ensure  is off in your cluster networking settings.\n                            Restricting your Amazon MSK cluster's access to the internet improves your security by limiting\n                            how many intermediaries handle your data. This is enabled by default on new Amazon MSK\n                            clusters.",
                                    "Once you have set up these resources, gather the following information from your AWS account to confirm that you are ready to continue.",
                                    {
                                        "code_example": "clusterSecurityGroups"
                                    },
                                    "The following permissions in your AWS account:"
                                ]
                            },
                            {
                                "sub_header": "Configure network connectivity for Lambda to communicate with Amazon MSK",
                                "content": [
                                    " Use AWS PrivateLink to connect Lambda and Amazon MSK. You can do so by creating interface            Amazon VPC endpoints in the Amazon VPC console. For more information about networking configuration, see Configure network security.        ",
                                    "When a Amazon MSK event source mapping runs on the behalf of a Lambda function, it assumes the Lambda function’s execution role. This IAM role            authorizes the mapping to access resources secured by IAM, such as your Amazon MSK cluster. Although the            components share an execution role, the Amazon MSK mapping and your Lambda function have separate connectivity            requirements for their respective tasks, as shown in the following diagram.",
                                    "\n\n",
                                    "Your event source mapping belongs to your Amazon MSK cluster security group. In this networking step, create            Amazon VPC endpoints from your Amazon MSK cluster VPC to connect the event source mapping to the Lambda and STS            services. Secure these endpoints to accept traffic from your Amazon MSK cluster security group. Then, adjust the            Amazon MSK cluster security groups to allow the event source mapping to communicate with the Amazon MSK            cluster.",
                                    " You can configure the following steps using the AWS Management Console.",
                                    {
                                        "code_example": "endpointSecurityGroup"
                                    }
                                ]
                            },
                            {
                                "sub_header": "Create an IAM role for Lambda to read from your Amazon MSK topic",
                                "content": [
                                    "Identify the auth requirements for Lambda to read from your Amazon MSK topic, then define them in a policy.            Create a role, lambdaAuthRole, that authorizes Lambda to use those            permissions. Authorize actions on your Amazon MSK cluster using kafka-cluster            IAM actions. Then, authorize Lambda to perform Amazon MSK kafka and Amazon EC2 actions            needed to discover and connect to your Amazon MSK cluster, as well as CloudWatch actions so Lambda can log what it has            done.",
                                    {
                                        "code_example": "clusterAuthPolicy"
                                    },
                                    "For details about the IAM policy language, see            the IAM documentation.",
                                    "Now that you have written your policy document, create an IAM policy so you can attach it to your role. You can do this using the console with the following procedure.",
                                    {
                                        "code_example": "clusterAuthPolicy"
                                    },
                                    "For more information, see Creating IAM policies in the IAM documentation.",
                                    "Now that you have appropriate IAM policies, create a role and attach them to it. You can do this using the console with the following procedure.",
                                    {
                                        "code_example": "clusterAuthPolicy"
                                    },
                                    "For more information, see Defining Lambda function permissions with an execution role."
                                ]
                            },
                            {
                                "sub_header": "Create a Lambda function to read from your Amazon MSK topic",
                                "content": [
                                    "Create a Lambda function configured to use your            IAM role. You can create your Lambda function using the console.",
                                    {
                                        "code_example": "Node.js"
                                    },
                                    "In a production environment, you usually need to add further policies to the execution role for your Lambda function to             meaningfully process your Amazon MSK events. For more information on adding policies to your role, see Add or            remove identity permissions in the IAM documentation."
                                ]
                            },
                            {
                                "sub_header": "Create an event source mapping to your Lambda function",
                                "content": [
                                    "Your Amazon MSK event source mapping provides the Lambda service the information necessary to invoke your            Lambda when appropriate Amazon MSK events occur. You can create a Amazon MSK mapping using the console. Create a Lambda            trigger, then the event source mapping is automatically set up.",
                                    "To create a Lambda trigger (and event source mapping)Navigate to your Lambda function's overview page.In the function overview section, choose Add trigger on the bottom left.In the Select a source dropdown, select Amazon MSK.Don't set authentication.For MSK cluster, select your cluster's name.For Batch size, enter 1. This step makes this feature easier to test, and is not an ideal value in production.For Topic name, provide the name of your Kafka topic.For Consumer group ID, provide the id of your Kafka consumer group."
                                ]
                            },
                            {
                                "sub_header": "Update your Lambda function to read your streaming data",
                                "content": [
                                    "            Lambda provides information about Kafka events through the event method parameter. For an example structure of a Amazon MSK event, see  Example event.            After you understand how to interpret Lambda forwarded Amazon MSK events, you can alter your Lambda function code to use the information they provide.        ",
                                    "            Provide the following code to your Lambda function to log the contents of a Lambda Amazon MSK event for testing purposes:        ",
                                    ".NETAWS SDK for .NETNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Consuming an Amazon MSK event with Lambda using .NET.using System.Text;using Amazon.Lambda.Core;using Amazon.Lambda.KafkaEvents;// Assembly attribute to enable the Lambda function's JSON input to be converted into a .NET class.[assembly: LambdaSerializer(typeof(Amazon.Lambda.Serialization.SystemTextJson.DefaultLambdaJsonSerializer))]namespace MSKLambda;public class Function{            /// <param name=\"input\">The event for the Lambda function handler to process.</param>    /// <param name=\"context\">The ILambdaContext that provides methods for logging and describing the Lambda environment.</param>    /// <returns></returns>    public void FunctionHandler(KafkaEvent evnt, ILambdaContext context)    {        foreach (var record in evnt.Records)        {            Console.WriteLine(\"Key:\" + record.Key);             foreach (var eventRecord in record.Value)            {                var valueBytes = eventRecord.Value.ToArray();                    var valueText = Encoding.UTF8.GetString(valueBytes);                                Console.WriteLine(\"Message:\" + valueText);            }        }    }    }GoSDK for Go V2Note        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Consuming an Amazon MSK event with Lambda using Go.package mainimport (\t\"encoding/base64\"\t\"fmt\"\t\"github.com/aws/aws-lambda-go/events\"\t\"github.com/aws/aws-lambda-go/lambda\")func handler(event events.KafkaEvent) {\tfor key, records := range event.Records {\t\tfmt.Println(\"Key:\", key)\t\tfor _, record := range records {\t\t\tfmt.Println(\"Record:\", record)\t\t\tdecodedValue, _ := base64.StdEncoding.DecodeString(record.Value)\t\t\tmessage := string(decodedValue)\t\t\tfmt.Println(\"Message:\", message)\t\t}\t}}func main() {\tlambda.Start(handler)}JavaSDK for Java 2.xNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Consuming an Amazon MSK event with Lambda using Java.import com.amazonaws.services.lambda.runtime.Context;import com.amazonaws.services.lambda.runtime.RequestHandler;import com.amazonaws.services.lambda.runtime.events.KafkaEvent;import com.amazonaws.services.lambda.runtime.events.KafkaEvent.KafkaEventRecord;import java.util.Base64;import java.util.Map;public class Example implements RequestHandler<KafkaEvent, Void> {    @Override    public Void handleRequest(KafkaEvent event, Context context) {        for (Map.Entry<String, java.util.List<KafkaEventRecord>> entry : event.getRecords().entrySet()) {            String key = entry.getKey();            System.out.println(\"Key: \" + key);            for (KafkaEventRecord record : entry.getValue()) {                System.out.println(\"Record: \" + record);                byte[] value = Base64.getDecoder().decode(record.getValue());                String message = new String(value);                System.out.println(\"Message: \" + message);            }        }        return null;    }}JavaScriptSDK for JavaScript (v3)Note        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Consuming an Amazon MSK event with Lambda using JavaScript.exports.handler = async (event) => {    // Iterate through keys    for (let key in event.records) {      console.log('Key: ', key)      // Iterate through records      event.records[key].map((record) => {        console.log('Record: ', record)        // Decode base64        const msg = Buffer.from(record.value, 'base64').toString()        console.log('Message:', msg)      })     }}PHPSDK for PHPNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Consuming an Amazon MSK event with Lambda using PHP.<?php// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.// SPDX-License-Identifier: Apache-2.0// using bref/bref and bref/logger for simplicityuse Bref\\Context\\Context;use Bref\\Event\\Kafka\\KafkaEvent;use Bref\\Event\\Handler as StdHandler;use Bref\\Logger\\StderrLogger;require __DIR__ . '/vendor/autoload.php';class Handler implements StdHandler{    private StderrLogger $logger;    public function __construct(StderrLogger $logger)    {        $this->logger = $logger;    }    /**     * @throws JsonException     * @throws \\Bref\\Event\\InvalidLambdaEvent     */    public function handle(mixed $event, Context $context): void    {        $kafkaEvent = new KafkaEvent($event);        $this->logger->info(\"Processing records\");        $records = $kafkaEvent->getRecords();        foreach ($records as $record) {            try {                $key = $record->getKey();                $this->logger->info(\"Key: $key\");                $values = $record->getValue();                $this->logger->info(json_encode($values));                foreach ($values as $value) {                    $this->logger->info(\"Value: $value\");                }                            } catch (Exception $e) {                $this->logger->error($e->getMessage());            }        }        $totalRecords = count($records);        $this->logger->info(\"Successfully processed $totalRecords records\");    }}$logger = new StderrLogger();return new Handler($logger);PythonSDK for Python (Boto3)Note        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Consuming an Amazon MSK event with Lambda using Python.import base64def lambda_handler(event, context):    # Iterate through keys    for key in event['records']:        print('Key:', key)        # Iterate through records        for record in event['records'][key]:            print('Record:', record)            # Decode base64            msg = base64.b64decode(record['value']).decode('utf-8')            print('Message:', msg)RubySDK for RubyNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Consuming an Amazon MSK event with Lambda using Ruby.require 'base64'def lambda_handler(event:, context:)  # Iterate through keys  event['records'].each do |key, records|    puts \"Key: #{key}\"    # Iterate through records    records.each do |record|      puts \"Record: #{record}\"      # Decode base64      msg = Base64.decode64(record['value'])      puts \"Message: #{msg}\"    end  endendanchoranchoranchoranchoranchoranchoranchor.NETGoJavaJavaScriptPHPPythonRubyAWS SDK for .NETNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Consuming an Amazon MSK event with Lambda using .NET.using System.Text;using Amazon.Lambda.Core;using Amazon.Lambda.KafkaEvents;// Assembly attribute to enable the Lambda function's JSON input to be converted into a .NET class.[assembly: LambdaSerializer(typeof(Amazon.Lambda.Serialization.SystemTextJson.DefaultLambdaJsonSerializer))]namespace MSKLambda;public class Function{            /// <param name=\"input\">The event for the Lambda function handler to process.</param>    /// <param name=\"context\">The ILambdaContext that provides methods for logging and describing the Lambda environment.</param>    /// <returns></returns>    public void FunctionHandler(KafkaEvent evnt, ILambdaContext context)    {        foreach (var record in evnt.Records)        {            Console.WriteLine(\"Key:\" + record.Key);             foreach (var eventRecord in record.Value)            {                var valueBytes = eventRecord.Value.ToArray();                    var valueText = Encoding.UTF8.GetString(valueBytes);                                Console.WriteLine(\"Message:\" + valueText);            }        }    }    }",
                                    "You can provide function code to your Lambda using the console.",
                                    "To update function code using the console code editor\nOpen the Functions page of the Lambda console and select your function.\n\nSelect the Code tab.\n\nIn the Code source pane, select your source code file and edit it in the integrated code editor.\n\nIn the DEPLOY section, choose Deploy to update your function's code:\n\n\n\n"
                                ]
                            },
                            {
                                "sub_header": "Test your Lambda function to verify it is connected to your Amazon MSK topic",
                                "content": [
                                    "You can now verify whether or not your Lambda is being invoked by the event source by inspecting CloudWatch event logs.",
                                    {
                                        "code_example": "kafka-console-producer"
                                    }
                                ]
                            }
                        ]
                    }
                ],
                "sections": [
                    "",
                    "NoteIf you want to send data to a target other than a Lambda function or enrich the data before sending it, see \n    Amazon EventBridge Pipes.",
                    "Amazon Managed Streaming for Apache Kafka (Amazon MSK) is a    fully managed service that you can use to build and run applications that use Apache Kafka to process streaming data.    Amazon MSK simplifies the setup, scaling, and management of clusters running Kafka. Amazon MSK also makes it easier to    configure your application for multiple Availability Zones and for security with AWS Identity and Access Management (IAM). Amazon MSK supports    multiple open-source versions of Kafka.",
                    "Amazon MSK as an event source operates similarly to using Amazon Simple Queue Service (Amazon SQS) or Amazon Kinesis. Lambda internally polls for    new messages from the event source and then synchronously invokes the target Lambda function. Lambda reads the    messages in batches and provides these to your function as an event payload. The maximum batch size is configurable    (the default is 100 messages). For more information, see    Batching behavior.",
                    "By default, Lambda autoscales the number of event pollers for    your Amazon MSK event source mapping. To optimize the throughput of your Amazon MSK event source mapping, configure    provisioned mode. In provisioned mode, you can define the minimum and maximum number of event pollers allocated    to your event source mapping. This can improve the ability of your event source mapping to handle unexpected    message spikes. For more information, see provisioned mode.",
                    "WarningLambda event source mappings process each event at least once, and duplicate processing of records can occur. To avoid potential issues \nrelated to duplicate events, we strongly recommend that you make your function code idempotent. To learn more, see How do I make my Lambda function idempotent \nin the AWS Knowledge Center.",
                    "For an example of how to configure Amazon MSK as an event source, see Using Amazon MSK as an event source for    AWS Lambda on the AWS Compute Blog. For a complete tutorial, see  Amazon MSK Lambda Integration in the Amazon MSK    Labs.",
                    "  1. Example event",
                    "  2.Configuring Amazon MSK event sources for Lambda",
                    "  3.Processing Amazon MSK messages with Lambda",
                    "  4.Using event filtering with an Amazon MSK event source",
                    "  5.Capturing discarded batches for an Amazon MSK event source",
                    "  6.Tutorial: Using an Amazon MSK event source mapping to invoke a Lambda function",
                    {
                        "sub_header": " Example event",
                        "content": [
                            "Lambda sends the batch of messages in the event parameter when it invokes your function. The event payload      contains an array of messages. Each array item contains details of the Amazon MSK topic and partition identifier,      together with a timestamp and a base64-encoded message.",
                            "{   \"eventSource\":\"aws:kafka\",   \"eventSourceArn\":\"arn:aws:kafka:us-east-1:123456789012:cluster/vpc-2priv-2pub/751d2973-a626-431c-9d4e-d7975eb44dd7-2\",   \"bootstrapServers\":\"b-2.demo-cluster-1.a1bcde.c1.kafka.us-east-1.amazonaws.com:9092,b-1.demo-cluster-1.a1bcde.c1.kafka.us-east-1.amazonaws.com:9092\",   \"records\":{      \"mytopic-0\":[         {            \"topic\":\"mytopic\",            \"partition\":0,            \"offset\":15,            \"timestamp\":1545084650987,            \"timestampType\":\"CREATE_TIME\",            \"key\":\"abcDEFghiJKLmnoPQRstuVWXyz1234==\",            \"value\":\"SGVsbG8sIHRoaXMgaXMgYSB0ZXN0Lg==\",            \"headers\":[               {                  \"headerKey\":[                     104,                     101,                     97,                     100,                     101,                     114,                     86,                     97,                     108,                     117,                     101                  ]               }            ]         }      ]   }}"
                        ]
                    }
                ]
            },
            {
                "title": "RDS",
                "href": "https://docs.aws.amazon.com/lambda/latest/dg/services-rds.html",
                "sections": [
                    "",
                    "You can connect a Lambda function to an Amazon Relational Database Service (Amazon RDS) database directly and through    an Amazon RDS Proxy. Direct connections are useful in simple scenarios, and proxies are recommended    for production. A database proxy manages a pool of shared database connections which enables    your function to reach high concurrency levels without exhausting database connections.",
                    "We recommend using Amazon RDS Proxy for Lambda functions that make frequent short database    connections, or open and close large numbers of database connections. For more information,    see     Automatically connecting a Lambda function and a DB instance in the Amazon Relational Database Service Developer Guide.",
                    {
                        "sub_header": "Configuring your function to work with RDS resources",
                        "content": [
                            "In the Lambda console, you can provision, and configure, Amazon RDS database instances and      proxy resources. You can do this by navigating to RDS databases under      the Configuration tab. Alternatively, you can also create and configure      connections to Lambda functions in the Amazon RDS console. When configuring an RDS database      instance to use with Lambda, note the following criteria:",
                            {
                                "code_example": "{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"ec2:CreateSecurityGroup\",\n        \"ec2:DescribeSecurityGroups\",\n        \"ec2:DescribeSubnets\",\n        \"ec2:DescribeVpcs\",\n        \"ec2:AuthorizeSecurityGroupIngress\",\n        \"ec2:AuthorizeSecurityGroupEgress\",\n        \"ec2:RevokeSecurityGroupEgress\",\n        \"ec2:CreateNetworkInterface\",\n        \"ec2:DeleteNetworkInterface\",\n        \"ec2:DescribeNetworkInterfaces\"\n      ],\n      \"Resource\": \"*\"\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"rds-db:connect\",\n        \"rds:CreateDBProxy\",\n        \"rds:CreateDBInstance\",\n        \"rds:CreateDBSubnetGroup\",\n        \"rds:DescribeDBClusters\",\n        \"rds:DescribeDBInstances\",\n        \"rds:DescribeDBSubnetGroups\",\n        \"rds:DescribeDBProxies\",\n        \"rds:DescribeDBProxyTargets\",\n        \"rds:DescribeDBProxyTargetGroups\",\n        \"rds:RegisterDBProxyTargets\",\n        \"rds:ModifyDBInstance\",\n        \"rds:ModifyDBProxy\"\n      ],\n      \"Resource\": \"*\"\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"lambda:CreateFunction\",\n        \"lambda:ListFunctions\",\n        \"lambda:UpdateFunctionConfiguration\"\n      ],\n      \"Resource\": \"*\"\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"iam:AttachRolePolicy\",\n        \"iam:AttachPolicy\",\n        \"iam:CreateRole\",\n        \"iam:CreatePolicy\"\n      ],\n      \"Resource\": \"*\"\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"secretsmanager:GetResourcePolicy\",\n        \"secretsmanager:GetSecretValue\",\n        \"secretsmanager:DescribeSecret\",\n        \"secretsmanager:ListSecretVersionIds\",\n        \"secretsmanager:CreateSecret\"\n      ],\n      \"Resource\": \"*\"\n    }\n  ]\n}\n"
                            },
                            "Amazon RDS charges an hourly rate for proxies based on the database instance size, see      RDS Proxy pricing for details.      For more information on proxy connections in general, see      Using Amazon RDS Proxy in the Amazon RDS User Guide.",
                            "Lambda and Amazon RDS setupBoth Lambda and Amazon RDS consoles will assist you in automatically configuring some of\n        the required resources to make a connection between Lambda and Amazon RDS."
                        ]
                    },
                    {
                        "sub_header": "Connecting to an Amazon RDS database in a Lambda function",
                        "content": [
                            "The following code example shows how to implement a Lambda function that connects      to an Amazon RDS database. The function makes a simple database request and returns the result.",
                            "GoSDK for Go V2Note        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Connecting to an Amazon RDS database in a Lambda function using Go./*Golang v2 code here.*/package mainimport (\t\"context\"\t\"database/sql\"\t\"encoding/json\"\t\"fmt\"\t\"os\"\t\"github.com/aws/aws-lambda-go/lambda\"\t\"github.com/aws/aws-sdk-go-v2/config\"\t\"github.com/aws/aws-sdk-go-v2/feature/rds/auth\"\t_ \"github.com/go-sql-driver/mysql\")type MyEvent struct {\tName string `json:\"name\"`}func HandleRequest(event *MyEvent) (map[string]interface{}, error) {\tvar dbName string = os.Getenv(\"DatabaseName\")\tvar dbUser string = os.Getenv(\"DatabaseUser\")\tvar dbHost string = os.Getenv(\"DBHost\") // Add hostname without https\tvar dbPort int = os.Getenv(\"Port\")      // Add port number\tvar dbEndpoint string = fmt.Sprintf(\"%s:%d\", dbHost, dbPort)\tvar region string = os.Getenv(\"AWS_REGION\")\tcfg, err := config.LoadDefaultConfig(context.TODO())\tif err != nil {\t\tpanic(\"configuration error: \" + err.Error())\t}\tauthenticationToken, err := auth.BuildAuthToken(\t\tcontext.TODO(), dbEndpoint, region, dbUser, cfg.Credentials)\tif err != nil {\t\tpanic(\"failed to create authentication token: \" + err.Error())\t}\tdsn := fmt.Sprintf(\"%s:%s@tcp(%s)/%s?tls=true&allowCleartextPasswords=true\",\t\tdbUser, authenticationToken, dbEndpoint, dbName,\t)\tdb, err := sql.Open(\"mysql\", dsn)\tif err != nil {\t\tpanic(err)\t}\tdefer db.Close()\tvar sum int\terr = db.QueryRow(\"SELECT ?+? AS sum\", 3, 2).Scan(&sum)\tif err != nil {\t\tpanic(err)\t}\ts := fmt.Sprint(sum)\tmessage := fmt.Sprintf(\"The selected sum is: %s\", s)\tmessageBytes, err := json.Marshal(message)\tif err != nil {\t\treturn nil, err\t}\tmessageString := string(messageBytes)\treturn map[string]interface{}{\t\t\"statusCode\": 200,\t\t\"headers\":    map[string]string{\"Content-Type\": \"application/json\"},\t\t\"body\":       messageString,\t}, nil}func main() {\tlambda.Start(HandleRequest)}JavaSDK for Java 2.xNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Connecting to an Amazon RDS database in a Lambda function using Java.import com.amazonaws.services.lambda.runtime.Context;import com.amazonaws.services.lambda.runtime.RequestHandler;import com.amazonaws.services.lambda.runtime.events.APIGatewayProxyRequestEvent;import com.amazonaws.services.lambda.runtime.events.APIGatewayProxyResponseEvent;import software.amazon.awssdk.auth.credentials.DefaultCredentialsProvider;import software.amazon.awssdk.regions.Region;import software.amazon.awssdk.services.rdsdata.RdsDataClient;import software.amazon.awssdk.services.rdsdata.model.ExecuteStatementRequest;import software.amazon.awssdk.services.rdsdata.model.ExecuteStatementResponse;import software.amazon.awssdk.services.rdsdata.model.Field;import java.sql.Connection;import java.sql.DriverManager;import java.sql.PreparedStatement;import java.sql.ResultSet;public class RdsLambdaHandler implements RequestHandler<APIGatewayProxyRequestEvent, APIGatewayProxyResponseEvent> {    @Override    public APIGatewayProxyResponseEvent handleRequest(APIGatewayProxyRequestEvent event, Context context) {        APIGatewayProxyResponseEvent response = new APIGatewayProxyResponseEvent();        try {            // Obtain auth token            String token = createAuthToken();            // Define connection configuration            String connectionString = String.format(\"jdbc:mysql://%s:%s/%s?useSSL=true&requireSSL=true\",                    System.getenv(\"ProxyHostName\"),                    System.getenv(\"Port\"),                    System.getenv(\"DBName\"));            // Establish a connection to the database            try (Connection connection = DriverManager.getConnection(connectionString, System.getenv(\"DBUserName\"), token);                 PreparedStatement statement = connection.prepareStatement(\"SELECT ? + ? AS sum\")) {                statement.setInt(1, 3);                statement.setInt(2, 2);                try (ResultSet resultSet = statement.executeQuery()) {                    if (resultSet.next()) {                        int sum = resultSet.getInt(\"sum\");                        response.setStatusCode(200);                        response.setBody(\"The selected sum is: \" + sum);                    }                }            }        } catch (Exception e) {            response.setStatusCode(500);            response.setBody(\"Error: \" + e.getMessage());        }        return response;    }    private String createAuthToken() {        // Create RDS Data Service client        RdsDataClient rdsDataClient = RdsDataClient.builder()                .region(Region.of(System.getenv(\"AWS_REGION\")))                .credentialsProvider(DefaultCredentialsProvider.create())                .build();        // Define authentication request        ExecuteStatementRequest request = ExecuteStatementRequest.builder()                .resourceArn(System.getenv(\"ProxyHostName\"))                .secretArn(System.getenv(\"DBUserName\"))                .database(System.getenv(\"DBName\"))                .sql(\"SELECT 'RDS IAM Authentication'\")                .build();        // Execute request and obtain authentication token        ExecuteStatementResponse response = rdsDataClient.executeStatement(request);        Field tokenField = response.records().get(0).get(0);        return tokenField.stringValue();    }}JavaScriptSDK for JavaScript (v3)Note        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Connecting to an Amazon RDS database in a Lambda function using JavaScript.// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.// SPDX-License-Identifier: Apache-2.0/* Node.js code here.*/// ES6+ exampleimport { Signer } from \"@aws-sdk/rds-signer\";import mysql from 'mysql2/promise';async function createAuthToken() {  // Define connection authentication parameters  const dbinfo = {    hostname: process.env.ProxyHostName,    port: process.env.Port,    username: process.env.DBUserName,    region: process.env.AWS_REGION,  }  // Create RDS Signer object  const signer = new Signer(dbinfo);  // Request authorization token from RDS, specifying the username  const token = await signer.getAuthToken();  return token;}async function dbOps() {  // Obtain auth token  const token = await createAuthToken();  // Define connection configuration  let connectionConfig = {    host: process.env.ProxyHostName,    user: process.env.DBUserName,    password: token,    database: process.env.DBName,    ssl: 'Amazon RDS'  }  // Create the connection to the DB  const conn = await mysql.createConnection(connectionConfig);  // Obtain the result of the query  const [res,] = await conn.execute('select ?+? as sum', [3, 2]);  return res;}export const handler = async (event) => {  // Execute database flow  const result = await dbOps();  // Return result  return {    statusCode: 200,    body: JSON.stringify(\"The selected sum is: \" + result[0].sum)  }};Connecting to an Amazon RDS database in a Lambda function using TypeScript.import { Signer } from \"@aws-sdk/rds-signer\";import mysql from 'mysql2/promise';// RDS settings// Using '!' (non-null assertion operator) to tell the TypeScript compiler that the DB settings are not null or undefined,const proxy_host_name = process.env.PROXY_HOST_NAME!const port = parseInt(process.env.PORT!)const db_name = process.env.DB_NAME!const db_user_name = process.env.DB_USER_NAME!const aws_region = process.env.AWS_REGION!async function createAuthToken(): Promise<string> {    // Create RDS Signer object    const signer = new Signer({        hostname: proxy_host_name,        port: port,        region: aws_region,        username: db_user_name    });    // Request authorization token from RDS, specifying the username    const token = await signer.getAuthToken();    return token;}async function dbOps(): Promise<mysql.QueryResult | undefined> {    try {        // Obtain auth token        const token = await createAuthToken();        const conn = await mysql.createConnection({            host: proxy_host_name,            user: db_user_name,            password: token,            database: db_name,            ssl: 'Amazon RDS' // Ensure you have the CA bundle for SSL connection        });        const [rows, fields] = await conn.execute('SELECT ? + ? AS sum', [3, 2]);        console.log('result:', rows);        return rows;    }    catch (err) {        console.log(err);    }}export const lambdaHandler = async (event: any): Promise<{ statusCode: number; body: string }> => {    // Execute database flow    const result = await dbOps();    // Return error is result is undefined    if (result == undefined)        return {            statusCode: 500,            body: JSON.stringify(`Error with connection to DB host`)        }    // Return result    return {        statusCode: 200,        body: JSON.stringify(`The selected sum is: ${result[0].sum}`)    };};PHPSDK for PHPNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Connecting to an Amazon RDS database in a Lambda function using PHP.<?php# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.# SPDX-License-Identifier: Apache-2.0# using bref/bref and bref/logger for simplicityuse Bref\\Context\\Context;use Bref\\Event\\Handler as StdHandler;use Bref\\Logger\\StderrLogger;use Aws\\Rds\\AuthTokenGenerator;use Aws\\Credentials\\CredentialProvider;require __DIR__ . '/vendor/autoload.php';class Handler implements StdHandler{    private StderrLogger $logger;    public function __construct(StderrLogger $logger)    {        $this->logger = $logger;    }    private function getAuthToken(): string {        // Define connection authentication parameters        $dbConnection = [            'hostname' => getenv('DB_HOSTNAME'),            'port' => getenv('DB_PORT'),            'username' => getenv('DB_USERNAME'),            'region' => getenv('AWS_REGION'),        ];        // Create RDS AuthTokenGenerator object        $generator = new AuthTokenGenerator(CredentialProvider::defaultProvider());        // Request authorization token from RDS, specifying the username        return $generator->createToken(            $dbConnection['hostname'] . ':' . $dbConnection['port'],            $dbConnection['region'],            $dbConnection['username']        );    }    private function getQueryResults() {        // Obtain auth token        $token = $this->getAuthToken();        // Define connection configuration        $connectionConfig = [            'host' => getenv('DB_HOSTNAME'),            'user' => getenv('DB_USERNAME'),            'password' => $token,            'database' => getenv('DB_NAME'),        ];        // Create the connection to the DB        $conn = new PDO(            \"mysql:host={$connectionConfig['host']};dbname={$connectionConfig['database']}\",            $connectionConfig['user'],            $connectionConfig['password'],            [                PDO::MYSQL_ATTR_SSL_CA => '/path/to/rds-ca-2019-root.pem',                PDO::MYSQL_ATTR_SSL_VERIFY_SERVER_CERT => true,            ]        );        // Obtain the result of the query        $stmt = $conn->prepare('SELECT ?+? AS sum');        $stmt->execute([3, 2]);        return $stmt->fetch(PDO::FETCH_ASSOC);    }    /**     * @param mixed $event     * @param Context $context     * @return array     */    public function handle(mixed $event, Context $context): array    {        $this->logger->info(\"Processing query\");        // Execute database flow        $result = $this->getQueryResults();        return [            'sum' => $result['sum']        ];    }}$logger = new StderrLogger();return new Handler($logger);PythonSDK for Python (Boto3)Note        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Connecting to an Amazon RDS database in a Lambda function using Python.import jsonimport osimport boto3import pymysql# RDS settingsproxy_host_name = os.environ['PROXY_HOST_NAME']port = int(os.environ['PORT'])db_name = os.environ['DB_NAME']db_user_name = os.environ['DB_USER_NAME']aws_region = os.environ['AWS_REGION']# Fetch RDS Auth Tokendef get_auth_token():    client = boto3.client('rds')    token = client.generate_db_auth_token(        DBHostname=proxy_host_name,        Port=port        DBUsername=db_user_name        Region=aws_region    )    return tokendef lambda_handler(event, context):    token = get_auth_token()    try:        connection = pymysql.connect(            host=proxy_host_name,            user=db_user_name,            password=token,            db=db_name,            port=port,            ssl={'ca': 'Amazon RDS'}  # Ensure you have the CA bundle for SSL connection        )                with connection.cursor() as cursor:            cursor.execute('SELECT %s + %s AS sum', (3, 2))            result = cursor.fetchone()        return result            except Exception as e:        return (f\"Error: {str(e)}\")  # Return an error message if an exception occurs     RubySDK for RubyNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Connecting to an Amazon RDS database in a Lambda function using Ruby.# Ruby code here.require 'aws-sdk-rds'require 'json'require 'mysql2'def lambda_handler(event:, context:)  endpoint = ENV['DBEndpoint'] # Add the endpoint without https\"  port = ENV['Port']           # 3306  user = ENV['DBUser']  region = ENV['DBRegion']     # 'us-east-1'  db_name = ENV['DBName']  credentials = Aws::Credentials.new(    ENV['AWS_ACCESS_KEY_ID'],    ENV['AWS_SECRET_ACCESS_KEY'],    ENV['AWS_SESSION_TOKEN']  )  rds_client = Aws::RDS::AuthTokenGenerator.new(    region: region,     credentials: credentials  )  token = rds_client.auth_token(    endpoint: endpoint+ ':' + port,    user_name: user,    region: region  )  begin    conn = Mysql2::Client.new(      host: endpoint,      username: user,      password: token,      port: port,      database: db_name,      sslca: '/var/task/global-bundle.pem',       sslverify: true,      enable_cleartext_plugin: true    )    a = 3    b = 2    result = conn.query(\"SELECT #{a} + #{b} AS sum\").first['sum']    puts result    conn.close    {      statusCode: 200,      body: result.to_json    }  rescue => e    puts \"Database connection failed due to #{e}\"  endendRustSDK for RustNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Connecting to an Amazon RDS database in a Lambda function using Rust.use aws_config::BehaviorVersion;use aws_credential_types::provider::ProvideCredentials;use aws_sigv4::{    http_request::{sign, SignableBody, SignableRequest, SigningSettings},    sign::v4,};use lambda_runtime::{run, service_fn, Error, LambdaEvent};use serde_json::{json, Value};use sqlx::postgres::PgConnectOptions;use std::env;use std::time::{Duration, SystemTime};const RDS_CERTS: &[u8] = include_bytes!(\"global-bundle.pem\");async fn generate_rds_iam_token(    db_hostname: &str,    port: u16,    db_username: &str,) -> Result<String, Error> {    let config = aws_config::load_defaults(BehaviorVersion::v2024_03_28()).await;    let credentials = config        .credentials_provider()        .expect(\"no credentials provider found\")        .provide_credentials()        .await        .expect(\"unable to load credentials\");    let identity = credentials.into();    let region = config.region().unwrap().to_string();    let mut signing_settings = SigningSettings::default();    signing_settings.expires_in = Some(Duration::from_secs(900));    signing_settings.signature_location = aws_sigv4::http_request::SignatureLocation::QueryParams;    let signing_params = v4::SigningParams::builder()        .identity(&identity)        .region(&region)        .name(\"rds-db\")        .time(SystemTime::now())        .settings(signing_settings)        .build()?;    let url = format!(        \"https://{db_hostname}:{port}/?Action=connect&DBUser={db_user}\",        db_hostname = db_hostname,        port = port,        db_user = db_username    );    let signable_request =        SignableRequest::new(\"GET\", &url, std::iter::empty(), SignableBody::Bytes(&[]))            .expect(\"signable request\");    let (signing_instructions, _signature) =        sign(signable_request, &signing_params.into())?.into_parts();    let mut url = url::Url::parse(&url).unwrap();    for (name, value) in signing_instructions.params() {        url.query_pairs_mut().append_pair(name, &value);    }    let response = url.to_string().split_off(\"https://\".len());    Ok(response)}#[tokio::main]async fn main() -> Result<(), Error> {    run(service_fn(handler)).await}async fn handler(_event: LambdaEvent<Value>) -> Result<Value, Error> {    let db_host = env::var(\"DB_HOSTNAME\").expect(\"DB_HOSTNAME must be set\");    let db_port = env::var(\"DB_PORT\")        .expect(\"DB_PORT must be set\")        .parse::<u16>()        .expect(\"PORT must be a valid number\");    let db_name = env::var(\"DB_NAME\").expect(\"DB_NAME must be set\");    let db_user_name = env::var(\"DB_USERNAME\").expect(\"DB_USERNAME must be set\");    let token = generate_rds_iam_token(&db_host, db_port, &db_user_name).await?;    let opts = PgConnectOptions::new()        .host(&db_host)        .port(db_port)        .username(&db_user_name)        .password(&token)        .database(&db_name)        .ssl_root_cert_from_pem(RDS_CERTS.to_vec())        .ssl_mode(sqlx::postgres::PgSslMode::Require);    let pool = sqlx::postgres::PgPoolOptions::new()        .connect_with(opts)        .await?;    let result: i32 = sqlx::query_scalar(\"SELECT $1 + $2\")        .bind(3)        .bind(2)        .fetch_one(&pool)        .await?;    println!(\"Result: {:?}\", result);    Ok(json!({        \"statusCode\": 200,        \"content-type\": \"text/plain\",        \"body\": format!(\"The selected sum is: {result}\")    }))}anchoranchoranchoranchoranchoranchoranchorGoJavaJavaScriptPHPPythonRubyRustSDK for Go V2Note        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Connecting to an Amazon RDS database in a Lambda function using Go./*Golang v2 code here.*/package mainimport (\t\"context\"\t\"database/sql\"\t\"encoding/json\"\t\"fmt\"\t\"os\"\t\"github.com/aws/aws-lambda-go/lambda\"\t\"github.com/aws/aws-sdk-go-v2/config\"\t\"github.com/aws/aws-sdk-go-v2/feature/rds/auth\"\t_ \"github.com/go-sql-driver/mysql\")type MyEvent struct {\tName string `json:\"name\"`}func HandleRequest(event *MyEvent) (map[string]interface{}, error) {\tvar dbName string = os.Getenv(\"DatabaseName\")\tvar dbUser string = os.Getenv(\"DatabaseUser\")\tvar dbHost string = os.Getenv(\"DBHost\") // Add hostname without https\tvar dbPort int = os.Getenv(\"Port\")      // Add port number\tvar dbEndpoint string = fmt.Sprintf(\"%s:%d\", dbHost, dbPort)\tvar region string = os.Getenv(\"AWS_REGION\")\tcfg, err := config.LoadDefaultConfig(context.TODO())\tif err != nil {\t\tpanic(\"configuration error: \" + err.Error())\t}\tauthenticationToken, err := auth.BuildAuthToken(\t\tcontext.TODO(), dbEndpoint, region, dbUser, cfg.Credentials)\tif err != nil {\t\tpanic(\"failed to create authentication token: \" + err.Error())\t}\tdsn := fmt.Sprintf(\"%s:%s@tcp(%s)/%s?tls=true&allowCleartextPasswords=true\",\t\tdbUser, authenticationToken, dbEndpoint, dbName,\t)\tdb, err := sql.Open(\"mysql\", dsn)\tif err != nil {\t\tpanic(err)\t}\tdefer db.Close()\tvar sum int\terr = db.QueryRow(\"SELECT ?+? AS sum\", 3, 2).Scan(&sum)\tif err != nil {\t\tpanic(err)\t}\ts := fmt.Sprint(sum)\tmessage := fmt.Sprintf(\"The selected sum is: %s\", s)\tmessageBytes, err := json.Marshal(message)\tif err != nil {\t\treturn nil, err\t}\tmessageString := string(messageBytes)\treturn map[string]interface{}{\t\t\"statusCode\": 200,\t\t\"headers\":    map[string]string{\"Content-Type\": \"application/json\"},\t\t\"body\":       messageString,\t}, nil}func main() {\tlambda.Start(HandleRequest)}"
                        ]
                    },
                    {
                        "sub_header": "Processing event notifications from Amazon RDS",
                        "content": [
                            "You can use Lambda to process event notifications from an Amazon RDS database. Amazon RDS sends      notifications to an Amazon Simple Notification Service (Amazon SNS) topic, which you can configure to invoke a Lambda function.      Amazon SNS wraps the message from Amazon RDS in its own event document and sends it to your function.",
                            "For more information about configuring an Amazon RDS database to send notifications, see      Using Amazon RDS      event notifications.    ",
                            {
                                "code_example": "{\n        \"Records\": [\n          {\n            \"EventVersion\": \"1.0\",\n            \"EventSubscriptionArn\": \"arn:aws:sns:us-east-2:123456789012:rds-lambda:21be56ed-a058-49f5-8c98-aedd2564c486\",\n            \"EventSource\": \"aws:sns\",\n            \"Sns\": {\n              \"SignatureVersion\": \"1\",\n              \"Timestamp\": \"2023-01-02T12:45:07.000Z\",\n              \"Signature\": \"tcc6faL2yUC6dgZdmrwh1Y4cGa/ebXEkAi6RibDsvpi+tE/1+82j...65r==\",\n              \"SigningCertUrl\": \"https://sns.us-east-2.amazonaws.com/SimpleNotificationService-ac565b8b1a6c5d002d285f9598aa1d9b.pem\",\n              \"MessageId\": \"95df01b4-ee98-5cb9-9903-4c221d41eb5e\",\n              \"Message\": \"{\\\"Event Source\\\":\\\"db-instance\\\",\\\"Event Time\\\":\\\"2023-01-02 12:45:06.000\\\",\\\"Identifier Link\\\":\\\"https://console.aws.amazon.com/rds/home?region=eu-west-1#dbinstance:id=dbinstanceid\\\",\\\"Source ID\\\":\\\"dbinstanceid\\\",\\\"Event ID\\\":\\\"http://docs.amazonwebservices.com/AmazonRDS/latest/UserGuide/USER_Events.html#RDS-EVENT-0002\\\",\\\"Event Message\\\":\\\"Finished DB Instance backup\\\"}\",\n              \"MessageAttributes\": {},\n              \"Type\": \"Notification\",\n              \"UnsubscribeUrl\": \"https://sns.us-east-2.amazonaws.com/?Action=Unsubscribe&amp;SubscriptionArn=arn:aws:sns:us-east-2:123456789012:test-lambda:21be56ed-a058-49f5-8c98-aedd2564c486\",\n              \"TopicArn\":\"arn:aws:sns:us-east-2:123456789012:sns-lambda\",\n              \"Subject\": \"RDS Notification Message\"\n            }\n          }\n        ]\n      }"
                            }
                        ]
                    },
                    {
                        "sub_header": "Complete Lambda and Amazon RDS tutorial",
                        "content": []
                    }
                ]
            },
            {
                "title": "S3",
                "href": "https://docs.aws.amazon.com/lambda/latest/dg/with-s3.html",
                "contents": [
                    {
                        "title": "Tutorial: Use an S3 trigger",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/with-s3-example.html",
                        "sections": [
                            "",
                            "In this tutorial, you use the console to create a Lambda function and configure a trigger for an Amazon Simple Storage Service (Amazon S3) bucket. Every time that you     add an object to your Amazon S3 bucket, your function runs and outputs the object type to Amazon CloudWatch Logs.",
                            "\n\n",
                            "This tutorial demonstrates how to:",
                            "\n\nCreate an Amazon S3 bucket.\n\nCreate a Lambda function that returns the object type of objects in an Amazon S3 bucket.\n\nConfigure a Lambda trigger that invokes your function when objects are uploaded to your bucket.\n\nTest your function, first with a dummy event, and then using the trigger.\n",
                            "By completing these steps, you’ll learn how to configure a Lambda function to run whenever objects are added to or deleted from an     Amazon S3 bucket. You can complete this tutorial using only the AWS Management Console.",
                            {
                                "sub_header": "Prerequisites",
                                "content": [
                                    "  5.Root user  Sign in to the AWS Management Console as the account owner by choosing  and entering your AWS account email address. On the next page, enter your password.",
                                    "  12.Root user  Sign in to the AWS Management Console as the account owner by choosing  and entering your AWS account email address. On the next page, enter your password."
                                ]
                            },
                            {
                                "sub_header": "Create an Amazon S3 bucket",
                                "content": [
                                    "\n\n",
                                    "To create an Amazon S3 bucket\nOpen the Amazon S3 console and select the Buckets page.\n\nChoose Create bucket.\n\nUnder General configuration, do the following:\n\nFor Bucket name, enter a globally unique name that meets the Amazon S3 Bucket naming rules. \n              Bucket names can contain only lower case letters, numbers, dots (.), and hyphens (-).\n\nFor AWS Region, choose a Region. Later in the tutorial, you must create your Lambda function in the same Region.\n\n\nLeave all other options set to their default values and choose Create bucket.\n"
                                ]
                            },
                            {
                                "sub_header": "Upload a test object to your bucket",
                                "content": [
                                    "\n\n",
                                    {
                                        "code_example": "HappyFace.jpg"
                                    },
                                    "Later in the tutorial, you’ll test your Lambda function using this object."
                                ]
                            },
                            {
                                "sub_header": "Create a permissions policy",
                                "content": [
                                    "\n\n",
                                    "Create a permissions policy that allows Lambda to get objects from an Amazon S3 bucket and to write to Amazon CloudWatch Logs. ",
                                    {
                                        "code_example": "{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"logs:PutLogEvents\",\n                \"logs:CreateLogGroup\",\n                \"logs:CreateLogStream\"\n            ],\n            \"Resource\": \"arn:aws:logs:*:*:*\"\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:GetObject\"\n            ],\n            \"Resource\": \"arn:aws:s3:::*/*\"\n        }\n    ]\n}"
                                    }
                                ]
                            },
                            {
                                "sub_header": "Create an execution role",
                                "content": [
                                    "\n\n",
                                    "An execution role is an AWS Identity and Access Management (IAM) role that grants a Lambda function permission to access AWS services and resources. In this step, create an execution role using the permissions policy that you created in the previous step.",
                                    {
                                        "code_example": "s3-trigger-tutorial"
                                    }
                                ]
                            },
                            {
                                "sub_header": "Create the Lambda function",
                                "content": [
                                    "\n\n",
                                    "Create a Lambda function in the console using the Python 3.12 runtime.",
                                    {
                                        "code_example": "s3-trigger-tutorial"
                                    }
                                ]
                            },
                            {
                                "sub_header": "Deploy the function code",
                                "content": [
                                    "\n\n",
                                    "This tutorial uses the Python 3.12 runtime, but we’ve also provided example code files for other runtimes. You can select the       tab in the following box to see the code for the runtime you’re interested in.",
                                    "The Lambda function retrieves the key name of the uploaded object and the name of the bucket from the event parameter it receives       from Amazon S3. The function then uses the get_object  method from the AWS SDK for Python (Boto3) to retrieve the object's metadata, including the content type (MIME type) of the uploaded object.",
                                    {
                                        "code_example": "// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n// SPDX-License-Identifier: Apache-2.0\n﻿using System.Threading.Tasks;\nusing Amazon.Lambda.Core;\nusing Amazon.S3;\nusing System;\nusing Amazon.Lambda.S3Events;\nusing System.Web;\n\n// Assembly attribute to enable the Lambda function's JSON input to be converted into a .NET class.\n[assembly: LambdaSerializer(typeof(Amazon.Lambda.Serialization.SystemTextJson.DefaultLambdaJsonSerializer))]\n\nnamespace S3Integration\n{\n    public class Function\n    {\n        private static AmazonS3Client _s3Client;\n        public Function() : this(null)\n        {\n        }\n\n        internal Function(AmazonS3Client s3Client)\n        {\n            _s3Client = s3Client ?? new AmazonS3Client();\n        }\n\n        public async Task<string> Handler(S3Event evt, ILambdaContext context)\n        {\n            try\n            {\n                if (evt.Records.Count <= 0)\n                {\n                    context.Logger.LogLine(\"Empty S3 Event received\");\n                    return string.Empty;\n                }\n\n                var bucket = evt.Records[0].S3.Bucket.Name;\n                var key = HttpUtility.UrlDecode(evt.Records[0].S3.Object.Key);\n\n                context.Logger.LogLine($\"Request is for {bucket} and {key}\");\n\n                var objectResult = await _s3Client.GetObjectAsync(bucket, key);\n\n                context.Logger.LogLine($\"Returning {objectResult.Key}\");\n\n                return objectResult.Key;\n            }\n            catch (Exception e)\n            {\n                context.Logger.LogLine($\"Error processing request - {e.Message}\");\n\n                return string.Empty;\n            }\n        }\n    }\n}\n"
                                    }
                                ]
                            },
                            {
                                "sub_header": "Create the Amazon S3 trigger",
                                "content": [
                                    "\n\n",
                                    "To create the Amazon S3 trigger\nIn the Function overview pane, choose Add trigger.\n\n\n\n\nSelect S3.\n\nUnder Bucket, select the bucket you created earlier in the tutorial.\n\nUnder Event types, be sure that All object create events is selected.\n\nUnder Recursive invocation, select the check box to acknowledge that using the same Amazon S3 bucket for input and \n          output is not recommended.\n\nChoose Add.\n",
                                    {
                                        "code_example": "An error occurred when creating the trigger: Unable to validate the following destination configurations."
                                    }
                                ]
                            },
                            {
                                "sub_header": "Test your Lambda function with a dummy event",
                                "content": [
                                    "\n\n",
                                    {
                                        "code_example": "MyTestEvent"
                                    },
                                    {
                                        "sub_header": "Test the Lambda function with the Amazon S3 trigger",
                                        "content": [
                                            "\n\n",
                                            "To test your function with the configured trigger, upload an object to your Amazon S3 bucket using the console. To verify that your Lambda         function ran as expected, use CloudWatch Logs to view your function’s output.",
                                            "To upload an object to your Amazon S3 bucket\nOpen the Buckets page of the Amazon S3 console and choose the bucket that you created earlier.\n\nChoose Upload.\n\nChoose Add files and use the file selector to choose an object you want to upload. This object can be any file \n            you choose.\n\nChoose Open, then choose Upload.\n",
                                            {
                                                "code_example": "/aws/lambda/s3-trigger-tutorial"
                                            }
                                        ]
                                    }
                                ]
                            },
                            {
                                "sub_header": "Clean up your resources",
                                "content": [
                                    "You can now delete the resources that you created for this tutorial, unless you want to retain them. By deleting AWS resources that you're no longer using, you prevent unnecessary charges to your AWS account.",
                                    {
                                        "code_example": "delete"
                                    },
                                    "To delete the execution role\nOpen the Roles page of the IAM console.\n\nSelect the execution role that you created.\n\nChoose Delete.\n\nEnter the name of the role in the text input field and choose Delete.\n",
                                    "To delete the S3 bucket\nOpen the Amazon S3 console.\n\nSelect the bucket you created.\n\nChoose Delete.\n\nEnter the name of the bucket in the text input field.\n\nChoose Delete bucket.\n"
                                ]
                            },
                            {
                                "sub_header": "Next steps",
                                "content": [
                                    "In Tutorial: Using an Amazon S3 trigger to create thumbnail images, the Amazon S3 trigger invokes a function that creates a thumbnail image for each image file that is uploaded to a      bucket. This tutorial requires a moderate level of AWS and Lambda domain knowledge. It demonstrates how to create resources using the AWS Command Line Interface (AWS CLI) and how to create a .zip file archive deployment package for the function and its dependencies."
                                ]
                            }
                        ]
                    },
                    {
                        "title": "Tutorial: Use an Amazon S3 trigger to create thumbnails",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/with-s3-tutorial.html",
                        "sections": [
                            "",
                            "In this tutorial, you create and configure a Lambda function that resizes images added to an Amazon Simple Storage Service (Amazon S3) bucket. When you add an image     file to your bucket, Amazon S3 invokes your Lambda function. The function then creates a thumbnail version of the image and outputs it to a different     Amazon S3 bucket.",
                            "\n\n",
                            "To complete this tutorial, you carry out the following steps:",
                            "\n\nCreate source and destination Amazon S3 buckets and upload a sample image.\n\nCreate a Lambda function that resizes an image and outputs a thumbnail to an Amazon S3 bucket.\n\nConfigure a Lambda trigger that invokes your function when objects are uploaded to your source bucket.\n\nTest your function, first with a dummy event, and then by uploading an image to your source bucket.\n",
                            "By completing these steps, you’ll learn how to use Lambda to carry out a file processing task on objects added to an Amazon S3 bucket. You can     complete this tutorial using the AWS Command Line Interface (AWS CLI) or the AWS Management Console.",
                            "If you're looking for a simpler example to learn how to configure an Amazon S3 trigger for Lambda, you can try Tutorial: Using an Amazon S3 trigger to invoke a Lambda function.",
                            "  1.Prerequisites",
                            "  2.Create two Amazon S3 buckets",
                            "  3.Upload a test image to your source bucket",
                            "  4.Create a permissions policy",
                            "  5.Create an execution role",
                            "  6.Create the function deployment package",
                            "  7.Create the Lambda function",
                            "  8.Configure Amazon S3 to invoke the function",
                            "  9.Test your Lambda function with a dummy event",
                            "  10.Test your function using the Amazon S3 trigger",
                            "  11.Clean up your resources",
                            {
                                "sub_header": "Prerequisites",
                                "content": [
                                    "  5.Root user  Sign in to the AWS Management Console as the account owner by choosing  and entering your AWS account email address. On the next page, enter your password.",
                                    "  12.Root user  Sign in to the AWS Management Console as the account owner by choosing  and entering your AWS account email address. On the next page, enter your password.",
                                    "If you want to use the AWS CLI to complete the tutorial, install the latest version of the AWS Command Line Interface.",
                                    "For your Lambda function code, you can use Python or Node.js. Install the language support tools and a package manager for the       language that you want to use. "
                                ]
                            },
                            {
                                "sub_header": "Create two Amazon S3 buckets",
                                "content": [
                                    "\n\n",
                                    "First create two Amazon S3 buckets. The first bucket is the source bucket you will upload your images to. The second bucket is used by     Lambda to save the resized thumbnail when you invoke your function.",
                                    "AWS Management ConsoleTo create the Amazon S3 buckets (console)Open the Buckets page of the Amazon S3 console.Choose Create bucket.Under General configuration, do the following:For Bucket name, enter a globally unique name that meets the Amazon S3 Bucket naming rules.                     Bucket names can contain only lower case letters, numbers, dots (.), and hyphens (-). For AWS Region, choose the AWS Region                     closest to your geographical location. Later in the tutorial, you must create your Lambda function in the same AWS Region, so                     make a note of the region you chose.Leave all other options set to their default values and choose Create bucket.Repeat steps 1 to 4 to create your destination bucket. For Bucket name, enter amzn-s3-demo-source-bucket-resized,                 where amzn-s3-demo-source-bucket is the name of the source bucket you just created.AWS CLITo create the Amazon S3 buckets (AWS CLI)Run the following CLI command to create your source bucket. The name you choose for your bucket must be globally unique and                 follow the Amazon S3 Bucket naming rules.                 Names can only contain lower case letters, numbers, dots (.), and hyphens (-). For region and LocationConstraint,                 choose the AWS Region closest to your geographical                 location.aws s3api create-bucket --bucket amzn-s3-demo-source-bucket --region us-east-1 \\--create-bucket-configuration LocationConstraint=us-east-1Later in the tutorial, you must create your Lambda function in the same AWS Region as your source bucket, so make a note of the                 region you chose.Run the following command to create your destination bucket. For the bucket name, you must use amzn-s3-demo-source-bucket-resized,                 where amzn-s3-demo-source-bucket is the name of the source bucket you created in step 1. For region                 and LocationConstraint, choose the same AWS Region you used to create your source bucket.aws s3api create-bucket --bucket amzn-s3-demo-source-bucket-resized --region us-east-1 \\--create-bucket-configuration LocationConstraint=us-east-1anchoranchorAWS Management ConsoleAWS CLITo create the Amazon S3 buckets (console)Open the Buckets page of the Amazon S3 console.Choose Create bucket.Under General configuration, do the following:For Bucket name, enter a globally unique name that meets the Amazon S3 Bucket naming rules.                     Bucket names can contain only lower case letters, numbers, dots (.), and hyphens (-). For AWS Region, choose the AWS Region                     closest to your geographical location. Later in the tutorial, you must create your Lambda function in the same AWS Region, so                     make a note of the region you chose.Leave all other options set to their default values and choose Create bucket.Repeat steps 1 to 4 to create your destination bucket. For Bucket name, enter amzn-s3-demo-source-bucket-resized,                 where amzn-s3-demo-source-bucket is the name of the source bucket you just created."
                                ]
                            },
                            {
                                "sub_header": "Upload a test image to your source bucket",
                                "content": [
                                    "\n\n",
                                    "Later in the tutorial, you’ll test your Lambda function by invoking it using the AWS CLI or the Lambda console. To confirm that your function       is operating correctly, your source bucket needs to contain a test image. This image can be any JPG or PNG file you choose.",
                                    "AWS Management ConsoleTo upload a test image to your source bucket (console)Open the Buckets page of the Amazon S3 console.Select the source bucket you created in the previous step.Choose Upload.Choose Add files and use the file selector to choose the object you want to upload.Choose Open, then choose Upload.AWS CLITo upload a test image to your source bucket (AWS CLI)From the directory containing the image you want to upload, run the following CLI command. Replace the --bucket                 parameter with the name of your source bucket. For the --key and --body parameters, use the filename of                 your test image.aws s3api put-object --bucket amzn-s3-demo-source-bucket --key HappyFace.jpg --body ./HappyFace.jpganchoranchorAWS Management ConsoleAWS CLITo upload a test image to your source bucket (console)Open the Buckets page of the Amazon S3 console.Select the source bucket you created in the previous step.Choose Upload.Choose Add files and use the file selector to choose the object you want to upload.Choose Open, then choose Upload."
                                ]
                            },
                            {
                                "sub_header": "Create a permissions policy",
                                "content": [
                                    "\n\n",
                                    "The first step in creating your Lambda function is to create a permissions policy. This policy gives your function the permissions it needs       to access other AWS resources. For this tutorial, the policy gives Lambda read and write permissions for Amazon S3 buckets and allows it to write       to Amazon CloudWatch Logs.",
                                    "AWS Management ConsoleTo create the policy (console)Open the Policies page of the AWS Identity and Access Management (IAM) console.Choose Create policy.Choose the JSON tab, and then paste the following custom policy into the JSON editor.{    \"Version\": \"2012-10-17\",    \"Statement\": [        {            \"Effect\": \"Allow\",            \"Action\": [                \"logs:PutLogEvents\",                \"logs:CreateLogGroup\",                \"logs:CreateLogStream\"            ],            \"Resource\": \"arn:aws:logs:*:*:*\"        },        {            \"Effect\": \"Allow\",            \"Action\": [                \"s3:GetObject\"            ],            \"Resource\": \"arn:aws:s3:::*/*\"        },        {            \"Effect\": \"Allow\",            \"Action\": [                \"s3:PutObject\"            ],            \"Resource\": \"arn:aws:s3:::*/*\"        }    ]}Choose Next.Under Policy details, for Policy name, enter LambdaS3Policy.Choose Create policy.AWS CLITo create the policy (AWS CLI)Save the following JSON in a file named policy.json.{    \"Version\": \"2012-10-17\",    \"Statement\": [        {            \"Effect\": \"Allow\",            \"Action\": [                \"logs:PutLogEvents\",                \"logs:CreateLogGroup\",                \"logs:CreateLogStream\"            ],            \"Resource\": \"arn:aws:logs:*:*:*\"        },        {            \"Effect\": \"Allow\",            \"Action\": [                \"s3:GetObject\"            ],            \"Resource\": \"arn:aws:s3:::*/*\"        },        {            \"Effect\": \"Allow\",            \"Action\": [                \"s3:PutObject\"            ],            \"Resource\": \"arn:aws:s3:::*/*\"        }    ]}From the directory you saved the JSON policy document in, run the following CLI command.aws iam create-policy --policy-name LambdaS3Policy --policy-document file://policy.jsonanchoranchorAWS Management ConsoleAWS CLITo create the policy (console)Open the Policies page of the AWS Identity and Access Management (IAM) console.Choose Create policy.Choose the JSON tab, and then paste the following custom policy into the JSON editor.{    \"Version\": \"2012-10-17\",    \"Statement\": [        {            \"Effect\": \"Allow\",            \"Action\": [                \"logs:PutLogEvents\",                \"logs:CreateLogGroup\",                \"logs:CreateLogStream\"            ],            \"Resource\": \"arn:aws:logs:*:*:*\"        },        {            \"Effect\": \"Allow\",            \"Action\": [                \"s3:GetObject\"            ],            \"Resource\": \"arn:aws:s3:::*/*\"        },        {            \"Effect\": \"Allow\",            \"Action\": [                \"s3:PutObject\"            ],            \"Resource\": \"arn:aws:s3:::*/*\"        }    ]}Choose Next.Under Policy details, for Policy name, enter LambdaS3Policy.Choose Create policy."
                                ]
                            },
                            {
                                "sub_header": "Create an execution role",
                                "content": [
                                    "\n\n",
                                    "An execution role is an IAM role that grants a Lambda function permission to access AWS services and resources. To give your function       read and write access to an Amazon S3 bucket, you attach the permissions policy you created in the previous step.",
                                    "AWS Management ConsoleTo create an execution role and attach your permissions policy (console)Open the Roles page of the (IAM) console.Choose Create role.For Trusted entity type, select AWS service, and for Use case,              select Lambda.Choose Next.Add the permissions policy you created in the previous step by doing the following:In the policy search box, enter LambdaS3Policy.In the search results, select the check box for LambdaS3Policy.Choose Next.Under Role details, for the Role name enter LambdaS3Role.Choose Create role.AWS CLITo create an execution role and attach your permissions policy (AWS CLI)Save the following JSON in a file named trust-policy.json. This trust policy allows Lambda to use the role’s                permissions by giving the service principal lambda.amazonaws.com permission to call the AWS Security Token Service (AWS STS) AssumeRole                action.{  \"Version\": \"2012-10-17\",  \"Statement\": [    {      \"Effect\": \"Allow\",      \"Principal\": {        \"Service\": \"lambda.amazonaws.com\"      },      \"Action\": \"sts:AssumeRole\"    }  ]}From the directory you saved the JSON trust policy document in, run the following CLI command to create the execution role.aws iam create-role --role-name LambdaS3Role --assume-role-policy-document file://trust-policy.jsonTo attach the permissions policy you created in the previous step, run the following CLI command. Replace the AWS account number                in the policy’s ARN with your own account number.aws iam attach-role-policy --role-name LambdaS3Role --policy-arn arn:aws:iam::123456789012:policy/LambdaS3PolicyanchoranchorAWS Management ConsoleAWS CLITo create an execution role and attach your permissions policy (console)Open the Roles page of the (IAM) console.Choose Create role.For Trusted entity type, select AWS service, and for Use case,              select Lambda.Choose Next.Add the permissions policy you created in the previous step by doing the following:In the policy search box, enter LambdaS3Policy.In the search results, select the check box for LambdaS3Policy.Choose Next.Under Role details, for the Role name enter LambdaS3Role.Choose Create role."
                                ]
                            },
                            {
                                "sub_header": "Create the function deployment package",
                                "content": [
                                    "\n\n",
                                    "To create your function, you create a deployment package containing your function code and its dependencies. For this       CreateThumbnail function, your function code uses a separate library for the image resizing. Follow the instructions for your       chosen language to create a deployment package containing the required library.",
                                    "Node.jsTo create the deployment package (Node.js)Create a directory named lambda-s3 for your function code and dependencies and navigate into it.mkdir lambda-s3cd lambda-s3Create a new Node.js project with npm. To accept the default options provided in the interactive experience, press Enter.npm initSave the following function code in a file named index.mjs. Make sure to replace us-east-1 with the               AWS Region in which you created your own source and destination buckets.// dependenciesimport { S3Client, GetObjectCommand, PutObjectCommand } from '@aws-sdk/client-s3';import { Readable } from 'stream';import sharp from 'sharp';import util from 'util';// create S3 clientconst s3 = new S3Client({region: 'us-east-1'});// define the handler functionexport const handler = async (event, context) => {// Read options from the event parameter and get the source bucketconsole.log(\"Reading options from event:\\n\", util.inspect(event, {depth: 5}));  const srcBucket = event.Records[0].s3.bucket.name;  // Object key may have spaces or unicode non-ASCII charactersconst srcKey    = decodeURIComponent(event.Records[0].s3.object.key.replace(/\\+/g, \" \"));const dstBucket = srcBucket + \"-resized\";const dstKey    = \"resized-\" + srcKey;// Infer the image type from the file suffixconst typeMatch = srcKey.match(/\\.([^.]*)$/);if (!typeMatch) {  console.log(\"Could not determine the image type.\");  return;}// Check that the image type is supportedconst imageType = typeMatch[1].toLowerCase();if (imageType != \"jpg\" && imageType != \"png\") {  console.log(`Unsupported image type: ${imageType}`);  return;}// Get the image from the source bucket. GetObjectCommand returns a stream.try {  const params = {    Bucket: srcBucket,    Key: srcKey  };  var response = await s3.send(new GetObjectCommand(params));  var stream = response.Body;  // Convert stream to buffer to pass to sharp resize function.  if (stream instanceof Readable) {    var content_buffer = Buffer.concat(await stream.toArray());      } else {    throw new Error('Unknown object stream type');  }} catch (error) {  console.log(error);  return;}  // set thumbnail width. Resize will set the height automatically to maintain aspect ratio.const width  = 200;// Use the sharp module to resize the image and save in a buffer.try {      var output_buffer = await sharp(content_buffer).resize(width).toBuffer();} catch (error) {  console.log(error);  return;}// Upload the thumbnail image to the destination buckettry {  const destparams = {    Bucket: dstBucket,    Key: dstKey,    Body: output_buffer,    ContentType: \"image\"  };  const putResult = await s3.send(new PutObjectCommand(destparams));  } catch (error) {    console.log(error);    return;  }  console.log('Successfully resized ' + srcBucket + '/' + srcKey +    ' and uploaded to ' + dstBucket + '/' + dstKey);  };In your lambda-s3 directory, install the sharp library using npm. Note that the latest version of sharp (0.33) isn't               compatible with Lambda. Install version 0.32.6 to complete this tutorial.npm install sharp@0.32.6The npm install command creates a node_modules directory for your modules. After this step, your                 directory structure should look like the following.lambda-s3|- index.mjs|- node_modules|  |- base64js|  |- bl|  |- buffer...|- package-lock.json|- package.jsonCreate a .zip deployment package containing your function code and its dependencies. In MacOS and Linux, run the following                 command.zip -r function.zip .In Windows, use your preferred zip utility to create a .zip file. Ensure that your index.mjs,                 package.json, and package-lock.json files and your node_modules directory are all at the root                 of your .zip file.PythonTo create the deployment package (Python)Save the example code as a file named          lambda_function.py.import boto3import osimport sysimport uuidfrom urllib.parse import unquote_plusfrom PIL import Imageimport PIL.Image            s3_client = boto3.client('s3')            def resize_image(image_path, resized_path):  with Image.open(image_path) as image:    image.thumbnail(tuple(x / 2 for x in image.size))    image.save(resized_path)            def lambda_handler(event, context):  for record in event['Records']:    bucket = record['s3']['bucket']['name']    key = unquote_plus(record['s3']['object']['key'])    tmpkey = key.replace('/', '')    download_path = '/tmp/{}{}'.format(uuid.uuid4(), tmpkey)    upload_path = '/tmp/resized-{}'.format(tmpkey)    s3_client.download_file(bucket, key, download_path)    resize_image(download_path, upload_path)    s3_client.upload_file(upload_path, '{}-resized'.format(bucket), 'resized-{}'.format(key))In the same directory in which you created your lambda_function.py file, create a new directory named           package and install the Pillow (PIL) library and the           AWS SDK for Python (Boto3). Although the Lambda Python runtime includes a version of the Boto3 SDK, we recommend that you add all of your function's           dependencies to your deployment package, even if they are included in the runtime. For more information, see           Runtime dependencies in Python.mkdir packagepip install \\--platform manylinux2014_x86_64 \\--target=package \\--implementation cp \\--python-version 3.12 \\--only-binary=:all: --upgrade \\pillow boto3The Pillow library contains C/C++ code. By using the --platform manylinux_2014_x86_64 and --only-binary=:all:           options, pip will download and install a version of Pillow that contains pre-compiled binaries compatible with the Amazon Linux 2 operating           system. This ensures that your deployment package will work in the Lambda execution environment, regardless of the operating system and           architecture of your local build machine.Create a .zip file containing your application code and the Pillow and Boto3 libraries. In Linux or MacOS, run the following commands from your           command line interface.cd packagezip -r ../lambda_function.zip .cd ..zip lambda_function.zip lambda_function.py In Windows, use your preferred zip tool to create the lambda_function.zip file. Make sure that your         lambda_function.py file and the folders containing your dependencies are all at the root of the .zip file.You can also create your deployment package using a Python virtual environment. See Working with .zip file archives for Python Lambda functionsanchoranchorNode.jsPythonTo create the deployment package (Node.js)Create a directory named lambda-s3 for your function code and dependencies and navigate into it.mkdir lambda-s3cd lambda-s3Create a new Node.js project with npm. To accept the default options provided in the interactive experience, press Enter.npm initSave the following function code in a file named index.mjs. Make sure to replace us-east-1 with the               AWS Region in which you created your own source and destination buckets.// dependenciesimport { S3Client, GetObjectCommand, PutObjectCommand } from '@aws-sdk/client-s3';import { Readable } from 'stream';import sharp from 'sharp';import util from 'util';// create S3 clientconst s3 = new S3Client({region: 'us-east-1'});// define the handler functionexport const handler = async (event, context) => {// Read options from the event parameter and get the source bucketconsole.log(\"Reading options from event:\\n\", util.inspect(event, {depth: 5}));  const srcBucket = event.Records[0].s3.bucket.name;  // Object key may have spaces or unicode non-ASCII charactersconst srcKey    = decodeURIComponent(event.Records[0].s3.object.key.replace(/\\+/g, \" \"));const dstBucket = srcBucket + \"-resized\";const dstKey    = \"resized-\" + srcKey;// Infer the image type from the file suffixconst typeMatch = srcKey.match(/\\.([^.]*)$/);if (!typeMatch) {  console.log(\"Could not determine the image type.\");  return;}// Check that the image type is supportedconst imageType = typeMatch[1].toLowerCase();if (imageType != \"jpg\" && imageType != \"png\") {  console.log(`Unsupported image type: ${imageType}`);  return;}// Get the image from the source bucket. GetObjectCommand returns a stream.try {  const params = {    Bucket: srcBucket,    Key: srcKey  };  var response = await s3.send(new GetObjectCommand(params));  var stream = response.Body;  // Convert stream to buffer to pass to sharp resize function.  if (stream instanceof Readable) {    var content_buffer = Buffer.concat(await stream.toArray());      } else {    throw new Error('Unknown object stream type');  }} catch (error) {  console.log(error);  return;}  // set thumbnail width. Resize will set the height automatically to maintain aspect ratio.const width  = 200;// Use the sharp module to resize the image and save in a buffer.try {      var output_buffer = await sharp(content_buffer).resize(width).toBuffer();} catch (error) {  console.log(error);  return;}// Upload the thumbnail image to the destination buckettry {  const destparams = {    Bucket: dstBucket,    Key: dstKey,    Body: output_buffer,    ContentType: \"image\"  };  const putResult = await s3.send(new PutObjectCommand(destparams));  } catch (error) {    console.log(error);    return;  }  console.log('Successfully resized ' + srcBucket + '/' + srcKey +    ' and uploaded to ' + dstBucket + '/' + dstKey);  };In your lambda-s3 directory, install the sharp library using npm. Note that the latest version of sharp (0.33) isn't               compatible with Lambda. Install version 0.32.6 to complete this tutorial.npm install sharp@0.32.6The npm install command creates a node_modules directory for your modules. After this step, your                 directory structure should look like the following.lambda-s3|- index.mjs|- node_modules|  |- base64js|  |- bl|  |- buffer...|- package-lock.json|- package.jsonCreate a .zip deployment package containing your function code and its dependencies. In MacOS and Linux, run the following                 command.zip -r function.zip .In Windows, use your preferred zip utility to create a .zip file. Ensure that your index.mjs,                 package.json, and package-lock.json files and your node_modules directory are all at the root                 of your .zip file."
                                ]
                            },
                            {
                                "sub_header": "Create the Lambda function",
                                "content": [
                                    "\n\n",
                                    "You can create your Lambda function using either the AWS CLI or the Lambda console. Follow the instructions for your chosen language to create       the function.",
                                    "AWS Management ConsoleTo create the function (console)To create your Lambda function using the console, you first create a basic function containing some ‘Hello world’ code. You then           replace this code with your own function code by uploading the.zip or JAR file you created in the previous step.Open the Functions page of the Lambda console.Make sure you're working in the same AWS Region you created your Amazon S3 bucket in. You can change your region using the drop-down             list at the top of the screen.Choose Create function.Choose Author from scratch.Under Basic information, do the following:For Function name, enter CreateThumbnail.For Runtime, choose either Node.js 22.x or Python 3.12 according to the language you chose for your function.For Architecture, choose x86_64.In the Change default execution role tab, do the following:Expand the tab, then choose Use an existing role.Select the LambdaS3Role you created earlier.Choose Create function.To upload the function code (console)In the Code source pane, choose Upload from.Choose .zip file. Choose Upload.In the file selector, select your .zip file and choose Open.Choose Save.AWS CLITo create the function (AWS CLI)Run the CLI command for the language you chose. For the role parameter, make sure to replace 123456789012             with your own AWS account ID. For the region parameter, replace us-east-1 with the region you created your             Amazon S3 buckets in.For Node.js, run the following command from the directory containing your function.zip               file.aws lambda create-function --function-name CreateThumbnail \\--zip-file fileb://function.zip --handler index.handler --runtime nodejs22.x \\--timeout 10 --memory-size 1024 \\--role arn:aws:iam::123456789012:role/LambdaS3Role --region us-east-1For Python, run the following command from the directory containing your lambda_function.zip                 file.aws lambda create-function --function-name CreateThumbnail \\--zip-file fileb://lambda_function.zip --handler lambda_function.lambda_handler \\--runtime python3.13 --timeout 10 --memory-size 1024 \\--role arn:aws:iam::123456789012:role/LambdaS3Role --region us-east-1anchoranchorAWS Management ConsoleAWS CLITo create the function (console)To create your Lambda function using the console, you first create a basic function containing some ‘Hello world’ code. You then           replace this code with your own function code by uploading the.zip or JAR file you created in the previous step.Open the Functions page of the Lambda console.Make sure you're working in the same AWS Region you created your Amazon S3 bucket in. You can change your region using the drop-down             list at the top of the screen.Choose Create function.Choose Author from scratch.Under Basic information, do the following:For Function name, enter CreateThumbnail.For Runtime, choose either Node.js 22.x or Python 3.12 according to the language you chose for your function.For Architecture, choose x86_64.In the Change default execution role tab, do the following:Expand the tab, then choose Use an existing role.Select the LambdaS3Role you created earlier.Choose Create function.To upload the function code (console)In the Code source pane, choose Upload from.Choose .zip file. Choose Upload.In the file selector, select your .zip file and choose Open.Choose Save."
                                ]
                            },
                            {
                                "sub_header": "Configure Amazon S3 to invoke the function",
                                "content": [
                                    "\n\n",
                                    "For your Lambda function to run when you upload an image to your source bucket, you need to configure a trigger for your function. You can     configure the Amazon S3 trigger using either the console or the AWS CLI.",
                                    "ImportantThis procedure configures the Amazon S3 bucket to invoke your function every time that an object is created in the bucket. Be sure to \n      configure this only on the source bucket. If your Lambda function creates objects in the same bucket that invokes it, your function can be \n        invoked continuously in a loop. This can result \n        in un expected charges being billed to your AWS account.",
                                    "AWS Management ConsoleTo configure the Amazon S3 trigger (console)Open the Functions page of the Lambda console and choose your function (CreateThumbnail).Choose Add trigger.Select S3.Under Bucket, select your source bucket.Under Event types, select All object create events.Under Recursive invocation, select the check box to acknowledge that using the same Amazon S3 bucket for input                 and output is not recommended. You can learn more about recursive invocation patterns in Lambda by reading                Recursive patterns that cause run-away Lambda functions                in Serverless Land.Choose Add.When you create a trigger using the Lambda console, Lambda automatically creates a resource based policy                 to give the service you select permission to invoke your function. AWS CLITo configure the Amazon S3 trigger (AWS CLI)For your Amazon S3 source bucket to invoke your function when you add an image file, you first need to configure permissions for your                 function using a resource based policy.                 A resource-based policy statement gives other AWS services permission to invoke your function. To give Amazon S3 permission to invoke                 your function, run the following CLI command. Be sure to replace the source-account parameter with your own AWS account ID and to                 use your own source bucket name.aws lambda add-permission --function-name CreateThumbnail \\--principal s3.amazonaws.com --statement-id s3invoke --action \"lambda:InvokeFunction\" \\--source-arn arn:aws:s3:::amzn-s3-demo-source-bucket \\--source-account 123456789012The policy you define with this command allows Amazon S3 to invoke your function only when an action takes place on your source bucket.NoteAlthough Amazon S3 bucket names are globally unique, when using resource-based policies it is best practice to specify that the                 bucket must belong to your account.  This is because if you delete a bucket, it is possible for another AWS account to create a                 bucket with the same Amazon Resource Name (ARN).Save the following JSON in a file named notification.json. When applied to your source bucket, this JSON                 configures the bucket to send a notification to your Lambda function every time a new object is added. Replace the AWS account               number and AWS Region in the Lambda function ARN with your own account number and region.{\"LambdaFunctionConfigurations\": [    {      \"Id\": \"CreateThumbnailEventConfiguration\",      \"LambdaFunctionArn\": \"arn:aws:lambda:us-east-1:123456789012:function:CreateThumbnail\",      \"Events\": [ \"s3:ObjectCreated:Put\" ]    }  ]}Run the following CLI command to apply the notification settings in the JSON file you created to your source bucket. Replace                 amzn-s3-demo-source-bucket with the name of your own source bucket.aws s3api put-bucket-notification-configuration --bucket amzn-s3-demo-source-bucket \\--notification-configuration file://notification.jsonTo learn more about the put-bucket-notification-configuration command and the                 notification-configuration option, see put-bucket-notification-configuration                 in the AWS CLI Command Reference.anchoranchorAWS Management ConsoleAWS CLITo configure the Amazon S3 trigger (console)Open the Functions page of the Lambda console and choose your function (CreateThumbnail).Choose Add trigger.Select S3.Under Bucket, select your source bucket.Under Event types, select All object create events.Under Recursive invocation, select the check box to acknowledge that using the same Amazon S3 bucket for input                 and output is not recommended. You can learn more about recursive invocation patterns in Lambda by reading                Recursive patterns that cause run-away Lambda functions                in Serverless Land.Choose Add.When you create a trigger using the Lambda console, Lambda automatically creates a resource based policy                 to give the service you select permission to invoke your function. "
                                ]
                            },
                            {
                                "sub_header": "Test your Lambda function with a dummy event",
                                "content": [
                                    "\n\n",
                                    "Before you test your whole setup by adding an image file to your Amazon S3 source bucket, you test that your Lambda function is working       correctly by invoking it with a dummy event. An event in Lambda is a JSON-formatted document that contains data for your function to process.       When your function is invoked by Amazon S3, the event sent to your function contains information such as the bucket name, bucket ARN, and object       key.",
                                    "AWS Management ConsoleTo test your Lambda function with a dummy event (console)Open the Functions page of the Lambda console and choose your              function (CreateThumbnail).Choose the Test tab.To create your test event, in the Test event pane, do the following:Under Test event action, select Create new event.For Event name, enter myTestEvent.For Template, select S3 Put.Replace the values for the following parameters with your own values.For awsRegion, replace us-east-1 with the AWS Region you created your Amazon S3 buckets in.For name, replace amzn-s3-demo-bucket with the name of your own Amazon S3 source bucket.For key, replace test%2Fkey with the filename of the test object you uploaded to your source                        bucket in the step Upload a test image to your source bucket.{  \"Records\": [    {      \"eventVersion\": \"2.0\",      \"eventSource\": \"aws:s3\",      \"awsRegion\": \"us-east-1\",      \"eventTime\": \"1970-01-01T00:00:00.000Z\",      \"eventName\": \"ObjectCreated:Put\",      \"userIdentity\": {        \"principalId\": \"EXAMPLE\"      },      \"requestParameters\": {        \"sourceIPAddress\": \"127.0.0.1\"      },      \"responseElements\": {        \"x-amz-request-id\": \"EXAMPLE123456789\",        \"x-amz-id-2\": \"EXAMPLE123/5678abcdefghijklambdaisawesome/mnopqrstuvwxyzABCDEFGH\"      },      \"s3\": {        \"s3SchemaVersion\": \"1.0\",        \"configurationId\": \"testConfigRule\",        \"bucket\": {          \"name\": \"amzn-s3-demo-bucket\",          \"ownerIdentity\": {            \"principalId\": \"EXAMPLE\"          },          \"arn\": \"arn:aws:s3:::amzn-s3-demo-bucket\"        },        \"object\": {          \"key\": \"test%2Fkey\",          \"size\": 1024,          \"eTag\": \"0123456789abcdef0123456789abcdef\",          \"sequencer\": \"0A1B2C3D4E5F678901\"        }      }    }  ]}Choose Save.In the Test event pane, choose Test.To check the your function has created a resized verison of your image and stored it in your target Amazon S3 bucket, do the following:Open the Buckets page of the Amazon S3 console.Choose your target bucket and confirm that your resized file is listed in the Objects pane.AWS CLITo test your Lambda function with a dummy event (AWS CLI)Save the following JSON in a file named dummyS3Event.json. Replace the values for the following parameters                 with your own values:For awsRegion, replace us-east-1 with the AWS Region you created your Amazon S3 buckets in.For name, replace amzn-s3-demo-bucket with the name of your own Amazon S3 source bucket.For key, replace test%2Fkey with the filename of the test object you uploaded to your source                        bucket in the step Upload a test image to your source bucket.{  \"Records\": [    {      \"eventVersion\": \"2.0\",      \"eventSource\": \"aws:s3\",      \"awsRegion\": \"us-east-1\",      \"eventTime\": \"1970-01-01T00:00:00.000Z\",      \"eventName\": \"ObjectCreated:Put\",      \"userIdentity\": {        \"principalId\": \"EXAMPLE\"      },      \"requestParameters\": {        \"sourceIPAddress\": \"127.0.0.1\"      },      \"responseElements\": {        \"x-amz-request-id\": \"EXAMPLE123456789\",        \"x-amz-id-2\": \"EXAMPLE123/5678abcdefghijklambdaisawesome/mnopqrstuvwxyzABCDEFGH\"      },      \"s3\": {        \"s3SchemaVersion\": \"1.0\",        \"configurationId\": \"testConfigRule\",        \"bucket\": {          \"name\": \"amzn-s3-demo-bucket\",          \"ownerIdentity\": {            \"principalId\": \"EXAMPLE\"          },          \"arn\": \"arn:aws:s3:::amzn-s3-demo-bucket\"        },        \"object\": {          \"key\": \"test%2Fkey\",          \"size\": 1024,          \"eTag\": \"0123456789abcdef0123456789abcdef\",          \"sequencer\": \"0A1B2C3D4E5F678901\"        }      }    }  ]}From the directory you saved your dummyS3Event.json file in, invoke the function by running the following                 CLI command. This command invokes your Lambda function synchronously by specifying RequestResponse as the value of the                 invocation-type parameter. To learn more about synchronous and asynchronous invocation, see Invoking Lambda functions.aws lambda invoke --function-name CreateThumbnail \\--invocation-type RequestResponse --cli-binary-format raw-in-base64-out \\--payload file://dummyS3Event.json outputfile.txtThe cli-binary-format option is required if you are using version 2 of the AWS CLI. To make this the default setting, run                 aws configure set cli-binary-format raw-in-base64-out. For more information, see AWS CLI supported global command line options.Verify that your function has created a thumbnail version of your image and saved it to your target Amazon S3 bucket. Run the                 following CLI command, replacing amzn-s3-demo-source-bucket-resized with the name of your own destination bucket.aws s3api list-objects-v2 --bucket amzn-s3-demo-source-bucket-resizedYou should see output similar to the following. The Key parameter shows the filename of your resized image file.{    \"Contents\": [        {            \"Key\": \"resized-HappyFace.jpg\",            \"LastModified\": \"2023-06-06T21:40:07+00:00\",            \"ETag\": \"\\\"d8ca652ffe83ba6b721ffc20d9d7174a\\\"\",            \"Size\": 2633,            \"StorageClass\": \"STANDARD\"        }    ]}anchoranchorAWS Management ConsoleAWS CLITo test your Lambda function with a dummy event (console)Open the Functions page of the Lambda console and choose your              function (CreateThumbnail).Choose the Test tab.To create your test event, in the Test event pane, do the following:Under Test event action, select Create new event.For Event name, enter myTestEvent.For Template, select S3 Put.Replace the values for the following parameters with your own values.For awsRegion, replace us-east-1 with the AWS Region you created your Amazon S3 buckets in.For name, replace amzn-s3-demo-bucket with the name of your own Amazon S3 source bucket.For key, replace test%2Fkey with the filename of the test object you uploaded to your source                        bucket in the step Upload a test image to your source bucket.{  \"Records\": [    {      \"eventVersion\": \"2.0\",      \"eventSource\": \"aws:s3\",      \"awsRegion\": \"us-east-1\",      \"eventTime\": \"1970-01-01T00:00:00.000Z\",      \"eventName\": \"ObjectCreated:Put\",      \"userIdentity\": {        \"principalId\": \"EXAMPLE\"      },      \"requestParameters\": {        \"sourceIPAddress\": \"127.0.0.1\"      },      \"responseElements\": {        \"x-amz-request-id\": \"EXAMPLE123456789\",        \"x-amz-id-2\": \"EXAMPLE123/5678abcdefghijklambdaisawesome/mnopqrstuvwxyzABCDEFGH\"      },      \"s3\": {        \"s3SchemaVersion\": \"1.0\",        \"configurationId\": \"testConfigRule\",        \"bucket\": {          \"name\": \"amzn-s3-demo-bucket\",          \"ownerIdentity\": {            \"principalId\": \"EXAMPLE\"          },          \"arn\": \"arn:aws:s3:::amzn-s3-demo-bucket\"        },        \"object\": {          \"key\": \"test%2Fkey\",          \"size\": 1024,          \"eTag\": \"0123456789abcdef0123456789abcdef\",          \"sequencer\": \"0A1B2C3D4E5F678901\"        }      }    }  ]}Choose Save.In the Test event pane, choose Test.To check the your function has created a resized verison of your image and stored it in your target Amazon S3 bucket, do the following:Open the Buckets page of the Amazon S3 console.Choose your target bucket and confirm that your resized file is listed in the Objects pane."
                                ]
                            },
                            {
                                "sub_header": "Test your function using the Amazon S3 trigger",
                                "content": [
                                    "\n\n",
                                    "Now that you’ve confirmed your Lambda function is operating correctly, you’re ready to test your complete setup by adding an image file to       your Amazon S3 source bucket. When you add your image to the source bucket, your Lambda function should be automatically invoked. Your function       creates a resized version of the file and stores it in your target bucket.",
                                    "AWS Management ConsoleTo test your Lambda function using the Amazon S3 trigger (console)To upload an image to your Amazon S3 bucket, do the following:Open the Buckets page of the Amazon S3 console and choose your source bucket.Choose Upload.Choose Add files and use the file selector to choose the image file you want to upload. Your image                   object can be any .jpg or .png file.Choose Open, then choose Upload.Verify that Lambda has saved a resized version of your image file in your target bucket by doing the following:Navigate back to the Buckets page of the Amazon S3 console and choose your destination bucket.In the Objects pane, you should now see two resized image files, one from each test of your Lambda function.                  To download your resized image, select the file, then choose Download.AWS CLITo test your Lambda function using the Amazon S3 trigger (AWS CLI)From the directory containing the image you want to upload, run the following CLI command. Replace the --bucket                 parameter with the name of your source bucket. For the --key and --body parameters, use the filename of                 your test image. Your test image can be any .jpg or .png file.aws s3api put-object --bucket amzn-s3-demo-source-bucket --key SmileyFace.jpg --body ./SmileyFace.jpgVerify that your function has created a thumbnail version of your image and saved it to your target Amazon S3 bucket. Run the                 following CLI command, replacing amzn-s3-demo-source-bucket-resized with the name of your own destination bucket.aws s3api list-objects-v2 --bucket amzn-s3-demo-source-bucket-resizedIf your function runs successfully, you’ll see output similar to the following. Your target bucket should now contain two resized files.{    \"Contents\": [        {            \"Key\": \"resized-HappyFace.jpg\",            \"LastModified\": \"2023-06-07T00:15:50+00:00\",            \"ETag\": \"\\\"7781a43e765a8301713f533d70968a1e\\\"\",            \"Size\": 2763,            \"StorageClass\": \"STANDARD\"        },        {            \"Key\": \"resized-SmileyFace.jpg\",            \"LastModified\": \"2023-06-07T00:13:18+00:00\",            \"ETag\": \"\\\"ca536e5a1b9e32b22cd549e18792cdbc\\\"\",            \"Size\": 1245,            \"StorageClass\": \"STANDARD\"        }    ]}anchoranchorAWS Management ConsoleAWS CLITo test your Lambda function using the Amazon S3 trigger (console)To upload an image to your Amazon S3 bucket, do the following:Open the Buckets page of the Amazon S3 console and choose your source bucket.Choose Upload.Choose Add files and use the file selector to choose the image file you want to upload. Your image                   object can be any .jpg or .png file.Choose Open, then choose Upload.Verify that Lambda has saved a resized version of your image file in your target bucket by doing the following:Navigate back to the Buckets page of the Amazon S3 console and choose your destination bucket.In the Objects pane, you should now see two resized image files, one from each test of your Lambda function.                  To download your resized image, select the file, then choose Download."
                                ]
                            },
                            {
                                "sub_header": "Clean up your resources",
                                "content": [
                                    "You can now delete the resources that you created for this tutorial, unless you want to retain them. By deleting AWS resources that you're no longer using, you prevent unnecessary charges to your AWS account.",
                                    {
                                        "code_example": "delete"
                                    },
                                    "To delete the policy that you created\nOpen the Policies page of the IAM console.\n\nSelect the policy that you created (AWSLambdaS3Policy).\n\nChoose Policy actions, Delete.\n\nChoose Delete.\n",
                                    "To delete the execution role\nOpen the Roles page of the IAM console.\n\nSelect the execution role that you created.\n\nChoose Delete.\n\nEnter the name of the role in the text input field and choose Delete.\n",
                                    "To delete the S3 bucket\nOpen the Amazon S3 console.\n\nSelect the bucket you created.\n\nChoose Delete.\n\nEnter the name of the bucket in the text input field.\n\nChoose Delete bucket.\n"
                                ]
                            }
                        ]
                    }
                ],
                "sections": [
                    "",
                    "You can use Lambda to process event notifications from    Amazon Simple Storage Service. Amazon S3 can send an event to a Lambda function when an object is created or deleted. You configure    notification settings on a bucket, and grant Amazon S3 permission to invoke a function on the function's resource-based    permissions policy.",
                    "WarningIf your Lambda function uses the same bucket that triggers it, it could cause\n      the function to run in a loop. For example, if the bucket triggers a function each time an object is uploaded,\n      and the function uploads an object to the bucket, then the function indirectly triggers itself. To avoid this, use\n      two buckets, or configure the trigger to only apply to a prefix used for incoming objects.",
                    "Amazon S3 invokes your function asynchronously with an event that contains    details about the object. The following example shows an event that Amazon S3 sent when a deployment package was uploaded    to Amazon S3.",
                    {
                        "code_example": "{\n  \"Records\": [\n    {\n      \"eventVersion\": \"2.1\",\n      \"eventSource\": \"aws:s3\",\n      \"awsRegion\": \"us-east-2\",\n      \"eventTime\": \"2019-09-03T19:37:27.192Z\",\n      \"eventName\": \"ObjectCreated:Put\",\n      \"userIdentity\": {\n        \"principalId\": \"AWS:AIDAINPONIXQXHT3IKHL2\"\n      },\n      \"requestParameters\": {\n        \"sourceIPAddress\": \"205.255.255.255\"\n      },\n      \"responseElements\": {\n        \"x-amz-request-id\": \"D82B88E5F771F645\",\n        \"x-amz-id-2\": \"vlR7PnpV2Ce81l0PRw6jlUpck7Jo5ZsQjryTjKlc5aLWGVHPZLj5NeC6qMa0emYBDXOo6QBU0Wo=\"\n      },\n      \"s3\": {\n        \"s3SchemaVersion\": \"1.0\",\n        \"configurationId\": \"828aa6fc-f7b5-4305-8584-487c791949c1\",\n        \"bucket\": {\n          \"name\": \"amzn-s3-demo-bucket\",\n          \"ownerIdentity\": {\n            \"principalId\": \"A3I5XTEXAMAI3E\"\n          },\n          \"arn\": \"arn:aws:s3:::lambda-artifacts-deafc19498e3f2df\"\n        },\n        \"object\": {\n          \"key\": \"b21b84d653bb07b05b1e6b33684dc11b\",\n          \"size\": 1305107,\n          \"eTag\": \"b21b84d653bb07b05b1e6b33684dc11b\",\n          \"sequencer\": \"0C0F6F405D6ED209E1\"\n        }\n      }\n    }\n  ]\n}"
                    },
                    "To invoke your function, Amazon S3 needs permission from the function's resource-based policy. When you configure an Amazon S3 trigger in the Lambda console, the console modifies the    resource-based policy to allow Amazon S3 to invoke the function if the bucket name and account ID match. If you configure    the notification in Amazon S3, you use the Lambda API to update the policy. You can also use the Lambda API to grant    permission to another account, or restrict permission to a designated alias.",
                    "If your function uses the AWS SDK to manage Amazon S3 resources, it also needs Amazon S3 permissions in its execution role. ",
                    "  1.Tutorial: Using an Amazon S3 trigger to invoke a Lambda function",
                    "  2.Tutorial: Using an Amazon S3 trigger to create thumbnail images"
                ]
            },
            {
                "title": "SQS",
                "href": "https://docs.aws.amazon.com/lambda/latest/dg/with-sqs.html",
                "contents": [
                    {
                        "title": "Create mapping",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/services-sqs-configure.html",
                        "sections": [
                            "",
                            "To process Amazon SQS messages with Lambda, configure your queue with the appropriate settings,        then create a Lambda event source mapping.",
                            {
                                "sub_header": "Configuring a queue to use with Lambda",
                                "content": [
                                    "If you don't already have an existing Amazon SQS queue, create one            to serve as an event source for your Lambda function. The Lambda function and the Amazon SQS queue must be in the same AWS Region, although they can be in different AWS accounts.",
                                    "To allow your function time to process each batch of records, set the source queue's                        visibility timeout to at least six times the configuration            timeout on your function. The extra time allows Lambda to retry if your function is throttled            while processing a previous batch.",
                                    "By default, if Lambda encounters an error at any point while processing a batch, all            messages in that batch return to the queue. After the             visibility timeout, the messages become visible to Lambda again. You can            configure your event source mapping to use             partial batch responses to return only the failed messages back to the queue. In            addition, if your function fails to process a message multiple times, Amazon SQS can send it to a                        dead-letter queue. We recommend setting the maxReceiveCount on your            source queue's             redrive policy to at least 5. This gives Lambda a few chances to retry before            sending failed messages directly to the dead-letter queue."
                                ]
                            },
                            {
                                "sub_header": "Setting up Lambda execution role permissions",
                                "content": [
                                    "The             AWSLambdaSQSQueueExecutionRole AWS managed policy includes the permissions that Lambda needs to read            from your Amazon SQS queue. You can add this managed policy to your function's            execution role.",
                                    "Optionally, if you're using an encrypted queue, you also need to add the following permission to your            execution role:"
                                ]
                            },
                            {
                                "sub_header": "Creating an SQS event source mapping",
                                "content": [
                                    "Create an event source mapping to tell Lambda to send items from your queue to a Lambda function.            You can create multiple event source mappings to process items from multiple queues with a single            function. When Lambda invokes the target function, the event can contain multiple items, up to a            configurable maximum batch size.",
                                    "To configure your function to read from Amazon SQS, attach the             AWSLambdaSQSQueueExecutionRole AWS managed policy to your execution role.            Then, create an SQS event source mapping from the console using            the following steps.",
                                    {
                                        "code_example": "AWSLambdaSQSQueueExecutionRole"
                                    },
                                    {
                                        "code_example": "MaximumBatchingWindowInSeconds"
                                    }
                                ]
                            }
                        ]
                    },
                    {
                        "title": "Scaling behavior",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/services-sqs-scaling.html",
                        "sections": [
                            "",
                            "For standard queues, Lambda uses         long polling to poll a queue until it becomes active. When messages are available,        Lambda starts processing five batches at a time with five concurrent invocations of your function.        If messages are still available, Lambda increases the number of processes that are reading batches by up to 300 more instances per minute. The maximum number of batches that an event source mapping can process simultaneously is 1,000.",
                            "For FIFO queues, Lambda sends messages to your function in the order that it receives them. When you send a        message to a FIFO queue, you specify a message group            ID. Amazon SQS ensures that messages in the same group are delivered to Lambda in order. When Lambda reads         your messages into batches, each batch may contain messages from more than one message group, but the order         of the messages is maintained. If your function returns an error, the function attempts all retries on the         affected messages before Lambda receives additional messages from the same        group.",
                            {
                                "sub_header": "Configuring maximum concurrency for Amazon SQS event sources",
                                "content": [
                                    "You can use the maximum concurrency setting to control scaling behavior for your SQS event sources.            The maximum concurrency setting limits the number of concurrent instances of the function that an Amazon SQS            event source can invoke. Maximum concurrency is an event source-level setting. If you have multiple Amazon SQS            event sources mapped to one function, each event source can have a separate maximum concurrency setting.            You can use maximum concurrency to prevent one queue from using all of the function's            reserved concurrency or the rest of the            account's concurrency quota. There is no charge for            configuring maximum concurrency on an Amazon SQS event source.",
                                    "Importantly, maximum concurrency and reserved concurrency are two independent settings. Don't set            maximum concurrency higher than the function's reserved concurrency. If you configure maximum concurrency,            make sure that your function's reserved concurrency is greater than or equal to the total maximum            concurrency for all Amazon SQS event sources on the function. Otherwise, Lambda may throttle your messages.",
                                    "When your account's concurrency quota is set to the default value of 1,000, an Amazon SQS event source mapping can scale             to invoke function instances up to this value, unless you specify a maximum concurrency.",
                                    "If you receive an increase to your account's default concurrency quota, Lambda may not be able to invoke concurrent functions             instances up to your new quota. By default, Lambda can scale to invoke up to 1,250 concurrent function instances             for an Amazon SQS event source mapping. If this is insufficient for your use case, contact AWS support to             discuss an increase to your account's Amazon SQS event source mapping concurrency.",
                                    {
                                        "code_example": "messageGroupId"
                                    },
                                    "You can configure maximum concurrency on new and existing Amazon SQS event source mappings.",
                                    "Configure maximum concurrency using the Lambda consoleOpen the Functions page of the Lambda console.\nChoose the name of a function.\n\nUnder Function overview, choose SQS. This opens the Configuration tab.\n\nSelect the Amazon SQS trigger and choose Edit.\n\nFor Maximum concurrency, enter a number between 2 and 1,000. To turn off maximum concurrency, leave the box empty.\n\nChoose Save.\n",
                                    "Configure maximum concurrency using the AWS Command Line Interface (AWS CLI)",
                                    "Use the update-event-source-mapping command with the --scaling-config option. Example:",
                                    "aws lambda update-event-source-mapping \\    --uuid \"a1b2c3d4-5678-90ab-cdef-11111EXAMPLE\" \\    --scaling-config '{\"MaximumConcurrency\":5}'",
                                    "To turn off maximum concurrency, enter an empty value for --scaling-config:",
                                    "aws lambda update-event-source-mapping \\    --uuid \"a1b2c3d4-5678-90ab-cdef-11111EXAMPLE\" \\    --scaling-config \"{}\"",
                                    "Configure maximum concurrency using the Lambda API",
                                    "Use the CreateEventSourceMapping or UpdateEventSourceMapping action with a ScalingConfig object."
                                ]
                            }
                        ]
                    },
                    {
                        "title": "Error handling",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/services-sqs-errorhandling.html",
                        "sections": [
                            "",
                            "To handle errors related to an SQS event source, Lambda automatically uses a retry strategy with a        backoff strategy. You can also customize error handling behavior by configuring your SQS event        source mapping to return partial batch responses.",
                            {
                                "sub_header": "Backoff strategy for failed invocations",
                                "content": [
                                    "When an invocation fails, Lambda attempts to retry the invocation while implementing a backoff strategy.            The backoff strategy differs slightly depending on whether Lambda encountered the failure due to an error in            your function code, or due to throttling.",
                                    "  1.function code \n                    If your  caused the error, Lambda will stop processing and retrying the invocation.\n                    In the meantime, Lambda gradually backs off, reducing the amount of concurrency allocated to your Amazon SQS event source mapping.\n                    After your queue's visibility timeout runs out, the message will again reappear in the queue.\n                ",
                                    "  2.throttling If the invocation fails due to , Lambda gradually backs off\n                    retries by reducing the amount of concurrency allocated to your Amazon SQS event source mapping. Lambda continues\n                    to retry the message until the message's timestamp exceeds your queue's visibility timeout, at which point\n                    Lambda drops the message."
                                ]
                            },
                            {
                                "sub_header": "Implementing partial batch responses",
                                "content": [
                                    "When your Lambda function encounters an error while processing a batch, all messages in that batch become            visible in the queue again by default, including messages that Lambda processed successfully. As a result, your            function can end up processing the same message several times.",
                                    "To avoid reprocessing successfully processed messages in a failed batch, you can configure your event            source mapping to make only the failed messages visible again. This is called a partial batch response.            To turn on partial batch responses, specify ReportBatchItemFailures for the            FunctionResponseTypes            action when configuring your event source mapping. This lets your function            return a partial success, which can help reduce the number of unnecessary retries on records.",
                                    "When ReportBatchItemFailures is activated, Lambda doesn't scale down message polling when function invocations fail. If you expect some messages to fail—and you don't want those failures to impact the message processing rate—use ReportBatchItemFailures.",
                                    {
                                        "code_example": "batchItemFailures"
                                    },
                                    {
                                        "code_example": "ReportBatchItemFailures"
                                    },
                                    "If the failed events do not return to the queue, see How do I troubleshoot Lambda function SQS ReportBatchItemFailures? in the AWS Knowledge Center.",
                                    {
                                        "sub_header": "Success and failure conditions",
                                        "content": [
                                            "Lambda treats a batch as a complete success if your function returns any of the following:",
                                            {
                                                "code_example": "batchItemFailures"
                                            },
                                            "Lambda treats a batch as a complete failure if your function returns any of the following:",
                                            {
                                                "code_example": "itemIdentifier"
                                            }
                                        ]
                                    },
                                    {
                                        "sub_header": "CloudWatch metrics",
                                        "content": [
                                            "To determine whether your function is correctly reporting batch item failures, you can monitor the                NumberOfMessagesDeleted and ApproximateAgeOfOldestMessage Amazon SQS metrics in                Amazon CloudWatch.",
                                            {
                                                "code_example": "NumberOfMessagesDeleted"
                                            }
                                        ]
                                    }
                                ]
                            }
                        ]
                    },
                    {
                        "title": "Parameters",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/services-sqs-parameters.html",
                        "sections": [
                            "",
                            "All Lambda event source types share the same CreateEventSourceMapping and UpdateEventSourceMapping        API operations. However, only some of the parameters apply to Amazon SQS.",
                            {
                                "code_example": "ReportBatchItemFailures"
                            }
                        ]
                    },
                    {
                        "title": "Event filtering",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/with-sqs-filtering.html",
                        "sections": [
                            "",
                            "You can use event filtering to control which records from a stream or queue Lambda sends to your function.    For general information about how event filtering works, see Control which events Lambda sends to your function.",
                            "This section focuses on event filtering for Amazon MSK event sources.",
                            "  1.Amazon SQS event filtering basics",
                            {
                                "sub_header": "Amazon SQS event filtering basics",
                                "content": [
                                    "Suppose your Amazon SQS queue contains messages in the following JSON format.",
                                    "{    \"RecordNumber\": 1234,    \"TimeStamp\": \"yyyy-mm-ddThh:mm:ss\",    \"RequestCode\": \"AAAA\"}",
                                    "An example record for this queue would look as follows.",
                                    "{    \"messageId\": \"059f36b4-87a3-44ab-83d2-661975830a7d\",    \"receiptHandle\": \"AQEBwJnKyrHigUMZj6rYigCgxlaS3SLy0a...\",    \"body\": \"{\\n \"RecordNumber\": 1234,\\n \"TimeStamp\": \"yyyy-mm-ddThh:mm:ss\",\\n \"RequestCode\": \"AAAA\"\\n}\",    \"attributes\": {        \"ApproximateReceiveCount\": \"1\",        \"SentTimestamp\": \"1545082649183\",        \"SenderId\": \"AIDAIENQZJOLO23YVJ4VO\",        \"ApproximateFirstReceiveTimestamp\": \"1545082649185\"        },    \"messageAttributes\": {},    \"md5OfBody\": \"e4e68fb7bd0e697a0ae8f1bb342846b3\",    \"eventSource\": \"aws:sqs\",    \"eventSourceARN\": \"arn:aws:sqs:us-west-2:123456789012:my-queue\",    \"awsRegion\": \"us-west-2\"}",
                                    "To filter based on the contents of your Amazon SQS messages, use the body key in the Amazon SQS message record. Suppose you want to process             only those records where the RequestCode in your Amazon SQS message is “BBBB.” The FilterCriteria object would be             as follows.",
                                    "{    \"Filters\": [        {            \"Pattern\": \"{ \\\"body\\\" : { \\\"RequestCode\\\" : [ \\\"BBBB\\\" ] } }\"        }    ]}",
                                    "For added clarity, here is the value of the filter's Pattern expanded in plain JSON. ",
                                    "{    \"body\": {        \"RequestCode\": [ \"BBBB\" ]        }}",
                                    "You can add your filter using the console, AWS CLI or an AWS SAM template.",
                                    "ConsoleTo add this filter using the console, follow the instructions in Attaching filter criteria to an event source mapping (console) and enter the following                         string for the Filter criteria.{ \"body\" : { \"RequestCode\" : [ \"BBBB\" ] } }AWS CLITo create a new event source mapping with these filter criteria using the AWS Command Line Interface (AWS CLI), run the following                        command.aws lambda create-event-source-mapping \\    --function-name my-function \\    --event-source-arn arn:aws:sqs:us-east-2:123456789012:my-queue \\    --filter-criteria '{\"Filters\": [{\"Pattern\": \"{ \\\"body\\\" : { \\\"RequestCode\\\" : [ \\\"BBBB\\\" ] } }\"}]}'To add these filter criteria to an existing event source mapping, run the following command.aws lambda update-event-source-mapping \\    --uuid \"a1b2c3d4-5678-90ab-cdef-11111EXAMPLE\" \\    --filter-criteria '{\"Filters\": [{\"Pattern\": \"{ \\\"body\\\" : { \\\"RequestCode\\\" : [ \\\"BBBB\\\" ] } }\"}]}'AWS SAMTo add this filter using AWS SAM, add the following snippet to the YAML template for your event source.FilterCriteria:  Filters:    - Pattern: '{ \"body\" : { \"RequestCode\" : [ \"BBBB\" ] } }'anchoranchoranchorConsoleAWS CLIAWS SAMTo add this filter using the console, follow the instructions in Attaching filter criteria to an event source mapping (console) and enter the following                         string for the Filter criteria.{ \"body\" : { \"RequestCode\" : [ \"BBBB\" ] } }",
                                    "Suppose you want your function to process only those records where RecordNumber is greater than 9999. The FilterCriteria             object would be as follows.",
                                    "{    \"Filters\": [        {            \"Pattern\": \"{ \\\"body\\\" : { \\\"RecordNumber\\\" : [ { \\\"numeric\\\": [ \\\">\\\", 9999 ] } ] } }\"        }    ]}",
                                    "For added clarity, here is the value of the filter's Pattern expanded in plain JSON. ",
                                    "{    \"body\": {        \"RecordNumber\": [            {                \"numeric\": [ \">\", 9999 ]            }        ]    }}",
                                    "You can add your filter using the console, AWS CLI or an AWS SAM template.",
                                    "ConsoleTo add this filter using the console, follow the instructions in Attaching filter criteria to an event source mapping (console) and enter the following                         string for the Filter criteria.{ \"body\" : { \"RecordNumber\" : [ { \"numeric\": [ \">\", 9999 ] } ] } }AWS CLITo create a new event source mapping with these filter criteria using the AWS Command Line Interface (AWS CLI), run the following                        command.aws lambda create-event-source-mapping \\    --function-name my-function \\    --event-source-arn arn:aws:sqs:us-east-2:123456789012:my-queue \\    --filter-criteria '{\"Filters\": [{\"Pattern\": \"{ \\\"body\\\" : { \\\"RecordNumber\\\" : [ { \\\"numeric\\\": [ \\\">\\\", 9999 ] } ] } }\"}]}'To add these filter criteria to an existing event source mapping, run the following command.aws lambda update-event-source-mapping \\    --uuid \"a1b2c3d4-5678-90ab-cdef-11111EXAMPLE\" \\    --filter-criteria '{\"Filters\": [{\"Pattern\": \"{ \\\"body\\\" : { \\\"RecordNumber\\\" : [ { \\\"numeric\\\": [ \\\">\\\", 9999 ] } ] } }\"}]}'AWS SAMTo add this filter using AWS SAM, add the following snippet to the YAML template for your event source.FilterCriteria:  Filters:    - Pattern: '{ \"body\" : { \"RecordNumber\" : [ { \"numeric\": [ \">\", 9999 ] } ] } }'anchoranchoranchorConsoleAWS CLIAWS SAMTo add this filter using the console, follow the instructions in Attaching filter criteria to an event source mapping (console) and enter the following                         string for the Filter criteria.{ \"body\" : { \"RecordNumber\" : [ { \"numeric\": [ \">\", 9999 ] } ] } }",
                                    "For Amazon SQS, the message body can be any string. However, this can be problematic if your FilterCriteria expect body             to be in a valid JSON format. The reverse scenario is also true—if the incoming message body is in JSON format but your filter criteria             expects body to be a plain string, this can lead to unintended behavior.",
                                    "To avoid this issue, ensure that the format of body in your FilterCriteria matches the expected format of body in messages             that you receive from your queue. Before filtering your messages, Lambda automatically evaluates the format of the incoming message body and             of your filter pattern for body. If there is a mismatch, Lambda drops the message. The following table summarizes this evaluation:",
                                    {
                                        "code_example": "body"
                                    }
                                ]
                            }
                        ]
                    },
                    {
                        "title": "Tutorial",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/with-sqs-example.html",
                        "sections": [
                            "",
                            "In this tutorial, you create a Lambda function that consumes messages from an Amazon Simple Queue Service (Amazon SQS) queue. The Lambda function runs whenever a new message is added to the queue. The function writes the messages to an Amazon CloudWatch Logs stream. The following diagram shows the AWS     resources you use to complete the tutorial.",
                            "\n\n",
                            "To complete this tutorial, you carry out the following steps:",
                            "\n\nCreate a Lambda function that writes messages to CloudWatch Logs.\n\nCreate an Amazon SQS queue.\n\nCreate a Lambda event source mapping. The event source mapping reads the Amazon SQS queue and invokes your Lambda function when a new message is added.\n\nTest the setup by adding messages to your queue and monitoring the results in \n        CloudWatch Logs.\n",
                            {
                                "sub_header": "Prerequisites",
                                "content": [
                                    {
                                        "code_example": "zip"
                                    }
                                ]
                            },
                            {
                                "sub_header": "Create the execution role",
                                "content": [
                                    "\n\n",
                                    "An execution role is an AWS Identity and Access Management (IAM) role that grants a Lambda function permission to access AWS services and resources. To allow       your function to read items from Amazon SQS, attach the AWSLambdaSQSQueueExecutionRole permissions policy.",
                                    {
                                        "code_example": "AWSLambdaSQSQueueExecutionRole"
                                    },
                                    "After role creation, note down the Amazon Resource Name (ARN) of your execution role. You'll      need it in later steps."
                                ]
                            },
                            {
                                "sub_header": "Create the function",
                                "content": [
                                    "\n\n",
                                    "Create a Lambda function that processes your Amazon SQS messages. The function code logs the body of      the Amazon SQS message to CloudWatch Logs.",
                                    "This tutorial uses the Node.js 18.x runtime, but we've also provided      example code in other runtime languages. You can select the tab in the following box to see code      for the runtime you're interested in. The JavaScript code you'll use in this step is in the first      example shown in the JavaScript tab.",
                                    ".NETAWS SDK for .NETNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Consuming an SQS event with Lambda using .NET.// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.// SPDX-License-Identifier: Apache-2.0﻿using Amazon.Lambda.Core;using Amazon.Lambda.SQSEvents;// Assembly attribute to enable the Lambda function's JSON input to be converted into a .NET class.[assembly: LambdaSerializer(typeof(Amazon.Lambda.Serialization.SystemTextJson.DefaultLambdaJsonSerializer))]namespace SqsIntegrationSampleCode{    public async Task FunctionHandler(SQSEvent evnt, ILambdaContext context)    {        foreach (var message in evnt.Records)        {            await ProcessMessageAsync(message, context);        }        context.Logger.LogInformation(\"done\");    }    private async Task ProcessMessageAsync(SQSEvent.SQSMessage message, ILambdaContext context)    {        try        {            context.Logger.LogInformation($\"Processed message {message.Body}\");            // TODO: Do interesting work based on the new message            await Task.CompletedTask;        }        catch (Exception e)        {            //You can use Dead Letter Queue to handle failures. By configuring a Lambda DLQ.            context.Logger.LogError($\"An error occurred\");            throw;        }    }}GoSDK for Go V2Note        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Consuming an SQS event with Lambda using Go.// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.// SPDX-License-Identifier: Apache-2.0package integration_sqs_to_lambdaimport (\t\"fmt\"\t\"github.com/aws/aws-lambda-go/events\"\t\"github.com/aws/aws-lambda-go/lambda\")func handler(event events.SQSEvent) error {\tfor _, record := range event.Records {\t\terr := processMessage(record)\t\tif err != nil {\t\t\treturn err\t\t}\t}\tfmt.Println(\"done\")\treturn nil}func processMessage(record events.SQSMessage) error {\tfmt.Printf(\"Processed message %s\\n\", record.Body)\t// TODO: Do interesting work based on the new message\treturn nil}func main() {\tlambda.Start(handler)}JavaSDK for Java 2.xNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Consuming an SQS event with Lambda using Java.// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.// SPDX-License-Identifier: Apache-2.0import com.amazonaws.services.lambda.runtime.Context;import com.amazonaws.services.lambda.runtime.RequestHandler;import com.amazonaws.services.lambda.runtime.events.SQSEvent;import com.amazonaws.services.lambda.runtime.events.SQSEvent.SQSMessage;public class Function implements RequestHandler<SQSEvent, Void> {    @Override    public Void handleRequest(SQSEvent sqsEvent, Context context) {        for (SQSMessage msg : sqsEvent.getRecords()) {            processMessage(msg, context);        }        context.getLogger().log(\"done\");        return null;    }    private void processMessage(SQSMessage msg, Context context) {        try {            context.getLogger().log(\"Processed message \" + msg.getBody());            // TODO: Do interesting work based on the new message        } catch (Exception e) {            context.getLogger().log(\"An error occurred\");            throw e;        }    }}JavaScriptSDK for JavaScript (v3)Note        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Consuming an SQS event with Lambda using JavaScript.// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.// SPDX-License-Identifier: Apache-2.0exports.handler = async (event, context) => {  for (const message of event.Records) {    await processMessageAsync(message);  }  console.info(\"done\");};async function processMessageAsync(message) {  try {    console.log(`Processed message ${message.body}`);    // TODO: Do interesting work based on the new message    await Promise.resolve(1); //Placeholder for actual async work  } catch (err) {    console.error(\"An error occurred\");    throw err;  }}Consuming an SQS event with Lambda using TypeScript.// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.// SPDX-License-Identifier: Apache-2.0import { SQSEvent, Context, SQSHandler, SQSRecord } from \"aws-lambda\";export const functionHandler: SQSHandler = async (  event: SQSEvent,  context: Context): Promise<void> => {  for (const message of event.Records) {    await processMessageAsync(message);  }  console.info(\"done\");};async function processMessageAsync(message: SQSRecord): Promise<any> {  try {    console.log(`Processed message ${message.body}`);    // TODO: Do interesting work based on the new message    await Promise.resolve(1); //Placeholder for actual async work  } catch (err) {    console.error(\"An error occurred\");    throw err;  }}PHPSDK for PHPNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Consuming an SQS event with Lambda using PHP.// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.// SPDX-License-Identifier: Apache-2.0<?php# using bref/bref and bref/logger for simplicityuse Bref\\Context\\Context;use Bref\\Event\\InvalidLambdaEvent;use Bref\\Event\\Sqs\\SqsEvent;use Bref\\Event\\Sqs\\SqsHandler;use Bref\\Logger\\StderrLogger;require __DIR__ . '/vendor/autoload.php';class Handler extends SqsHandler{    private StderrLogger $logger;    public function __construct(StderrLogger $logger)    {        $this->logger = $logger;    }    /**     * @throws InvalidLambdaEvent     */    public function handleSqs(SqsEvent $event, Context $context): void    {        foreach ($event->getRecords() as $record) {            $body = $record->getBody();            // TODO: Do interesting work based on the new message        }    }}$logger = new StderrLogger();return new Handler($logger);PythonSDK for Python (Boto3)Note        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Consuming an SQS event with Lambda using Python.# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.# SPDX-License-Identifier: Apache-2.0def lambda_handler(event, context):    for message in event['Records']:        process_message(message)    print(\"done\")def process_message(message):    try:        print(f\"Processed message {message['body']}\")        # TODO: Do interesting work based on the new message    except Exception as err:        print(\"An error occurred\")        raise errRubySDK for RubyNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Consuming an SQS event with Lambda using Ruby.# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.# SPDX-License-Identifier: Apache-2.0def lambda_handler(event:, context:)  event['Records'].each do |message|    process_message(message)  end  puts \"done\"enddef process_message(message)  begin    puts \"Processed message #{message['body']}\"    # TODO: Do interesting work based on the new message  rescue StandardError => err    puts \"An error occurred\"    raise err  endendRustSDK for RustNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Consuming an SQS event with Lambda using Rust.// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.// SPDX-License-Identifier: Apache-2.0use aws_lambda_events::event::sqs::SqsEvent;use lambda_runtime::{run, service_fn, Error, LambdaEvent};async fn function_handler(event: LambdaEvent<SqsEvent>) -> Result<(), Error> {    event.payload.records.iter().for_each(|record| {        // process the record        tracing::info!(\"Message body: {}\", record.body.as_deref().unwrap_or_default())    });    Ok(())}#[tokio::main]async fn main() -> Result<(), Error> {    tracing_subscriber::fmt()        .with_max_level(tracing::Level::INFO)        // disable printing the name of the module in every log line.        .with_target(false)        // disabling time is handy because CloudWatch will add the ingestion time.        .without_time()        .init();    run(service_fn(function_handler)).await}anchoranchoranchoranchoranchoranchoranchoranchor.NETGoJavaJavaScriptPHPPythonRubyRustAWS SDK for .NETNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Consuming an SQS event with Lambda using .NET.// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.// SPDX-License-Identifier: Apache-2.0﻿using Amazon.Lambda.Core;using Amazon.Lambda.SQSEvents;// Assembly attribute to enable the Lambda function's JSON input to be converted into a .NET class.[assembly: LambdaSerializer(typeof(Amazon.Lambda.Serialization.SystemTextJson.DefaultLambdaJsonSerializer))]namespace SqsIntegrationSampleCode{    public async Task FunctionHandler(SQSEvent evnt, ILambdaContext context)    {        foreach (var message in evnt.Records)        {            await ProcessMessageAsync(message, context);        }        context.Logger.LogInformation(\"done\");    }    private async Task ProcessMessageAsync(SQSEvent.SQSMessage message, ILambdaContext context)    {        try        {            context.Logger.LogInformation($\"Processed message {message.Body}\");            // TODO: Do interesting work based on the new message            await Task.CompletedTask;        }        catch (Exception e)        {            //You can use Dead Letter Queue to handle failures. By configuring a Lambda DLQ.            context.Logger.LogError($\"An error occurred\");            throw;        }    }}",
                                    {
                                        "code_example": "mkdir sqs-tutorial\ncd sqs-tutorial"
                                    }
                                ]
                            },
                            {
                                "sub_header": "Test the function",
                                "content": [
                                    "\n\n",
                                    "Invoke your Lambda function manually using the invoke AWS CLI command and a sample Amazon SQS      event.",
                                    {
                                        "code_example": "input.json"
                                    }
                                ]
                            },
                            {
                                "sub_header": "Create an Amazon SQS queue",
                                "content": [
                                    "\n\n",
                                    "Create an Amazon SQS queue that the Lambda function can use as an event source. The Lambda function and the Amazon SQS queue must be in the same AWS Region.",
                                    "To create a queue\nOpen the Amazon SQS console.\n\nChoose Create queue.\n\nEnter a name for the queue. Leave all other options at the default settings.\n\nChoose Create queue.\n",
                                    "After creating the queue, note down its ARN. You need this in the next step when you      associate the queue with your Lambda function."
                                ]
                            },
                            {
                                "sub_header": "Configure the event source",
                                "content": [
                                    "\n\n",
                                    "Connect the Amazon SQS queue to your Lambda function by creating an event source mapping. The event source mapping reads the Amazon SQS queue and invokes your Lambda function when a new message is added.",
                                    "To create a mapping between your Amazon SQS queue and your Lambda function, use the create-event-source-mapping AWS CLI command. Example:",
                                    "aws lambda create-event-source-mapping --function-name ProcessSQSRecord  --batch-size 10 \\--event-source-arn arn:aws:sqs:us-east-1:111122223333:my-queue",
                                    "To get a list of your event source mappings, use the list-event-source-mappings command. Example:",
                                    "aws lambda list-event-source-mappings --function-name ProcessSQSRecord"
                                ]
                            },
                            {
                                "sub_header": "Send a test message",
                                "content": [
                                    "\n\n",
                                    "To send an Amazon SQS message to the Lambda function\nOpen the Amazon SQS console.\n\nChoose the queue that you created earlier.\n\nChoose Send and receive messages.\n\nUnder Message body, enter a test message, such as \"this is a test message.\"\n\nChoose Send message.\n",
                                    "Lambda polls the queue for updates. When there is a new message, Lambda invokes your function with this new      event data from the queue. If the function handler returns without exceptions, Lambda considers the message successfully processed and      begins reading new messages in the queue. After successfully processing a message, Lambda automatically deletes it      from the queue. If the handler throws an exception, Lambda considers the batch of messages not successfully      processed, and Lambda invokes the function with the same batch of messages."
                                ]
                            },
                            {
                                "sub_header": "Check the CloudWatch logs",
                                "content": [
                                    "\n\n",
                                    {
                                        "code_example": "INFO"
                                    }
                                ]
                            },
                            {
                                "sub_header": "Clean up your resources",
                                "content": [
                                    "You can now delete the resources that you created for this tutorial, unless you want to retain them. By deleting AWS resources that you're no longer using, you prevent unnecessary charges to your AWS account.",
                                    "To delete the execution role\nOpen the Roles page of the IAM console.\n\nSelect the execution role that you created.\n\nChoose Delete.\n\nEnter the name of the role in the text input field and choose Delete.\n",
                                    {
                                        "code_example": "delete"
                                    },
                                    {
                                        "code_example": "confirm"
                                    }
                                ]
                            }
                        ]
                    },
                    {
                        "title": "SQS cross-account tutorial",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/with-sqs-cross-account-example.html",
                        "sections": [
                            "",
                            "In this tutorial, you create a Lambda function that consumes messages from an Amazon Simple Queue Service (Amazon SQS) queue in a    different AWS account. This tutorial involves two AWS accounts: Account A refers to the    account that contains your Lambda function, and Account B refers to the account that contains    the Amazon SQS queue.",
                            {
                                "sub_header": "Prerequisites",
                                "content": [
                                    "This tutorial assumes that you have some knowledge of basic Lambda operations and the Lambda console. If you      haven't already, follow the instructions in Create a Lambda function with the console to create your first Lambda function.",
                                    "To complete the following steps, you need the AWS CLI version 2. Commands and the expected output are listed in separate blocks:",
                                    "aws --version",
                                    "You should see the following output:",
                                    "aws-cli/2.13.27 Python/3.11.6 Linux/4.14.328-248.540.amzn2.x86_64 exe/x86_64.amzn.2",
                                    "For long commands, an escape character (\\) is used to split a command over multiple lines.",
                                    "On Linux and macOS, use your preferred shell and package manager.",
                                    {
                                        "code_example": "zip"
                                    }
                                ]
                            },
                            {
                                "sub_header": "Create the execution role (Account A)",
                                "content": [
                                    "In Account A, create an execution role      that gives your function permission to access the required AWS resources.",
                                    {
                                        "code_example": "cross-account-lambda-sqs-role"
                                    },
                                    "The AWSLambdaSQSQueueExecutionRole policy has the permissions that the function needs to      read items from Amazon SQS and to write logs to Amazon CloudWatch Logs."
                                ]
                            },
                            {
                                "sub_header": "Create the function (Account A)",
                                "content": [
                                    "In Account A, create a Lambda function that processes your Amazon SQS messages. The Lambda function and the Amazon SQS queue must be in the same AWS Region.",
                                    "The following Node.js 18 code example writes each message to a log in CloudWatch Logs.",
                                    {
                                        "code_example": "export const handler = async function(event, context) {\n  event.Records.forEach(record => {\n    const { body } = record;\n    console.log(body);\n  });\n  return {};\n}"
                                    },
                                    {
                                        "code_example": "index.mjs"
                                    }
                                ]
                            },
                            {
                                "sub_header": "Test the function (Account A)",
                                "content": [
                                    "In Account A, test your Lambda function manually using the invoke AWS CLI      command and a sample Amazon SQS event.",
                                    "If the handler returns normally without exceptions, Lambda considers the message to be successfully processed      and begins reading new messages in the queue. After successfully processing a message, Lambda automatically deletes      it from the queue. If the handler throws an exception, Lambda considers the batch of messages not successfully      processed, and Lambda invokes the function with the same batch of messages.",
                                    {
                                        "code_example": "input.txt"
                                    }
                                ]
                            },
                            {
                                "sub_header": "Create an Amazon SQS queue (Account B)",
                                "content": [
                                    "In Account B, create an Amazon SQS queue that the Lambda function in Account      A can use as an event source. The Lambda function and the Amazon SQS queue must be in the same AWS Region.",
                                    {
                                        "code_example": "{\n   \"Version\": \"2012-10-17\",\n   \"Id\": \"Queue1_Policy_UUID\",\n   \"Statement\": [{\n      \"Sid\":\"Queue1_AllActions\",\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n         \"AWS\": [\n            \"arn:aws:iam::<AccountA_ID>:role/cross-account-lambda-sqs-role\"\n         ]\n      },\n      \"Action\": \"sqs:*\",\n      \"Resource\": \"arn:aws:sqs:us-east-1:<AccountB_ID>:LambdaCrossAccountQueue\"\n    }\n  ]\n}"
                                    }
                                ]
                            },
                            {
                                "sub_header": "Configure the event source (Account A)",
                                "content": [
                                    "In Account A, create an event source mapping between the Amazon SQS queue in Account        B and your Lambda function by running the following create-event-source-mapping AWS CLI      command.",
                                    "aws lambda create-event-source-mapping --function-name CrossAccountSQSExample --batch-size 10 \\--event-source-arn arn:aws:sqs:us-east-1:<AccountB_ID>:LambdaCrossAccountQueue",
                                    "To get a list of your event source mappings, run the following command.",
                                    "aws lambda list-event-source-mappings --function-name CrossAccountSQSExample \\--event-source-arn arn:aws:sqs:us-east-1:<AccountB_ID>:LambdaCrossAccountQueue"
                                ]
                            },
                            {
                                "sub_header": "Test the setup",
                                "content": [
                                    "You can now test the setup as follows:",
                                    "\n\nIn Account B, open the Amazon SQS console.\n\nChoose LambdaCrossAccountQueue, which you created earlier.\n\nChoose Send and receive messages.\n\nUnder Message body, enter a test message.\n\nChoose Send message.\n",
                                    "Your Lambda function in Account A should receive the message. Lambda will continue to poll      the queue for updates. When there is a new message, Lambda invokes your function with this new event data from the      queue. Your function runs and creates logs in Amazon CloudWatch. You can view the logs in the CloudWatch console."
                                ]
                            },
                            {
                                "sub_header": "Clean up your resources",
                                "content": [
                                    "You can now delete the resources that you created for this tutorial, unless you want to retain them. By deleting AWS resources that you're no longer using, you prevent unnecessary charges to your AWS account.",
                                    "In Account A, clean up your execution role and Lambda function.",
                                    "To delete the execution role\nOpen the Roles page of the IAM console.\n\nSelect the execution role that you created.\n\nChoose Delete.\n\nEnter the name of the role in the text input field and choose Delete.\n",
                                    {
                                        "code_example": "delete"
                                    },
                                    "In Account B, clean up the Amazon SQS queue.",
                                    {
                                        "code_example": "confirm"
                                    }
                                ]
                            }
                        ]
                    }
                ],
                "sections": [
                    "",
                    "NoteIf you want to send data to a target other than a Lambda function or enrich the data before sending it, see \n    Amazon EventBridge Pipes.",
                    "You can use a Lambda function to process messages in an Amazon Simple Queue Service (Amazon SQS) queue. Lambda    supports both     standard queues and       first-in, first-out (FIFO) queues for event source mappings. The Lambda function and the Amazon SQS queue must be in the same AWS Region, although they can be in different AWS accounts.",
                    "  1.Understanding polling and batching behavior for Amazon SQS event source mappings",
                    "  2.Example standard queue message event",
                    "  3. Example FIFO queue message event",
                    "  4.Creating and configuring an Amazon SQS event source mapping",
                    "  5.Configuring scaling behavior for SQS event source mappings",
                    "  6.Handling errors for an SQS event source in Lambda",
                    "  7.Lambda parameters for Amazon SQS event source mappings",
                    "  8.Using event filtering with an Amazon SQS event source",
                    "  9.Tutorial: Using Lambda with Amazon SQS",
                    "  10.Tutorial: Using a cross-account Amazon SQS queue as an event      source",
                    {
                        "sub_header": "Understanding polling and batching behavior for Amazon SQS event source mappings",
                        "content": [
                            "With Amazon SQS event source mappings, Lambda polls the queue and invokes your function       synchronously with an event. Each event can contain a batch of multiple messages from the queue. Lambda receives      these events one batch at a time, and invokes your function once for each batch. When your function successfully      processes a batch, Lambda deletes its messages from the queue.",
                            "When Lambda receives a batch, the messages stay in the queue but are hidden for the length of the queue's            visibility timeout. If your function successfully processes all messages in the batch, Lambda deletes      the messages from the queue. By default, if your function encounters an error while processing a batch, all      messages in that batch become visible in the queue again after the visibility timeout expires. For this reason,      your function code must be able to process the same message multiple times without unintended side effects.",
                            "WarningLambda event source mappings process each event at least once, and duplicate processing of records can occur. To avoid potential issues \nrelated to duplicate events, we strongly recommend that you make your function code idempotent. To learn more, see How do I make my Lambda function idempotent \nin the AWS Knowledge Center.",
                            "To prevent Lambda from processing a message multiple times, you can either configure your event source      mapping to include batch item failures in your      function response, or you can use the DeleteMessage API to      remove messages from the queue as  your Lambda function successfully processes them.",
                            "For more information about configuration parameters that Lambda supports for SQS event source      mappings, see Creating an SQS event source mapping."
                        ]
                    },
                    {
                        "sub_header": "Example standard queue message event",
                        "content": [
                            {
                                "code_example": "{\n    \"Records\": [\n        {\n            \"messageId\": \"059f36b4-87a3-44ab-83d2-661975830a7d\",\n            \"receiptHandle\": \"AQEBwJnKyrHigUMZj6rYigCgxlaS3SLy0a...\",\n            \"body\": \"Test message.\",\n            \"attributes\": {\n                \"ApproximateReceiveCount\": \"1\",\n                \"SentTimestamp\": \"1545082649183\",\n                \"SenderId\": \"AIDAIENQZJOLO23YVJ4VO\",\n                \"ApproximateFirstReceiveTimestamp\": \"1545082649185\"\n            },\n            \"messageAttributes\": {\n                \"myAttribute\": {\n                    \"stringValue\": \"myValue\", \n                    \"stringListValues\": [], \n                    \"binaryListValues\": [], \n                    \"dataType\": 'String'\n                }\n            },\n            \"md5OfBody\": \"e4e68fb7bd0e697a0ae8f1bb342846b3\",\n            \"eventSource\": \"aws:sqs\",\n            \"eventSourceARN\": \"arn:aws:sqs:us-east-2:123456789012:my-queue\",\n            \"awsRegion\": \"us-east-2\"\n        },\n        {\n            \"messageId\": \"2e1424d4-f796-459a-8184-9c92662be6da\",\n            \"receiptHandle\": \"AQEBzWwaftRI0KuVm4tP+/7q1rGgNqicHq...\",\n            \"body\": \"Test message.\",\n            \"attributes\": {\n                \"ApproximateReceiveCount\": \"1\",\n                \"SentTimestamp\": \"1545082650636\",\n                \"SenderId\": \"AIDAIENQZJOLO23YVJ4VO\",\n                \"ApproximateFirstReceiveTimestamp\": \"1545082650649\"\n            },\n            \"messageAttributes\": {},\n            \"md5OfBody\": \"e4e68fb7bd0e697a0ae8f1bb342846b3\",\n            \"eventSource\": \"aws:sqs\",\n            \"eventSourceARN\": \"arn:aws:sqs:us-east-2:123456789012:my-queue\",\n            \"awsRegion\": \"us-east-2\"\n        }\n    ]\n}"
                            },
                            "By default, Lambda polls up to 10 messages in your queue at once and sends that batch to your function. To avoid      invoking the function with a small number of records, you can configure the event source to buffer records for up      to 5 minutes by configuring a batch window. Before invoking the function, Lambda continues to poll messages from the      standard queue until the batch window expires, the invocation payload size      quota is reached, or the configured maximum batch size is reached.",
                            "If you're using a batch window and your SQS queue contains very low traffic, Lambda might wait for up to 20      seconds before invoking your function. This is true even if you set a batch window lower than 20 seconds.    ",
                            "NoteIn Java, you might experience null pointer errors when deserializing JSON. This could be due to how case of  \"Records\" and \"eventSourceARN\" is converted by the JSON object mapper."
                        ]
                    },
                    {
                        "sub_header": " Example FIFO queue message event",
                        "content": [
                            "For FIFO queues, records contain additional attributes that are related to deduplication and sequencing.",
                            {
                                "code_example": "{\n    \"Records\": [\n        {\n            \"messageId\": \"11d6ee51-4cc7-4302-9e22-7cd8afdaadf5\",\n            \"receiptHandle\": \"AQEBBX8nesZEXmkhsmZeyIE8iQAMig7qw...\",\n            \"body\": \"Test message.\",\n            \"attributes\": {\n                \"ApproximateReceiveCount\": \"1\",\n                \"SentTimestamp\": \"1573251510774\",\n                \"SequenceNumber\": \"18849496460467696128\",\n                \"MessageGroupId\": \"1\",\n                \"SenderId\": \"AIDAIO23YVJENQZJOL4VO\",\n                \"MessageDeduplicationId\": \"1\",\n                \"ApproximateFirstReceiveTimestamp\": \"1573251510774\"\n            },\n            \"messageAttributes\": {},\n            \"md5OfBody\": \"e4e68fb7bd0e697a0ae8f1bb342846b3\",\n            \"eventSource\": \"aws:sqs\",\n            \"eventSourceARN\": \"arn:aws:sqs:us-east-2:123456789012:fifo.fifo\",\n            \"awsRegion\": \"us-east-2\"\n        }\n    ]\n}"
                            }
                        ]
                    }
                ]
            },
            {
                "title": "S3 Batch",
                "href": "https://docs.aws.amazon.com/lambda/latest/dg/services-s3-batch.html",
                "sections": [
                    "",
                    "You can use Amazon S3 batch operations to invoke a Lambda function on a large set of Amazon S3 objects. Amazon S3    tracks the progress of batch operations, sends notifications, and stores a completion report that shows the status    of each action. ",
                    "To run a batch operation, you create an Amazon S3 batch operations job. When you create    the job, you provide a manifest (the list of objects) and configure the action to perform on those objects. ",
                    "When the batch job starts, Amazon S3 invokes the Lambda function synchronously for each object in the manifest.    The event parameter includes the names of the bucket and the object. ",
                    "The following example shows the event that Amazon S3 sends to the Lambda function for an object that is named      customerImage1.jpg in the amzn-s3-demo-bucket    bucket.",
                    {
                        "code_example": "\n{\n\"invocationSchemaVersion\": \"1.0\",\n    \"invocationId\": \"YXNkbGZqYWRmaiBhc2RmdW9hZHNmZGpmaGFzbGtkaGZza2RmaAo\",\n    \"job\": {\n        \"id\": \"f3cc4f60-61f6-4a2b-8a21-d07600c373ce\"\n    },\n    \"tasks\": [\n        {\n            \"taskId\": \"dGFza2lkZ29lc2hlcmUK\",\n            \"s3Key\": \"customerImage1.jpg\",\n            \"s3VersionId\": \"1\",\n            \"s3BucketArn\": \"arn:aws:s3:::amzn-s3-demo-bucket\"\n        }\n    ]  \n}"
                    },
                    "Your Lambda function must return a JSON object with the fields as shown in the following example. You can copy the    invocationId and taskId from the event parameter. You can return a string in the resultString.    Amazon S3 saves the resultString values in the completion report. ",
                    {
                        "code_example": "\n{\n  \"invocationSchemaVersion\": \"1.0\",\n  \"treatMissingKeysAs\" : \"PermanentFailure\",\n  \"invocationId\" : \"YXNkbGZqYWRmaiBhc2RmdW9hZHNmZGpmaGFzbGtkaGZza2RmaAo\",\n  \"results\": [\n    {\n      \"taskId\": \"dGFza2lkZ29lc2hlcmUK\",\n      \"resultCode\": \"Succeeded\",\n      \"resultString\": \"[\\\"Alice\\\", \\\"Bob\\\"]\"\n    }\n  ]\n}\n  "
                    },
                    {
                        "sub_header": "Invoking Lambda functions from Amazon S3 batch operations ",
                        "content": [
                            "You can invoke the Lambda function with an unqualified or qualified function ARN. If you want to use the same      function version for the entire batch job, configure a specific function version in the FunctionARN      parameter when you create your job. If you configure an alias or the $LATEST qualifier, the batch job immediately      starts calling the new version of the function if the alias or $LATEST is updated during the job execution. ",
                            "Note that you can't reuse an existing Amazon S3 event-based function for batch operations. This is because the Amazon S3      batch operation passes a different event parameter to the Lambda function and expects a return message with a      specific JSON structure.",
                            "In the resource-based policy that you create for the Amazon S3      batch job, ensure that you set permission for the job to invoke your Lambda function.",
                            "In the execution role for the function, set a trust policy for Amazon S3 to assume the role when it runs your      function.",
                            "If your function uses the AWS SDK to manage Amazon S3 resources, you need to add Amazon S3 permissions in the  execution role. ",
                            "When the job runs, Amazon S3 starts multiple function instances to process the Amazon S3 objects in parallel, up to      the concurrency limit of the function. Amazon S3 limits the initial ramp-up of instances      to avoid excess cost for smaller jobs. ",
                            "If the Lambda function returns a TemporaryFailure response code, Amazon S3 retries the operation. ",
                            "For more information about Amazon S3 batch operations, see Performing batch operations in the        Amazon S3 Developer Guide. ",
                            "For an example of how to use a Lambda function in Amazon S3 batch operations, see Invoking a Lambda function from Amazon S3        batch operations in the Amazon S3 Developer Guide. "
                        ]
                    }
                ]
            },
            {
                "title": "SNS",
                "href": "https://docs.aws.amazon.com/lambda/latest/dg/with-sns.html",
                "contents": [
                    {
                        "title": "Tutorial",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/with-sns-example.html",
                        "sections": [
                            "",
                            "In this tutorial, you use a Lambda function in one AWS account to subscribe to an Amazon Simple Notification Service (Amazon SNS) topic in a separate AWS account. When     you publish messages to your Amazon SNS topic, your Lambda function reads the contents of the message and outputs it to Amazon CloudWatch Logs. To complete this     tutorial, you use the AWS Command Line Interface (AWS CLI).",
                            "\n\n",
                            "To complete this tutorial, you perform the following steps:",
                            "  1.account A In , create an Amazon SNS topic.",
                            "  2.account B In , create a Lambda function that will read messages from the topic.",
                            "  3.account B In , create a subscription to the topic.",
                            "  4.account A Publish messages to the Amazon SNS topic in  and confirm that the Lambda function in \n        account B outputs them to CloudWatch Logs.",
                            "By completing these steps, you will learn how to configure an Amazon SNS topic to invoke a Lambda function. You will also learn how to create an    AWS Identity and Access Management (IAM) policy that gives permission for a resource in another AWS account to invoke Lambda.",
                            "In the tutorial, you use two separate AWS accounts. The AWS CLI commands illustrate this by using two named profiles called accountA     and accountB, each configured for use with a different AWS account. To learn how to configure the AWS CLI to use different profiles,     see Configuration and credential file settings in the     AWS Command Line Interface User Guide for Version 2. Be sure to configure the same default AWS Region for both profiles.",
                            "If the AWS CLI profiles you create for the two AWS accounts use different names, or if you use the default profile and one named profile,     modify the AWS CLI commands in the following steps as needed.",
                            {
                                "sub_header": "Prerequisites",
                                "content": [
                                    {
                                        "code_example": "zip"
                                    }
                                ]
                            },
                            {
                                "sub_header": "Create an Amazon SNS topic (account A)",
                                "content": [
                                    "\n\n",
                                    {
                                        "code_example": "aws sns create-topic --name sns-topic-for-lambda --profile accountA"
                                    }
                                ]
                            },
                            {
                                "sub_header": "Create a function execution role (account B)",
                                "content": [
                                    "\n\n",
                                    "An execution role is an IAM role that grants a Lambda function permission to access AWS services and resources. Before you create your       function in account B, you create a role that gives the function basic permissions to write logs to       CloudWatch Logs. We’ll add the permissions to read from your Amazon SNS topic in a later step.",
                                    {
                                        "code_example": "AWSLambdaBasicExecutionRole"
                                    }
                                ]
                            },
                            {
                                "sub_header": "Create a Lambda function (account B)",
                                "content": [
                                    "\n\n",
                                    "Create a Lambda function that processes your Amazon SNS messages. The function code logs the message      contents of each record to Amazon CloudWatch Logs.",
                                    "This tutorial uses the Node.js 18.x runtime, but we've also provided example code in other      runtime languages. You can select the tab in the following box to see code for the runtime      you're interested in. The JavaScript code you'll use in this step is in the first example      shown in the JavaScript tab.",
                                    ".NETAWS SDK for .NETNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Consuming an SNS event with Lambda using .NET.// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.// SPDX-License-Identifier: Apache-2.0using Amazon.Lambda.Core;using Amazon.Lambda.SNSEvents;// Assembly attribute to enable the Lambda function's JSON input to be converted into a .NET class.[assembly: LambdaSerializer(typeof(Amazon.Lambda.Serialization.SystemTextJson.DefaultLambdaJsonSerializer))]namespace SnsIntegration;public class Function{    public async Task FunctionHandler(SNSEvent evnt, ILambdaContext context)    {        foreach (var record in evnt.Records)        {            await ProcessRecordAsync(record, context);        }        context.Logger.LogInformation(\"done\");    }    private async Task ProcessRecordAsync(SNSEvent.SNSRecord record, ILambdaContext context)    {        try        {            context.Logger.LogInformation($\"Processed record {record.Sns.Message}\");            // TODO: Do interesting work based on the new message            await Task.CompletedTask;        }        catch (Exception e)        {            //You can use Dead Letter Queue to handle failures. By configuring a Lambda DLQ.            context.Logger.LogError($\"An error occurred\");            throw;        }    }}GoSDK for Go V2Note        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Consuming an SNS event with Lambda using Go.// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.// SPDX-License-Identifier: Apache-2.0package mainimport (\t\"context\"\t\"fmt\"\t\"github.com/aws/aws-lambda-go/events\"\t\"github.com/aws/aws-lambda-go/lambda\")func handler(ctx context.Context, snsEvent events.SNSEvent) {\tfor _, record := range snsEvent.Records {\t\tprocessMessage(record)\t}\tfmt.Println(\"done\")}func processMessage(record events.SNSEventRecord) {\tmessage := record.SNS.Message\tfmt.Printf(\"Processed message: %s\\n\", message)\t// TODO: Process your record here}func main() {\tlambda.Start(handler)}JavaSDK for Java 2.xNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Consuming an SNS event with Lambda using Java.// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.// SPDX-License-Identifier: Apache-2.0package example;import com.amazonaws.services.lambda.runtime.Context;import com.amazonaws.services.lambda.runtime.LambdaLogger;import com.amazonaws.services.lambda.runtime.RequestHandler;import com.amazonaws.services.lambda.runtime.events.SNSEvent;import com.amazonaws.services.lambda.runtime.events.SNSEvent.SNSRecord;import java.util.Iterator;import java.util.List;public class SNSEventHandler implements RequestHandler<SNSEvent, Boolean> {    LambdaLogger logger;    @Override    public Boolean handleRequest(SNSEvent event, Context context) {        logger = context.getLogger();        List<SNSRecord> records = event.getRecords();        if (!records.isEmpty()) {            Iterator<SNSRecord> recordsIter = records.iterator();            while (recordsIter.hasNext()) {                processRecord(recordsIter.next());            }        }        return Boolean.TRUE;    }    public void processRecord(SNSRecord record) {        try {            String message = record.getSNS().getMessage();            logger.log(\"message: \" + message);        } catch (Exception e) {            throw new RuntimeException(e);        }    }}JavaScriptSDK for JavaScript (v3)Note        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Consuming an SNS event with Lambda using JavaScript.// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.// SPDX-License-Identifier: Apache-2.0exports.handler = async (event, context) => {  for (const record of event.Records) {    await processMessageAsync(record);  }  console.info(\"done\");};async function processMessageAsync(record) {  try {    const message = JSON.stringify(record.Sns.Message);    console.log(`Processed message ${message}`);    await Promise.resolve(1); //Placeholder for actual async work  } catch (err) {    console.error(\"An error occurred\");    throw err;  }}Consuming an SNS event with Lambda using TypeScript.// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.// SPDX-License-Identifier: Apache-2.0import { SNSEvent, Context, SNSHandler, SNSEventRecord } from \"aws-lambda\";export const functionHandler: SNSHandler = async (  event: SNSEvent,  context: Context): Promise<void> => {  for (const record of event.Records) {    await processMessageAsync(record);  }  console.info(\"done\");};async function processMessageAsync(record: SNSEventRecord): Promise<any> {  try {    const message: string = JSON.stringify(record.Sns.Message);    console.log(`Processed message ${message}`);    await Promise.resolve(1); //Placeholder for actual async work  } catch (err) {    console.error(\"An error occurred\");    throw err;  }}PHPSDK for PHPNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Consuming an SNS event with Lambda using PHP.// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.// SPDX-License-Identifier: Apache-2.0<?php/* Since native PHP support for AWS Lambda is not available, we are utilizing Bref's PHP functions runtime for AWS Lambda.For more information on Bref's PHP runtime for Lambda, refer to: https://bref.sh/docs/runtimes/functionAnother approach would be to create a custom runtime. A practical example can be found here: https://aws.amazon.com/blogs/apn/aws-lambda-custom-runtime-for-php-a-practical-example/*/// Additional composer packages may be required when using Bref or any other PHP functions runtime.// require __DIR__ . '/vendor/autoload.php';use Bref\\Context\\Context;use Bref\\Event\\Sns\\SnsEvent;use Bref\\Event\\Sns\\SnsHandler;class Handler extends SnsHandler{    public function handleSns(SnsEvent $event, Context $context): void    {        foreach ($event->getRecords() as $record) {            $message = $record->getMessage();            // TODO: Implement your custom processing logic here            // Any exception thrown will be logged and the invocation will be marked as failed            echo \"Processed Message: $message\" . PHP_EOL;        }    }}return new Handler();PythonSDK for Python (Boto3)Note        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Consuming an SNS event with Lambda using Python.# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.# SPDX-License-Identifier: Apache-2.0def lambda_handler(event, context):    for record in event['Records']:        process_message(record)    print(\"done\")def process_message(record):    try:        message = record['Sns']['Message']        print(f\"Processed message {message}\")        # TODO; Process your record here            except Exception as e:        print(\"An error occurred\")        raise eRubySDK for RubyNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Consuming an SNS event with Lambda using Ruby.# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.# SPDX-License-Identifier: Apache-2.0def lambda_handler(event:, context:)  event['Records'].map { |record| process_message(record) }enddef process_message(record)  message = record['Sns']['Message']  puts(\"Processing message: #{message}\")rescue StandardError => e  puts(\"Error processing message: #{e}\")  raiseendRustSDK for RustNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Consuming an SNS event with Lambda using Rust.// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.// SPDX-License-Identifier: Apache-2.0use aws_lambda_events::event::sns::SnsEvent;use aws_lambda_events::sns::SnsRecord;use lambda_runtime::{run, service_fn, Error, LambdaEvent};use tracing::info;// Built with the following dependencies://  aws_lambda_events = { version = \"0.10.0\", default-features = false, features = [\"sns\"] }//  lambda_runtime = \"0.8.1\"//  tokio = { version = \"1\", features = [\"macros\"] }//  tracing = { version = \"0.1\", features = [\"log\"] }//  tracing-subscriber = { version = \"0.3\", default-features = false, features = [\"fmt\"] }async fn function_handler(event: LambdaEvent<SnsEvent>) -> Result<(), Error> {    for event in event.payload.records {        process_record(&event)?;    }        Ok(())}fn process_record(record: &SnsRecord) -> Result<(), Error> {    info!(\"Processing SNS Message: {}\", record.sns.message);    // Implement your record handling code here.    Ok(())}#[tokio::main]async fn main() -> Result<(), Error> {    tracing_subscriber::fmt()        .with_max_level(tracing::Level::INFO)        .with_target(false)        .without_time()        .init();    run(service_fn(function_handler)).await}anchoranchoranchoranchoranchoranchoranchoranchor.NETGoJavaJavaScriptPHPPythonRubyRustAWS SDK for .NETNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Consuming an SNS event with Lambda using .NET.// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.// SPDX-License-Identifier: Apache-2.0using Amazon.Lambda.Core;using Amazon.Lambda.SNSEvents;// Assembly attribute to enable the Lambda function's JSON input to be converted into a .NET class.[assembly: LambdaSerializer(typeof(Amazon.Lambda.Serialization.SystemTextJson.DefaultLambdaJsonSerializer))]namespace SnsIntegration;public class Function{    public async Task FunctionHandler(SNSEvent evnt, ILambdaContext context)    {        foreach (var record in evnt.Records)        {            await ProcessRecordAsync(record, context);        }        context.Logger.LogInformation(\"done\");    }    private async Task ProcessRecordAsync(SNSEvent.SNSRecord record, ILambdaContext context)    {        try        {            context.Logger.LogInformation($\"Processed record {record.Sns.Message}\");            // TODO: Do interesting work based on the new message            await Task.CompletedTask;        }        catch (Exception e)        {            //You can use Dead Letter Queue to handle failures. By configuring a Lambda DLQ.            context.Logger.LogError($\"An error occurred\");            throw;        }    }}",
                                    {
                                        "code_example": "mkdir sns-tutorial\ncd sns-tutorial"
                                    }
                                ]
                            },
                            {
                                "sub_header": "Add permissions to function (account B)",
                                "content": [
                                    "\n\n",
                                    "For Amazon SNS to invoke your function, you need to grant it permission in a statement on a resource-based policy.       You add this statement using the AWS CLI add-permission command.",
                                    {
                                        "code_example": "aws lambda add-permission --function-name Function-With-SNS \\\n    --source-arn arn:aws:sns:us-east-1:<AccountA_ID>:sns-topic-for-lambda \\\n    --statement-id function-with-sns --action \"lambda:InvokeFunction\" \\\n    --principal sns.amazonaws.com --profile accountB"
                                    },
                                    {
                                        "code_example": "sns.ap-east-1.amazonaws.com"
                                    }
                                ]
                            },
                            {
                                "sub_header": "Grant cross-account permission for Amazon SNS subscription (account A)",
                                "content": [
                                    "\n\n",
                                    "For your Lambda function in account B to subscribe to the Amazon SNS topic you created in account A,       you need to grant permission for account B to subscribe to your topic. You grant this permission using the       AWS CLI add-permission command. ",
                                    {
                                        "code_example": "aws sns add-permission --label lambda-access --aws-account-id <AccountB_ID> \\\n    --topic-arn arn:aws:sns:us-east-1:<AccountA_ID>:sns-topic-for-lambda \\  \n    --action-name Subscribe ListSubscriptionsByTopic --profile accountA"
                                    }
                                ]
                            },
                            {
                                "sub_header": "Create a subscription (account B)",
                                "content": [
                                    "\n\n",
                                    "In account B, you now subscribe your Lambda function to the Amazon SNS topic you created at the beginning of the     tutorial in account A. When a message is sent to this topic (sns-topic-for-lambda), Amazon SNS invokes     your Lambda function Function-With-SNS in account B. ",
                                    {
                                        "code_example": "aws sns subscribe --protocol lambda \\\n    --region us-east-1 \\\n    --topic-arn arn:aws:sns:us-east-1:<AccountA_ID>:sns-topic-for-lambda \\\n    --notification-endpoint arn:aws:lambda:us-east-1:<AccountB_ID>:function:Function-With-SNS \\\n    --profile accountB"
                                    }
                                ]
                            },
                            {
                                "sub_header": "Publish messages to topic (account A and account B)",
                                "content": [
                                    "\n\n",
                                    "Now that your Lambda function in account B is subscribed to your Amazon SNS topic in account A,       it’s time to test your setup by publishing messages to your topic. To confirm that Amazon SNS has invoked your Lambda function, you use CloudWatch Logs to view       your function’s output.",
                                    {
                                        "code_example": "Hello World"
                                    }
                                ]
                            },
                            {
                                "sub_header": "Clean up your resources",
                                "content": [
                                    "You can now delete the resources that you created for this tutorial, unless you want to retain them. By deleting AWS resources that you're no longer using, you prevent unnecessary charges to your AWS account.",
                                    "In Account A, clean up your Amazon SNS topic.",
                                    {
                                        "code_example": "delete me"
                                    },
                                    "In Account B, clean up your execution role, Lambda function, and Amazon SNS subscription.",
                                    "To delete the execution role\nOpen the Roles page of the IAM console.\n\nSelect the execution role that you created.\n\nChoose Delete.\n\nEnter the name of the role in the text input field and choose Delete.\n",
                                    {
                                        "code_example": "delete"
                                    },
                                    "To delete the Amazon SNS subscription\nOpen the Subscriptions page of the Amazon SNS console.\n\nSelect the subscription you created.\n\nChoose Delete, Delete.\n"
                                ]
                            }
                        ]
                    }
                ],
                "sections": [
                    "",
                    "You can use a Lambda function to process Amazon Simple Notification Service (Amazon SNS) notifications. Amazon SNS supports Lambda functions as a    target for messages sent to a topic. You can subscribe your function to topics in the same account or in other AWS    accounts. For a detailed walkthrough, see Tutorial: Using AWS Lambda with Amazon Simple Notification Service.",
                    "Lambda supports SNS triggers for standard SNS topics only. FIFO topics aren't supported.",
                    "For asynchronous invocation, Lambda queues the message and handles retries. If Amazon SNS can't reach Lambda or the    message is rejected, Amazon SNS retries at increasing intervals over several hours. For details, see Reliability in the Amazon SNS FAQs.",
                    "WarningLambda event source mappings process each event at least once, and duplicate processing of records can occur. To avoid potential issues \nrelated to duplicate events, we strongly recommend that you make your function code idempotent. To learn more, see How do I make my Lambda function idempotent \nin the AWS Knowledge Center.",
                    "  1.Adding an Amazon SNS topic trigger for a Lambda function using the console",
                    "  2.Manually adding an Amazon SNS topic trigger for a Lambda function",
                    "  3.Sample SNS event shape",
                    "  4.Tutorial: Using AWS Lambda with Amazon Simple Notification Service",
                    {
                        "sub_header": "Adding an Amazon SNS topic trigger for a Lambda function using the console",
                        "content": [
                            "To add an SNS topic as a trigger for a Lambda function, the easiest way is to use      the Lambda console. When you add the trigger via the console, Lambda automatically      sets up the necessary permissions and subscriptions to start receiving events from      the SNS topic.",
                            "To add an SNS topic as a trigger for a Lambda function (console)Open the Functions page of the Lambda console.\nChoose the name of a function you want to add the trigger for.\n\nChoose Configuration, and then choose Triggers.\n\nChoose Add trigger.\n\nUnder Trigger configuration, in the dropdown menu, choose\n          SNS.\n\nFor SNS topic, choose the SNS topic to subscribe to.\n"
                        ]
                    },
                    {
                        "sub_header": "Manually adding an Amazon SNS topic trigger for a Lambda function",
                        "content": [
                            "To set up an SNS trigger for a Lambda function manually, you need to complete the following      steps:",
                            "You can use the AWS Command Line Interface (AWS CLI) to complete both of these steps. First, to define      a resource-based policy for a Lambda function that allows SNS invocations, use the following      AWS CLI command. Be sure to replace the value of --function-name with your      Lambda function name, and the value of --source-arn with your SNS topic ARN.",
                            "aws lambda add-permission --function-name example-function \\    --source-arn arn:aws:sns:us-east-1:123456789012:sns-topic-for-lambda \\    --statement-id function-with-sns --action \"lambda:InvokeFunction\" \\    --principal sns.amazonaws.com",
                            "To subscribe your function to the SNS topic, use the following AWS CLI command. Replace      the value of --topic-arn with your SNS topic ARN, and the value of      --notification-endpoint with your Lambda function ARN.",
                            "aws sns subscribe --protocol lambda \\    --region us-east-1 \\    --topic-arn arn:aws:sns:us-east-1:123456789012:sns-topic-for-lambda \\    --notification-endpoint arn:aws:lambda:us-east-1:123456789012:function:example-function"
                        ]
                    },
                    {
                        "sub_header": "Sample SNS event shape",
                        "content": [
                            "Amazon SNS invokes your function asynchronously with an event that contains a      message and metadata.",
                            {
                                "code_example": "{\n  \"Records\": [\n    {\n      \"EventVersion\": \"1.0\",\n      \"EventSubscriptionArn\": \"arn:aws:sns:us-east-1:123456789012:sns-lambda:21be56ed-a058-49f5-8c98-aedd2564c486\",\n      \"EventSource\": \"aws:sns\",\n      \"Sns\": {\n        \"SignatureVersion\": \"1\",\n        \"Timestamp\": \"2019-01-02T12:45:07.000Z\",\n        \"Signature\": \"tcc6faL2yUC6dgZdmrwh1Y4cGa/ebXEkAi6RibDsvpi+tE/1+82j...65r==\",\n        \"SigningCertURL\": \"https://sns.us-east-1.amazonaws.com/SimpleNotificationService-ac565b8b1a6c5d002d285f9598aa1d9b.pem\",\n        \"MessageId\": \"95df01b4-ee98-5cb9-9903-4c221d41eb5e\",\n        \"Message\": \"Hello from SNS!\",\n        \"MessageAttributes\": {\n          \"Test\": {\n            \"Type\": \"String\",\n            \"Value\": \"TestString\"\n          },\n          \"TestBinary\": {\n            \"Type\": \"Binary\",\n            \"Value\": \"TestBinary\"\n          }\n        },\n        \"Type\": \"Notification\",\n        \"UnsubscribeUrl\": \"https://sns.us-east-1.amazonaws.com/?Action=Unsubscribe&amp;SubscriptionArn=arn:aws:sns:us-east-1:123456789012:test-lambda:21be56ed-a058-49f5-8c98-aedd2564c486\",\n        \"TopicArn\":\"arn:aws:sns:us-east-1:123456789012:sns-lambda\",\n        \"Subject\": \"TestInvoke\"\n      }\n    }\n  ]\n}"
                            }
                        ]
                    }
                ]
            }
        ],
        "sections": [
            "",
            "Some AWS services can directly invoke Lambda functions using triggers. These services push events to Lambda, and the function is invoked immediately when the specified event occurs. Triggers are suitable for discrete events and real-time processing. When you create a trigger using the Lambda console, the console interacts with the corresponding AWS service to configure the event notification on that service. The trigger is actually stored and managed by the service that generates the events, not by Lambda.",
            "The events are data structured in JSON format. The JSON structure varies depending on the service that    generates it and the event type, but they all contain the data that the function needs to process the    event.",
            "A function can have multiple triggers. Each trigger acts as a client invoking your function independently, and each event that    Lambda passes to your function has data from only one trigger. Lambda converts the event document into an object and passes it to your function handler.",
            "Depending on the service, the event-driven invocation can be synchronous or asynchronous.",
            {
                "sub_header": "Creating a trigger",
                "content": [
                    "The easiest way to create a trigger is to use the Lambda console. When you create a trigger using the console, Lambda automatically adds the required permissions to the function's resource-based policy.",
                    "To create a trigger using the Lambda console\nOpen the Functions page of the Lambda console.\n\nSelect the function you want to create a trigger for.\n\nIn the Function overview pane, choose\n          Add trigger.\n\nSelect the AWS service you want to invoke your function.\n\nFill out the options in the Trigger configuration pane\n          and choose Add. Depending on the AWS service you choose to\n          invoke your function, the trigger configuration options will be different.\n"
                ]
            },
            {
                "sub_header": "Services that can invoke Lambda functions",
                "content": [
                    "The following table lists services that can invoke Lambda functions.",
                    "\n\nService\nMethod of invocation\n\n\n\n\nAmazon Managed Streaming for Apache Kafka\n\n\nEvent source mapping\n\n\n\n\nSelf-managed Apache Kafka\n\n\nEvent source mapping\n\n\n\n\nAmazon API Gateway\n\n\nEvent-driven; synchronous invocation\n\n\n\n\nAWS CloudFormation\n\n\nEvent-driven; asynchronous invocation\n\n\n\n\nAmazon CloudWatch Logs\n\n\nEvent-driven; asynchronous invocation\n\n\n\n\nAWS CodeCommit\n\n\nEvent-driven; asynchronous invocation\n\n\n\n\nAWS CodePipeline\n\n\nEvent-driven; asynchronous invocation\n\n\n\n\nAmazon Cognito\n\n\nEvent-driven; synchronous invocation\n\n\n\n\nAWS Config\n\n\nEvent-driven; asynchronous invocation\n\n\n\n\nAmazon Connect\n\n\nEvent-driven; synchronous invocation\n\n\n\n\nAmazon DynamoDB\n\n\nEvent source mapping\n\n\n\n\nAmazon Elastic File System\n\n\nSpecial integration\n\n\n\n\nElastic Load Balancing (Application Load Balancer)\n\n\nEvent-driven; synchronous invocation\n\n\n\n\nAmazon EventBridge (CloudWatch Events)\n\n\nEvent-driven; asynchronous invocation (event buses), synchronous or asynchronous invocation (pipes and schedules)\n\n\n\n\nAWS IoT\n\n\nEvent-driven; asynchronous invocation\n\n\n\n\nAmazon Kinesis\n\n\nEvent source mapping\n\n\n\n\nAmazon Data Firehose\n\n\nEvent-driven; synchronous invocation\n\n\n\n\nAmazon Lex\n\n\nEvent-driven; synchronous invocation\n\n\n\n\nAmazon MQ\n\n\nEvent source mapping\n\n\n\n\nAmazon Simple Email Service\n\n\nEvent-driven; asynchronous invocation\n\n\n\n\nAmazon Simple Notification Service\n\n\nEvent-driven; asynchronous invocation\n\n\n\n\nAmazon Simple Queue Service\n\n\nEvent source mapping\n\n\n\n\nAmazon Simple Storage Service (Amazon S3)\n\n\nEvent-driven; asynchronous invocation\n\n\n\n\nAmazon Simple Storage Service Batch\n\n\nEvent-driven; synchronous invocation\n\n\n\n\nSecrets Manager\n\n\nSpecial integration\n\n\n\n\nAWS Step Functions\n\n\nEvent-driven; synchronous or asynchronous invocation\n\n\n\n\nAmazon VPC Lattice\n\n\nEvent-driven; synchronous invocation\n\n\n\n\nAWS X-Ray\n\n\nSpecial integration\n\n\n"
                ]
            }
        ]
    },
    {
        "title": "Code examples",
        "href": "https://docs.aws.amazon.com/lambda/latest/dg/service_code_examples.html",
        "contents": [
            {
                "title": "Basics",
                "href": "https://docs.aws.amazon.com/lambda/latest/dg/service_code_examples_basics.html",
                "contents": [
                    {
                        "title": "Hello Lambda",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_lambda_Hello_section.html",
                        "sections": [
                            "",
                            "The following code examples show how to get started using Lambda.",
                            ".NETAWS SDK for .NETNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        AWS Code            Examples Repository.    namespace LambdaActions;using Amazon.Lambda;public class HelloLambda{    static async Task Main(string[] args)    {        var lambdaClient = new AmazonLambdaClient();        Console.WriteLine(\"Hello AWS Lambda\");        Console.WriteLine(\"Let's get started with AWS Lambda by listing your existing Lambda functions:\");        var response = await lambdaClient.ListFunctionsAsync();        response.Functions.ForEach(function =>        {            Console.WriteLine($\"{function.FunctionName}\\t{function.Description}\");        });    }}                        For API details, see                        ListFunctions                        in AWS SDK for .NET API Reference.                    C++SDK for C++Note        There's more on GitHub. Find the complete example and learn how to set up and run in the        AWS Code            Examples Repository.    Code for the CMakeLists.txt CMake file.# Set the minimum required version of CMake for this project.cmake_minimum_required(VERSION 3.13)# Set the AWS service components used by this project.set(SERVICE_COMPONENTS lambda)# Set this project's name.project(\"hello_lambda\")# Set the C++ standard to use to build this target.# At least C++ 11 is required for the AWS SDK for C++.set(CMAKE_CXX_STANDARD 11)# Use the MSVC variable to determine if this is a Windows build.set(WINDOWS_BUILD ${MSVC})if (WINDOWS_BUILD) # Set the location where CMake can find the installed libraries for the AWS SDK.    string(REPLACE \";\" \"/aws-cpp-sdk-all;\" SYSTEM_MODULE_PATH \"${CMAKE_SYSTEM_PREFIX_PATH}/aws-cpp-sdk-all\")    list(APPEND CMAKE_PREFIX_PATH ${SYSTEM_MODULE_PATH})endif ()# Find the AWS SDK for C++ package.find_package(AWSSDK REQUIRED COMPONENTS ${SERVICE_COMPONENTS})if (WINDOWS_BUILD AND AWSSDK_INSTALL_AS_SHARED_LIBS)     # Copy relevant AWS SDK for C++ libraries into the current binary directory for running and debugging.     # set(BIN_SUB_DIR \"/Debug\") # if you are building from the command line you may need to uncomment this                                     # and set the proper subdirectory to the executables' location.     AWSSDK_CPY_DYN_LIBS(SERVICE_COMPONENTS \"\" ${CMAKE_CURRENT_BINARY_DIR}${BIN_SUB_DIR})endif ()add_executable(${PROJECT_NAME}        hello_lambda.cpp)target_link_libraries(${PROJECT_NAME}        ${AWSSDK_LINK_LIBRARIES})Code for the hello_lambda.cpp source file.#include <aws/core/Aws.h>#include <aws/lambda/LambdaClient.h>#include <aws/lambda/model/ListFunctionsRequest.h>#include <iostream>/* *  A \"Hello Lambda\" starter application which initializes an AWS Lambda (Lambda) client and lists the Lambda functions. * *  main function * *  Usage: 'hello_lambda' * */int main(int argc, char **argv) {    Aws::SDKOptions options;    // Optionally change the log level for debugging.//   options.loggingOptions.logLevel = Utils::Logging::LogLevel::Debug;    Aws::InitAPI(options); // Should only be called once.    int result = 0;    {        Aws::Client::ClientConfiguration clientConfig;        // Optional: Set to the AWS Region (overrides config file).        // clientConfig.region = \"us-east-1\";        Aws::Lambda::LambdaClient lambdaClient(clientConfig);        std::vector<Aws::String> functions;        Aws::String marker; // Used for pagination.        do {            Aws::Lambda::Model::ListFunctionsRequest request;            if (!marker.empty()) {                request.SetMarker(marker);            }            Aws::Lambda::Model::ListFunctionsOutcome outcome = lambdaClient.ListFunctions(                    request);            if (outcome.IsSuccess()) {                const Aws::Lambda::Model::ListFunctionsResult &listFunctionsResult = outcome.GetResult();                std::cout << listFunctionsResult.GetFunctions().size()                          << \" lambda functions were retrieved.\" << std::endl;                for (const Aws::Lambda::Model::FunctionConfiguration &functionConfiguration: listFunctionsResult.GetFunctions()) {                    functions.push_back(functionConfiguration.GetFunctionName());                    std::cout << functions.size() << \"  \"                              << functionConfiguration.GetDescription() << std::endl;                    std::cout << \"   \"                              << Aws::Lambda::Model::RuntimeMapper::GetNameForRuntime(                                      functionConfiguration.GetRuntime()) << \": \"                              << functionConfiguration.GetHandler()                              << std::endl;                }                marker = listFunctionsResult.GetNextMarker();            } else {                std::cerr << \"Error with Lambda::ListFunctions. \"                          << outcome.GetError().GetMessage()                          << std::endl;                result = 1;                break;            }        } while (!marker.empty());    }    Aws::ShutdownAPI(options); // Should only be called once.    return result;}                        For API details, see                        ListFunctions                        in AWS SDK for C++ API Reference.                    GoSDK for Go V2Note        There's more on GitHub. Find the complete example and learn how to set up and run in the        AWS Code            Examples Repository.    package mainimport (\t\"context\"\t\"fmt\"\t\"github.com/aws/aws-sdk-go-v2/aws\"\t\"github.com/aws/aws-sdk-go-v2/config\"\t\"github.com/aws/aws-sdk-go-v2/service/lambda\")// main uses the AWS SDK for Go (v2) to create an AWS Lambda client and list up to 10// functions in your account.// This example uses the default settings specified in your shared credentials// and config files.func main() {\tctx := context.Background()\tsdkConfig, err := config.LoadDefaultConfig(ctx)\tif err != nil {\t\tfmt.Println(\"Couldn't load default configuration. Have you set up your AWS account?\")\t\tfmt.Println(err)\t\treturn\t}\tlambdaClient := lambda.NewFromConfig(sdkConfig)\tmaxItems := 10\tfmt.Printf(\"Let's list up to %v functions for your account.\\n\", maxItems)\tresult, err := lambdaClient.ListFunctions(ctx, &lambda.ListFunctionsInput{\t\tMaxItems: aws.Int32(int32(maxItems)),\t})\tif err != nil {\t\tfmt.Printf(\"Couldn't list functions for your account. Here's why: %v\\n\", err)\t\treturn\t}\tif len(result.Functions) == 0 {\t\tfmt.Println(\"You don't have any functions!\")\t} else {\t\tfor _, function := range result.Functions {\t\t\tfmt.Printf(\"\\t%v\\n\", *function.FunctionName)\t\t}\t}}                        For API details, see                        ListFunctions                        in AWS SDK for Go API Reference.                    JavaSDK for Java 2.xNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        AWS Code            Examples Repository.        /**     * Lists the AWS Lambda functions associated with the current AWS account.     *     * @param awsLambda an instance of the {@link LambdaClient} class, which is used to interact with the AWS Lambda service     *     * @throws LambdaException if an error occurs while interacting with the AWS Lambda service     */    public static void listFunctions(LambdaClient awsLambda) {        try {            ListFunctionsResponse functionResult = awsLambda.listFunctions();            List<FunctionConfiguration> list = functionResult.functions();            for (FunctionConfiguration config : list) {                System.out.println(\"The function name is \" + config.functionName());            }        } catch (LambdaException e) {            System.err.println(e.getMessage());            System.exit(1);        }    }                        For API details, see                        ListFunctions                        in AWS SDK for Java 2.x API Reference.                    JavaScriptSDK for JavaScript (v3)Note        There's more on GitHub. Find the complete example and learn how to set up and run in the        AWS Code            Examples Repository.    import { LambdaClient, paginateListFunctions } from \"@aws-sdk/client-lambda\";const client = new LambdaClient({});export const helloLambda = async () => {  const paginator = paginateListFunctions({ client }, {});  const functions = [];  for await (const page of paginator) {    const funcNames = page.Functions.map((f) => f.FunctionName);    functions.push(...funcNames);  }  console.log(\"Functions:\");  console.log(functions.join(\"\\n\"));  return functions;};                        For API details, see                        ListFunctions                        in AWS SDK for JavaScript API Reference.                    PythonSDK for Python (Boto3)Note        There's more on GitHub. Find the complete example and learn how to set up and run in the        AWS Code            Examples Repository.    import boto3def main():    \"\"\"    List the Lambda functions in your AWS account.    \"\"\"    # Create the Lambda client    lambda_client = boto3.client(\"lambda\")    # Use the paginator to list the functions    paginator = lambda_client.get_paginator(\"list_functions\")    response_iterator = paginator.paginate()    print(\"Here are the Lambda functions in your account:\")    for page in response_iterator:        for function in page[\"Functions\"]:            print(f\"  {function['FunctionName']}\")if __name__ == \"__main__\":    main()                        For API details, see                        ListFunctions                        in AWS SDK for Python (Boto3) API Reference.                    RubySDK for RubyNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        AWS Code            Examples Repository.    require 'aws-sdk-lambda'# Creates an AWS Lambda client using the default credentials and configurationdef lambda_client  Aws::Lambda::Client.newend# Lists the Lambda functions in your AWS account, paginating the results if necessarydef list_lambda_functions  lambda = lambda_client  # Use a pagination iterator to list all functions  functions = []  lambda.list_functions.each_page do |page|    functions.concat(page.functions)  end  # Print the name and ARN of each function  functions.each do |function|    puts \"Function name: #{function.function_name}\"    puts \"Function ARN: #{function.function_arn}\"    puts  end  puts \"Total functions: #{functions.count}\"endlist_lambda_functions if __FILE__ == $PROGRAM_NAME                        For API details, see                        ListFunctions                        in AWS SDK for Ruby API Reference.                    anchoranchoranchoranchoranchoranchoranchor.NETC++GoJavaJavaScriptPythonRubyAWS SDK for .NETNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        AWS Code            Examples Repository.    namespace LambdaActions;using Amazon.Lambda;public class HelloLambda{    static async Task Main(string[] args)    {        var lambdaClient = new AmazonLambdaClient();        Console.WriteLine(\"Hello AWS Lambda\");        Console.WriteLine(\"Let's get started with AWS Lambda by listing your existing Lambda functions:\");        var response = await lambdaClient.ListFunctionsAsync();        response.Functions.ForEach(function =>        {            Console.WriteLine($\"{function.FunctionName}\\t{function.Description}\");        });    }}                        For API details, see                        ListFunctions                        in AWS SDK for .NET API Reference.                    ",
                            "For a complete list of AWS SDK developer guides and code examples, see    Using Lambda with an AWS SDK.    This topic also includes information about getting started and details about previous SDK versions."
                        ]
                    },
                    {
                        "title": "Learn the basics",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_lambda_Scenario_GettingStartedFunctions_section.html",
                        "sections": [
                            "",
                            "The following code examples show how to:",
                            "For more information, see Create a Lambda function with the console.",
                            ".NETAWS SDK for .NETNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        AWS Code            Examples Repository.    Create methods that perform Lambda actions.namespace LambdaActions;using Amazon.Lambda;using Amazon.Lambda.Model;/// <summary>/// A class that implements AWS Lambda methods./// </summary>public class LambdaWrapper{    private readonly IAmazonLambda _lambdaService;    /// <summary>    /// Constructor for the LambdaWrapper class.    /// </summary>    /// <param name=\"lambdaService\">An initialized Lambda service client.</param>    public LambdaWrapper(IAmazonLambda lambdaService)    {        _lambdaService = lambdaService;    }    /// <summary>    /// Creates a new Lambda function.    /// </summary>    /// <param name=\"functionName\">The name of the function.</param>    /// <param name=\"s3Bucket\">The Amazon Simple Storage Service (Amazon S3)    /// bucket where the zip file containing the code is located.</param>    /// <param name=\"s3Key\">The Amazon S3 key of the zip file.</param>    /// <param name=\"role\">The Amazon Resource Name (ARN) of a role with the    /// appropriate Lambda permissions.</param>    /// <param name=\"handler\">The name of the handler function.</param>    /// <returns>The Amazon Resource Name (ARN) of the newly created    /// Lambda function.</returns>    public async Task<string> CreateLambdaFunctionAsync(        string functionName,        string s3Bucket,        string s3Key,        string role,        string handler)    {        // Defines the location for the function code.        // S3Bucket - The S3 bucket where the file containing        //            the source code is stored.        // S3Key    - The name of the file containing the code.        var functionCode = new FunctionCode        {            S3Bucket = s3Bucket,            S3Key = s3Key,        };        var createFunctionRequest = new CreateFunctionRequest        {            FunctionName = functionName,            Description = \"Created by the Lambda .NET API\",            Code = functionCode,            Handler = handler,            Runtime = Runtime.Dotnet6,            Role = role,        };        var reponse = await _lambdaService.CreateFunctionAsync(createFunctionRequest);        return reponse.FunctionArn;    }    /// <summary>    /// Delete an AWS Lambda function.    /// </summary>    /// <param name=\"functionName\">The name of the Lambda function to    /// delete.</param>    /// <returns>A Boolean value that indicates the success of the action.</returns>    public async Task<bool> DeleteFunctionAsync(string functionName)    {        var request = new DeleteFunctionRequest        {            FunctionName = functionName,        };        var response = await _lambdaService.DeleteFunctionAsync(request);        // A return value of NoContent means that the request was processed.        // In this case, the function was deleted, and the return value        // is intentionally blank.        return response.HttpStatusCode == System.Net.HttpStatusCode.NoContent;    }    /// <summary>    /// Gets information about a Lambda function.    /// </summary>    /// <param name=\"functionName\">The name of the Lambda function for    /// which to retrieve information.</param>    /// <returns>Async Task.</returns>    public async Task<FunctionConfiguration> GetFunctionAsync(string functionName)    {        var functionRequest = new GetFunctionRequest        {            FunctionName = functionName,        };        var response = await _lambdaService.GetFunctionAsync(functionRequest);        return response.Configuration;    }    /// <summary>    /// Invoke a Lambda function.    /// </summary>    /// <param name=\"functionName\">The name of the Lambda function to    /// invoke.</param    /// <param name=\"parameters\">The parameter values that will be passed to the function.</param>    /// <returns>A System Threading Task.</returns>    public async Task<string> InvokeFunctionAsync(        string functionName,        string parameters)    {        var payload = parameters;        var request = new InvokeRequest        {            FunctionName = functionName,            Payload = payload,        };        var response = await _lambdaService.InvokeAsync(request);        MemoryStream stream = response.Payload;        string returnValue = System.Text.Encoding.UTF8.GetString(stream.ToArray());        return returnValue;    }    /// <summary>    /// Get a list of Lambda functions.    /// </summary>    /// <returns>A list of FunctionConfiguration objects.</returns>    public async Task<List<FunctionConfiguration>> ListFunctionsAsync()    {        var functionList = new List<FunctionConfiguration>();        var functionPaginator =            _lambdaService.Paginators.ListFunctions(new ListFunctionsRequest());        await foreach (var function in functionPaginator.Functions)        {            functionList.Add(function);        }        return functionList;    }    /// <summary>    /// Update an existing Lambda function.    /// </summary>    /// <param name=\"functionName\">The name of the Lambda function to update.</param>    /// <param name=\"bucketName\">The bucket where the zip file containing    /// the Lambda function code is stored.</param>    /// <param name=\"key\">The key name of the source code file.</param>    /// <returns>Async Task.</returns>    public async Task UpdateFunctionCodeAsync(        string functionName,        string bucketName,        string key)    {        var functionCodeRequest = new UpdateFunctionCodeRequest        {            FunctionName = functionName,            Publish = true,            S3Bucket = bucketName,            S3Key = key,        };        var response = await _lambdaService.UpdateFunctionCodeAsync(functionCodeRequest);        Console.WriteLine($\"The Function was last modified at {response.LastModified}.\");    }    /// <summary>    /// Update the code of a Lambda function.    /// </summary>    /// <param name=\"functionName\">The name of the function to update.</param>    /// <param name=\"functionHandler\">The code that performs the function's actions.</param>    /// <param name=\"environmentVariables\">A dictionary of environment variables.</param>    /// <returns>A Boolean value indicating the success of the action.</returns>    public async Task<bool> UpdateFunctionConfigurationAsync(        string functionName,        string functionHandler,        Dictionary<string, string> environmentVariables)    {        var request = new UpdateFunctionConfigurationRequest        {            Handler = functionHandler,            FunctionName = functionName,            Environment = new Amazon.Lambda.Model.Environment { Variables = environmentVariables },        };        var response = await _lambdaService.UpdateFunctionConfigurationAsync(request);        Console.WriteLine(response.LastModified);        return response.HttpStatusCode == System.Net.HttpStatusCode.OK;    }}Create a function that runs the scenario.global using System.Threading.Tasks;global using Amazon.IdentityManagement;global using Amazon.Lambda;global using LambdaActions;global using LambdaScenarioCommon;global using Microsoft.Extensions.DependencyInjection;global using Microsoft.Extensions.Hosting;global using Microsoft.Extensions.Logging;global using Microsoft.Extensions.Logging.Console;global using Microsoft.Extensions.Logging.Debug;using Amazon.Lambda.Model;using Microsoft.Extensions.Configuration;namespace LambdaBasics;public class LambdaBasics{    private static ILogger logger = null!;    static async Task Main(string[] args)    {        // Set up dependency injection for the Amazon service.        using var host = Host.CreateDefaultBuilder(args)            .ConfigureLogging(logging =>                logging.AddFilter(\"System\", LogLevel.Debug)                    .AddFilter<DebugLoggerProvider>(\"Microsoft\", LogLevel.Information)                    .AddFilter<ConsoleLoggerProvider>(\"Microsoft\", LogLevel.Trace))            .ConfigureServices((_, services) =>            services.AddAWSService<IAmazonLambda>()            .AddAWSService<IAmazonIdentityManagementService>()            .AddTransient<LambdaWrapper>()            .AddTransient<LambdaRoleWrapper>()            .AddTransient<UIWrapper>()        )        .Build();        var configuration = new ConfigurationBuilder()            .SetBasePath(Directory.GetCurrentDirectory())            .AddJsonFile(\"settings.json\") // Load test settings from .json file.            .AddJsonFile(\"settings.local.json\",            true) // Optionally load local settings.        .Build();        logger = LoggerFactory.Create(builder => { builder.AddConsole(); })            .CreateLogger<LambdaBasics>();        var lambdaWrapper = host.Services.GetRequiredService<LambdaWrapper>();        var lambdaRoleWrapper = host.Services.GetRequiredService<LambdaRoleWrapper>();        var uiWrapper = host.Services.GetRequiredService<UIWrapper>();        string functionName = configuration[\"FunctionName\"]!;        string roleName = configuration[\"RoleName\"]!;        string policyDocument = \"{\" +            \" \\\"Version\\\": \\\"2012-10-17\\\",\" +            \" \\\"Statement\\\": [ \" +            \"    {\" +            \"        \\\"Effect\\\": \\\"Allow\\\",\" +            \"        \\\"Principal\\\": {\" +            \"            \\\"Service\\\": \\\"lambda.amazonaws.com\\\" \" +            \"    },\" +            \"        \\\"Action\\\": \\\"sts:AssumeRole\\\" \" +            \"    }\" +            \"]\" +        \"}\";        var incrementHandler = configuration[\"IncrementHandler\"];        var calculatorHandler = configuration[\"CalculatorHandler\"];        var bucketName = configuration[\"BucketName\"];        var incrementKey = configuration[\"IncrementKey\"];        var calculatorKey = configuration[\"CalculatorKey\"];        var policyArn = configuration[\"PolicyArn\"];        uiWrapper.DisplayLambdaBasicsOverview();        // Create the policy to use with the AWS Lambda functions and then attach the        // policy to a new role.        var roleArn = await lambdaRoleWrapper.CreateLambdaRoleAsync(roleName, policyDocument);        Console.WriteLine(\"Waiting for role to become active.\");        uiWrapper.WaitABit(15, \"Wait until the role is active before trying to use it.\");        // Attach the appropriate AWS Identity and Access Management (IAM) role policy to the new role.        var success = await lambdaRoleWrapper.AttachLambdaRolePolicyAsync(policyArn, roleName);        uiWrapper.WaitABit(10, \"Allow time for the IAM policy to be attached to the role.\");        // Create the Lambda function using a zip file stored in an Amazon Simple Storage Service        // (Amazon S3) bucket.        uiWrapper.DisplayTitle(\"Create Lambda Function\");        Console.WriteLine($\"Creating the AWS Lambda function: {functionName}.\");        var lambdaArn = await lambdaWrapper.CreateLambdaFunctionAsync(            functionName,            bucketName,            incrementKey,            roleArn,            incrementHandler);        Console.WriteLine(\"Waiting for the new function to be available.\");        Console.WriteLine($\"The AWS Lambda ARN is {lambdaArn}\");        // Get the Lambda function.        Console.WriteLine($\"Getting the {functionName} AWS Lambda function.\");        FunctionConfiguration config;        do        {            config = await lambdaWrapper.GetFunctionAsync(functionName);            Console.Write(\".\");        }        while (config.State != State.Active);        Console.WriteLine($\"\\nThe function, {functionName} has been created.\");        Console.WriteLine($\"The runtime of this Lambda function is {config.Runtime}.\");        uiWrapper.PressEnter();        // List the Lambda functions.        uiWrapper.DisplayTitle(\"Listing all Lambda functions.\");        var functions = await lambdaWrapper.ListFunctionsAsync();        DisplayFunctionList(functions);        uiWrapper.DisplayTitle(\"Invoke increment function\");        Console.WriteLine(\"Now that it has been created, invoke the Lambda increment function.\");        string? value;        do        {            Console.Write(\"Enter a value to increment: \");            value = Console.ReadLine();        }        while (string.IsNullOrEmpty(value));        string functionParameters = \"{\" +            \"\\\"action\\\": \\\"increment\\\", \" +            \"\\\"x\\\": \\\"\" + value + \"\\\"\" +        \"}\";        var answer = await lambdaWrapper.InvokeFunctionAsync(functionName, functionParameters);        Console.WriteLine($\"{value} + 1 = {answer}.\");        uiWrapper.DisplayTitle(\"Update function\");        Console.WriteLine(\"Now update the Lambda function code.\");        await lambdaWrapper.UpdateFunctionCodeAsync(functionName, bucketName, calculatorKey);        do        {            config = await lambdaWrapper.GetFunctionAsync(functionName);            Console.Write(\".\");        }        while (config.LastUpdateStatus == LastUpdateStatus.InProgress);        await lambdaWrapper.UpdateFunctionConfigurationAsync(            functionName,            calculatorHandler,            new Dictionary<string, string> { { \"LOG_LEVEL\", \"DEBUG\" } });        do        {            config = await lambdaWrapper.GetFunctionAsync(functionName);            Console.Write(\".\");        }        while (config.LastUpdateStatus == LastUpdateStatus.InProgress);        uiWrapper.DisplayTitle(\"Call updated function\");        Console.WriteLine(\"Now call the updated function...\");        bool done = false;        do        {            string? opSelected;            Console.WriteLine(\"Select the operation to perform:\");            Console.WriteLine(\"\\t1. add\");            Console.WriteLine(\"\\t2. subtract\");            Console.WriteLine(\"\\t3. multiply\");            Console.WriteLine(\"\\t4. divide\");            Console.WriteLine(\"\\tOr enter \\\"q\\\" to quit.\");            Console.WriteLine(\"Enter the number (1, 2, 3, 4, or q) of the operation you want to perform: \");            do            {                Console.Write(\"Your choice? \");                opSelected = Console.ReadLine();            }            while (opSelected == string.Empty);            var operation = (opSelected) switch            {                \"1\" => \"add\",                \"2\" => \"subtract\",                \"3\" => \"multiply\",                \"4\" => \"divide\",                \"q\" => \"quit\",                _ => \"add\",            };            if (operation == \"quit\")            {                done = true;            }            else            {                // Get two numbers and an action from the user.                value = string.Empty;                do                {                    Console.Write(\"Enter the first value: \");                    value = Console.ReadLine();                }                while (value == string.Empty);                string? value2;                do                {                    Console.Write(\"Enter a second value: \");                    value2 = Console.ReadLine();                }                while (value2 == string.Empty);                functionParameters = \"{\" +                    \"\\\"action\\\": \\\"\" + operation + \"\\\", \" +                    \"\\\"x\\\": \\\"\" + value + \"\\\",\" +                    \"\\\"y\\\": \\\"\" + value2 + \"\\\"\" +                \"}\";                answer = await lambdaWrapper.InvokeFunctionAsync(functionName, functionParameters);                Console.WriteLine($\"The answer when we {operation} the two numbers is: {answer}.\");            }            uiWrapper.PressEnter();        } while (!done);        // Delete the function created earlier.        uiWrapper.DisplayTitle(\"Clean up resources\");        // Detach the IAM policy from the IAM role.        Console.WriteLine(\"First detach the IAM policy from the role.\");        success = await lambdaRoleWrapper.DetachLambdaRolePolicyAsync(policyArn, roleName);        uiWrapper.WaitABit(15, \"Let's wait for the policy to be fully detached from the role.\");        Console.WriteLine(\"Delete the AWS Lambda function.\");        success = await lambdaWrapper.DeleteFunctionAsync(functionName);        if (success)        {            Console.WriteLine($\"The {functionName} function was deleted.\");        }        else        {            Console.WriteLine($\"Could not remove the function {functionName}\");        }        // Now delete the IAM role created for use with the functions        // created by the application.        Console.WriteLine(\"Now we can delete the role that we created.\");        success = await lambdaRoleWrapper.DeleteLambdaRoleAsync(roleName);        if (success)        {            Console.WriteLine(\"The role has been successfully removed.\");        }        else        {            Console.WriteLine(\"Couldn't delete the role.\");        }        Console.WriteLine(\"The Lambda Scenario is now complete.\");        uiWrapper.PressEnter();        // Displays a formatted list of existing functions returned by the        // LambdaMethods.ListFunctions.        void DisplayFunctionList(List<FunctionConfiguration> functions)        {            functions.ForEach(functionConfig =>            {                Console.WriteLine($\"{functionConfig.FunctionName}\\t{functionConfig.Description}\");            });        }    }}namespace LambdaActions;using Amazon.IdentityManagement;using Amazon.IdentityManagement.Model;public class LambdaRoleWrapper{    private readonly IAmazonIdentityManagementService _lambdaRoleService;    public LambdaRoleWrapper(IAmazonIdentityManagementService lambdaRoleService)    {        _lambdaRoleService = lambdaRoleService;    }    /// <summary>    /// Attach an AWS Identity and Access Management (IAM) role policy to the    /// IAM role to be assumed by the AWS Lambda functions created for the scenario.    /// </summary>    /// <param name=\"policyArn\">The Amazon Resource Name (ARN) of the IAM policy.</param>    /// <param name=\"roleName\">The name of the IAM role to attach the IAM policy to.</param>    /// <returns>A Boolean value indicating the success of the action.</returns>    public async Task<bool> AttachLambdaRolePolicyAsync(string policyArn, string roleName)    {        var response = await _lambdaRoleService.AttachRolePolicyAsync(new AttachRolePolicyRequest { PolicyArn = policyArn, RoleName = roleName });        return response.HttpStatusCode == System.Net.HttpStatusCode.OK;    }    /// <summary>    /// Create a new IAM role.    /// </summary>    /// <param name=\"roleName\">The name of the IAM role to create.</param>    /// <param name=\"policyDocument\">The policy document for the new IAM role.</param>    /// <returns>A string representing the ARN for newly created role.</returns>    public async Task<string> CreateLambdaRoleAsync(string roleName, string policyDocument)    {        var request = new CreateRoleRequest        {            AssumeRolePolicyDocument = policyDocument,            RoleName = roleName,        };        var response = await _lambdaRoleService.CreateRoleAsync(request);        return response.Role.Arn;    }    /// <summary>    /// Deletes an IAM role.    /// </summary>    /// <param name=\"roleName\">The name of the role to delete.</param>    /// <returns>A Boolean value indicating the success of the operation.</returns>    public async Task<bool> DeleteLambdaRoleAsync(string roleName)    {        var request = new DeleteRoleRequest        {            RoleName = roleName,        };        var response = await _lambdaRoleService.DeleteRoleAsync(request);        return response.HttpStatusCode == System.Net.HttpStatusCode.OK;    }    public async Task<bool> DetachLambdaRolePolicyAsync(string policyArn, string roleName)    {        var response = await _lambdaRoleService.DetachRolePolicyAsync(new DetachRolePolicyRequest { PolicyArn = policyArn, RoleName = roleName });        return response.HttpStatusCode == System.Net.HttpStatusCode.OK;    }}namespace LambdaScenarioCommon;public class UIWrapper{    public readonly string SepBar = new('-', Console.WindowWidth);    /// <summary>    /// Show information about the AWS Lambda Basics scenario.    /// </summary>    public void DisplayLambdaBasicsOverview()    {        Console.Clear();        DisplayTitle(\"Welcome to AWS Lambda Basics\");        Console.WriteLine(\"This example application does the following:\");        Console.WriteLine(\"\\t1. Creates an AWS Identity and Access Management (IAM) role that will be assumed by the functions we create.\");        Console.WriteLine(\"\\t2. Attaches an IAM role policy that has Lambda permissions.\");        Console.WriteLine(\"\\t3. Creates a Lambda function that increments the value passed to it.\");        Console.WriteLine(\"\\t4. Calls the increment function and passes a value.\");        Console.WriteLine(\"\\t5. Updates the code so that the function is a simple calculator.\");        Console.WriteLine(\"\\t6. Calls the calculator function with the values entered.\");        Console.WriteLine(\"\\t7. Deletes the Lambda function.\");        Console.WriteLine(\"\\t7. Detaches the IAM role policy.\");        Console.WriteLine(\"\\t8. Deletes the IAM role.\");        PressEnter();    }    /// <summary>    /// Display a message and wait until the user presses enter.    /// </summary>    public void PressEnter()    {        Console.Write(\"\\nPress <Enter> to continue. \");        _ = Console.ReadLine();        Console.WriteLine();    }    /// <summary>    /// Pad a string with spaces to center it on the console display.    /// </summary>    /// <param name=\"strToCenter\">The string to be centered.</param>    /// <returns>The padded string.</returns>    public string CenterString(string strToCenter)    {        var padAmount = (Console.WindowWidth - strToCenter.Length) / 2;        var leftPad = new string(' ', padAmount);        return $\"{leftPad}{strToCenter}\";    }    /// <summary>    /// Display a line of hyphens, the centered text of the title and another    /// line of hyphens.    /// </summary>    /// <param name=\"strTitle\">The string to be displayed.</param>    public void DisplayTitle(string strTitle)    {        Console.WriteLine(SepBar);        Console.WriteLine(CenterString(strTitle));        Console.WriteLine(SepBar);    }    /// <summary>    /// Display a countdown and wait for a number of seconds.    /// </summary>    /// <param name=\"numSeconds\">The number of seconds to wait.</param>    public void WaitABit(int numSeconds, string msg)    {        Console.WriteLine(msg);        // Wait for the requested number of seconds.        for (int i = numSeconds; i > 0; i--)        {            System.Threading.Thread.Sleep(1000);            Console.Write($\"{i}...\");        }        PressEnter();    }}Define a Lambda handler that increments a number.using Amazon.Lambda.Core;// Assembly attribute to enable the Lambda function's JSON input to be converted into a .NET class.[assembly: LambdaSerializer(typeof(Amazon.Lambda.Serialization.SystemTextJson.DefaultLambdaJsonSerializer))]namespace LambdaIncrement;public class Function{    /// <summary>    /// A simple function increments the integer parameter.    /// </summary>    /// <param name=\"input\">A JSON string containing an action, which must be    /// \"increment\" and a string representing the value to increment.</param>    /// <param name=\"context\">The context object passed by Lambda containing    /// information about invocation, function, and execution environment.</param>    /// <returns>A string representing the incremented value of the parameter.</returns>    public int FunctionHandler(Dictionary<string, string> input, ILambdaContext context)    {        if (input[\"action\"] == \"increment\")        {            int inputValue = Convert.ToInt32(input[\"x\"]);            return inputValue + 1;        }        else        {            return 0;        }    }}Define a second Lambda handler that performs arithmetic operations.using Amazon.Lambda.Core;// Assembly attribute to enable the Lambda function's JSON input to be converted into a .NET class.[assembly: LambdaSerializer(typeof(Amazon.Lambda.Serialization.SystemTextJson.DefaultLambdaJsonSerializer))]namespace LambdaCalculator;public class Function{    /// <summary>    /// A simple function that takes two number in string format and performs    /// the requested arithmetic function.    /// </summary>    /// <param name=\"input\">JSON data containing an action, and x and y values.    /// Valid actions include: add, subtract, multiply, and divide.</param>    /// <param name=\"context\">The context object passed by Lambda containing    /// information about invocation, function, and execution environment.</param>    /// <returns>A string representing the results of the calculation.</returns>    public int FunctionHandler(Dictionary<string, string> input, ILambdaContext context)    {        var action = input[\"action\"];        int x = Convert.ToInt32(input[\"x\"]);        int y = Convert.ToInt32(input[\"y\"]);        int result;        switch (action)        {            case \"add\":                result = x + y;                break;            case \"subtract\":                result = x - y;                break;            case \"multiply\":                result = x * y;                break;            case \"divide\":                if (y == 0)                {                    Console.Error.WriteLine(\"Divide by zero error.\");                    result = 0;                }                else                    result = x / y;                break;            default:                Console.Error.WriteLine($\"{action} is not a valid operation.\");                result = 0;                break;        }        return result;    }}For API details, see the following topics in AWS SDK for .NET API Reference.CreateFunctionDeleteFunctionGetFunctionInvokeListFunctionsUpdateFunctionCodeUpdateFunctionConfigurationC++SDK for C++Note        There's more on GitHub. Find the complete example and learn how to set up and run in the        AWS Code            Examples Repository.    //! Get started with functions scenario./*! \\param clientConfig: AWS client configuration. \\return bool: Successful completion. */bool AwsDoc::Lambda::getStartedWithFunctionsScenario(        const Aws::Client::ClientConfiguration &clientConfig) {    Aws::Lambda::LambdaClient client(clientConfig);    // 1. Create an AWS Identity and Access Management (IAM) role for Lambda function.    Aws::String roleArn;    if (!getIamRoleArn(roleArn, clientConfig)) {        return false;    }    // 2. Create a Lambda function.    int seconds = 0;    do {        Aws::Lambda::Model::CreateFunctionRequest request;        request.SetFunctionName(LAMBDA_NAME);        request.SetDescription(LAMBDA_DESCRIPTION); // Optional.#if USE_CPP_LAMBDA_FUNCTION        request.SetRuntime(Aws::Lambda::Model::Runtime::provided_al2);        request.SetTimeout(15);        request.SetMemorySize(128);        // Assume the AWS Lambda function was built in Docker with same architecture        // as this code.#if  defined(__x86_64__)        request.SetArchitectures({Aws::Lambda::Model::Architecture::x86_64});#elif defined(__aarch64__)        request.SetArchitectures({Aws::Lambda::Model::Architecture::arm64});#else#error \"Unimplemented architecture\"#endif // defined(architecture)#else        request.SetRuntime(Aws::Lambda::Model::Runtime::python3_9);#endif        request.SetRole(roleArn);        request.SetHandler(LAMBDA_HANDLER_NAME);        request.SetPublish(true);        Aws::Lambda::Model::FunctionCode code;        std::ifstream ifstream(INCREMENT_LAMBDA_CODE.c_str(),                               std::ios_base::in | std::ios_base::binary);        if (!ifstream.is_open()) {            std::cerr << \"Error opening file \" << INCREMENT_LAMBDA_CODE << \".\" << std::endl;#if USE_CPP_LAMBDA_FUNCTION            std::cerr                    << \"The cpp Lambda function must be built following the instructions in the cpp_lambda/README.md file. \"                    << std::endl;#endif            deleteIamRole(clientConfig);            return false;        }        Aws::StringStream buffer;        buffer << ifstream.rdbuf();        code.SetZipFile(Aws::Utils::ByteBuffer((unsigned char *) buffer.str().c_str(),                                               buffer.str().length()));        request.SetCode(code);        Aws::Lambda::Model::CreateFunctionOutcome outcome = client.CreateFunction(                request);        if (outcome.IsSuccess()) {            std::cout << \"The lambda function was successfully created. \" << seconds                      << \" seconds elapsed.\" << std::endl;            break;        }        else if (outcome.GetError().GetErrorType() ==                 Aws::Lambda::LambdaErrors::INVALID_PARAMETER_VALUE &&                 outcome.GetError().GetMessage().find(\"role\") >= 0) {            if ((seconds % 5) == 0) { // Log status every 10 seconds.                std::cout                        << \"Waiting for the IAM role to become available as a CreateFunction parameter. \"                        << seconds                        << \" seconds elapsed.\" << std::endl;                std::cout << outcome.GetError().GetMessage() << std::endl;            }        }        else {            std::cerr << \"Error with CreateFunction. \"                      << outcome.GetError().GetMessage()                      << std::endl;            deleteIamRole(clientConfig);            return false;        }        ++seconds;        std::this_thread::sleep_for(std::chrono::seconds(1));    } while (60 > seconds);    std::cout << \"The current Lambda function increments 1 by an input.\" << std::endl;    // 3.  Invoke the Lambda function.    {        int increment = askQuestionForInt(\"Enter an increment integer: \");        Aws::Lambda::Model::InvokeResult invokeResult;        Aws::Utils::Json::JsonValue jsonPayload;        jsonPayload.WithString(\"action\", \"increment\");        jsonPayload.WithInteger(\"number\", increment);        if (invokeLambdaFunction(jsonPayload, Aws::Lambda::Model::LogType::Tail,                                 invokeResult, client)) {            Aws::Utils::Json::JsonValue jsonValue(invokeResult.GetPayload());            Aws::Map<Aws::String, Aws::Utils::Json::JsonView> values =                    jsonValue.View().GetAllObjects();            auto iter = values.find(\"result\");            if (iter != values.end() && iter->second.IsIntegerType()) {                {                    std::cout << INCREMENT_RESUlT_PREFIX                              << iter->second.AsInteger() << std::endl;                }            }            else {                std::cout << \"There was an error in execution. Here is the log.\"                          << std::endl;                Aws::Utils::ByteBuffer buffer = Aws::Utils::HashingUtils::Base64Decode(                        invokeResult.GetLogResult());                std::cout << \"With log \" << buffer.GetUnderlyingData() << std::endl;            }        }    }    std::cout            << \"The Lambda function will now be updated with new code. Press return to continue, \";    Aws::String answer;    std::getline(std::cin, answer);    // 4.  Update the Lambda function code.    {        Aws::Lambda::Model::UpdateFunctionCodeRequest request;        request.SetFunctionName(LAMBDA_NAME);        std::ifstream ifstream(CALCULATOR_LAMBDA_CODE.c_str(),                               std::ios_base::in | std::ios_base::binary);        if (!ifstream.is_open()) {            std::cerr << \"Error opening file \" << INCREMENT_LAMBDA_CODE << \".\" << std::endl;#if USE_CPP_LAMBDA_FUNCTION            std::cerr                    << \"The cpp Lambda function must be built following the instructions in the cpp_lambda/README.md file. \"                    << std::endl;#endif            deleteLambdaFunction(client);            deleteIamRole(clientConfig);            return false;        }        Aws::StringStream buffer;        buffer << ifstream.rdbuf();        request.SetZipFile(                Aws::Utils::ByteBuffer((unsigned char *) buffer.str().c_str(),                                       buffer.str().length()));        request.SetPublish(true);        Aws::Lambda::Model::UpdateFunctionCodeOutcome outcome = client.UpdateFunctionCode(                request);        if (outcome.IsSuccess()) {            std::cout << \"The lambda code was successfully updated.\" << std::endl;        }        else {            std::cerr << \"Error with Lambda::UpdateFunctionCode. \"                      << outcome.GetError().GetMessage()                      << std::endl;        }    }    std::cout            << \"This function uses an environment variable to control the logging level.\"            << std::endl;    std::cout            << \"UpdateFunctionConfiguration will be used to set the LOG_LEVEL to DEBUG.\"            << std::endl;    seconds = 0;    // 5.  Update the Lambda function configuration.    do {        ++seconds;        std::this_thread::sleep_for(std::chrono::seconds(1));        Aws::Lambda::Model::UpdateFunctionConfigurationRequest request;        request.SetFunctionName(LAMBDA_NAME);        Aws::Lambda::Model::Environment environment;        environment.AddVariables(\"LOG_LEVEL\", \"DEBUG\");        request.SetEnvironment(environment);        Aws::Lambda::Model::UpdateFunctionConfigurationOutcome outcome = client.UpdateFunctionConfiguration(                request);        if (outcome.IsSuccess()) {            std::cout << \"The lambda configuration was successfully updated.\"                      << std::endl;            break;        }            // RESOURCE_IN_USE: function code update not completed.        else if (outcome.GetError().GetErrorType() !=                 Aws::Lambda::LambdaErrors::RESOURCE_IN_USE) {            if ((seconds % 10) == 0) { // Log status every 10 seconds.                std::cout << \"Lambda function update in progress . After \" << seconds                          << \" seconds elapsed.\" << std::endl;            }        }        else {            std::cerr << \"Error with Lambda::UpdateFunctionConfiguration. \"                      << outcome.GetError().GetMessage()                      << std::endl;        }    } while (0 < seconds);    if (0 > seconds) {        std::cerr << \"Function failed to become active.\" << std::endl;    }    else {        std::cout << \"Updated function active after \" << seconds << \" seconds.\"                  << std::endl;    }    std::cout            << \"\\nThe new code applies an arithmetic operator to two variables, x an y.\"            << std::endl;    std::vector<Aws::String> operators = {\"plus\", \"minus\", \"times\", \"divided-by\"};    for (size_t i = 0; i < operators.size(); ++i) {        std::cout << \"   \" << i + 1 << \" \" << operators[i] << std::endl;    }    // 6.  Invoke the updated Lambda function.    do {        int operatorIndex = askQuestionForIntRange(\"Select an operator index 1 - 4 \", 1,                                                   4);        int x = askQuestionForInt(\"Enter an integer for the x value \");        int y = askQuestionForInt(\"Enter an integer for the y value \");        Aws::Utils::Json::JsonValue calculateJsonPayload;        calculateJsonPayload.WithString(\"action\", operators[operatorIndex - 1]);        calculateJsonPayload.WithInteger(\"x\", x);        calculateJsonPayload.WithInteger(\"y\", y);        Aws::Lambda::Model::InvokeResult calculatedResult;        if (invokeLambdaFunction(calculateJsonPayload,                                 Aws::Lambda::Model::LogType::Tail,                                 calculatedResult, client)) {            Aws::Utils::Json::JsonValue jsonValue(calculatedResult.GetPayload());            Aws::Map<Aws::String, Aws::Utils::Json::JsonView> values =                    jsonValue.View().GetAllObjects();            auto iter = values.find(\"result\");            if (iter != values.end() && iter->second.IsIntegerType()) {                std::cout << ARITHMETIC_RESUlT_PREFIX << x << \" \"                          << operators[operatorIndex - 1] << \" \"                          << y << \" is \" << iter->second.AsInteger() << std::endl;            }            else if (iter != values.end() && iter->second.IsFloatingPointType()) {                std::cout << ARITHMETIC_RESUlT_PREFIX << x << \" \"                          << operators[operatorIndex - 1] << \" \"                          << y << \" is \" << iter->second.AsDouble() << std::endl;            }            else {                std::cout << \"There was an error in execution. Here is the log.\"                          << std::endl;                Aws::Utils::ByteBuffer buffer = Aws::Utils::HashingUtils::Base64Decode(                        calculatedResult.GetLogResult());                std::cout << \"With log \" << buffer.GetUnderlyingData() << std::endl;            }        }        answer = askQuestion(\"Would you like to try another operation? (y/n) \");    } while (answer == \"y\");    std::cout            << \"A list of the lambda functions will be retrieved. Press return to continue, \";    std::getline(std::cin, answer);    // 7.  List the Lambda functions.    std::vector<Aws::String> functions;    Aws::String marker;    do {        Aws::Lambda::Model::ListFunctionsRequest request;        if (!marker.empty()) {            request.SetMarker(marker);        }        Aws::Lambda::Model::ListFunctionsOutcome outcome = client.ListFunctions(                request);        if (outcome.IsSuccess()) {            const Aws::Lambda::Model::ListFunctionsResult &result = outcome.GetResult();            std::cout << result.GetFunctions().size()                      << \" lambda functions were retrieved.\" << std::endl;            for (const Aws::Lambda::Model::FunctionConfiguration &functionConfiguration: result.GetFunctions()) {                functions.push_back(functionConfiguration.GetFunctionName());                std::cout << functions.size() << \"  \"                          << functionConfiguration.GetDescription() << std::endl;                std::cout << \"   \"                          << Aws::Lambda::Model::RuntimeMapper::GetNameForRuntime(                                  functionConfiguration.GetRuntime()) << \": \"                          << functionConfiguration.GetHandler()                          << std::endl;            }            marker = result.GetNextMarker();        }        else {            std::cerr << \"Error with Lambda::ListFunctions. \"                      << outcome.GetError().GetMessage()                      << std::endl;        }    } while (!marker.empty());    // 8.  Get a Lambda function.    if (!functions.empty()) {        std::stringstream question;        question << \"Choose a function to retrieve between 1 and \" << functions.size()                 << \" \";        int functionIndex = askQuestionForIntRange(question.str(), 1,                                                   static_cast<int>(functions.size()));        Aws::String functionName = functions[functionIndex - 1];        Aws::Lambda::Model::GetFunctionRequest request;        request.SetFunctionName(functionName);        Aws::Lambda::Model::GetFunctionOutcome outcome = client.GetFunction(request);        if (outcome.IsSuccess()) {            std::cout << \"Function retrieve.\\n\" <<                      outcome.GetResult().GetConfiguration().Jsonize().View().WriteReadable()                      << std::endl;        }        else {            std::cerr << \"Error with Lambda::GetFunction. \"                      << outcome.GetError().GetMessage()                      << std::endl;        }    }    std::cout << \"The resources will be deleted. Press return to continue, \";    std::getline(std::cin, answer);    // 9.  Delete the Lambda function.    bool result = deleteLambdaFunction(client);    // 10. Delete the IAM role.    return result && deleteIamRole(clientConfig);}//! Routine which invokes a Lambda function and returns the result./*! \\param jsonPayload: Payload for invoke function. \\param logType: Log type setting for invoke function. \\param invokeResult: InvokeResult object to receive the result. \\param client: Lambda client. \\return bool: Successful completion. */boolAwsDoc::Lambda::invokeLambdaFunction(const Aws::Utils::Json::JsonValue &jsonPayload,                                     Aws::Lambda::Model::LogType logType,                                     Aws::Lambda::Model::InvokeResult &invokeResult,                                     const Aws::Lambda::LambdaClient &client) {    int seconds = 0;    bool result = false;    /*     * In this example, the Invoke function can be called before recently created resources are     * available.  The Invoke function is called repeatedly until the resources are     * available.     */    do {        Aws::Lambda::Model::InvokeRequest request;        request.SetFunctionName(LAMBDA_NAME);        request.SetLogType(logType);        std::shared_ptr<Aws::IOStream> payload = Aws::MakeShared<Aws::StringStream>(                \"FunctionTest\");        *payload << jsonPayload.View().WriteReadable();        request.SetBody(payload);        request.SetContentType(\"application/json\");        Aws::Lambda::Model::InvokeOutcome outcome = client.Invoke(request);        if (outcome.IsSuccess()) {            invokeResult = std::move(outcome.GetResult());            result = true;            break;        }            // ACCESS_DENIED: because the role is not available yet.            // RESOURCE_CONFLICT: because the Lambda function is being created or updated.        else if ((outcome.GetError().GetErrorType() ==                  Aws::Lambda::LambdaErrors::ACCESS_DENIED) ||                 (outcome.GetError().GetErrorType() ==                  Aws::Lambda::LambdaErrors::RESOURCE_CONFLICT)) {            if ((seconds % 5) == 0) { // Log status every 10 seconds.                std::cout << \"Waiting for the invoke api to be available, status \" <<                          ((outcome.GetError().GetErrorType() ==                            Aws::Lambda::LambdaErrors::ACCESS_DENIED ?                            \"ACCESS_DENIED\" : \"RESOURCE_CONFLICT\")) << \". \" << seconds                          << \" seconds elapsed.\" << std::endl;            }        }        else {            std::cerr << \"Error with Lambda::InvokeRequest. \"                      << outcome.GetError().GetMessage()                      << std::endl;            break;        }        ++seconds;        std::this_thread::sleep_for(std::chrono::seconds(1));    } while (seconds < 60);    return result;}For API details, see the following topics in AWS SDK for C++ API Reference.CreateFunctionDeleteFunctionGetFunctionInvokeListFunctionsUpdateFunctionCodeUpdateFunctionConfigurationGoSDK for Go V2Note        There's more on GitHub. Find the complete example and learn how to set up and run in the        AWS Code            Examples Repository.    Create an interactive scenario that shows you how to get started with Lambda functions.import (\t\"archive/zip\"\t\"bytes\"\t\"context\"\t\"encoding/base64\"\t\"encoding/json\"\t\"errors\"\t\"fmt\"\t\"log\"\t\"os\"\t\"strings\"\t\"time\"\t\"github.com/aws/aws-sdk-go-v2/aws\"\t\"github.com/aws/aws-sdk-go-v2/service/iam\"\tiamtypes \"github.com/aws/aws-sdk-go-v2/service/iam/types\"\t\"github.com/aws/aws-sdk-go-v2/service/lambda\"\t\"github.com/awsdocs/aws-doc-sdk-examples/gov2/demotools\"\t\"github.com/awsdocs/aws-doc-sdk-examples/gov2/lambda/actions\")// GetStartedFunctionsScenario shows you how to use AWS Lambda to perform the following// actions:////  1. Create an AWS Identity and Access Management (IAM) role and Lambda function, then upload handler code.//  2. Invoke the function with a single parameter and get results.//  3. Update the function code and configure with an environment variable.//  4. Invoke the function with new parameters and get results. Display the returned execution log.//  5. List the functions for your account, then clean up resources.type GetStartedFunctionsScenario struct {\tsdkConfig       aws.Config\tfunctionWrapper actions.FunctionWrapper\tquestioner      demotools.IQuestioner\thelper          IScenarioHelper\tisTestRun       bool}// NewGetStartedFunctionsScenario constructs a GetStartedFunctionsScenario instance from a configuration.// It uses the specified config to get a Lambda client and create wrappers for the actions// used in the scenario.func NewGetStartedFunctionsScenario(sdkConfig aws.Config, questioner demotools.IQuestioner,\thelper IScenarioHelper) GetStartedFunctionsScenario {\tlambdaClient := lambda.NewFromConfig(sdkConfig)\treturn GetStartedFunctionsScenario{\t\tsdkConfig:       sdkConfig,\t\tfunctionWrapper: actions.FunctionWrapper{LambdaClient: lambdaClient},\t\tquestioner:      questioner,\t\thelper:          helper,\t}}// Run runs the interactive scenario.func (scenario GetStartedFunctionsScenario) Run(ctx context.Context) {\tdefer func() {\t\tif r := recover(); r != nil {\t\t\tlog.Printf(\"Something went wrong with the demo.\\n\")\t\t}\t}()\tlog.Println(strings.Repeat(\"-\", 88))\tlog.Println(\"Welcome to the AWS Lambda get started with functions demo.\")\tlog.Println(strings.Repeat(\"-\", 88))\trole := scenario.GetOrCreateRole(ctx)\tfuncName := scenario.CreateFunction(ctx, role)\tscenario.InvokeIncrement(ctx, funcName)\tscenario.UpdateFunction(ctx, funcName)\tscenario.InvokeCalculator(ctx, funcName)\tscenario.ListFunctions(ctx)\tscenario.Cleanup(ctx, role, funcName)\tlog.Println(strings.Repeat(\"-\", 88))\tlog.Println(\"Thanks for watching!\")\tlog.Println(strings.Repeat(\"-\", 88))}// GetOrCreateRole checks whether the specified role exists and returns it if it does.// Otherwise, a role is created that specifies Lambda as a trusted principal.// The AWSLambdaBasicExecutionRole managed policy is attached to the role and the role// is returned.func (scenario GetStartedFunctionsScenario) GetOrCreateRole(ctx context.Context) *iamtypes.Role {\tvar role *iamtypes.Role\tiamClient := iam.NewFromConfig(scenario.sdkConfig)\tlog.Println(\"First, we need an IAM role that Lambda can assume.\")\troleName := scenario.questioner.Ask(\"Enter a name for the role:\", demotools.NotEmpty{})\tgetOutput, err := iamClient.GetRole(ctx, &iam.GetRoleInput{\t\tRoleName: aws.String(roleName)})\tif err != nil {\t\tvar noSuch *iamtypes.NoSuchEntityException\t\tif errors.As(err, &noSuch) {\t\t\tlog.Printf(\"Role %v doesn't exist. Creating it....\\n\", roleName)\t\t} else {\t\t\tlog.Panicf(\"Couldn't check whether role %v exists. Here's why: %v\\n\",\t\t\t\troleName, err)\t\t}\t} else {\t\trole = getOutput.Role\t\tlog.Printf(\"Found role %v.\\n\", *role.RoleName)\t}\tif role == nil {\t\ttrustPolicy := PolicyDocument{\t\t\tVersion: \"2012-10-17\",\t\t\tStatement: []PolicyStatement{{\t\t\t\tEffect:    \"Allow\",\t\t\t\tPrincipal: map[string]string{\"Service\": \"lambda.amazonaws.com\"},\t\t\t\tAction:    []string{\"sts:AssumeRole\"},\t\t\t}},\t\t}\t\tpolicyArn := \"arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole\"\t\tcreateOutput, err := iamClient.CreateRole(ctx, &iam.CreateRoleInput{\t\t\tAssumeRolePolicyDocument: aws.String(trustPolicy.String()),\t\t\tRoleName:                 aws.String(roleName),\t\t})\t\tif err != nil {\t\t\tlog.Panicf(\"Couldn't create role %v. Here's why: %v\\n\", roleName, err)\t\t}\t\trole = createOutput.Role\t\t_, err = iamClient.AttachRolePolicy(ctx, &iam.AttachRolePolicyInput{\t\t\tPolicyArn: aws.String(policyArn),\t\t\tRoleName:  aws.String(roleName),\t\t})\t\tif err != nil {\t\t\tlog.Panicf(\"Couldn't attach a policy to role %v. Here's why: %v\\n\", roleName, err)\t\t}\t\tlog.Printf(\"Created role %v.\\n\", *role.RoleName)\t\tlog.Println(\"Let's give AWS a few seconds to propagate resources...\")\t\tscenario.helper.Pause(10)\t}\tlog.Println(strings.Repeat(\"-\", 88))\treturn role}// CreateFunction creates a Lambda function and uploads a handler written in Python.// The code for the Python handler is packaged as a []byte in .zip format.func (scenario GetStartedFunctionsScenario) CreateFunction(ctx context.Context, role *iamtypes.Role) string {\tlog.Println(\"Let's create a function that increments a number.\\n\" +\t\t\"The function uses the 'lambda_handler_basic.py' script found in the \\n\" +\t\t\"'handlers' directory of this project.\")\tfuncName := scenario.questioner.Ask(\"Enter a name for the Lambda function:\", demotools.NotEmpty{})\tzipPackage := scenario.helper.CreateDeploymentPackage(\"lambda_handler_basic.py\", fmt.Sprintf(\"%v.py\", funcName))\tlog.Printf(\"Creating function %v and waiting for it to be ready.\", funcName)\tfuncState := scenario.functionWrapper.CreateFunction(ctx, funcName, fmt.Sprintf(\"%v.lambda_handler\", funcName),\t\trole.Arn, zipPackage)\tlog.Printf(\"Your function is %v.\", funcState)\tlog.Println(strings.Repeat(\"-\", 88))\treturn funcName}// InvokeIncrement invokes a Lambda function that increments a number. The function// parameters are contained in a Go struct that is used to serialize the parameters to// a JSON payload that is passed to the function.// The result payload is deserialized into a Go struct that contains an int value.func (scenario GetStartedFunctionsScenario) InvokeIncrement(ctx context.Context, funcName string) {\tparameters := actions.IncrementParameters{Action: \"increment\"}\tlog.Println(\"Let's invoke our function. This function increments a number.\")\tparameters.Number = scenario.questioner.AskInt(\"Enter a number to increment:\", demotools.NotEmpty{})\tlog.Printf(\"Invoking %v with %v...\\n\", funcName, parameters.Number)\tinvokeOutput := scenario.functionWrapper.Invoke(ctx, funcName, parameters, false)\tvar payload actions.LambdaResultInt\terr := json.Unmarshal(invokeOutput.Payload, &payload)\tif err != nil {\t\tlog.Panicf(\"Couldn't unmarshal payload from invoking %v. Here's why: %v\\n\",\t\t\tfuncName, err)\t}\tlog.Printf(\"Invoking %v with %v returned %v.\\n\", funcName, parameters.Number, payload)\tlog.Println(strings.Repeat(\"-\", 88))}// UpdateFunction updates the code for a Lambda function by uploading a simple arithmetic// calculator written in Python. The code for the Python handler is packaged as a// []byte in .zip format.// After the code is updated, the configuration is also updated with a new log// level that instructs the handler to log additional information.func (scenario GetStartedFunctionsScenario) UpdateFunction(ctx context.Context, funcName string) {\tlog.Println(\"Let's update the function to an arithmetic calculator.\\n\" +\t\t\"The function uses the 'lambda_handler_calculator.py' script found in the \\n\" +\t\t\"'handlers' directory of this project.\")\tscenario.questioner.Ask(\"Press Enter when you're ready.\")\tlog.Println(\"Creating deployment package...\")\tzipPackage := scenario.helper.CreateDeploymentPackage(\"lambda_handler_calculator.py\",\t\tfmt.Sprintf(\"%v.py\", funcName))\tlog.Println(\"...and updating the Lambda function and waiting for it to be ready.\")\tfuncState := scenario.functionWrapper.UpdateFunctionCode(ctx, funcName, zipPackage)\tlog.Printf(\"Updated function %v. Its current state is %v.\", funcName, funcState)\tlog.Println(\"This function uses an environment variable to control logging level.\")\tlog.Println(\"Let's set it to DEBUG to get the most logging.\")\tscenario.functionWrapper.UpdateFunctionConfiguration(ctx, funcName,\t\tmap[string]string{\"LOG_LEVEL\": \"DEBUG\"})\tlog.Println(strings.Repeat(\"-\", 88))}// InvokeCalculator invokes the Lambda calculator function. The parameters are stored in a// Go struct that is used to serialize the parameters to a JSON payload. That payload is then passed// to the function.// The result payload is deserialized to a Go struct that stores the result as either an// int or float32, depending on the kind of operation that was specified.func (scenario GetStartedFunctionsScenario) InvokeCalculator(ctx context.Context, funcName string) {\twantInvoke := true\tchoices := []string{\"plus\", \"minus\", \"times\", \"divided-by\"}\tfor wantInvoke {\t\tchoice := scenario.questioner.AskChoice(\"Select an arithmetic operation:\\n\", choices)\t\tx := scenario.questioner.AskInt(\"Enter a value for x:\", demotools.NotEmpty{})\t\ty := scenario.questioner.AskInt(\"Enter a value for y:\", demotools.NotEmpty{})\t\tlog.Printf(\"Invoking %v %v %v...\", x, choices[choice], y)\t\tcalcParameters := actions.CalculatorParameters{\t\t\tAction: choices[choice],\t\t\tX:      x,\t\t\tY:      y,\t\t}\t\tinvokeOutput := scenario.functionWrapper.Invoke(ctx, funcName, calcParameters, true)\t\tvar payload any\t\tif choice == 3 { // divide-by results in a float.\t\t\tpayload = actions.LambdaResultFloat{}\t\t} else {\t\t\tpayload = actions.LambdaResultInt{}\t\t}\t\terr := json.Unmarshal(invokeOutput.Payload, &payload)\t\tif err != nil {\t\t\tlog.Panicf(\"Couldn't unmarshal payload from invoking %v. Here's why: %v\\n\",\t\t\t\tfuncName, err)\t\t}\t\tlog.Printf(\"Invoking %v with %v %v %v returned %v.\\n\", funcName,\t\t\tcalcParameters.X, calcParameters.Action, calcParameters.Y, payload)\t\tscenario.questioner.Ask(\"Press Enter to see the logs from the call.\")\t\tlogRes, err := base64.StdEncoding.DecodeString(*invokeOutput.LogResult)\t\tif err != nil {\t\t\tlog.Panicf(\"Couldn't decode log result. Here's why: %v\\n\", err)\t\t}\t\tlog.Println(string(logRes))\t\twantInvoke = scenario.questioner.AskBool(\"Do you want to calculate again? (y/n)\", \"y\")\t}\tlog.Println(strings.Repeat(\"-\", 88))}// ListFunctions lists up to the specified number of functions for your account.func (scenario GetStartedFunctionsScenario) ListFunctions(ctx context.Context) {\tcount := scenario.questioner.AskInt(\t\t\"Let's list functions for your account. How many do you want to see?\", demotools.NotEmpty{})\tfunctions := scenario.functionWrapper.ListFunctions(ctx, count)\tlog.Printf(\"Found %v functions:\", len(functions))\tfor _, function := range functions {\t\tlog.Printf(\"\\t%v\", *function.FunctionName)\t}\tlog.Println(strings.Repeat(\"-\", 88))}// Cleanup removes the IAM and Lambda resources created by the example.func (scenario GetStartedFunctionsScenario) Cleanup(ctx context.Context, role *iamtypes.Role, funcName string) {\tif scenario.questioner.AskBool(\"Do you want to clean up resources created for this example? (y/n)\",\t\t\"y\") {\t\tiamClient := iam.NewFromConfig(scenario.sdkConfig)\t\tpoliciesOutput, err := iamClient.ListAttachedRolePolicies(ctx,\t\t\t&iam.ListAttachedRolePoliciesInput{RoleName: role.RoleName})\t\tif err != nil {\t\t\tlog.Panicf(\"Couldn't get policies attached to role %v. Here's why: %v\\n\",\t\t\t\t*role.RoleName, err)\t\t}\t\tfor _, policy := range policiesOutput.AttachedPolicies {\t\t\t_, err = iamClient.DetachRolePolicy(ctx, &iam.DetachRolePolicyInput{\t\t\t\tPolicyArn: policy.PolicyArn, RoleName: role.RoleName,\t\t\t})\t\t\tif err != nil {\t\t\t\tlog.Panicf(\"Couldn't detach policy %v from role %v. Here's why: %v\\n\",\t\t\t\t\t*policy.PolicyArn, *role.RoleName, err)\t\t\t}\t\t}\t\t_, err = iamClient.DeleteRole(ctx, &iam.DeleteRoleInput{RoleName: role.RoleName})\t\tif err != nil {\t\t\tlog.Panicf(\"Couldn't delete role %v. Here's why: %v\\n\", *role.RoleName, err)\t\t}\t\tlog.Printf(\"Deleted role %v.\\n\", *role.RoleName)\t\tscenario.functionWrapper.DeleteFunction(ctx, funcName)\t\tlog.Printf(\"Deleted function %v.\\n\", funcName)\t} else {\t\tlog.Println(\"Okay. Don't forget to delete the resources when you're done with them.\")\t}}// IScenarioHelper abstracts I/O and wait functions from a scenario so that they// can be mocked for unit testing.type IScenarioHelper interface {\tPause(secs int)\tCreateDeploymentPackage(sourceFile string, destinationFile string) *bytes.Buffer}// ScenarioHelper lets the caller specify the path to Lambda handler functions.type ScenarioHelper struct {\tHandlerPath string}// Pause waits for the specified number of seconds.func (helper *ScenarioHelper) Pause(secs int) {\ttime.Sleep(time.Duration(secs) * time.Second)}// CreateDeploymentPackage creates an AWS Lambda deployment package from a source file. The// deployment package is stored in .zip format in a bytes.Buffer. The buffer can be// used to pass a []byte to Lambda when creating the function.// The specified destinationFile is the name to give the file when it's deployed to Lambda.func (helper *ScenarioHelper) CreateDeploymentPackage(sourceFile string, destinationFile string) *bytes.Buffer {\tvar err error\tbuffer := &bytes.Buffer{}\twriter := zip.NewWriter(buffer)\tzFile, err := writer.Create(destinationFile)\tif err != nil {\t\tlog.Panicf(\"Couldn't create destination archive %v. Here's why: %v\\n\", destinationFile, err)\t}\tsourceBody, err := os.ReadFile(fmt.Sprintf(\"%v/%v\", helper.HandlerPath, sourceFile))\tif err != nil {\t\tlog.Panicf(\"Couldn't read handler source file %v. Here's why: %v\\n\",\t\t\tsourceFile, err)\t} else {\t\t_, err = zFile.Write(sourceBody)\t\tif err != nil {\t\t\tlog.Panicf(\"Couldn't write handler %v to zip archive. Here's why: %v\\n\",\t\t\t\tsourceFile, err)\t\t}\t}\terr = writer.Close()\tif err != nil {\t\tlog.Panicf(\"Couldn't close zip writer. Here's why: %v\\n\", err)\t}\treturn buffer}Create a struct that wraps individual Lambda actions.import (\t\"bytes\"\t\"context\"\t\"encoding/json\"\t\"errors\"\t\"log\"\t\"time\"\t\"github.com/aws/aws-sdk-go-v2/aws\"\t\"github.com/aws/aws-sdk-go-v2/service/lambda\"\t\"github.com/aws/aws-sdk-go-v2/service/lambda/types\")// FunctionWrapper encapsulates function actions used in the examples.// It contains an AWS Lambda service client that is used to perform user actions.type FunctionWrapper struct {\tLambdaClient *lambda.Client}// GetFunction gets data about the Lambda function specified by functionName.func (wrapper FunctionWrapper) GetFunction(ctx context.Context, functionName string) types.State {\tvar state types.State\tfuncOutput, err := wrapper.LambdaClient.GetFunction(ctx, &lambda.GetFunctionInput{\t\tFunctionName: aws.String(functionName),\t})\tif err != nil {\t\tlog.Panicf(\"Couldn't get function %v. Here's why: %v\\n\", functionName, err)\t} else {\t\tstate = funcOutput.Configuration.State\t}\treturn state}// CreateFunction creates a new Lambda function from code contained in the zipPackage// buffer. The specified handlerName must match the name of the file and function// contained in the uploaded code. The role specified by iamRoleArn is assumed by// Lambda and grants specific permissions.// When the function already exists, types.StateActive is returned.// When the function is created, a lambda.FunctionActiveV2Waiter is used to wait until the// function is active.func (wrapper FunctionWrapper) CreateFunction(ctx context.Context, functionName string, handlerName string,\tiamRoleArn *string, zipPackage *bytes.Buffer) types.State {\tvar state types.State\t_, err := wrapper.LambdaClient.CreateFunction(ctx, &lambda.CreateFunctionInput{\t\tCode:         &types.FunctionCode{ZipFile: zipPackage.Bytes()},\t\tFunctionName: aws.String(functionName),\t\tRole:         iamRoleArn,\t\tHandler:      aws.String(handlerName),\t\tPublish:      true,\t\tRuntime:      types.RuntimePython39,\t})\tif err != nil {\t\tvar resConflict *types.ResourceConflictException\t\tif errors.As(err, &resConflict) {\t\t\tlog.Printf(\"Function %v already exists.\\n\", functionName)\t\t\tstate = types.StateActive\t\t} else {\t\t\tlog.Panicf(\"Couldn't create function %v. Here's why: %v\\n\", functionName, err)\t\t}\t} else {\t\twaiter := lambda.NewFunctionActiveV2Waiter(wrapper.LambdaClient)\t\tfuncOutput, err := waiter.WaitForOutput(ctx, &lambda.GetFunctionInput{\t\t\tFunctionName: aws.String(functionName)}, 1*time.Minute)\t\tif err != nil {\t\t\tlog.Panicf(\"Couldn't wait for function %v to be active. Here's why: %v\\n\", functionName, err)\t\t} else {\t\t\tstate = funcOutput.Configuration.State\t\t}\t}\treturn state}// UpdateFunctionCode updates the code for the Lambda function specified by functionName.// The existing code for the Lambda function is entirely replaced by the code in the// zipPackage buffer. After the update action is called, a lambda.FunctionUpdatedV2Waiter// is used to wait until the update is successful.func (wrapper FunctionWrapper) UpdateFunctionCode(ctx context.Context, functionName string, zipPackage *bytes.Buffer) types.State {\tvar state types.State\t_, err := wrapper.LambdaClient.UpdateFunctionCode(ctx, &lambda.UpdateFunctionCodeInput{\t\tFunctionName: aws.String(functionName), ZipFile: zipPackage.Bytes(),\t})\tif err != nil {\t\tlog.Panicf(\"Couldn't update code for function %v. Here's why: %v\\n\", functionName, err)\t} else {\t\twaiter := lambda.NewFunctionUpdatedV2Waiter(wrapper.LambdaClient)\t\tfuncOutput, err := waiter.WaitForOutput(ctx, &lambda.GetFunctionInput{\t\t\tFunctionName: aws.String(functionName)}, 1*time.Minute)\t\tif err != nil {\t\t\tlog.Panicf(\"Couldn't wait for function %v to be active. Here's why: %v\\n\", functionName, err)\t\t} else {\t\t\tstate = funcOutput.Configuration.State\t\t}\t}\treturn state}// UpdateFunctionConfiguration updates a map of environment variables configured for// the Lambda function specified by functionName.func (wrapper FunctionWrapper) UpdateFunctionConfiguration(ctx context.Context, functionName string, envVars map[string]string) {\t_, err := wrapper.LambdaClient.UpdateFunctionConfiguration(ctx, &lambda.UpdateFunctionConfigurationInput{\t\tFunctionName: aws.String(functionName),\t\tEnvironment:  &types.Environment{Variables: envVars},\t})\tif err != nil {\t\tlog.Panicf(\"Couldn't update configuration for %v. Here's why: %v\", functionName, err)\t}}// ListFunctions lists up to maxItems functions for the account. This function uses a// lambda.ListFunctionsPaginator to paginate the results.func (wrapper FunctionWrapper) ListFunctions(ctx context.Context, maxItems int) []types.FunctionConfiguration {\tvar functions []types.FunctionConfiguration\tpaginator := lambda.NewListFunctionsPaginator(wrapper.LambdaClient, &lambda.ListFunctionsInput{\t\tMaxItems: aws.Int32(int32(maxItems)),\t})\tfor paginator.HasMorePages() && len(functions) < maxItems {\t\tpageOutput, err := paginator.NextPage(ctx)\t\tif err != nil {\t\t\tlog.Panicf(\"Couldn't list functions for your account. Here's why: %v\\n\", err)\t\t}\t\tfunctions = append(functions, pageOutput.Functions...)\t}\treturn functions}// DeleteFunction deletes the Lambda function specified by functionName.func (wrapper FunctionWrapper) DeleteFunction(ctx context.Context, functionName string) {\t_, err := wrapper.LambdaClient.DeleteFunction(ctx, &lambda.DeleteFunctionInput{\t\tFunctionName: aws.String(functionName),\t})\tif err != nil {\t\tlog.Panicf(\"Couldn't delete function %v. Here's why: %v\\n\", functionName, err)\t}}// Invoke invokes the Lambda function specified by functionName, passing the parameters// as a JSON payload. When getLog is true, types.LogTypeTail is specified, which tells// Lambda to include the last few log lines in the returned result.func (wrapper FunctionWrapper) Invoke(ctx context.Context, functionName string, parameters any, getLog bool) *lambda.InvokeOutput {\tlogType := types.LogTypeNone\tif getLog {\t\tlogType = types.LogTypeTail\t}\tpayload, err := json.Marshal(parameters)\tif err != nil {\t\tlog.Panicf(\"Couldn't marshal parameters to JSON. Here's why %v\\n\", err)\t}\tinvokeOutput, err := wrapper.LambdaClient.Invoke(ctx, &lambda.InvokeInput{\t\tFunctionName: aws.String(functionName),\t\tLogType:      logType,\t\tPayload:      payload,\t})\tif err != nil {\t\tlog.Panicf(\"Couldn't invoke function %v. Here's why: %v\\n\", functionName, err)\t}\treturn invokeOutput}// IncrementParameters is used to serialize parameters to the increment Lambda handler.type IncrementParameters struct {\tAction string `json:\"action\"`\tNumber int    `json:\"number\"`}// CalculatorParameters is used to serialize parameters to the calculator Lambda handler.type CalculatorParameters struct {\tAction string `json:\"action\"`\tX      int    `json:\"x\"`\tY      int    `json:\"y\"`}// LambdaResultInt is used to deserialize an int result from a Lambda handler.type LambdaResultInt struct {\tResult int `json:\"result\"`}// LambdaResultFloat is used to deserialize a float32 result from a Lambda handler.type LambdaResultFloat struct {\tResult float32 `json:\"result\"`}Define a Lambda handler that increments a number.import logginglogger = logging.getLogger()logger.setLevel(logging.INFO)def lambda_handler(event, context):    \"\"\"    Accepts an action and a single number, performs the specified action on the number,    and returns the result. The only allowable action is 'increment'.    :param event: The event dict that contains the parameters sent when the function                  is invoked.    :param context: The context in which the function is called.    :return: The result of the action.    \"\"\"    result = None    action = event.get(\"action\")    if action == \"increment\":        result = event.get(\"number\", 0) + 1        logger.info(\"Calculated result of %s\", result)    else:        logger.error(\"%s is not a valid action.\", action)    response = {\"result\": result}    return responseDefine a second Lambda handler that performs arithmetic operations.import loggingimport oslogger = logging.getLogger()# Define a list of Python lambda functions that are called by this AWS Lambda function.ACTIONS = {    \"plus\": lambda x, y: x + y,    \"minus\": lambda x, y: x - y,    \"times\": lambda x, y: x * y,    \"divided-by\": lambda x, y: x / y,}def lambda_handler(event, context):    \"\"\"    Accepts an action and two numbers, performs the specified action on the numbers,    and returns the result.    :param event: The event dict that contains the parameters sent when the function                  is invoked.    :param context: The context in which the function is called.    :return: The result of the specified action.    \"\"\"    # Set the log level based on a variable configured in the Lambda environment.    logger.setLevel(os.environ.get(\"LOG_LEVEL\", logging.INFO))    logger.debug(\"Event: %s\", event)    action = event.get(\"action\")    func = ACTIONS.get(action)    x = event.get(\"x\")    y = event.get(\"y\")    result = None    try:        if func is not None and x is not None and y is not None:            result = func(x, y)            logger.info(\"%s %s %s is %s\", x, action, y, result)        else:            logger.error(\"I can't calculate %s %s %s.\", x, action, y)    except ZeroDivisionError:        logger.warning(\"I can't divide %s by 0!\", x)    response = {\"result\": result}    return responseFor API details, see the following topics in AWS SDK for Go API Reference.CreateFunctionDeleteFunctionGetFunctionInvokeListFunctionsUpdateFunctionCodeUpdateFunctionConfigurationJavaSDK for Java 2.xNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        AWS Code            Examples Repository.    /* *  Lambda function names appear as: * *  arn:aws:lambda:us-west-2:335556666777:function:HelloFunction * *  To find this value, look at the function in the AWS Management Console. * *  Before running this Java code example, set up your development environment, including your credentials. * *  For more information, see this documentation topic: * *  https://docs.aws.amazon.com/sdk-for-java/latest/developer-guide/get-started.html * *  This example performs the following tasks: * * 1. Creates an AWS Lambda function. * 2. Gets a specific AWS Lambda function. * 3. Lists all Lambda functions. * 4. Invokes a Lambda function. * 5. Updates the Lambda function code and invokes it again. * 6. Updates a Lambda function's configuration value. * 7. Deletes a Lambda function. */public class LambdaScenario {    public static final String DASHES = new String(new char[80]).replace(\"\\0\", \"-\");    public static void main(String[] args) throws InterruptedException {        final String usage = \"\"\"            Usage:                <functionName> <role> <handler> <bucketName> <key>\\s            Where:                functionName - The name of the Lambda function.\\s                role - The AWS Identity and Access Management (IAM) service role that has Lambda permissions.\\s                handler - The fully qualified method name (for example, example.Handler::handleRequest).\\s                bucketName - The Amazon Simple Storage Service (Amazon S3) bucket name that contains the .zip or .jar used to update the Lambda function's code.\\s                key - The Amazon S3 key name that represents the .zip or .jar (for example, LambdaHello-1.0-SNAPSHOT.jar).                \"\"\";        if (args.length != 5) {              System.out.println(usage);              return;        }        String functionName = args[0];        String role = args[1];        String handler = args[2];        String bucketName = args[3];        String key = args[4];        LambdaClient awsLambda = LambdaClient.builder()            .build();        System.out.println(DASHES);        System.out.println(\"Welcome to the AWS Lambda Basics scenario.\");        System.out.println(DASHES);        System.out.println(DASHES);        System.out.println(\"1. Create an AWS Lambda function.\");        String funArn = createLambdaFunction(awsLambda, functionName, key, bucketName, role, handler);        System.out.println(\"The AWS Lambda ARN is \" + funArn);        System.out.println(DASHES);        System.out.println(DASHES);        System.out.println(\"2. Get the \" + functionName + \" AWS Lambda function.\");        getFunction(awsLambda, functionName);        System.out.println(DASHES);        System.out.println(DASHES);        System.out.println(\"3. List all AWS Lambda functions.\");        listFunctions(awsLambda);        System.out.println(DASHES);        System.out.println(DASHES);        System.out.println(\"4. Invoke the Lambda function.\");        System.out.println(\"*** Sleep for 1 min to get Lambda function ready.\");        Thread.sleep(60000);        invokeFunction(awsLambda, functionName);        System.out.println(DASHES);        System.out.println(DASHES);        System.out.println(\"5. Update the Lambda function code and invoke it again.\");        updateFunctionCode(awsLambda, functionName, bucketName, key);        System.out.println(\"*** Sleep for 1 min to get Lambda function ready.\");        Thread.sleep(60000);        invokeFunction(awsLambda, functionName);        System.out.println(DASHES);        System.out.println(DASHES);        System.out.println(\"6. Update a Lambda function's configuration value.\");        updateFunctionConfiguration(awsLambda, functionName, handler);        System.out.println(DASHES);        System.out.println(DASHES);        System.out.println(\"7. Delete the AWS Lambda function.\");        LambdaScenario.deleteLambdaFunction(awsLambda, functionName);        System.out.println(DASHES);        System.out.println(DASHES);        System.out.println(\"The AWS Lambda scenario completed successfully\");        System.out.println(DASHES);        awsLambda.close();    }    /**     * Creates a new Lambda function in AWS using the AWS Lambda Java API.     *     * @param awsLambda    the AWS Lambda client used to interact with the AWS Lambda service     * @param functionName the name of the Lambda function to create     * @param key          the S3 key of the function code     * @param bucketName   the name of the S3 bucket containing the function code     * @param role         the IAM role to assign to the Lambda function     * @param handler      the fully qualified class name of the function handler     * @return the Amazon Resource Name (ARN) of the created Lambda function     */    public static String createLambdaFunction(LambdaClient awsLambda,                                              String functionName,                                              String key,                                              String bucketName,                                              String role,                                              String handler) {        try {            LambdaWaiter waiter = awsLambda.waiter();            FunctionCode code = FunctionCode.builder()                .s3Key(key)                .s3Bucket(bucketName)                .build();            CreateFunctionRequest functionRequest = CreateFunctionRequest.builder()                .functionName(functionName)                .description(\"Created by the Lambda Java API\")                .code(code)                .handler(handler)                .runtime(Runtime.JAVA17)                .role(role)                .build();            // Create a Lambda function using a waiter            CreateFunctionResponse functionResponse = awsLambda.createFunction(functionRequest);            GetFunctionRequest getFunctionRequest = GetFunctionRequest.builder()                .functionName(functionName)                .build();            WaiterResponse<GetFunctionResponse> waiterResponse = waiter.waitUntilFunctionExists(getFunctionRequest);            waiterResponse.matched().response().ifPresent(System.out::println);            return functionResponse.functionArn();        } catch (LambdaException e) {            System.err.println(e.getMessage());            System.exit(1);        }        return \"\";    }    /**     * Retrieves information about an AWS Lambda function.     *     * @param awsLambda    an instance of the {@link LambdaClient} class, which is used to interact with the AWS Lambda service     * @param functionName the name of the AWS Lambda function to retrieve information about     */    public static void getFunction(LambdaClient awsLambda, String functionName) {        try {            GetFunctionRequest functionRequest = GetFunctionRequest.builder()                .functionName(functionName)                .build();            GetFunctionResponse response = awsLambda.getFunction(functionRequest);            System.out.println(\"The runtime of this Lambda function is \" + response.configuration().runtime());        } catch (LambdaException e) {            System.err.println(e.getMessage());            System.exit(1);        }    }    /**     * Lists the AWS Lambda functions associated with the current AWS account.     *     * @param awsLambda an instance of the {@link LambdaClient} class, which is used to interact with the AWS Lambda service     *     * @throws LambdaException if an error occurs while interacting with the AWS Lambda service     */    public static void listFunctions(LambdaClient awsLambda) {        try {            ListFunctionsResponse functionResult = awsLambda.listFunctions();            List<FunctionConfiguration> list = functionResult.functions();            for (FunctionConfiguration config : list) {                System.out.println(\"The function name is \" + config.functionName());            }        } catch (LambdaException e) {            System.err.println(e.getMessage());            System.exit(1);        }    }    /**     * Invokes a specific AWS Lambda function.     *     * @param awsLambda    an instance of {@link LambdaClient} to interact with the AWS Lambda service     * @param functionName the name of the AWS Lambda function to be invoked     */    public static void invokeFunction(LambdaClient awsLambda, String functionName) {        InvokeResponse res;        try {            // Need a SdkBytes instance for the payload.            JSONObject jsonObj = new JSONObject();            jsonObj.put(\"inputValue\", \"2000\");            String json = jsonObj.toString();            SdkBytes payload = SdkBytes.fromUtf8String(json);            InvokeRequest request = InvokeRequest.builder()                .functionName(functionName)                .payload(payload)                .build();            res = awsLambda.invoke(request);            String value = res.payload().asUtf8String();            System.out.println(value);        } catch (LambdaException e) {            System.err.println(e.getMessage());            System.exit(1);        }    }    /**     * Updates the code for an AWS Lambda function.     *     * @param awsLambda  the AWS Lambda client     * @param functionName the name of the Lambda function to update     * @param bucketName the name of the S3 bucket where the function code is located     * @param key the key (file name) of the function code in the S3 bucket     * @throws LambdaException if there is an error updating the function code     */    public static void updateFunctionCode(LambdaClient awsLambda, String functionName, String bucketName, String key) {        try {            LambdaWaiter waiter = awsLambda.waiter();            UpdateFunctionCodeRequest functionCodeRequest = UpdateFunctionCodeRequest.builder()                .functionName(functionName)                .publish(true)                .s3Bucket(bucketName)                .s3Key(key)                .build();            UpdateFunctionCodeResponse response = awsLambda.updateFunctionCode(functionCodeRequest);            GetFunctionConfigurationRequest getFunctionConfigRequest = GetFunctionConfigurationRequest.builder()                .functionName(functionName)                .build();            WaiterResponse<GetFunctionConfigurationResponse> waiterResponse = waiter                .waitUntilFunctionUpdated(getFunctionConfigRequest);            waiterResponse.matched().response().ifPresent(System.out::println);            System.out.println(\"The last modified value is \" + response.lastModified());        } catch (LambdaException e) {            System.err.println(e.getMessage());            System.exit(1);        }    }    /**     * Updates the configuration of an AWS Lambda function.     *     * @param awsLambda     the {@link LambdaClient} instance to use for the AWS Lambda operation     * @param functionName  the name of the AWS Lambda function to update     * @param handler       the new handler for the AWS Lambda function     *     * @throws LambdaException if there is an error while updating the function configuration     */    public static void updateFunctionConfiguration(LambdaClient awsLambda, String functionName, String handler) {        try {            UpdateFunctionConfigurationRequest configurationRequest = UpdateFunctionConfigurationRequest.builder()                .functionName(functionName)                .handler(handler)                .runtime(Runtime.JAVA17)                .build();            awsLambda.updateFunctionConfiguration(configurationRequest);        } catch (LambdaException e) {            System.err.println(e.getMessage());            System.exit(1);        }    }    /**     * Deletes an AWS Lambda function.     *     * @param awsLambda     an instance of the {@link LambdaClient} class, which is used to interact with the AWS Lambda service     * @param functionName  the name of the Lambda function to be deleted     *     * @throws LambdaException if an error occurs while deleting the Lambda function     */    public static void deleteLambdaFunction(LambdaClient awsLambda, String functionName) {        try {            DeleteFunctionRequest request = DeleteFunctionRequest.builder()                .functionName(functionName)                .build();            awsLambda.deleteFunction(request);            System.out.println(\"The \" + functionName + \" function was deleted\");        } catch (LambdaException e) {            System.err.println(e.getMessage());            System.exit(1);        }    }}For API details, see the following topics in AWS SDK for Java 2.x API Reference.CreateFunctionDeleteFunctionGetFunctionInvokeListFunctionsUpdateFunctionCodeUpdateFunctionConfigurationJavaScriptSDK for JavaScript (v3)Note        There's more on GitHub. Find the complete example and learn how to set up and run in the        AWS Code            Examples Repository.    Create an AWS Identity and Access Management (IAM) role that grants Lambda permission to write to logs.    logger.log(`Creating role (${NAME_ROLE_LAMBDA})...`);    const response = await createRole(NAME_ROLE_LAMBDA);import { AttachRolePolicyCommand, IAMClient } from \"@aws-sdk/client-iam\";const client = new IAMClient({});/** * * @param {string} policyArn * @param {string} roleName */export const attachRolePolicy = (policyArn, roleName) => {  const command = new AttachRolePolicyCommand({    PolicyArn: policyArn,    RoleName: roleName,  });  return client.send(command);};Create a Lambda function and upload handler code.const createFunction = async (funcName, roleArn) => {  const client = new LambdaClient({});  const code = await readFile(`${dirname}../functions/${funcName}.zip`);  const command = new CreateFunctionCommand({    Code: { ZipFile: code },    FunctionName: funcName,    Role: roleArn,    Architectures: [Architecture.arm64],    Handler: \"index.handler\", // Required when sending a .zip file    PackageType: PackageType.Zip, // Required when sending a .zip file    Runtime: Runtime.nodejs16x, // Required when sending a .zip file  });  return client.send(command);};Invoke the function with a single parameter and get results.const invoke = async (funcName, payload) => {  const client = new LambdaClient({});  const command = new InvokeCommand({    FunctionName: funcName,    Payload: JSON.stringify(payload),    LogType: LogType.Tail,  });  const { Payload, LogResult } = await client.send(command);  const result = Buffer.from(Payload).toString();  const logs = Buffer.from(LogResult, \"base64\").toString();  return { logs, result };};Update the function code and configure its Lambda environment with an environment variable.const updateFunctionCode = async (funcName, newFunc) => {  const client = new LambdaClient({});  const code = await readFile(`${dirname}../functions/${newFunc}.zip`);  const command = new UpdateFunctionCodeCommand({    ZipFile: code,    FunctionName: funcName,    Architectures: [Architecture.arm64],    Handler: \"index.handler\", // Required when sending a .zip file    PackageType: PackageType.Zip, // Required when sending a .zip file    Runtime: Runtime.nodejs16x, // Required when sending a .zip file  });  return client.send(command);};const updateFunctionConfiguration = (funcName) => {  const client = new LambdaClient({});  const config = readFileSync(`${dirname}../functions/config.json`).toString();  const command = new UpdateFunctionConfigurationCommand({    ...JSON.parse(config),    FunctionName: funcName,  });  const result = client.send(command);  waitForFunctionUpdated({ FunctionName: funcName });  return result;};List the functions for your account.const listFunctions = () => {  const client = new LambdaClient({});  const command = new ListFunctionsCommand({});  return client.send(command);};Delete the IAM role and the Lambda function.import { DeleteRoleCommand, IAMClient } from \"@aws-sdk/client-iam\";const client = new IAMClient({});/** * * @param {string} roleName */export const deleteRole = (roleName) => {  const command = new DeleteRoleCommand({ RoleName: roleName });  return client.send(command);};/** * @param {string} funcName */const deleteFunction = (funcName) => {  const client = new LambdaClient({});  const command = new DeleteFunctionCommand({ FunctionName: funcName });  return client.send(command);};For API details, see the following topics in AWS SDK for JavaScript API Reference.CreateFunctionDeleteFunctionGetFunctionInvokeListFunctionsUpdateFunctionCodeUpdateFunctionConfigurationKotlinSDK for KotlinNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        AWS Code            Examples Repository.    suspend fun main(args: Array<String>) {    val usage = \"\"\"        Usage:            <functionName> <role> <handler> <bucketName> <updatedBucketName> <key>         Where:            functionName - The name of the AWS Lambda function.             role - The AWS Identity and Access Management (IAM) service role that has AWS Lambda permissions.             handler - The fully qualified method name (for example, example.Handler::handleRequest).             bucketName - The Amazon Simple Storage Service (Amazon S3) bucket name that contains the ZIP or JAR used for the Lambda function's code.            updatedBucketName - The Amazon S3 bucket name that contains the .zip or .jar used to update the Lambda function's code.             key - The Amazon S3 key name that represents the .zip or .jar file (for example, LambdaHello-1.0-SNAPSHOT.jar).            \"\"\"    if (args.size != 6) {        println(usage)        exitProcess(1)    }    val functionName = args[0]    val role = args[1]    val handler = args[2]    val bucketName = args[3]    val updatedBucketName = args[4]    val key = args[5]    println(\"Creating a Lambda function named $functionName.\")    val funArn = createScFunction(functionName, bucketName, key, handler, role)    println(\"The AWS Lambda ARN is $funArn\")    // Get a specific Lambda function.    println(\"Getting the $functionName AWS Lambda function.\")    getFunction(functionName)    // List the Lambda functions.    println(\"Listing all AWS Lambda functions.\")    listFunctionsSc()    // Invoke the Lambda function.    println(\"*** Invoke the Lambda function.\")    invokeFunctionSc(functionName)    // Update the AWS Lambda function code.    println(\"*** Update the Lambda function code.\")    updateFunctionCode(functionName, updatedBucketName, key)    // println(\"*** Invoke the function again after updating the code.\")    invokeFunctionSc(functionName)    // Update the AWS Lambda function configuration.    println(\"Update the run time of the function.\")    updateFunctionConfiguration(functionName, handler)    // Delete the AWS Lambda function.    println(\"Delete the AWS Lambda function.\")    delFunction(functionName)}suspend fun createScFunction(    myFunctionName: String,    s3BucketName: String,    myS3Key: String,    myHandler: String,    myRole: String,): String {    val functionCode =        FunctionCode {            s3Bucket = s3BucketName            s3Key = myS3Key        }    val request =        CreateFunctionRequest {            functionName = myFunctionName            code = functionCode            description = \"Created by the Lambda Kotlin API\"            handler = myHandler            role = myRole            runtime = Runtime.Java8        }    // Create a Lambda function using a waiter    LambdaClient { region = \"us-west-2\" }.use { awsLambda ->        val functionResponse = awsLambda.createFunction(request)        awsLambda.waitUntilFunctionActive {            functionName = myFunctionName        }        return functionResponse.functionArn.toString()    }}suspend fun getFunction(functionNameVal: String) {    val functionRequest =        GetFunctionRequest {            functionName = functionNameVal        }    LambdaClient { region = \"us-west-2\" }.use { awsLambda ->        val response = awsLambda.getFunction(functionRequest)        println(\"The runtime of this Lambda function is ${response.configuration?.runtime}\")    }}suspend fun listFunctionsSc() {    val request =        ListFunctionsRequest {            maxItems = 10        }    LambdaClient { region = \"us-west-2\" }.use { awsLambda ->        val response = awsLambda.listFunctions(request)        response.functions?.forEach { function ->            println(\"The function name is ${function.functionName}\")        }    }}suspend fun invokeFunctionSc(functionNameVal: String) {    val json = \"\"\"{\"inputValue\":\"1000\"}\"\"\"    val byteArray = json.trimIndent().encodeToByteArray()    val request =        InvokeRequest {            functionName = functionNameVal            payload = byteArray            logType = LogType.Tail        }    LambdaClient { region = \"us-west-2\" }.use { awsLambda ->        val res = awsLambda.invoke(request)        println(\"The function payload is ${res.payload?.toString(Charsets.UTF_8)}\")    }}suspend fun updateFunctionCode(    functionNameVal: String?,    bucketName: String?,    key: String?,) {    val functionCodeRequest =        UpdateFunctionCodeRequest {            functionName = functionNameVal            publish = true            s3Bucket = bucketName            s3Key = key        }    LambdaClient { region = \"us-west-2\" }.use { awsLambda ->        val response = awsLambda.updateFunctionCode(functionCodeRequest)        awsLambda.waitUntilFunctionUpdated {            functionName = functionNameVal        }        println(\"The last modified value is \" + response.lastModified)    }}suspend fun updateFunctionConfiguration(    functionNameVal: String?,    handlerVal: String?,) {    val configurationRequest =        UpdateFunctionConfigurationRequest {            functionName = functionNameVal            handler = handlerVal            runtime = Runtime.Java11        }    LambdaClient { region = \"us-west-2\" }.use { awsLambda ->        awsLambda.updateFunctionConfiguration(configurationRequest)    }}suspend fun delFunction(myFunctionName: String) {    val request =        DeleteFunctionRequest {            functionName = myFunctionName        }    LambdaClient { region = \"us-west-2\" }.use { awsLambda ->        awsLambda.deleteFunction(request)        println(\"$myFunctionName was deleted\")    }}For API details, see the following topics in AWS SDK for Kotlin API reference.CreateFunctionDeleteFunctionGetFunctionInvokeListFunctionsUpdateFunctionCodeUpdateFunctionConfigurationPHPSDK for PHPNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        AWS Code            Examples Repository.    namespace Lambda;use Aws\\S3\\S3Client;use GuzzleHttp\\Psr7\\Stream;use Iam\\IAMService;class GettingStartedWithLambda{    public function run()    {        echo(\"\\n\");        echo(\"--------------------------------------\\n\");        print(\"Welcome to the AWS Lambda getting started demo using PHP!\\n\");        echo(\"--------------------------------------\\n\");        $clientArgs = [            'region' => 'us-west-2',            'version' => 'latest',            'profile' => 'default',        ];        $uniqid = uniqid();        $iamService = new IAMService();        $s3client = new S3Client($clientArgs);        $lambdaService = new LambdaService();        echo \"First, let's create a role to run our Lambda code.\\n\";        $roleName = \"test-lambda-role-$uniqid\";        $rolePolicyDocument = \"{            \\\"Version\\\": \\\"2012-10-17\\\",            \\\"Statement\\\": [                {                    \\\"Effect\\\": \\\"Allow\\\",                    \\\"Principal\\\": {                        \\\"Service\\\": \\\"lambda.amazonaws.com\\\"                    },                    \\\"Action\\\": \\\"sts:AssumeRole\\\"                }            ]        }\";        $role = $iamService->createRole($roleName, $rolePolicyDocument);        echo \"Created role {$role['RoleName']}.\\n\";        $iamService->attachRolePolicy(            $role['RoleName'],            \"arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole\"        );        echo \"Attached the AWSLambdaBasicExecutionRole to {$role['RoleName']}.\\n\";        echo \"\\nNow let's create an S3 bucket and upload our Lambda code there.\\n\";        $bucketName = \"test-example-bucket-$uniqid\";        $s3client->createBucket([            'Bucket' => $bucketName,        ]);        echo \"Created bucket $bucketName.\\n\";        $functionName = \"doc_example_lambda_$uniqid\";        $codeBasic = __DIR__ . \"/lambda_handler_basic.zip\";        $handler = \"lambda_handler_basic\";        $file = file_get_contents($codeBasic);        $s3client->putObject([            'Bucket' => $bucketName,            'Key' => $functionName,            'Body' => $file,        ]);        echo \"Uploaded the Lambda code.\\n\";        $createLambdaFunction = $lambdaService->createFunction($functionName, $role, $bucketName, $handler);        // Wait until the function has finished being created.        do {            $getLambdaFunction = $lambdaService->getFunction($createLambdaFunction['FunctionName']);        } while ($getLambdaFunction['Configuration']['State'] == \"Pending\");        echo \"Created Lambda function {$getLambdaFunction['Configuration']['FunctionName']}.\\n\";        sleep(1);        echo \"\\nOk, let's invoke that Lambda code.\\n\";        $basicParams = [            'action' => 'increment',            'number' => 3,        ];        /** @var Stream $invokeFunction */        $invokeFunction = $lambdaService->invoke($functionName, $basicParams)['Payload'];        $result = json_decode($invokeFunction->getContents())->result;        echo \"After invoking the Lambda code with the input of {$basicParams['number']} we received $result.\\n\";        echo \"\\nSince that's working, let's update the Lambda code.\\n\";        $codeCalculator = \"lambda_handler_calculator.zip\";        $handlerCalculator = \"lambda_handler_calculator\";        echo \"First, put the new code into the S3 bucket.\\n\";        $file = file_get_contents($codeCalculator);        $s3client->putObject([            'Bucket' => $bucketName,            'Key' => $functionName,            'Body' => $file,        ]);        echo \"New code uploaded.\\n\";        $lambdaService->updateFunctionCode($functionName, $bucketName, $functionName);        // Wait for the Lambda code to finish updating.        do {            $getLambdaFunction = $lambdaService->getFunction($createLambdaFunction['FunctionName']);        } while ($getLambdaFunction['Configuration']['LastUpdateStatus'] !== \"Successful\");        echo \"New Lambda code uploaded.\\n\";        $environment = [            'Variable' => ['Variables' => ['LOG_LEVEL' => 'DEBUG']],        ];        $lambdaService->updateFunctionConfiguration($functionName, $handlerCalculator, $environment);        do {            $getLambdaFunction = $lambdaService->getFunction($createLambdaFunction['FunctionName']);        } while ($getLambdaFunction['Configuration']['LastUpdateStatus'] !== \"Successful\");        echo \"Lambda code updated with new handler and a LOG_LEVEL of DEBUG for more information.\\n\";        echo \"Invoke the new code with some new data.\\n\";        $calculatorParams = [            'action' => 'plus',            'x' => 5,            'y' => 4,        ];        $invokeFunction = $lambdaService->invoke($functionName, $calculatorParams, \"Tail\");        $result = json_decode($invokeFunction['Payload']->getContents())->result;        echo \"Indeed, {$calculatorParams['x']} + {$calculatorParams['y']} does equal $result.\\n\";        echo \"Here's the extra debug info: \";        echo base64_decode($invokeFunction['LogResult']) . \"\\n\";        echo \"\\nBut what happens if you try to divide by zero?\\n\";        $divZeroParams = [            'action' => 'divide',            'x' => 5,            'y' => 0,        ];        $invokeFunction = $lambdaService->invoke($functionName, $divZeroParams, \"Tail\");        $result = json_decode($invokeFunction['Payload']->getContents())->result;        echo \"You get a |$result| result.\\n\";        echo \"And an error message: \";        echo base64_decode($invokeFunction['LogResult']) . \"\\n\";        echo \"\\nHere's all the Lambda functions you have in this Region:\\n\";        $listLambdaFunctions = $lambdaService->listFunctions(5);        $allLambdaFunctions = $listLambdaFunctions['Functions'];        $next = $listLambdaFunctions->get('NextMarker');        while ($next != false) {            $listLambdaFunctions = $lambdaService->listFunctions(5, $next);            $next = $listLambdaFunctions->get('NextMarker');            $allLambdaFunctions = array_merge($allLambdaFunctions, $listLambdaFunctions['Functions']);        }        foreach ($allLambdaFunctions as $function) {            echo \"{$function['FunctionName']}\\n\";        }        echo \"\\n\\nAnd don't forget to clean up your data!\\n\";        $lambdaService->deleteFunction($functionName);        echo \"Deleted Lambda function.\\n\";        $iamService->deleteRole($role['RoleName']);        echo \"Deleted Role.\\n\";        $deleteObjects = $s3client->listObjectsV2([            'Bucket' => $bucketName,        ]);        $deleteObjects = $s3client->deleteObjects([            'Bucket' => $bucketName,            'Delete' => [                'Objects' => $deleteObjects['Contents'],            ]        ]);        echo \"Deleted all objects from the S3 bucket.\\n\";        $s3client->deleteBucket(['Bucket' => $bucketName]);        echo \"Deleted the bucket.\\n\";    }}For API details, see the following topics in AWS SDK for PHP API Reference.CreateFunctionDeleteFunctionGetFunctionInvokeListFunctionsUpdateFunctionCodeUpdateFunctionConfigurationPythonSDK for Python (Boto3)Note        There's more on GitHub. Find the complete example and learn how to set up and run in the        AWS Code            Examples Repository.    Define a Lambda handler that increments a number.import logginglogger = logging.getLogger()logger.setLevel(logging.INFO)def lambda_handler(event, context):    \"\"\"    Accepts an action and a single number, performs the specified action on the number,    and returns the result. The only allowable action is 'increment'.    :param event: The event dict that contains the parameters sent when the function                  is invoked.    :param context: The context in which the function is called.    :return: The result of the action.    \"\"\"    result = None    action = event.get(\"action\")    if action == \"increment\":        result = event.get(\"number\", 0) + 1        logger.info(\"Calculated result of %s\", result)    else:        logger.error(\"%s is not a valid action.\", action)    response = {\"result\": result}    return responseDefine a second Lambda handler that performs arithmetic operations.import loggingimport oslogger = logging.getLogger()# Define a list of Python lambda functions that are called by this AWS Lambda function.ACTIONS = {    \"plus\": lambda x, y: x + y,    \"minus\": lambda x, y: x - y,    \"times\": lambda x, y: x * y,    \"divided-by\": lambda x, y: x / y,}def lambda_handler(event, context):    \"\"\"    Accepts an action and two numbers, performs the specified action on the numbers,    and returns the result.    :param event: The event dict that contains the parameters sent when the function                  is invoked.    :param context: The context in which the function is called.    :return: The result of the specified action.    \"\"\"    # Set the log level based on a variable configured in the Lambda environment.    logger.setLevel(os.environ.get(\"LOG_LEVEL\", logging.INFO))    logger.debug(\"Event: %s\", event)    action = event.get(\"action\")    func = ACTIONS.get(action)    x = event.get(\"x\")    y = event.get(\"y\")    result = None    try:        if func is not None and x is not None and y is not None:            result = func(x, y)            logger.info(\"%s %s %s is %s\", x, action, y, result)        else:            logger.error(\"I can't calculate %s %s %s.\", x, action, y)    except ZeroDivisionError:        logger.warning(\"I can't divide %s by 0!\", x)    response = {\"result\": result}    return responseCreate functions that wrap Lambda actions.class LambdaWrapper:    def __init__(self, lambda_client, iam_resource):        self.lambda_client = lambda_client        self.iam_resource = iam_resource    @staticmethod    def create_deployment_package(source_file, destination_file):        \"\"\"        Creates a Lambda deployment package in .zip format in an in-memory buffer. This        buffer can be passed directly to Lambda when creating the function.        :param source_file: The name of the file that contains the Lambda handler                            function.        :param destination_file: The name to give the file when it's deployed to Lambda.        :return: The deployment package.        \"\"\"        buffer = io.BytesIO()        with zipfile.ZipFile(buffer, \"w\") as zipped:            zipped.write(source_file, destination_file)        buffer.seek(0)        return buffer.read()    def get_iam_role(self, iam_role_name):        \"\"\"        Get an AWS Identity and Access Management (IAM) role.        :param iam_role_name: The name of the role to retrieve.        :return: The IAM role.        \"\"\"        role = None        try:            temp_role = self.iam_resource.Role(iam_role_name)            temp_role.load()            role = temp_role            logger.info(\"Got IAM role %s\", role.name)        except ClientError as err:            if err.response[\"Error\"][\"Code\"] == \"NoSuchEntity\":                logger.info(\"IAM role %s does not exist.\", iam_role_name)            else:                logger.error(                    \"Couldn't get IAM role %s. Here's why: %s: %s\",                    iam_role_name,                    err.response[\"Error\"][\"Code\"],                    err.response[\"Error\"][\"Message\"],                )                raise        return role    def create_iam_role_for_lambda(self, iam_role_name):        \"\"\"        Creates an IAM role that grants the Lambda function basic permissions. If a        role with the specified name already exists, it is used for the demo.        :param iam_role_name: The name of the role to create.        :return: The role and a value that indicates whether the role is newly created.        \"\"\"        role = self.get_iam_role(iam_role_name)        if role is not None:            return role, False        lambda_assume_role_policy = {            \"Version\": \"2012-10-17\",            \"Statement\": [                {                    \"Effect\": \"Allow\",                    \"Principal\": {\"Service\": \"lambda.amazonaws.com\"},                    \"Action\": \"sts:AssumeRole\",                }            ],        }        policy_arn = \"arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole\"        try:            role = self.iam_resource.create_role(                RoleName=iam_role_name,                AssumeRolePolicyDocument=json.dumps(lambda_assume_role_policy),            )            logger.info(\"Created role %s.\", role.name)            role.attach_policy(PolicyArn=policy_arn)            logger.info(\"Attached basic execution policy to role %s.\", role.name)        except ClientError as error:            if error.response[\"Error\"][\"Code\"] == \"EntityAlreadyExists\":                role = self.iam_resource.Role(iam_role_name)                logger.warning(\"The role %s already exists. Using it.\", iam_role_name)            else:                logger.exception(                    \"Couldn't create role %s or attach policy %s.\",                    iam_role_name,                    policy_arn,                )                raise        return role, True    def get_function(self, function_name):        \"\"\"        Gets data about a Lambda function.        :param function_name: The name of the function.        :return: The function data.        \"\"\"        response = None        try:            response = self.lambda_client.get_function(FunctionName=function_name)        except ClientError as err:            if err.response[\"Error\"][\"Code\"] == \"ResourceNotFoundException\":                logger.info(\"Function %s does not exist.\", function_name)            else:                logger.error(                    \"Couldn't get function %s. Here's why: %s: %s\",                    function_name,                    err.response[\"Error\"][\"Code\"],                    err.response[\"Error\"][\"Message\"],                )                raise        return response    def create_function(        self, function_name, handler_name, iam_role, deployment_package    ):        \"\"\"        Deploys a Lambda function.        :param function_name: The name of the Lambda function.        :param handler_name: The fully qualified name of the handler function. This                             must include the file name and the function name.        :param iam_role: The IAM role to use for the function.        :param deployment_package: The deployment package that contains the function                                   code in .zip format.        :return: The Amazon Resource Name (ARN) of the newly created function.        \"\"\"        try:            response = self.lambda_client.create_function(                FunctionName=function_name,                Description=\"AWS Lambda doc example\",                Runtime=\"python3.9\",                Role=iam_role.arn,                Handler=handler_name,                Code={\"ZipFile\": deployment_package},                Publish=True,            )            function_arn = response[\"FunctionArn\"]            waiter = self.lambda_client.get_waiter(\"function_active_v2\")            waiter.wait(FunctionName=function_name)            logger.info(                \"Created function '%s' with ARN: '%s'.\",                function_name,                response[\"FunctionArn\"],            )        except ClientError:            logger.error(\"Couldn't create function %s.\", function_name)            raise        else:            return function_arn    def delete_function(self, function_name):        \"\"\"        Deletes a Lambda function.        :param function_name: The name of the function to delete.        \"\"\"        try:            self.lambda_client.delete_function(FunctionName=function_name)        except ClientError:            logger.exception(\"Couldn't delete function %s.\", function_name)            raise    def invoke_function(self, function_name, function_params, get_log=False):        \"\"\"        Invokes a Lambda function.        :param function_name: The name of the function to invoke.        :param function_params: The parameters of the function as a dict. This dict                                is serialized to JSON before it is sent to Lambda.        :param get_log: When true, the last 4 KB of the execution log are included in                        the response.        :return: The response from the function invocation.        \"\"\"        try:            response = self.lambda_client.invoke(                FunctionName=function_name,                Payload=json.dumps(function_params),                LogType=\"Tail\" if get_log else \"None\",            )            logger.info(\"Invoked function %s.\", function_name)        except ClientError:            logger.exception(\"Couldn't invoke function %s.\", function_name)            raise        return response    def update_function_code(self, function_name, deployment_package):        \"\"\"        Updates the code for a Lambda function by submitting a .zip archive that contains        the code for the function.        :param function_name: The name of the function to update.        :param deployment_package: The function code to update, packaged as bytes in                                   .zip format.        :return: Data about the update, including the status.        \"\"\"        try:            response = self.lambda_client.update_function_code(                FunctionName=function_name, ZipFile=deployment_package            )        except ClientError as err:            logger.error(                \"Couldn't update function %s. Here's why: %s: %s\",                function_name,                err.response[\"Error\"][\"Code\"],                err.response[\"Error\"][\"Message\"],            )            raise        else:            return response    def update_function_configuration(self, function_name, env_vars):        \"\"\"        Updates the environment variables for a Lambda function.        :param function_name: The name of the function to update.        :param env_vars: A dict of environment variables to update.        :return: Data about the update, including the status.        \"\"\"        try:            response = self.lambda_client.update_function_configuration(                FunctionName=function_name, Environment={\"Variables\": env_vars}            )        except ClientError as err:            logger.error(                \"Couldn't update function configuration %s. Here's why: %s: %s\",                function_name,                err.response[\"Error\"][\"Code\"],                err.response[\"Error\"][\"Message\"],            )            raise        else:            return response    def list_functions(self):        \"\"\"        Lists the Lambda functions for the current account.        \"\"\"        try:            func_paginator = self.lambda_client.get_paginator(\"list_functions\")            for func_page in func_paginator.paginate():                for func in func_page[\"Functions\"]:                    print(func[\"FunctionName\"])                    desc = func.get(\"Description\")                    if desc:                        print(f\"\\t{desc}\")                    print(f\"\\t{func['Runtime']}: {func['Handler']}\")        except ClientError as err:            logger.error(                \"Couldn't list functions. Here's why: %s: %s\",                err.response[\"Error\"][\"Code\"],                err.response[\"Error\"][\"Message\"],            )            raiseCreate a function that runs the scenario.class UpdateFunctionWaiter(CustomWaiter):    \"\"\"A custom waiter that waits until a function is successfully updated.\"\"\"    def __init__(self, client):        super().__init__(            \"UpdateSuccess\",            \"GetFunction\",            \"Configuration.LastUpdateStatus\",            {\"Successful\": WaitState.SUCCESS, \"Failed\": WaitState.FAILURE},            client,        )    def wait(self, function_name):        self._wait(FunctionName=function_name)def run_scenario(lambda_client, iam_resource, basic_file, calculator_file, lambda_name):    \"\"\"    Runs the scenario.    :param lambda_client: A Boto3 Lambda client.    :param iam_resource: A Boto3 IAM resource.    :param basic_file: The name of the file that contains the basic Lambda handler.    :param calculator_file: The name of the file that contains the calculator Lambda handler.    :param lambda_name: The name to give resources created for the scenario, such as the                        IAM role and the Lambda function.    \"\"\"    logging.basicConfig(level=logging.INFO, format=\"%(levelname)s: %(message)s\")    print(\"-\" * 88)    print(\"Welcome to the AWS Lambda getting started with functions demo.\")    print(\"-\" * 88)    wrapper = LambdaWrapper(lambda_client, iam_resource)    print(\"Checking for IAM role for Lambda...\")    iam_role, should_wait = wrapper.create_iam_role_for_lambda(lambda_name)    if should_wait:        logger.info(\"Giving AWS time to create resources...\")        wait(10)    print(f\"Looking for function {lambda_name}...\")    function = wrapper.get_function(lambda_name)    if function is None:        print(\"Zipping the Python script into a deployment package...\")        deployment_package = wrapper.create_deployment_package(            basic_file, f\"{lambda_name}.py\"        )        print(f\"...and creating the {lambda_name} Lambda function.\")        wrapper.create_function(            lambda_name, f\"{lambda_name}.lambda_handler\", iam_role, deployment_package        )    else:        print(f\"Function {lambda_name} already exists.\")    print(\"-\" * 88)    print(f\"Let's invoke {lambda_name}. This function increments a number.\")    action_params = {        \"action\": \"increment\",        \"number\": q.ask(\"Give me a number to increment: \", q.is_int),    }    print(f\"Invoking {lambda_name}...\")    response = wrapper.invoke_function(lambda_name, action_params)    print(        f\"Incrementing {action_params['number']} resulted in \"        f\"{json.load(response['Payload'])}\"    )    print(\"-\" * 88)    print(f\"Let's update the function to an arithmetic calculator.\")    q.ask(\"Press Enter when you're ready.\")    print(\"Creating a new deployment package...\")    deployment_package = wrapper.create_deployment_package(        calculator_file, f\"{lambda_name}.py\"    )    print(f\"...and updating the {lambda_name} Lambda function.\")    update_waiter = UpdateFunctionWaiter(lambda_client)    wrapper.update_function_code(lambda_name, deployment_package)    update_waiter.wait(lambda_name)    print(f\"This function uses an environment variable to control logging level.\")    print(f\"Let's set it to DEBUG to get the most logging.\")    wrapper.update_function_configuration(        lambda_name, {\"LOG_LEVEL\": logging.getLevelName(logging.DEBUG)}    )    actions = [\"plus\", \"minus\", \"times\", \"divided-by\"]    want_invoke = True    while want_invoke:        print(f\"Let's invoke {lambda_name}. You can invoke these actions:\")        for index, action in enumerate(actions):            print(f\"{index + 1}: {action}\")        action_params = {}        action_index = q.ask(            \"Enter the number of the action you want to take: \",            q.is_int,            q.in_range(1, len(actions)),        )        action_params[\"action\"] = actions[action_index - 1]        print(f\"You've chosen to invoke 'x {action_params['action']} y'.\")        action_params[\"x\"] = q.ask(\"Enter a value for x: \", q.is_int)        action_params[\"y\"] = q.ask(\"Enter a value for y: \", q.is_int)        print(f\"Invoking {lambda_name}...\")        response = wrapper.invoke_function(lambda_name, action_params, True)        print(            f\"Calculating {action_params['x']} {action_params['action']} {action_params['y']} \"            f\"resulted in {json.load(response['Payload'])}\"        )        q.ask(\"Press Enter to see the logs from the call.\")        print(base64.b64decode(response[\"LogResult\"]).decode())        want_invoke = q.ask(\"That was fun. Shall we do it again? (y/n) \", q.is_yesno)    print(\"-\" * 88)    if q.ask(        \"Do you want to list all of the functions in your account? (y/n) \", q.is_yesno    ):        wrapper.list_functions()    print(\"-\" * 88)    if q.ask(\"Ready to delete the function and role? (y/n) \", q.is_yesno):        for policy in iam_role.attached_policies.all():            policy.detach_role(RoleName=iam_role.name)        iam_role.delete()        print(f\"Deleted role {lambda_name}.\")        wrapper.delete_function(lambda_name)        print(f\"Deleted function {lambda_name}.\")    print(\"\\nThanks for watching!\")    print(\"-\" * 88)if __name__ == \"__main__\":    try:        run_scenario(            boto3.client(\"lambda\"),            boto3.resource(\"iam\"),            \"lambda_handler_basic.py\",            \"lambda_handler_calculator.py\",            \"doc_example_lambda_calculator\",        )    except Exception:        logging.exception(\"Something went wrong with the demo!\")For API details, see the following topics in AWS SDK for Python (Boto3) API Reference.CreateFunctionDeleteFunctionGetFunctionInvokeListFunctionsUpdateFunctionCodeUpdateFunctionConfigurationRubySDK for RubyNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        AWS Code            Examples Repository.    Set up pre-requisite IAM permissions for a Lambda function capable of writing logs.  # Get an AWS Identity and Access Management (IAM) role.  #  # @param iam_role_name: The name of the role to retrieve.  # @param action: Whether to create or destroy the IAM apparatus.  # @return: The IAM role.  def manage_iam(iam_role_name, action)    case action    when 'create'      create_iam_role(iam_role_name)    when 'destroy'      destroy_iam_role(iam_role_name)    else      raise \"Incorrect action provided. Must provide 'create' or 'destroy'\"    end  end  private  def create_iam_role(iam_role_name)    role_policy = {      'Version': '2012-10-17',      'Statement': [        {          'Effect': 'Allow',          'Principal': { 'Service': 'lambda.amazonaws.com' },          'Action': 'sts:AssumeRole'        }      ]    }    role = @iam_client.create_role(      role_name: iam_role_name,      assume_role_policy_document: role_policy.to_json    )    @iam_client.attach_role_policy(      {        policy_arn: 'arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole',        role_name: iam_role_name      }    )    wait_for_role_to_exist(iam_role_name)    @logger.debug(\"Successfully created IAM role: #{role['role']['arn']}\")    sleep(10)    [role, role_policy.to_json]  end  def destroy_iam_role(iam_role_name)    @iam_client.detach_role_policy(      {        policy_arn: 'arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole',        role_name: iam_role_name      }    )    @iam_client.delete_role(role_name: iam_role_name)    @logger.debug(\"Detached policy & deleted IAM role: #{iam_role_name}\")  end  def wait_for_role_to_exist(iam_role_name)    @iam_client.wait_until(:role_exists, { role_name: iam_role_name }) do |w|      w.max_attempts = 5      w.delay = 5    end  endDefine a Lambda handler that increments a number provided as an invocation parameter.require 'logger'# A function that increments a whole number by one (1) and logs the result.# Requires a manually-provided runtime parameter, 'number', which must be Int## @param event [Hash] Parameters sent when the function is invoked# @param context [Hash] Methods and properties that provide information# about the invocation, function, and execution environment.# @return incremented_number [String] The incremented number.def lambda_handler(event:, context:)  logger = Logger.new($stdout)  log_level = ENV['LOG_LEVEL']  logger.level = case log_level                 when 'debug'                   Logger::DEBUG                 when 'info'                   Logger::INFO                 else                   Logger::ERROR                 end  logger.debug('This is a debug log message.')  logger.info('This is an info log message. Code executed successfully!')  number = event['number'].to_i  incremented_number = number + 1  logger.info(\"You provided #{number.round} and it was incremented to #{incremented_number.round}\")  incremented_number.round.to_sendZip your Lambda function into a deployment package.  # Creates a Lambda deployment package in .zip format.  #  # @param source_file: The name of the object, without suffix, for the Lambda file and zip.  # @return: The deployment package.  def create_deployment_package(source_file)    Dir.chdir(File.dirname(__FILE__))    if File.exist?('lambda_function.zip')      File.delete('lambda_function.zip')      @logger.debug('Deleting old zip: lambda_function.zip')    end    Zip::File.open('lambda_function.zip', create: true) do |zipfile|      zipfile.add('lambda_function.rb', \"#{source_file}.rb\")    end    @logger.debug(\"Zipping #{source_file}.rb into: lambda_function.zip.\")    File.read('lambda_function.zip').to_s  rescue StandardError => e    @logger.error(\"There was an error creating deployment package:\\n #{e.message}\")  endCreate a new Lambda function.  # Deploys a Lambda function.  #  # @param function_name: The name of the Lambda function.  # @param handler_name: The fully qualified name of the handler function.  # @param role_arn: The IAM role to use for the function.  # @param deployment_package: The deployment package that contains the function code in .zip format.  # @return: The Amazon Resource Name (ARN) of the newly created function.  def create_function(function_name, handler_name, role_arn, deployment_package)    response = @lambda_client.create_function({                                                role: role_arn.to_s,                                                function_name: function_name,                                                handler: handler_name,                                                runtime: 'ruby2.7',                                                code: {                                                  zip_file: deployment_package                                                },                                                environment: {                                                  variables: {                                                    'LOG_LEVEL' => 'info'                                                  }                                                }                                              })    @lambda_client.wait_until(:function_active_v2, { function_name: function_name }) do |w|      w.max_attempts = 5      w.delay = 5    end    response  rescue Aws::Lambda::Errors::ServiceException => e    @logger.error(\"There was an error creating #{function_name}:\\n #{e.message}\")  rescue Aws::Waiters::Errors::WaiterFailed => e    @logger.error(\"Failed waiting for #{function_name} to activate:\\n #{e.message}\")  endInvoke your Lambda function with optional runtime parameters.  # Invokes a Lambda function.  # @param function_name [String] The name of the function to invoke.  # @param payload [nil] Payload containing runtime parameters.  # @return [Object] The response from the function invocation.  def invoke_function(function_name, payload = nil)    params = { function_name: function_name }    params[:payload] = payload unless payload.nil?    @lambda_client.invoke(params)  rescue Aws::Lambda::Errors::ServiceException => e    @logger.error(\"There was an error executing #{function_name}:\\n #{e.message}\")  endUpdate your Lambda function's configuration to inject a new environment variable.  # Updates the environment variables for a Lambda function.  # @param function_name: The name of the function to update.  # @param log_level: The log level of the function.  # @return: Data about the update, including the status.  def update_function_configuration(function_name, log_level)    @lambda_client.update_function_configuration({                                                   function_name: function_name,                                                   environment: {                                                     variables: {                                                       'LOG_LEVEL' => log_level                                                     }                                                   }                                                 })    @lambda_client.wait_until(:function_updated_v2, { function_name: function_name }) do |w|      w.max_attempts = 5      w.delay = 5    end  rescue Aws::Lambda::Errors::ServiceException => e    @logger.error(\"There was an error updating configurations for #{function_name}:\\n #{e.message}\")  rescue Aws::Waiters::Errors::WaiterFailed => e    @logger.error(\"Failed waiting for #{function_name} to activate:\\n #{e.message}\")  endUpdate your Lambda function's code with a different deployment package containing different code.  # Updates the code for a Lambda function by submitting a .zip archive that contains  # the code for the function.  #  # @param function_name: The name of the function to update.  # @param deployment_package: The function code to update, packaged as bytes in  #                            .zip format.  # @return: Data about the update, including the status.  def update_function_code(function_name, deployment_package)    @lambda_client.update_function_code(      function_name: function_name,      zip_file: deployment_package    )    @lambda_client.wait_until(:function_updated_v2, { function_name: function_name }) do |w|      w.max_attempts = 5      w.delay = 5    end  rescue Aws::Lambda::Errors::ServiceException => e    @logger.error(\"There was an error updating function code for: #{function_name}:\\n #{e.message}\")    nil  rescue Aws::Waiters::Errors::WaiterFailed => e    @logger.error(\"Failed waiting for #{function_name} to update:\\n #{e.message}\")  endList all existing Lambda functions using the built-in paginator.  # Lists the Lambda functions for the current account.  def list_functions    functions = []    @lambda_client.list_functions.each do |response|      response['functions'].each do |function|        functions.append(function['function_name'])      end    end    functions  rescue Aws::Lambda::Errors::ServiceException => e    @logger.error(\"There was an error listing functions:\\n #{e.message}\")  endDelete a specific Lambda function.  # Deletes a Lambda function.  # @param function_name: The name of the function to delete.  def delete_function(function_name)    print \"Deleting function: #{function_name}...\"    @lambda_client.delete_function(      function_name: function_name    )    print 'Done!'.green  rescue Aws::Lambda::Errors::ServiceException => e    @logger.error(\"There was an error deleting #{function_name}:\\n #{e.message}\")  endFor API details, see the following topics in AWS SDK for Ruby API Reference.CreateFunctionDeleteFunctionGetFunctionInvokeListFunctionsUpdateFunctionCodeUpdateFunctionConfigurationRustSDK for RustNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        AWS Code            Examples Repository.    The Cargo.toml with dependencies used in this scenario.[package]name = \"lambda-code-examples\"version = \"0.1.0\"edition = \"2021\"# See more keys and their definitions at https://doc.rust-lang.org/cargo/reference/manifest.html[dependencies]aws-config = { version = \"1.0.1\", features = [\"behavior-version-latest\"] }aws-sdk-ec2 = { version = \"1.3.0\" }aws-sdk-iam = { version = \"1.3.0\" }aws-sdk-lambda = { version = \"1.3.0\" }aws-sdk-s3 = { version = \"1.4.0\" }aws-smithy-types = { version = \"1.0.1\" }aws-types = { version = \"1.0.1\" }clap = { version = \"4.4\", features = [\"derive\"] }tokio = { version = \"1.20.1\", features = [\"full\"] }tracing-subscriber = { version = \"0.3.15\", features = [\"env-filter\"] }tracing = \"0.1.37\"serde_json = \"1.0.94\"anyhow = \"1.0.71\"uuid = { version = \"1.3.3\", features = [\"v4\"] }lambda_runtime = \"0.8.0\"serde = \"1.0.164\"A collection of utilities that streamline calling Lambda for this scenario. This file is src/ations.rs in the crate.use anyhow::anyhow;use aws_sdk_iam::operation::{create_role::CreateRoleError, delete_role::DeleteRoleOutput};use aws_sdk_lambda::{    operation::{        delete_function::DeleteFunctionOutput, get_function::GetFunctionOutput,        invoke::InvokeOutput, list_functions::ListFunctionsOutput,        update_function_code::UpdateFunctionCodeOutput,        update_function_configuration::UpdateFunctionConfigurationOutput,    },    primitives::ByteStream,    types::{Environment, FunctionCode, LastUpdateStatus, State},};use aws_sdk_s3::{    error::ErrorMetadata,    operation::{delete_bucket::DeleteBucketOutput, delete_object::DeleteObjectOutput},    types::CreateBucketConfiguration,};use aws_smithy_types::Blob;use serde::{ser::SerializeMap, Serialize};use std::{fmt::Display, path::PathBuf, str::FromStr, time::Duration};use tracing::{debug, info, warn};/* Operation describes  */#[derive(Clone, Copy, Debug, Serialize)]pub enum Operation {    #[serde(rename = \"plus\")]    Plus,    #[serde(rename = \"minus\")]    Minus,    #[serde(rename = \"times\")]    Times,    #[serde(rename = \"divided-by\")]    DividedBy,}impl FromStr for Operation {    type Err = anyhow::Error;    fn from_str(s: &str) -> Result<Self, Self::Err> {        match s {            \"plus\" => Ok(Operation::Plus),            \"minus\" => Ok(Operation::Minus),            \"times\" => Ok(Operation::Times),            \"divided-by\" => Ok(Operation::DividedBy),            _ => Err(anyhow!(\"Unknown operation {s}\")),        }    }}impl Display for Operation {    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {        match self {            Operation::Plus => write!(f, \"plus\"),            Operation::Minus => write!(f, \"minus\"),            Operation::Times => write!(f, \"times\"),            Operation::DividedBy => write!(f, \"divided-by\"),        }    }}/** * InvokeArgs will be serialized as JSON and sent to the AWS Lambda handler. */#[derive(Debug)]pub enum InvokeArgs {    Increment(i32),    Arithmetic(Operation, i32, i32),}impl Serialize for InvokeArgs {    fn serialize<S>(&self, serializer: S) -> Result<S::Ok, S::Error>    where        S: serde::Serializer,    {        match self {            InvokeArgs::Increment(i) => serializer.serialize_i32(*i),            InvokeArgs::Arithmetic(o, i, j) => {                let mut map: S::SerializeMap = serializer.serialize_map(Some(3))?;                map.serialize_key(&\"op\".to_string())?;                map.serialize_value(&o.to_string())?;                map.serialize_key(&\"i\".to_string())?;                map.serialize_value(&i)?;                map.serialize_key(&\"j\".to_string())?;                map.serialize_value(&j)?;                map.end()            }        }    }}/** A policy document allowing Lambda to execute this function on the account's behalf. */const ROLE_POLICY_DOCUMENT: &str = r#\"{    \"Version\": \"2012-10-17\",    \"Statement\": [        {            \"Effect\": \"Allow\",            \"Principal\": { \"Service\": \"lambda.amazonaws.com\" },            \"Action\": \"sts:AssumeRole\"        }    ]}\"#;/** * A LambdaManager gathers all the resources necessary to run the Lambda example scenario. * This includes instantiated aws_sdk clients and details of resource names. */pub struct LambdaManager {    iam_client: aws_sdk_iam::Client,    lambda_client: aws_sdk_lambda::Client,    s3_client: aws_sdk_s3::Client,    lambda_name: String,    role_name: String,    bucket: String,    own_bucket: bool,}// These unit type structs provide nominal typing on top of String parameters for LambdaManager::newpub struct LambdaName(pub String);pub struct RoleName(pub String);pub struct Bucket(pub String);pub struct OwnBucket(pub bool);impl LambdaManager {    pub fn new(        iam_client: aws_sdk_iam::Client,        lambda_client: aws_sdk_lambda::Client,        s3_client: aws_sdk_s3::Client,        lambda_name: LambdaName,        role_name: RoleName,        bucket: Bucket,        own_bucket: OwnBucket,    ) -> Self {        Self {            iam_client,            lambda_client,            s3_client,            lambda_name: lambda_name.0,            role_name: role_name.0,            bucket: bucket.0,            own_bucket: own_bucket.0,        }    }    /**     * Load the AWS configuration from the environment.     * Look up lambda_name and bucket if none are given, or generate a random name if not present in the environment.     * If the bucket name is provided, the caller needs to have created the bucket.     * If the bucket name is generated, it will be created.     */    pub async fn load_from_env(lambda_name: Option<String>, bucket: Option<String>) -> Self {        let sdk_config = aws_config::load_from_env().await;        let lambda_name = LambdaName(lambda_name.unwrap_or_else(|| {            std::env::var(\"LAMBDA_NAME\").unwrap_or_else(|_| \"rust_lambda_example\".to_string())        }));        let role_name = RoleName(format!(\"{}_role\", lambda_name.0));        let (bucket, own_bucket) =            match bucket {                Some(bucket) => (Bucket(bucket), false),                None => (                    Bucket(std::env::var(\"LAMBDA_BUCKET\").unwrap_or_else(|_| {                        format!(\"rust-lambda-example-{}\", uuid::Uuid::new_v4())                    })),                    true,                ),            };        let s3_client = aws_sdk_s3::Client::new(&sdk_config);        if own_bucket {            info!(\"Creating bucket for demo: {}\", bucket.0);            s3_client                .create_bucket()                .bucket(bucket.0.clone())                .create_bucket_configuration(                    CreateBucketConfiguration::builder()                        .location_constraint(aws_sdk_s3::types::BucketLocationConstraint::from(                            sdk_config.region().unwrap().as_ref(),                        ))                        .build(),                )                .send()                .await                .unwrap();        }        Self::new(            aws_sdk_iam::Client::new(&sdk_config),            aws_sdk_lambda::Client::new(&sdk_config),            s3_client,            lambda_name,            role_name,            bucket,            OwnBucket(own_bucket),        )    }    /**     * Upload function code from a path to a zip file.     * The zip file must have an AL2 Linux-compatible binary called `bootstrap`.     * The easiest way to create such a zip is to use `cargo lambda build --output-format Zip`.     */    async fn prepare_function(        &self,        zip_file: PathBuf,        key: Option<String>,    ) -> Result<FunctionCode, anyhow::Error> {        let body = ByteStream::from_path(zip_file).await?;        let key = key.unwrap_or_else(|| format!(\"{}_code\", self.lambda_name));        info!(\"Uploading function code to s3://{}/{}\", self.bucket, key);        let _ = self            .s3_client            .put_object()            .bucket(self.bucket.clone())            .key(key.clone())            .body(body)            .send()            .await?;        Ok(FunctionCode::builder()            .s3_bucket(self.bucket.clone())            .s3_key(key)            .build())    }    /**     * Create a function, uploading from a zip file.     */    pub async fn create_function(&self, zip_file: PathBuf) -> Result<String, anyhow::Error> {        let code = self.prepare_function(zip_file, None).await?;        let key = code.s3_key().unwrap().to_string();        let role = self.create_role().await.map_err(|e| anyhow!(e))?;        info!(\"Created iam role, waiting 15s for it to become active\");        tokio::time::sleep(Duration::from_secs(15)).await;        info!(\"Creating lambda function {}\", self.lambda_name);        let _ = self            .lambda_client            .create_function()            .function_name(self.lambda_name.clone())            .code(code)            .role(role.arn())            .runtime(aws_sdk_lambda::types::Runtime::Providedal2)            .handler(\"_unused\")            .send()            .await            .map_err(anyhow::Error::from)?;        self.wait_for_function_ready().await?;        self.lambda_client            .publish_version()            .function_name(self.lambda_name.clone())            .send()            .await?;        Ok(key)    }    /**     * Create an IAM execution role for the managed Lambda function.     * If the role already exists, use that instead.     */    async fn create_role(&self) -> Result<aws_sdk_iam::types::Role, CreateRoleError> {        info!(\"Creating execution role for function\");        let get_role = self            .iam_client            .get_role()            .role_name(self.role_name.clone())            .send()            .await;        if let Ok(get_role) = get_role {            if let Some(role) = get_role.role {                return Ok(role);            }        }        let create_role = self            .iam_client            .create_role()            .role_name(self.role_name.clone())            .assume_role_policy_document(ROLE_POLICY_DOCUMENT)            .send()            .await;        match create_role {            Ok(create_role) => match create_role.role {                Some(role) => Ok(role),                None => Err(CreateRoleError::generic(                    ErrorMetadata::builder()                        .message(\"CreateRole returned empty success\")                        .build(),                )),            },            Err(err) => Err(err.into_service_error()),        }    }    /**     * Poll `is_function_ready` with a 1-second delay. It returns when the function is ready or when there's an error checking the function's state.     */    pub async fn wait_for_function_ready(&self) -> Result<(), anyhow::Error> {        info!(\"Waiting for function\");        while !self.is_function_ready(None).await? {            info!(\"Function is not ready, sleeping 1s\");            tokio::time::sleep(Duration::from_secs(1)).await;        }        Ok(())    }    /**     * Check if a Lambda function is ready to be invoked.     * A Lambda function is ready for this scenario when its state is active and its LastUpdateStatus is Successful.     * Additionally, if a sha256 is provided, the function must have that as its current code hash.     * Any missing properties or failed requests will be reported as an Err.     */    async fn is_function_ready(        &self,        expected_code_sha256: Option<&str>,    ) -> Result<bool, anyhow::Error> {        match self.get_function().await {            Ok(func) => {                if let Some(config) = func.configuration() {                    if let Some(state) = config.state() {                        info!(?state, \"Checking if function is active\");                        if !matches!(state, State::Active) {                            return Ok(false);                        }                    }                    match config.last_update_status() {                        Some(last_update_status) => {                            info!(?last_update_status, \"Checking if function is ready\");                            match last_update_status {                                LastUpdateStatus::Successful => {                                    // continue                                }                                LastUpdateStatus::Failed | LastUpdateStatus::InProgress => {                                    return Ok(false);                                }                                unknown => {                                    warn!(                                        status_variant = unknown.as_str(),                                        \"LastUpdateStatus unknown\"                                    );                                    return Err(anyhow!(                                        \"Unknown LastUpdateStatus, fn config is {config:?}\"                                    ));                                }                            }                        }                        None => {                            warn!(\"Missing last update status\");                            return Ok(false);                        }                    };                    if expected_code_sha256.is_none() {                        return Ok(true);                    }                    if let Some(code_sha256) = config.code_sha256() {                        return Ok(code_sha256 == expected_code_sha256.unwrap_or_default());                    }                }            }            Err(e) => {                warn!(?e, \"Could not get function while waiting\");            }        }        Ok(false)    }    /** Get the Lambda function with this Manager's name. */    pub async fn get_function(&self) -> Result<GetFunctionOutput, anyhow::Error> {        info!(\"Getting lambda function\");        self.lambda_client            .get_function()            .function_name(self.lambda_name.clone())            .send()            .await            .map_err(anyhow::Error::from)    }    /** List all Lambda functions in the current Region. */    pub async fn list_functions(&self) -> Result<ListFunctionsOutput, anyhow::Error> {        info!(\"Listing lambda functions\");        self.lambda_client            .list_functions()            .send()            .await            .map_err(anyhow::Error::from)    }    /** Invoke the lambda function using calculator InvokeArgs. */    pub async fn invoke(&self, args: InvokeArgs) -> Result<InvokeOutput, anyhow::Error> {        info!(?args, \"Invoking {}\", self.lambda_name);        let payload = serde_json::to_string(&args)?;        debug!(?payload, \"Sending payload\");        self.lambda_client            .invoke()            .function_name(self.lambda_name.clone())            .payload(Blob::new(payload))            .send()            .await            .map_err(anyhow::Error::from)    }    /** Given a Path to a zip file, update the function's code and wait for the update to finish. */    pub async fn update_function_code(        &self,        zip_file: PathBuf,        key: String,    ) -> Result<UpdateFunctionCodeOutput, anyhow::Error> {        let function_code = self.prepare_function(zip_file, Some(key)).await?;        info!(\"Updating code for {}\", self.lambda_name);        let update = self            .lambda_client            .update_function_code()            .function_name(self.lambda_name.clone())            .s3_bucket(self.bucket.clone())            .s3_key(function_code.s3_key().unwrap().to_string())            .send()            .await            .map_err(anyhow::Error::from)?;        self.wait_for_function_ready().await?;        Ok(update)    }    /** Update the environment for a function. */    pub async fn update_function_configuration(        &self,        environment: Environment,    ) -> Result<UpdateFunctionConfigurationOutput, anyhow::Error> {        info!(            ?environment,            \"Updating environment for {}\", self.lambda_name        );        let updated = self            .lambda_client            .update_function_configuration()            .function_name(self.lambda_name.clone())            .environment(environment)            .send()            .await            .map_err(anyhow::Error::from)?;        self.wait_for_function_ready().await?;        Ok(updated)    }    /** Delete a function and its role, and if possible or necessary, its associated code object and bucket. */    pub async fn delete_function(        &self,        location: Option<String>,    ) -> (        Result<DeleteFunctionOutput, anyhow::Error>,        Result<DeleteRoleOutput, anyhow::Error>,        Option<Result<DeleteObjectOutput, anyhow::Error>>,    ) {        info!(\"Deleting lambda function {}\", self.lambda_name);        let delete_function = self            .lambda_client            .delete_function()            .function_name(self.lambda_name.clone())            .send()            .await            .map_err(anyhow::Error::from);        info!(\"Deleting iam role {}\", self.role_name);        let delete_role = self            .iam_client            .delete_role()            .role_name(self.role_name.clone())            .send()            .await            .map_err(anyhow::Error::from);        let delete_object: Option<Result<DeleteObjectOutput, anyhow::Error>> =            if let Some(location) = location {                info!(\"Deleting object {location}\");                Some(                    self.s3_client                        .delete_object()                        .bucket(self.bucket.clone())                        .key(location)                        .send()                        .await                        .map_err(anyhow::Error::from),                )            } else {                info!(?location, \"Skipping delete object\");                None            };        (delete_function, delete_role, delete_object)    }    pub async fn cleanup(        &self,        location: Option<String>,    ) -> (        (            Result<DeleteFunctionOutput, anyhow::Error>,            Result<DeleteRoleOutput, anyhow::Error>,            Option<Result<DeleteObjectOutput, anyhow::Error>>,        ),        Option<Result<DeleteBucketOutput, anyhow::Error>>,    ) {        let delete_function = self.delete_function(location).await;        let delete_bucket = if self.own_bucket {            info!(\"Deleting bucket {}\", self.bucket);            if delete_function.2.is_none() || delete_function.2.as_ref().unwrap().is_ok() {                Some(                    self.s3_client                        .delete_bucket()                        .bucket(self.bucket.clone())                        .send()                        .await                        .map_err(anyhow::Error::from),                )            } else {                None            }        } else {            info!(\"No bucket to clean up\");            None        };        (delete_function, delete_bucket)    }}/** * Testing occurs primarily as an integration test running the `scenario` bin successfully. * Each action relies deeply on the internal workings and state of Amazon Simple Storage Service (Amazon S3), Lambda, and IAM working together. * It is therefore infeasible to mock the clients to test the individual actions. */#[cfg(test)]mod test {    use super::{InvokeArgs, Operation};    use serde_json::json;    /** Make sure that the JSON output of serializing InvokeArgs is what's expected by the calculator. */    #[test]    fn test_serialize() {        assert_eq!(json!(InvokeArgs::Increment(5)), 5);        assert_eq!(            json!(InvokeArgs::Arithmetic(Operation::Plus, 5, 7)).to_string(),            r#\"{\"op\":\"plus\",\"i\":5,\"j\":7}\"#.to_string(),        );    }}A binary to run the scenario from front to end, using command line flags to control some behavior. This file is src/bin/scenario.rs in the crate./*## Service actionsService actions wrap the SDK call, taking a client and any specific parameters necessary for the call.* CreateFunction* GetFunction* ListFunctions* Invoke* UpdateFunctionCode* UpdateFunctionConfiguration* DeleteFunction## ScenarioA scenario runs at a command prompt and prints output to the user on the result of each service action. A scenario can run in one of two ways: straight through, printing out progress as it goes, or as an interactive question/answer script.## Getting started with functionsUse an SDK to manage AWS Lambda functions: create a function, invoke it, update its code, invoke it again, view its output and logs, and delete it.This scenario uses two Lambda handlers:_Note: Handlers don't use AWS SDK API calls._The increment handler is straightforward:1. It accepts a number, increments it, and returns the new value.2. It performs simple logging of the result.The arithmetic handler is more complex:1. It accepts a set of actions ['plus', 'minus', 'times', 'divided-by'] and two numbers, and returns the result of the calculation.2. It uses an environment variable to control log level (such as DEBUG, INFO, WARNING, ERROR).It logs a few things at different levels, such as:    * DEBUG: Full event data.    * INFO: The calculation result.    * WARN~ING~: When a divide by zero error occurs.    * This will be the typical `RUST_LOG` variable.The steps of the scenario are:1. Create an AWS Identity and Access Management (IAM) role that meets the following requirements:    * Has an assume_role policy that grants 'lambda.amazonaws.com' the 'sts:AssumeRole' action.    * Attaches the 'arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole' managed role.    * _You must wait for ~10 seconds after the role is created before you can use it!_2. Create a function (CreateFunction) for the increment handler by packaging it as a zip and doing one of the following:    * Adding it with CreateFunction Code.ZipFile.    * --or--    * Uploading it to Amazon Simple Storage Service (Amazon S3) and adding it with CreateFunction Code.S3Bucket/S3Key.    * _Note: Zipping the file does not have to be done in code._    * If you have a waiter, use it to wait until the function is active. Otherwise, call GetFunction until State is Active.3. Invoke the function with a number and print the result.4. Update the function (UpdateFunctionCode) to the arithmetic handler by packaging it as a zip and doing one of the following:    * Adding it with UpdateFunctionCode ZipFile.    * --or--    * Uploading it to Amazon S3 and adding it with UpdateFunctionCode S3Bucket/S3Key.5. Call GetFunction until Configuration.LastUpdateStatus is 'Successful' (or 'Failed').6. Update the environment variable by calling UpdateFunctionConfiguration and pass it a log level, such as:    * Environment={'Variables': {'RUST_LOG': 'TRACE'}}7. Invoke the function with an action from the list and a couple of values. Include LogType='Tail' to get logs in the result. Print the result of the calculation and the log.8. [Optional] Invoke the function to provoke a divide-by-zero error and show the log result.9. List all functions for the account, using pagination (ListFunctions).10. Delete the function (DeleteFunction).11. Delete the role.Each step should use the function created in Service Actions to abstract calling the SDK. */use aws_sdk_lambda::{operation::invoke::InvokeOutput, types::Environment};use clap::Parser;use std::{collections::HashMap, path::PathBuf};use tracing::{debug, info, warn};use tracing_subscriber::EnvFilter;use lambda_code_examples::actions::{    InvokeArgs::{Arithmetic, Increment},    LambdaManager, Operation,};#[derive(Debug, Parser)]pub struct Opt {    /// The AWS Region.    #[structopt(short, long)]    pub region: Option<String>,    // The bucket to use for the FunctionCode.    #[structopt(short, long)]    pub bucket: Option<String>,    // The name of the Lambda function.    #[structopt(short, long)]    pub lambda_name: Option<String>,    // The number to increment.    #[structopt(short, long, default_value = \"12\")]    pub inc: i32,    // The left operand.    #[structopt(long, default_value = \"19\")]    pub num_a: i32,    // The right operand.    #[structopt(long, default_value = \"23\")]    pub num_b: i32,    // The arithmetic operation.    #[structopt(short, long, default_value = \"plus\")]    pub operation: Operation,    #[structopt(long)]    pub cleanup: Option<bool>,    #[structopt(long)]    pub no_cleanup: Option<bool>,}fn code_path(lambda: &str) -> PathBuf {    PathBuf::from(format!(\"../target/lambda/{lambda}/bootstrap.zip\"))}fn log_invoke_output(invoke: &InvokeOutput, message: &str) {    if let Some(payload) = invoke.payload().cloned() {        let payload = String::from_utf8(payload.into_inner());        info!(?payload, message);    } else {        info!(\"Could not extract payload\")    }    if let Some(logs) = invoke.log_result() {        debug!(?logs, \"Invoked function logs\")    } else {        debug!(\"Invoked function had no logs\")    }}async fn main_block(    opt: &Opt,    manager: &LambdaManager,    code_location: String,) -> Result<(), anyhow::Error> {    let invoke = manager.invoke(Increment(opt.inc)).await?;    log_invoke_output(&invoke, \"Invoked function configured as increment\");    let update_code = manager        .update_function_code(code_path(\"arithmetic\"), code_location.clone())        .await?;    let code_sha256 = update_code.code_sha256().unwrap_or(\"Unknown SHA\");    info!(?code_sha256, \"Updated function code with arithmetic.zip\");    let arithmetic_args = Arithmetic(opt.operation, opt.num_a, opt.num_b);    let invoke = manager.invoke(arithmetic_args).await?;    log_invoke_output(&invoke, \"Invoked function configured as arithmetic\");    let update = manager        .update_function_configuration(            Environment::builder()                .set_variables(Some(HashMap::from([(                    \"RUST_LOG\".to_string(),                    \"trace\".to_string(),                )])))                .build(),        )        .await?;    let updated_environment = update.environment();    info!(?updated_environment, \"Updated function configuration\");    let invoke = manager        .invoke(Arithmetic(opt.operation, opt.num_a, opt.num_b))        .await?;    log_invoke_output(        &invoke,        \"Invoked function configured as arithmetic with increased logging\",    );    let invoke = manager        .invoke(Arithmetic(Operation::DividedBy, opt.num_a, 0))        .await?;    log_invoke_output(        &invoke,        \"Invoked function configured as arithmetic with divide by zero\",    );    Ok::<(), anyhow::Error>(())}#[tokio::main]async fn main() {    tracing_subscriber::fmt()        .without_time()        .with_file(true)        .with_line_number(true)        .with_env_filter(EnvFilter::from_default_env())        .init();    let opt = Opt::parse();    let manager = LambdaManager::load_from_env(opt.lambda_name.clone(), opt.bucket.clone()).await;    let key = match manager.create_function(code_path(\"increment\")).await {        Ok(init) => {            info!(?init, \"Created function, initially with increment.zip\");            let run_block = main_block(&opt, &manager, init.clone()).await;            info!(?run_block, \"Finished running example, cleaning up\");            Some(init)        }        Err(err) => {            warn!(?err, \"Error happened when initializing function\");            None        }    };    if Some(false) == opt.cleanup || Some(true) == opt.no_cleanup {        info!(\"Skipping cleanup\")    } else {        let delete = manager.cleanup(key).await;        info!(?delete, \"Deleted function & cleaned up resources\");    }}For API details, see the following topics in AWS SDK for Rust API reference.CreateFunctionDeleteFunctionGetFunctionInvokeListFunctionsUpdateFunctionCodeUpdateFunctionConfigurationSAP ABAPSDK for SAP ABAPNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        AWS Code            Examples Repository.        TRY.        \"Create an AWS Identity and Access Management (IAM) role that grants AWS Lambda permission to write to logs.\"        DATA(lv_policy_document) = `{` &&            `\"Version\":\"2012-10-17\",` &&                  `\"Statement\": [` &&                    `{` &&                      `\"Effect\": \"Allow\",` &&                      `\"Action\": [` &&                        `\"sts:AssumeRole\"` &&                      `],` &&                      `\"Principal\": {` &&                        `\"Service\": [` &&                          `\"lambda.amazonaws.com\"` &&                        `]` &&                      `}` &&                    `}` &&                  `]` &&                `}`.        TRY.            DATA(lo_create_role_output) =  lo_iam->createrole(                    iv_rolename = iv_role_name                    iv_assumerolepolicydocument = lv_policy_document                    iv_description = 'Grant lambda permission to write to logs'                ).            MESSAGE 'IAM role created.' TYPE 'I'.            WAIT UP TO 10 SECONDS.            \" Make sure that the IAM role is ready for use. \"          CATCH /aws1/cx_iamentityalrdyexex.            MESSAGE 'IAM role already exists.' TYPE 'E'.          CATCH /aws1/cx_iaminvalidinputex.            MESSAGE 'The request contains a non-valid parameter.' TYPE 'E'.          CATCH /aws1/cx_iammalformedplydocex.            MESSAGE 'Policy document in the request is malformed.' TYPE 'E'.        ENDTRY.        TRY.            lo_iam->attachrolepolicy(                iv_rolename  = iv_role_name                iv_policyarn = 'arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole'            ).            MESSAGE 'Attached policy to the IAM role.' TYPE 'I'.          CATCH /aws1/cx_iaminvalidinputex.            MESSAGE 'The request contains a non-valid parameter.' TYPE 'E'.          CATCH /aws1/cx_iamnosuchentityex.            MESSAGE 'The requested resource entity does not exist.' TYPE 'E'.          CATCH /aws1/cx_iamplynotattachableex.            MESSAGE 'Service role policies can only be attached to the service-linked role for their service.' TYPE 'E'.          CATCH /aws1/cx_iamunmodableentityex.            MESSAGE 'Service that depends on the service-linked role is not modifiable.' TYPE 'E'.        ENDTRY.        \" Create a Lambda function and upload handler code. \"        \" Lambda function performs 'increment' action on a number. \"        TRY.            lo_lmd->createfunction(                 iv_functionname = iv_function_name                 iv_runtime = `python3.9`                 iv_role = lo_create_role_output->get_role( )->get_arn( )                 iv_handler = iv_handler                 io_code = io_initial_zip_file                 iv_description = 'AWS Lambda code example'             ).            MESSAGE 'Lambda function created.' TYPE 'I'.          CATCH /aws1/cx_lmdcodestorageexcdex.            MESSAGE 'Maximum total code size per account exceeded.' TYPE 'E'.          CATCH /aws1/cx_lmdinvparamvalueex.            MESSAGE 'The request contains a non-valid parameter.' TYPE 'E'.          CATCH /aws1/cx_lmdresourcenotfoundex.            MESSAGE 'The requested resource does not exist.' TYPE 'E'.        ENDTRY.        \" Verify the function is in Active state \"        WHILE lo_lmd->getfunction( iv_functionname = iv_function_name )->get_configuration( )->ask_state( ) <> 'Active'.          IF sy-index = 10.            EXIT.               \" Maximum 10 seconds. \"          ENDIF.          WAIT UP TO 1 SECONDS.        ENDWHILE.        \"Invoke the function with a single parameter and get results.\"        TRY.            DATA(lv_json) = /aws1/cl_rt_util=>string_to_xstring(              `{`  &&                `\"action\": \"increment\",`  &&                `\"number\": 10` &&              `}`            ).            DATA(lo_initial_invoke_output) =  lo_lmd->invoke(                       iv_functionname = iv_function_name                       iv_payload = lv_json                   ).            ov_initial_invoke_payload = lo_initial_invoke_output->get_payload( ).           \" ov_initial_invoke_payload is returned for testing purposes. \"            DATA(lo_writer_json) = cl_sxml_string_writer=>create( type = if_sxml=>co_xt_json ).            CALL TRANSFORMATION id SOURCE XML ov_initial_invoke_payload RESULT XML lo_writer_json.            DATA(lv_result) = cl_abap_codepage=>convert_from( lo_writer_json->get_output( ) ).            MESSAGE 'Lambda function invoked.' TYPE 'I'.          CATCH /aws1/cx_lmdinvparamvalueex.            MESSAGE 'The request contains a non-valid parameter.' TYPE 'E'.          CATCH /aws1/cx_lmdinvrequestcontex.            MESSAGE 'Unable to parse request body as JSON.' TYPE 'E'.          CATCH /aws1/cx_lmdresourcenotfoundex.            MESSAGE 'The requested resource does not exist.' TYPE 'E'.          CATCH /aws1/cx_lmdunsuppedmediatyp00.            MESSAGE 'Invoke request body does not have JSON as its content type.' TYPE 'E'.        ENDTRY.        \" Update the function code and configure its Lambda environment with an environment variable. \"        \" Lambda function is updated to perform 'decrement' action also. \"        TRY.            lo_lmd->updatefunctioncode(                  iv_functionname = iv_function_name                  iv_zipfile = io_updated_zip_file              ).            WAIT UP TO 10 SECONDS.            \" Make sure that the update is completed. \"            MESSAGE 'Lambda function code updated.' TYPE 'I'.          CATCH /aws1/cx_lmdcodestorageexcdex.            MESSAGE 'Maximum total code size per account exceeded.' TYPE 'E'.          CATCH /aws1/cx_lmdinvparamvalueex.            MESSAGE 'The request contains a non-valid parameter.' TYPE 'E'.          CATCH /aws1/cx_lmdresourcenotfoundex.            MESSAGE 'The requested resource does not exist.' TYPE 'E'.        ENDTRY.        TRY.            DATA lt_variables TYPE /aws1/cl_lmdenvironmentvaria00=>tt_environmentvariables.            DATA ls_variable LIKE LINE OF lt_variables.            ls_variable-key = 'LOG_LEVEL'.            ls_variable-value = NEW /aws1/cl_lmdenvironmentvaria00( iv_value = 'info' ).            INSERT ls_variable INTO TABLE lt_variables.            lo_lmd->updatefunctionconfiguration(                  iv_functionname = iv_function_name                  io_environment = NEW /aws1/cl_lmdenvironment( it_variables = lt_variables )              ).            WAIT UP TO 10 SECONDS.            \" Make sure that the update is completed. \"            MESSAGE 'Lambda function configuration/settings updated.' TYPE 'I'.          CATCH /aws1/cx_lmdinvparamvalueex.            MESSAGE 'The request contains a non-valid parameter.' TYPE 'E'.          CATCH /aws1/cx_lmdresourceconflictex.            MESSAGE 'Resource already exists or another operation is in progress.' TYPE 'E'.          CATCH /aws1/cx_lmdresourcenotfoundex.            MESSAGE 'The requested resource does not exist.' TYPE 'E'.        ENDTRY.        \"Invoke the function with new parameters and get results. Display the execution log that's returned from the invocation.\"        TRY.            lv_json = /aws1/cl_rt_util=>string_to_xstring(              `{`  &&                `\"action\": \"decrement\",`  &&                `\"number\": 10` &&              `}`            ).            DATA(lo_updated_invoke_output) =  lo_lmd->invoke(                       iv_functionname = iv_function_name                       iv_payload = lv_json                   ).            ov_updated_invoke_payload = lo_updated_invoke_output->get_payload( ).           \" ov_updated_invoke_payload is returned for testing purposes. \"            lo_writer_json = cl_sxml_string_writer=>create( type = if_sxml=>co_xt_json ).            CALL TRANSFORMATION id SOURCE XML ov_updated_invoke_payload RESULT XML lo_writer_json.            lv_result = cl_abap_codepage=>convert_from( lo_writer_json->get_output( ) ).            MESSAGE 'Lambda function invoked.' TYPE 'I'.          CATCH /aws1/cx_lmdinvparamvalueex.            MESSAGE 'The request contains a non-valid parameter.' TYPE 'E'.          CATCH /aws1/cx_lmdinvrequestcontex.            MESSAGE 'Unable to parse request body as JSON.' TYPE 'E'.          CATCH /aws1/cx_lmdresourcenotfoundex.            MESSAGE 'The requested resource does not exist.' TYPE 'E'.          CATCH /aws1/cx_lmdunsuppedmediatyp00.            MESSAGE 'Invoke request body does not have JSON as its content type.' TYPE 'E'.        ENDTRY.        \" List the functions for your account. \"        TRY.            DATA(lo_list_output) = lo_lmd->listfunctions( ).            DATA(lt_functions) = lo_list_output->get_functions( ).            MESSAGE 'Retrieved list of Lambda functions.' TYPE 'I'.          CATCH /aws1/cx_lmdinvparamvalueex.            MESSAGE 'The request contains a non-valid parameter.' TYPE 'E'.        ENDTRY.        \" Delete the Lambda function. \"        TRY.            lo_lmd->deletefunction( iv_functionname = iv_function_name ).            MESSAGE 'Lambda function deleted.' TYPE 'I'.          CATCH /aws1/cx_lmdinvparamvalueex.            MESSAGE 'The request contains a non-valid parameter.' TYPE 'E'.          CATCH /aws1/cx_lmdresourcenotfoundex.            MESSAGE 'The requested resource does not exist.' TYPE 'E'.        ENDTRY.        \" Detach role policy. \"        TRY.            lo_iam->detachrolepolicy(                iv_rolename  = iv_role_name                iv_policyarn = 'arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole'            ).            MESSAGE 'Detached policy from the IAM role.' TYPE 'I'.          CATCH /aws1/cx_iaminvalidinputex.            MESSAGE 'The request contains a non-valid parameter.' TYPE 'E'.          CATCH /aws1/cx_iamnosuchentityex.            MESSAGE 'The requested resource entity does not exist.' TYPE 'E'.          CATCH /aws1/cx_iamplynotattachableex.            MESSAGE 'Service role policies can only be attached to the service-linked role for their service.' TYPE 'E'.          CATCH /aws1/cx_iamunmodableentityex.            MESSAGE 'Service that depends on the service-linked role is not modifiable.' TYPE 'E'.        ENDTRY.        \" Delete the IAM role. \"        TRY.            lo_iam->deleterole( iv_rolename = iv_role_name ).            MESSAGE 'IAM role deleted.' TYPE 'I'.          CATCH /aws1/cx_iamnosuchentityex.            MESSAGE 'The requested resource entity does not exist.' TYPE 'E'.          CATCH /aws1/cx_iamunmodableentityex.            MESSAGE 'Service that depends on the service-linked role is not modifiable.' TYPE 'E'.        ENDTRY.      CATCH /aws1/cx_rt_service_generic INTO lo_exception.        DATA(lv_error) = lo_exception->get_longtext( ).        MESSAGE lv_error TYPE 'E'.    ENDTRY.For API details, see the following topics in AWS SDK for SAP ABAP API reference.CreateFunctionDeleteFunctionGetFunctionInvokeListFunctionsUpdateFunctionCodeUpdateFunctionConfigurationanchoranchoranchoranchoranchoranchoranchoranchoranchoranchoranchor.NETC++GoJavaJavaScriptKotlinPHPPythonRubyRustSAP ABAPAWS SDK for .NETNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        AWS Code            Examples Repository.    Create methods that perform Lambda actions.namespace LambdaActions;using Amazon.Lambda;using Amazon.Lambda.Model;/// <summary>/// A class that implements AWS Lambda methods./// </summary>public class LambdaWrapper{    private readonly IAmazonLambda _lambdaService;    /// <summary>    /// Constructor for the LambdaWrapper class.    /// </summary>    /// <param name=\"lambdaService\">An initialized Lambda service client.</param>    public LambdaWrapper(IAmazonLambda lambdaService)    {        _lambdaService = lambdaService;    }    /// <summary>    /// Creates a new Lambda function.    /// </summary>    /// <param name=\"functionName\">The name of the function.</param>    /// <param name=\"s3Bucket\">The Amazon Simple Storage Service (Amazon S3)    /// bucket where the zip file containing the code is located.</param>    /// <param name=\"s3Key\">The Amazon S3 key of the zip file.</param>    /// <param name=\"role\">The Amazon Resource Name (ARN) of a role with the    /// appropriate Lambda permissions.</param>    /// <param name=\"handler\">The name of the handler function.</param>    /// <returns>The Amazon Resource Name (ARN) of the newly created    /// Lambda function.</returns>    public async Task<string> CreateLambdaFunctionAsync(        string functionName,        string s3Bucket,        string s3Key,        string role,        string handler)    {        // Defines the location for the function code.        // S3Bucket - The S3 bucket where the file containing        //            the source code is stored.        // S3Key    - The name of the file containing the code.        var functionCode = new FunctionCode        {            S3Bucket = s3Bucket,            S3Key = s3Key,        };        var createFunctionRequest = new CreateFunctionRequest        {            FunctionName = functionName,            Description = \"Created by the Lambda .NET API\",            Code = functionCode,            Handler = handler,            Runtime = Runtime.Dotnet6,            Role = role,        };        var reponse = await _lambdaService.CreateFunctionAsync(createFunctionRequest);        return reponse.FunctionArn;    }    /// <summary>    /// Delete an AWS Lambda function.    /// </summary>    /// <param name=\"functionName\">The name of the Lambda function to    /// delete.</param>    /// <returns>A Boolean value that indicates the success of the action.</returns>    public async Task<bool> DeleteFunctionAsync(string functionName)    {        var request = new DeleteFunctionRequest        {            FunctionName = functionName,        };        var response = await _lambdaService.DeleteFunctionAsync(request);        // A return value of NoContent means that the request was processed.        // In this case, the function was deleted, and the return value        // is intentionally blank.        return response.HttpStatusCode == System.Net.HttpStatusCode.NoContent;    }    /// <summary>    /// Gets information about a Lambda function.    /// </summary>    /// <param name=\"functionName\">The name of the Lambda function for    /// which to retrieve information.</param>    /// <returns>Async Task.</returns>    public async Task<FunctionConfiguration> GetFunctionAsync(string functionName)    {        var functionRequest = new GetFunctionRequest        {            FunctionName = functionName,        };        var response = await _lambdaService.GetFunctionAsync(functionRequest);        return response.Configuration;    }    /// <summary>    /// Invoke a Lambda function.    /// </summary>    /// <param name=\"functionName\">The name of the Lambda function to    /// invoke.</param    /// <param name=\"parameters\">The parameter values that will be passed to the function.</param>    /// <returns>A System Threading Task.</returns>    public async Task<string> InvokeFunctionAsync(        string functionName,        string parameters)    {        var payload = parameters;        var request = new InvokeRequest        {            FunctionName = functionName,            Payload = payload,        };        var response = await _lambdaService.InvokeAsync(request);        MemoryStream stream = response.Payload;        string returnValue = System.Text.Encoding.UTF8.GetString(stream.ToArray());        return returnValue;    }    /// <summary>    /// Get a list of Lambda functions.    /// </summary>    /// <returns>A list of FunctionConfiguration objects.</returns>    public async Task<List<FunctionConfiguration>> ListFunctionsAsync()    {        var functionList = new List<FunctionConfiguration>();        var functionPaginator =            _lambdaService.Paginators.ListFunctions(new ListFunctionsRequest());        await foreach (var function in functionPaginator.Functions)        {            functionList.Add(function);        }        return functionList;    }    /// <summary>    /// Update an existing Lambda function.    /// </summary>    /// <param name=\"functionName\">The name of the Lambda function to update.</param>    /// <param name=\"bucketName\">The bucket where the zip file containing    /// the Lambda function code is stored.</param>    /// <param name=\"key\">The key name of the source code file.</param>    /// <returns>Async Task.</returns>    public async Task UpdateFunctionCodeAsync(        string functionName,        string bucketName,        string key)    {        var functionCodeRequest = new UpdateFunctionCodeRequest        {            FunctionName = functionName,            Publish = true,            S3Bucket = bucketName,            S3Key = key,        };        var response = await _lambdaService.UpdateFunctionCodeAsync(functionCodeRequest);        Console.WriteLine($\"The Function was last modified at {response.LastModified}.\");    }    /// <summary>    /// Update the code of a Lambda function.    /// </summary>    /// <param name=\"functionName\">The name of the function to update.</param>    /// <param name=\"functionHandler\">The code that performs the function's actions.</param>    /// <param name=\"environmentVariables\">A dictionary of environment variables.</param>    /// <returns>A Boolean value indicating the success of the action.</returns>    public async Task<bool> UpdateFunctionConfigurationAsync(        string functionName,        string functionHandler,        Dictionary<string, string> environmentVariables)    {        var request = new UpdateFunctionConfigurationRequest        {            Handler = functionHandler,            FunctionName = functionName,            Environment = new Amazon.Lambda.Model.Environment { Variables = environmentVariables },        };        var response = await _lambdaService.UpdateFunctionConfigurationAsync(request);        Console.WriteLine(response.LastModified);        return response.HttpStatusCode == System.Net.HttpStatusCode.OK;    }}Create a function that runs the scenario.global using System.Threading.Tasks;global using Amazon.IdentityManagement;global using Amazon.Lambda;global using LambdaActions;global using LambdaScenarioCommon;global using Microsoft.Extensions.DependencyInjection;global using Microsoft.Extensions.Hosting;global using Microsoft.Extensions.Logging;global using Microsoft.Extensions.Logging.Console;global using Microsoft.Extensions.Logging.Debug;using Amazon.Lambda.Model;using Microsoft.Extensions.Configuration;namespace LambdaBasics;public class LambdaBasics{    private static ILogger logger = null!;    static async Task Main(string[] args)    {        // Set up dependency injection for the Amazon service.        using var host = Host.CreateDefaultBuilder(args)            .ConfigureLogging(logging =>                logging.AddFilter(\"System\", LogLevel.Debug)                    .AddFilter<DebugLoggerProvider>(\"Microsoft\", LogLevel.Information)                    .AddFilter<ConsoleLoggerProvider>(\"Microsoft\", LogLevel.Trace))            .ConfigureServices((_, services) =>            services.AddAWSService<IAmazonLambda>()            .AddAWSService<IAmazonIdentityManagementService>()            .AddTransient<LambdaWrapper>()            .AddTransient<LambdaRoleWrapper>()            .AddTransient<UIWrapper>()        )        .Build();        var configuration = new ConfigurationBuilder()            .SetBasePath(Directory.GetCurrentDirectory())            .AddJsonFile(\"settings.json\") // Load test settings from .json file.            .AddJsonFile(\"settings.local.json\",            true) // Optionally load local settings.        .Build();        logger = LoggerFactory.Create(builder => { builder.AddConsole(); })            .CreateLogger<LambdaBasics>();        var lambdaWrapper = host.Services.GetRequiredService<LambdaWrapper>();        var lambdaRoleWrapper = host.Services.GetRequiredService<LambdaRoleWrapper>();        var uiWrapper = host.Services.GetRequiredService<UIWrapper>();        string functionName = configuration[\"FunctionName\"]!;        string roleName = configuration[\"RoleName\"]!;        string policyDocument = \"{\" +            \" \\\"Version\\\": \\\"2012-10-17\\\",\" +            \" \\\"Statement\\\": [ \" +            \"    {\" +            \"        \\\"Effect\\\": \\\"Allow\\\",\" +            \"        \\\"Principal\\\": {\" +            \"            \\\"Service\\\": \\\"lambda.amazonaws.com\\\" \" +            \"    },\" +            \"        \\\"Action\\\": \\\"sts:AssumeRole\\\" \" +            \"    }\" +            \"]\" +        \"}\";        var incrementHandler = configuration[\"IncrementHandler\"];        var calculatorHandler = configuration[\"CalculatorHandler\"];        var bucketName = configuration[\"BucketName\"];        var incrementKey = configuration[\"IncrementKey\"];        var calculatorKey = configuration[\"CalculatorKey\"];        var policyArn = configuration[\"PolicyArn\"];        uiWrapper.DisplayLambdaBasicsOverview();        // Create the policy to use with the AWS Lambda functions and then attach the        // policy to a new role.        var roleArn = await lambdaRoleWrapper.CreateLambdaRoleAsync(roleName, policyDocument);        Console.WriteLine(\"Waiting for role to become active.\");        uiWrapper.WaitABit(15, \"Wait until the role is active before trying to use it.\");        // Attach the appropriate AWS Identity and Access Management (IAM) role policy to the new role.        var success = await lambdaRoleWrapper.AttachLambdaRolePolicyAsync(policyArn, roleName);        uiWrapper.WaitABit(10, \"Allow time for the IAM policy to be attached to the role.\");        // Create the Lambda function using a zip file stored in an Amazon Simple Storage Service        // (Amazon S3) bucket.        uiWrapper.DisplayTitle(\"Create Lambda Function\");        Console.WriteLine($\"Creating the AWS Lambda function: {functionName}.\");        var lambdaArn = await lambdaWrapper.CreateLambdaFunctionAsync(            functionName,            bucketName,            incrementKey,            roleArn,            incrementHandler);        Console.WriteLine(\"Waiting for the new function to be available.\");        Console.WriteLine($\"The AWS Lambda ARN is {lambdaArn}\");        // Get the Lambda function.        Console.WriteLine($\"Getting the {functionName} AWS Lambda function.\");        FunctionConfiguration config;        do        {            config = await lambdaWrapper.GetFunctionAsync(functionName);            Console.Write(\".\");        }        while (config.State != State.Active);        Console.WriteLine($\"\\nThe function, {functionName} has been created.\");        Console.WriteLine($\"The runtime of this Lambda function is {config.Runtime}.\");        uiWrapper.PressEnter();        // List the Lambda functions.        uiWrapper.DisplayTitle(\"Listing all Lambda functions.\");        var functions = await lambdaWrapper.ListFunctionsAsync();        DisplayFunctionList(functions);        uiWrapper.DisplayTitle(\"Invoke increment function\");        Console.WriteLine(\"Now that it has been created, invoke the Lambda increment function.\");        string? value;        do        {            Console.Write(\"Enter a value to increment: \");            value = Console.ReadLine();        }        while (string.IsNullOrEmpty(value));        string functionParameters = \"{\" +            \"\\\"action\\\": \\\"increment\\\", \" +            \"\\\"x\\\": \\\"\" + value + \"\\\"\" +        \"}\";        var answer = await lambdaWrapper.InvokeFunctionAsync(functionName, functionParameters);        Console.WriteLine($\"{value} + 1 = {answer}.\");        uiWrapper.DisplayTitle(\"Update function\");        Console.WriteLine(\"Now update the Lambda function code.\");        await lambdaWrapper.UpdateFunctionCodeAsync(functionName, bucketName, calculatorKey);        do        {            config = await lambdaWrapper.GetFunctionAsync(functionName);            Console.Write(\".\");        }        while (config.LastUpdateStatus == LastUpdateStatus.InProgress);        await lambdaWrapper.UpdateFunctionConfigurationAsync(            functionName,            calculatorHandler,            new Dictionary<string, string> { { \"LOG_LEVEL\", \"DEBUG\" } });        do        {            config = await lambdaWrapper.GetFunctionAsync(functionName);            Console.Write(\".\");        }        while (config.LastUpdateStatus == LastUpdateStatus.InProgress);        uiWrapper.DisplayTitle(\"Call updated function\");        Console.WriteLine(\"Now call the updated function...\");        bool done = false;        do        {            string? opSelected;            Console.WriteLine(\"Select the operation to perform:\");            Console.WriteLine(\"\\t1. add\");            Console.WriteLine(\"\\t2. subtract\");            Console.WriteLine(\"\\t3. multiply\");            Console.WriteLine(\"\\t4. divide\");            Console.WriteLine(\"\\tOr enter \\\"q\\\" to quit.\");            Console.WriteLine(\"Enter the number (1, 2, 3, 4, or q) of the operation you want to perform: \");            do            {                Console.Write(\"Your choice? \");                opSelected = Console.ReadLine();            }            while (opSelected == string.Empty);            var operation = (opSelected) switch            {                \"1\" => \"add\",                \"2\" => \"subtract\",                \"3\" => \"multiply\",                \"4\" => \"divide\",                \"q\" => \"quit\",                _ => \"add\",            };            if (operation == \"quit\")            {                done = true;            }            else            {                // Get two numbers and an action from the user.                value = string.Empty;                do                {                    Console.Write(\"Enter the first value: \");                    value = Console.ReadLine();                }                while (value == string.Empty);                string? value2;                do                {                    Console.Write(\"Enter a second value: \");                    value2 = Console.ReadLine();                }                while (value2 == string.Empty);                functionParameters = \"{\" +                    \"\\\"action\\\": \\\"\" + operation + \"\\\", \" +                    \"\\\"x\\\": \\\"\" + value + \"\\\",\" +                    \"\\\"y\\\": \\\"\" + value2 + \"\\\"\" +                \"}\";                answer = await lambdaWrapper.InvokeFunctionAsync(functionName, functionParameters);                Console.WriteLine($\"The answer when we {operation} the two numbers is: {answer}.\");            }            uiWrapper.PressEnter();        } while (!done);        // Delete the function created earlier.        uiWrapper.DisplayTitle(\"Clean up resources\");        // Detach the IAM policy from the IAM role.        Console.WriteLine(\"First detach the IAM policy from the role.\");        success = await lambdaRoleWrapper.DetachLambdaRolePolicyAsync(policyArn, roleName);        uiWrapper.WaitABit(15, \"Let's wait for the policy to be fully detached from the role.\");        Console.WriteLine(\"Delete the AWS Lambda function.\");        success = await lambdaWrapper.DeleteFunctionAsync(functionName);        if (success)        {            Console.WriteLine($\"The {functionName} function was deleted.\");        }        else        {            Console.WriteLine($\"Could not remove the function {functionName}\");        }        // Now delete the IAM role created for use with the functions        // created by the application.        Console.WriteLine(\"Now we can delete the role that we created.\");        success = await lambdaRoleWrapper.DeleteLambdaRoleAsync(roleName);        if (success)        {            Console.WriteLine(\"The role has been successfully removed.\");        }        else        {            Console.WriteLine(\"Couldn't delete the role.\");        }        Console.WriteLine(\"The Lambda Scenario is now complete.\");        uiWrapper.PressEnter();        // Displays a formatted list of existing functions returned by the        // LambdaMethods.ListFunctions.        void DisplayFunctionList(List<FunctionConfiguration> functions)        {            functions.ForEach(functionConfig =>            {                Console.WriteLine($\"{functionConfig.FunctionName}\\t{functionConfig.Description}\");            });        }    }}namespace LambdaActions;using Amazon.IdentityManagement;using Amazon.IdentityManagement.Model;public class LambdaRoleWrapper{    private readonly IAmazonIdentityManagementService _lambdaRoleService;    public LambdaRoleWrapper(IAmazonIdentityManagementService lambdaRoleService)    {        _lambdaRoleService = lambdaRoleService;    }    /// <summary>    /// Attach an AWS Identity and Access Management (IAM) role policy to the    /// IAM role to be assumed by the AWS Lambda functions created for the scenario.    /// </summary>    /// <param name=\"policyArn\">The Amazon Resource Name (ARN) of the IAM policy.</param>    /// <param name=\"roleName\">The name of the IAM role to attach the IAM policy to.</param>    /// <returns>A Boolean value indicating the success of the action.</returns>    public async Task<bool> AttachLambdaRolePolicyAsync(string policyArn, string roleName)    {        var response = await _lambdaRoleService.AttachRolePolicyAsync(new AttachRolePolicyRequest { PolicyArn = policyArn, RoleName = roleName });        return response.HttpStatusCode == System.Net.HttpStatusCode.OK;    }    /// <summary>    /// Create a new IAM role.    /// </summary>    /// <param name=\"roleName\">The name of the IAM role to create.</param>    /// <param name=\"policyDocument\">The policy document for the new IAM role.</param>    /// <returns>A string representing the ARN for newly created role.</returns>    public async Task<string> CreateLambdaRoleAsync(string roleName, string policyDocument)    {        var request = new CreateRoleRequest        {            AssumeRolePolicyDocument = policyDocument,            RoleName = roleName,        };        var response = await _lambdaRoleService.CreateRoleAsync(request);        return response.Role.Arn;    }    /// <summary>    /// Deletes an IAM role.    /// </summary>    /// <param name=\"roleName\">The name of the role to delete.</param>    /// <returns>A Boolean value indicating the success of the operation.</returns>    public async Task<bool> DeleteLambdaRoleAsync(string roleName)    {        var request = new DeleteRoleRequest        {            RoleName = roleName,        };        var response = await _lambdaRoleService.DeleteRoleAsync(request);        return response.HttpStatusCode == System.Net.HttpStatusCode.OK;    }    public async Task<bool> DetachLambdaRolePolicyAsync(string policyArn, string roleName)    {        var response = await _lambdaRoleService.DetachRolePolicyAsync(new DetachRolePolicyRequest { PolicyArn = policyArn, RoleName = roleName });        return response.HttpStatusCode == System.Net.HttpStatusCode.OK;    }}namespace LambdaScenarioCommon;public class UIWrapper{    public readonly string SepBar = new('-', Console.WindowWidth);    /// <summary>    /// Show information about the AWS Lambda Basics scenario.    /// </summary>    public void DisplayLambdaBasicsOverview()    {        Console.Clear();        DisplayTitle(\"Welcome to AWS Lambda Basics\");        Console.WriteLine(\"This example application does the following:\");        Console.WriteLine(\"\\t1. Creates an AWS Identity and Access Management (IAM) role that will be assumed by the functions we create.\");        Console.WriteLine(\"\\t2. Attaches an IAM role policy that has Lambda permissions.\");        Console.WriteLine(\"\\t3. Creates a Lambda function that increments the value passed to it.\");        Console.WriteLine(\"\\t4. Calls the increment function and passes a value.\");        Console.WriteLine(\"\\t5. Updates the code so that the function is a simple calculator.\");        Console.WriteLine(\"\\t6. Calls the calculator function with the values entered.\");        Console.WriteLine(\"\\t7. Deletes the Lambda function.\");        Console.WriteLine(\"\\t7. Detaches the IAM role policy.\");        Console.WriteLine(\"\\t8. Deletes the IAM role.\");        PressEnter();    }    /// <summary>    /// Display a message and wait until the user presses enter.    /// </summary>    public void PressEnter()    {        Console.Write(\"\\nPress <Enter> to continue. \");        _ = Console.ReadLine();        Console.WriteLine();    }    /// <summary>    /// Pad a string with spaces to center it on the console display.    /// </summary>    /// <param name=\"strToCenter\">The string to be centered.</param>    /// <returns>The padded string.</returns>    public string CenterString(string strToCenter)    {        var padAmount = (Console.WindowWidth - strToCenter.Length) / 2;        var leftPad = new string(' ', padAmount);        return $\"{leftPad}{strToCenter}\";    }    /// <summary>    /// Display a line of hyphens, the centered text of the title and another    /// line of hyphens.    /// </summary>    /// <param name=\"strTitle\">The string to be displayed.</param>    public void DisplayTitle(string strTitle)    {        Console.WriteLine(SepBar);        Console.WriteLine(CenterString(strTitle));        Console.WriteLine(SepBar);    }    /// <summary>    /// Display a countdown and wait for a number of seconds.    /// </summary>    /// <param name=\"numSeconds\">The number of seconds to wait.</param>    public void WaitABit(int numSeconds, string msg)    {        Console.WriteLine(msg);        // Wait for the requested number of seconds.        for (int i = numSeconds; i > 0; i--)        {            System.Threading.Thread.Sleep(1000);            Console.Write($\"{i}...\");        }        PressEnter();    }}Define a Lambda handler that increments a number.using Amazon.Lambda.Core;// Assembly attribute to enable the Lambda function's JSON input to be converted into a .NET class.[assembly: LambdaSerializer(typeof(Amazon.Lambda.Serialization.SystemTextJson.DefaultLambdaJsonSerializer))]namespace LambdaIncrement;public class Function{    /// <summary>    /// A simple function increments the integer parameter.    /// </summary>    /// <param name=\"input\">A JSON string containing an action, which must be    /// \"increment\" and a string representing the value to increment.</param>    /// <param name=\"context\">The context object passed by Lambda containing    /// information about invocation, function, and execution environment.</param>    /// <returns>A string representing the incremented value of the parameter.</returns>    public int FunctionHandler(Dictionary<string, string> input, ILambdaContext context)    {        if (input[\"action\"] == \"increment\")        {            int inputValue = Convert.ToInt32(input[\"x\"]);            return inputValue + 1;        }        else        {            return 0;        }    }}Define a second Lambda handler that performs arithmetic operations.using Amazon.Lambda.Core;// Assembly attribute to enable the Lambda function's JSON input to be converted into a .NET class.[assembly: LambdaSerializer(typeof(Amazon.Lambda.Serialization.SystemTextJson.DefaultLambdaJsonSerializer))]namespace LambdaCalculator;public class Function{    /// <summary>    /// A simple function that takes two number in string format and performs    /// the requested arithmetic function.    /// </summary>    /// <param name=\"input\">JSON data containing an action, and x and y values.    /// Valid actions include: add, subtract, multiply, and divide.</param>    /// <param name=\"context\">The context object passed by Lambda containing    /// information about invocation, function, and execution environment.</param>    /// <returns>A string representing the results of the calculation.</returns>    public int FunctionHandler(Dictionary<string, string> input, ILambdaContext context)    {        var action = input[\"action\"];        int x = Convert.ToInt32(input[\"x\"]);        int y = Convert.ToInt32(input[\"y\"]);        int result;        switch (action)        {            case \"add\":                result = x + y;                break;            case \"subtract\":                result = x - y;                break;            case \"multiply\":                result = x * y;                break;            case \"divide\":                if (y == 0)                {                    Console.Error.WriteLine(\"Divide by zero error.\");                    result = 0;                }                else                    result = x / y;                break;            default:                Console.Error.WriteLine($\"{action} is not a valid operation.\");                result = 0;                break;        }        return result;    }}For API details, see the following topics in AWS SDK for .NET API Reference.CreateFunctionDeleteFunctionGetFunctionInvokeListFunctionsUpdateFunctionCodeUpdateFunctionConfiguration",
                            "For a complete list of AWS SDK developer guides and code examples, see    Using Lambda with an AWS SDK.    This topic also includes information about getting started and details about previous SDK versions."
                        ]
                    },
                    {
                        "title": "Actions",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/service_code_examples_actions.html",
                        "contents": [
                            {
                                "title": "CreateAlias",
                                "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_lambda_CreateAlias_section.html",
                                "sections": [
                                    "",
                                    "The following code examples show how to use CreateAlias.",
                                    "CLIAWS CLITo create an alias for a Lambda functionThe following create-alias example creates an alias named LIVE that points to version 1 of the my-function Lambda function.aws lambda create-alias \\    --function-name my-function \\    --description \"alias for live version of function\" \\    --function-version 1 \\    --name LIVEOutput:{    \"FunctionVersion\": \"1\",    \"Name\": \"LIVE\",    \"AliasArn\": \"arn:aws:lambda:us-west-2:123456789012:function:my-function:LIVE\",    \"RevisionId\": \"873282ed-4cd3-4dc8-a069-d0c647e470c6\",    \"Description\": \"alias for live version of function\"}For more information, see Configuring AWS Lambda Function Aliases in the AWS Lambda Developer Guide.                        For API details, see                        CreateAlias                        in AWS CLI Command Reference.                    PowerShellTools for PowerShellExample 1: This example creates a New Lambda Alias for specified version and routing configuration to specify the percentage of invocation requests that it receives.New-LMAlias -FunctionName \"MylambdaFunction123\" -RoutingConfig_AdditionalVersionWeight @{Name=\"1\";Value=\"0.6} -Description \"Alias for version 4\" -FunctionVersion 4 -Name \"PowershellAlias\"                        For API details, see                        CreateAlias                        in AWS Tools for PowerShell Cmdlet Reference.                    anchoranchorCLIPowerShellAWS CLITo create an alias for a Lambda functionThe following create-alias example creates an alias named LIVE that points to version 1 of the my-function Lambda function.aws lambda create-alias \\    --function-name my-function \\    --description \"alias for live version of function\" \\    --function-version 1 \\    --name LIVEOutput:{    \"FunctionVersion\": \"1\",    \"Name\": \"LIVE\",    \"AliasArn\": \"arn:aws:lambda:us-west-2:123456789012:function:my-function:LIVE\",    \"RevisionId\": \"873282ed-4cd3-4dc8-a069-d0c647e470c6\",    \"Description\": \"alias for live version of function\"}For more information, see Configuring AWS Lambda Function Aliases in the AWS Lambda Developer Guide.                        For API details, see                        CreateAlias                        in AWS CLI Command Reference.                    ",
                                    "For a complete list of AWS SDK developer guides and code examples, see    Using Lambda with an AWS SDK.    This topic also includes information about getting started and details about previous SDK versions."
                                ]
                            },
                            {
                                "title": "CreateFunction",
                                "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_lambda_CreateFunction_section.html",
                                "sections": [
                                    "",
                                    "The following code examples show how to use CreateFunction.",
                                    "Action examples are code excerpts from larger programs and must be run in context. You can see this action in            context in the following code example:            ",
                                    ".NETAWS SDK for .NETNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        AWS Code            Examples Repository.        /// <summary>    /// Creates a new Lambda function.    /// </summary>    /// <param name=\"functionName\">The name of the function.</param>    /// <param name=\"s3Bucket\">The Amazon Simple Storage Service (Amazon S3)    /// bucket where the zip file containing the code is located.</param>    /// <param name=\"s3Key\">The Amazon S3 key of the zip file.</param>    /// <param name=\"role\">The Amazon Resource Name (ARN) of a role with the    /// appropriate Lambda permissions.</param>    /// <param name=\"handler\">The name of the handler function.</param>    /// <returns>The Amazon Resource Name (ARN) of the newly created    /// Lambda function.</returns>    public async Task<string> CreateLambdaFunctionAsync(        string functionName,        string s3Bucket,        string s3Key,        string role,        string handler)    {        // Defines the location for the function code.        // S3Bucket - The S3 bucket where the file containing        //            the source code is stored.        // S3Key    - The name of the file containing the code.        var functionCode = new FunctionCode        {            S3Bucket = s3Bucket,            S3Key = s3Key,        };        var createFunctionRequest = new CreateFunctionRequest        {            FunctionName = functionName,            Description = \"Created by the Lambda .NET API\",            Code = functionCode,            Handler = handler,            Runtime = Runtime.Dotnet6,            Role = role,        };        var reponse = await _lambdaService.CreateFunctionAsync(createFunctionRequest);        return reponse.FunctionArn;    }                        For API details, see                        CreateFunction                        in AWS SDK for .NET API Reference.                    C++SDK for C++Note        There's more on GitHub. Find the complete example and learn how to set up and run in the        AWS Code            Examples Repository.            Aws::Client::ClientConfiguration clientConfig;        // Optional: Set to the AWS Region in which the bucket was created (overrides config file).        // clientConfig.region = \"us-east-1\";    Aws::Lambda::LambdaClient client(clientConfig);        Aws::Lambda::Model::CreateFunctionRequest request;        request.SetFunctionName(LAMBDA_NAME);        request.SetDescription(LAMBDA_DESCRIPTION); // Optional.#if USE_CPP_LAMBDA_FUNCTION        request.SetRuntime(Aws::Lambda::Model::Runtime::provided_al2);        request.SetTimeout(15);        request.SetMemorySize(128);        // Assume the AWS Lambda function was built in Docker with same architecture        // as this code.#if  defined(__x86_64__)        request.SetArchitectures({Aws::Lambda::Model::Architecture::x86_64});#elif defined(__aarch64__)        request.SetArchitectures({Aws::Lambda::Model::Architecture::arm64});#else#error \"Unimplemented architecture\"#endif // defined(architecture)#else        request.SetRuntime(Aws::Lambda::Model::Runtime::python3_9);#endif        request.SetRole(roleArn);        request.SetHandler(LAMBDA_HANDLER_NAME);        request.SetPublish(true);        Aws::Lambda::Model::FunctionCode code;        std::ifstream ifstream(INCREMENT_LAMBDA_CODE.c_str(),                               std::ios_base::in | std::ios_base::binary);        if (!ifstream.is_open()) {            std::cerr << \"Error opening file \" << INCREMENT_LAMBDA_CODE << \".\" << std::endl;#if USE_CPP_LAMBDA_FUNCTION            std::cerr                    << \"The cpp Lambda function must be built following the instructions in the cpp_lambda/README.md file. \"                    << std::endl;#endif            deleteIamRole(clientConfig);            return false;        }        Aws::StringStream buffer;        buffer << ifstream.rdbuf();        code.SetZipFile(Aws::Utils::ByteBuffer((unsigned char *) buffer.str().c_str(),                                               buffer.str().length()));        request.SetCode(code);        Aws::Lambda::Model::CreateFunctionOutcome outcome = client.CreateFunction(                request);        if (outcome.IsSuccess()) {            std::cout << \"The lambda function was successfully created. \" << seconds                      << \" seconds elapsed.\" << std::endl;            break;        }        else {            std::cerr << \"Error with CreateFunction. \"                      << outcome.GetError().GetMessage()                      << std::endl;            deleteIamRole(clientConfig);            return false;        }                        For API details, see                        CreateFunction                        in AWS SDK for C++ API Reference.                    CLIAWS CLITo create a Lambda functionThe following create-function example creates a Lambda function named my-function.aws lambda create-function \\    --function-name my-function \\    --runtime nodejs18.x \\    --zip-file fileb://my-function.zip \\    --handler my-function.handler \\    --role arn:aws:iam::123456789012:role/service-role/MyTestFunction-role-tges6bf4Contents of my-function.zip:This file is a deployment package that contains your function code and any dependencies.Output:{    \"TracingConfig\": {        \"Mode\": \"PassThrough\"    },    \"CodeSha256\": \"PFn4S+er27qk+UuZSTKEQfNKG/XNn7QJs90mJgq6oH8=\",    \"FunctionName\": \"my-function\",    \"CodeSize\": 308,    \"RevisionId\": \"873282ed-4cd3-4dc8-a069-d0c647e470c6\",    \"MemorySize\": 128,    \"FunctionArn\": \"arn:aws:lambda:us-west-2:123456789012:function:my-function\",    \"Version\": \"$LATEST\",    \"Role\": \"arn:aws:iam::123456789012:role/service-role/MyTestFunction-role-zgur6bf4\",    \"Timeout\": 3,    \"LastModified\": \"2023-10-14T22:26:11.234+0000\",    \"Handler\": \"my-function.handler\",    \"Runtime\": \"nodejs18.x\",    \"Description\": \"\"}For more information, see AWS Lambda Function Configuration in the AWS Lambda Developer Guide.                        For API details, see                        CreateFunction                        in AWS CLI Command Reference.                    GoSDK for Go V2Note        There's more on GitHub. Find the complete example and learn how to set up and run in the        AWS Code            Examples Repository.    import (\t\"bytes\"\t\"context\"\t\"encoding/json\"\t\"errors\"\t\"log\"\t\"time\"\t\"github.com/aws/aws-sdk-go-v2/aws\"\t\"github.com/aws/aws-sdk-go-v2/service/lambda\"\t\"github.com/aws/aws-sdk-go-v2/service/lambda/types\")// FunctionWrapper encapsulates function actions used in the examples.// It contains an AWS Lambda service client that is used to perform user actions.type FunctionWrapper struct {\tLambdaClient *lambda.Client}// CreateFunction creates a new Lambda function from code contained in the zipPackage// buffer. The specified handlerName must match the name of the file and function// contained in the uploaded code. The role specified by iamRoleArn is assumed by// Lambda and grants specific permissions.// When the function already exists, types.StateActive is returned.// When the function is created, a lambda.FunctionActiveV2Waiter is used to wait until the// function is active.func (wrapper FunctionWrapper) CreateFunction(ctx context.Context, functionName string, handlerName string,\tiamRoleArn *string, zipPackage *bytes.Buffer) types.State {\tvar state types.State\t_, err := wrapper.LambdaClient.CreateFunction(ctx, &lambda.CreateFunctionInput{\t\tCode:         &types.FunctionCode{ZipFile: zipPackage.Bytes()},\t\tFunctionName: aws.String(functionName),\t\tRole:         iamRoleArn,\t\tHandler:      aws.String(handlerName),\t\tPublish:      true,\t\tRuntime:      types.RuntimePython39,\t})\tif err != nil {\t\tvar resConflict *types.ResourceConflictException\t\tif errors.As(err, &resConflict) {\t\t\tlog.Printf(\"Function %v already exists.\\n\", functionName)\t\t\tstate = types.StateActive\t\t} else {\t\t\tlog.Panicf(\"Couldn't create function %v. Here's why: %v\\n\", functionName, err)\t\t}\t} else {\t\twaiter := lambda.NewFunctionActiveV2Waiter(wrapper.LambdaClient)\t\tfuncOutput, err := waiter.WaitForOutput(ctx, &lambda.GetFunctionInput{\t\t\tFunctionName: aws.String(functionName)}, 1*time.Minute)\t\tif err != nil {\t\t\tlog.Panicf(\"Couldn't wait for function %v to be active. Here's why: %v\\n\", functionName, err)\t\t} else {\t\t\tstate = funcOutput.Configuration.State\t\t}\t}\treturn state}                        For API details, see                        CreateFunction                        in AWS SDK for Go API Reference.                    JavaSDK for Java 2.xNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        AWS Code            Examples Repository.        /**     * Creates a new Lambda function in AWS using the AWS Lambda Java API.     *     * @param awsLambda    the AWS Lambda client used to interact with the AWS Lambda service     * @param functionName the name of the Lambda function to create     * @param key          the S3 key of the function code     * @param bucketName   the name of the S3 bucket containing the function code     * @param role         the IAM role to assign to the Lambda function     * @param handler      the fully qualified class name of the function handler     * @return the Amazon Resource Name (ARN) of the created Lambda function     */    public static String createLambdaFunction(LambdaClient awsLambda,                                              String functionName,                                              String key,                                              String bucketName,                                              String role,                                              String handler) {        try {            LambdaWaiter waiter = awsLambda.waiter();            FunctionCode code = FunctionCode.builder()                .s3Key(key)                .s3Bucket(bucketName)                .build();            CreateFunctionRequest functionRequest = CreateFunctionRequest.builder()                .functionName(functionName)                .description(\"Created by the Lambda Java API\")                .code(code)                .handler(handler)                .runtime(Runtime.JAVA17)                .role(role)                .build();            // Create a Lambda function using a waiter            CreateFunctionResponse functionResponse = awsLambda.createFunction(functionRequest);            GetFunctionRequest getFunctionRequest = GetFunctionRequest.builder()                .functionName(functionName)                .build();            WaiterResponse<GetFunctionResponse> waiterResponse = waiter.waitUntilFunctionExists(getFunctionRequest);            waiterResponse.matched().response().ifPresent(System.out::println);            return functionResponse.functionArn();        } catch (LambdaException e) {            System.err.println(e.getMessage());            System.exit(1);        }        return \"\";    }                        For API details, see                        CreateFunction                        in AWS SDK for Java 2.x API Reference.                    JavaScriptSDK for JavaScript (v3)Note        There's more on GitHub. Find the complete example and learn how to set up and run in the        AWS Code            Examples Repository.    const createFunction = async (funcName, roleArn) => {  const client = new LambdaClient({});  const code = await readFile(`${dirname}../functions/${funcName}.zip`);  const command = new CreateFunctionCommand({    Code: { ZipFile: code },    FunctionName: funcName,    Role: roleArn,    Architectures: [Architecture.arm64],    Handler: \"index.handler\", // Required when sending a .zip file    PackageType: PackageType.Zip, // Required when sending a .zip file    Runtime: Runtime.nodejs16x, // Required when sending a .zip file  });  return client.send(command);};                        For API details, see                        CreateFunction                        in AWS SDK for JavaScript API Reference.                    KotlinSDK for KotlinNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        AWS Code            Examples Repository.    suspend fun createNewFunction(    myFunctionName: String,    s3BucketName: String,    myS3Key: String,    myHandler: String,    myRole: String,): String? {    val functionCode =        FunctionCode {            s3Bucket = s3BucketName            s3Key = myS3Key        }    val request =        CreateFunctionRequest {            functionName = myFunctionName            code = functionCode            description = \"Created by the Lambda Kotlin API\"            handler = myHandler            role = myRole            runtime = Runtime.Java8        }    LambdaClient { region = \"us-west-2\" }.use { awsLambda ->        val functionResponse = awsLambda.createFunction(request)        awsLambda.waitUntilFunctionActive {            functionName = myFunctionName        }        return functionResponse.functionArn    }}                        For API details, see                        CreateFunction                        in AWS SDK for Kotlin API reference.                    PHPSDK for PHPNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        AWS Code            Examples Repository.        public function createFunction($functionName, $role, $bucketName, $handler)    {        //This assumes the Lambda function is in an S3 bucket.        return $this->customWaiter(function () use ($functionName, $role, $bucketName, $handler) {            return $this->lambdaClient->createFunction([                'Code' => [                    'S3Bucket' => $bucketName,                    'S3Key' => $functionName,                ],                'FunctionName' => $functionName,                'Role' => $role['Arn'],                'Runtime' => 'python3.9',                'Handler' => \"$handler.lambda_handler\",            ]);        });    }                        For API details, see                        CreateFunction                        in AWS SDK for PHP API Reference.                    PowerShellTools for PowerShellExample 1: This example creates a new C# (dotnetcore1.0 runtime) function named MyFunction in AWS Lambda, providing the compiled binaries for the function from a zip file on the local file system (relative or absolute paths may be used). C# Lambda functions specify the handler for the function using the designation AssemblyName::Namespace.ClassName::MethodName. You should replace the assembly name (without .dll suffix), namespace, class name and method name parts of the handler spec appropriately. The new function will have environment variables 'envvar1' and 'envvar2' set up from the provided values.Publish-LMFunction -Description \"My C# Lambda Function\" `        -FunctionName MyFunction `        -ZipFilename .\\MyFunctionBinaries.zip `        -Handler \"AssemblyName::Namespace.ClassName::MethodName\" `        -Role \"arn:aws:iam::123456789012:role/LambdaFullExecRole\" `        -Runtime dotnetcore1.0 `        -Environment_Variable @{ \"envvar1\"=\"value\";\"envvar2\"=\"value\" }Output:CodeSha256       : /NgBMd...gq71I=CodeSize         : 214784DeadLetterConfig :Description      : My C# Lambda FunctionEnvironment      : Amazon.Lambda.Model.EnvironmentResponseFunctionArn      : arn:aws:lambda:us-west-2:123456789012:function:ToUpperFunctionName     : MyFunctionHandler          : AssemblyName::Namespace.ClassName::MethodNameKMSKeyArn        :LastModified     : 2016-12-29T23:50:14.207+0000MemorySize       : 128Role             : arn:aws:iam::123456789012:role/LambdaFullExecRoleRuntime          : dotnetcore1.0Timeout          : 3Version          : $LATESTVpcConfig        :Example 2: This example is similar to the previous one except the function binaries are first uploaded to an Amazon S3 bucket (which must be in the same region as the intended Lambda function) and the resulting S3 object is then referenced when creating the function.Write-S3Object -BucketName amzn-s3-demo-bucket -Key MyFunctionBinaries.zip -File .\\MyFunctionBinaries.zip    Publish-LMFunction -Description \"My C# Lambda Function\" `        -FunctionName MyFunction `        -BucketName amzn-s3-demo-bucket `        -Key MyFunctionBinaries.zip `        -Handler \"AssemblyName::Namespace.ClassName::MethodName\" `        -Role \"arn:aws:iam::123456789012:role/LambdaFullExecRole\" `        -Runtime dotnetcore1.0 `        -Environment_Variable @{ \"envvar1\"=\"value\";\"envvar2\"=\"value\" }                        For API details, see                        CreateFunction                        in AWS Tools for PowerShell Cmdlet Reference.                    PythonSDK for Python (Boto3)Note        There's more on GitHub. Find the complete example and learn how to set up and run in the        AWS Code            Examples Repository.    class LambdaWrapper:    def __init__(self, lambda_client, iam_resource):        self.lambda_client = lambda_client        self.iam_resource = iam_resource    def create_function(        self, function_name, handler_name, iam_role, deployment_package    ):        \"\"\"        Deploys a Lambda function.        :param function_name: The name of the Lambda function.        :param handler_name: The fully qualified name of the handler function. This                             must include the file name and the function name.        :param iam_role: The IAM role to use for the function.        :param deployment_package: The deployment package that contains the function                                   code in .zip format.        :return: The Amazon Resource Name (ARN) of the newly created function.        \"\"\"        try:            response = self.lambda_client.create_function(                FunctionName=function_name,                Description=\"AWS Lambda doc example\",                Runtime=\"python3.9\",                Role=iam_role.arn,                Handler=handler_name,                Code={\"ZipFile\": deployment_package},                Publish=True,            )            function_arn = response[\"FunctionArn\"]            waiter = self.lambda_client.get_waiter(\"function_active_v2\")            waiter.wait(FunctionName=function_name)            logger.info(                \"Created function '%s' with ARN: '%s'.\",                function_name,                response[\"FunctionArn\"],            )        except ClientError:            logger.error(\"Couldn't create function %s.\", function_name)            raise        else:            return function_arn                        For API details, see                        CreateFunction                        in AWS SDK for Python (Boto3) API Reference.                    RubySDK for RubyNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        AWS Code            Examples Repository.    class LambdaWrapper  attr_accessor :lambda_client, :cloudwatch_client, :iam_client  def initialize    @lambda_client = Aws::Lambda::Client.new    @cloudwatch_client = Aws::CloudWatchLogs::Client.new(region: 'us-east-1')    @iam_client = Aws::IAM::Client.new(region: 'us-east-1')    @logger = Logger.new($stdout)    @logger.level = Logger::WARN  end  # Deploys a Lambda function.  #  # @param function_name: The name of the Lambda function.  # @param handler_name: The fully qualified name of the handler function.  # @param role_arn: The IAM role to use for the function.  # @param deployment_package: The deployment package that contains the function code in .zip format.  # @return: The Amazon Resource Name (ARN) of the newly created function.  def create_function(function_name, handler_name, role_arn, deployment_package)    response = @lambda_client.create_function({                                                role: role_arn.to_s,                                                function_name: function_name,                                                handler: handler_name,                                                runtime: 'ruby2.7',                                                code: {                                                  zip_file: deployment_package                                                },                                                environment: {                                                  variables: {                                                    'LOG_LEVEL' => 'info'                                                  }                                                }                                              })    @lambda_client.wait_until(:function_active_v2, { function_name: function_name }) do |w|      w.max_attempts = 5      w.delay = 5    end    response  rescue Aws::Lambda::Errors::ServiceException => e    @logger.error(\"There was an error creating #{function_name}:\\n #{e.message}\")  rescue Aws::Waiters::Errors::WaiterFailed => e    @logger.error(\"Failed waiting for #{function_name} to activate:\\n #{e.message}\")  end                        For API details, see                        CreateFunction                        in AWS SDK for Ruby API Reference.                    RustSDK for RustNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        AWS Code            Examples Repository.        /**     * Create a function, uploading from a zip file.     */    pub async fn create_function(&self, zip_file: PathBuf) -> Result<String, anyhow::Error> {        let code = self.prepare_function(zip_file, None).await?;        let key = code.s3_key().unwrap().to_string();        let role = self.create_role().await.map_err(|e| anyhow!(e))?;        info!(\"Created iam role, waiting 15s for it to become active\");        tokio::time::sleep(Duration::from_secs(15)).await;        info!(\"Creating lambda function {}\", self.lambda_name);        let _ = self            .lambda_client            .create_function()            .function_name(self.lambda_name.clone())            .code(code)            .role(role.arn())            .runtime(aws_sdk_lambda::types::Runtime::Providedal2)            .handler(\"_unused\")            .send()            .await            .map_err(anyhow::Error::from)?;        self.wait_for_function_ready().await?;        self.lambda_client            .publish_version()            .function_name(self.lambda_name.clone())            .send()            .await?;        Ok(key)    }    /**     * Upload function code from a path to a zip file.     * The zip file must have an AL2 Linux-compatible binary called `bootstrap`.     * The easiest way to create such a zip is to use `cargo lambda build --output-format Zip`.     */    async fn prepare_function(        &self,        zip_file: PathBuf,        key: Option<String>,    ) -> Result<FunctionCode, anyhow::Error> {        let body = ByteStream::from_path(zip_file).await?;        let key = key.unwrap_or_else(|| format!(\"{}_code\", self.lambda_name));        info!(\"Uploading function code to s3://{}/{}\", self.bucket, key);        let _ = self            .s3_client            .put_object()            .bucket(self.bucket.clone())            .key(key.clone())            .body(body)            .send()            .await?;        Ok(FunctionCode::builder()            .s3_bucket(self.bucket.clone())            .s3_key(key)            .build())    }                        For API details, see                        CreateFunction                        in AWS SDK for Rust API reference.                    SAP ABAPSDK for SAP ABAPNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        AWS Code            Examples Repository.        TRY.        lo_lmd->createfunction(            iv_functionname = iv_function_name            iv_runtime = `python3.9`            iv_role = iv_role_arn            iv_handler = iv_handler            io_code = io_zip_file            iv_description = 'AWS Lambda code example'        ).        MESSAGE 'Lambda function created.' TYPE 'I'.      CATCH /aws1/cx_lmdcodesigningcfgno00.        MESSAGE 'Code signing configuration does not exist.' TYPE 'E'.      CATCH /aws1/cx_lmdcodestorageexcdex.        MESSAGE 'Maximum total code size per account exceeded.' TYPE 'E'.      CATCH /aws1/cx_lmdcodeverification00.        MESSAGE 'Code signature failed one or more validation checks for signature mismatch or expiration.' TYPE 'E'.      CATCH /aws1/cx_lmdinvalidcodesigex.        MESSAGE 'Code signature failed the integrity check.' TYPE 'E'.      CATCH /aws1/cx_lmdinvparamvalueex.        MESSAGE 'The request contains a non-valid parameter.' TYPE 'E'.      CATCH /aws1/cx_lmdresourceconflictex.        MESSAGE 'Resource already exists or another operation is in progress.' TYPE 'E'.      CATCH /aws1/cx_lmdresourcenotfoundex.        MESSAGE 'The requested resource does not exist.' TYPE 'E'.      CATCH /aws1/cx_lmdserviceexception.        MESSAGE 'An internal problem was encountered by the AWS Lambda service.' TYPE 'E'.      CATCH /aws1/cx_lmdtoomanyrequestsex.        MESSAGE 'The maximum request throughput was reached.' TYPE 'E'.    ENDTRY.                        For API details, see                        CreateFunction                        in AWS SDK for SAP ABAP API reference.                    anchoranchoranchoranchoranchoranchoranchoranchoranchoranchoranchoranchoranchor.NETC++CLIGoJavaJavaScriptKotlinPHPPowerShellPythonRubyRustSAP ABAPAWS SDK for .NETNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        AWS Code            Examples Repository.        /// <summary>    /// Creates a new Lambda function.    /// </summary>    /// <param name=\"functionName\">The name of the function.</param>    /// <param name=\"s3Bucket\">The Amazon Simple Storage Service (Amazon S3)    /// bucket where the zip file containing the code is located.</param>    /// <param name=\"s3Key\">The Amazon S3 key of the zip file.</param>    /// <param name=\"role\">The Amazon Resource Name (ARN) of a role with the    /// appropriate Lambda permissions.</param>    /// <param name=\"handler\">The name of the handler function.</param>    /// <returns>The Amazon Resource Name (ARN) of the newly created    /// Lambda function.</returns>    public async Task<string> CreateLambdaFunctionAsync(        string functionName,        string s3Bucket,        string s3Key,        string role,        string handler)    {        // Defines the location for the function code.        // S3Bucket - The S3 bucket where the file containing        //            the source code is stored.        // S3Key    - The name of the file containing the code.        var functionCode = new FunctionCode        {            S3Bucket = s3Bucket,            S3Key = s3Key,        };        var createFunctionRequest = new CreateFunctionRequest        {            FunctionName = functionName,            Description = \"Created by the Lambda .NET API\",            Code = functionCode,            Handler = handler,            Runtime = Runtime.Dotnet6,            Role = role,        };        var reponse = await _lambdaService.CreateFunctionAsync(createFunctionRequest);        return reponse.FunctionArn;    }                        For API details, see                        CreateFunction                        in AWS SDK for .NET API Reference.                    ",
                                    "For a complete list of AWS SDK developer guides and code examples, see    Using Lambda with an AWS SDK.    This topic also includes information about getting started and details about previous SDK versions."
                                ]
                            },
                            {
                                "title": "DeleteAlias",
                                "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_lambda_DeleteAlias_section.html",
                                "sections": [
                                    "",
                                    "The following code examples show how to use DeleteAlias.",
                                    "CLIAWS CLITo delete an alias of a Lambda functionThe following delete-alias example deletes the alias named LIVE from the my-function Lambda function.aws lambda delete-alias \\    --function-name my-function \\    --name LIVEThis command produces no output.For more information, see Configuring AWS Lambda Function Aliases in the AWS Lambda Developer Guide.                        For API details, see                        DeleteAlias                        in AWS CLI Command Reference.                    PowerShellTools for PowerShellExample 1: This example deletes the Lambda function Alias mentioned in the command.Remove-LMAlias -FunctionName \"MylambdaFunction123\" -Name \"NewAlias\"                        For API details, see                        DeleteAlias                        in AWS Tools for PowerShell Cmdlet Reference.                    anchoranchorCLIPowerShellAWS CLITo delete an alias of a Lambda functionThe following delete-alias example deletes the alias named LIVE from the my-function Lambda function.aws lambda delete-alias \\    --function-name my-function \\    --name LIVEThis command produces no output.For more information, see Configuring AWS Lambda Function Aliases in the AWS Lambda Developer Guide.                        For API details, see                        DeleteAlias                        in AWS CLI Command Reference.                    ",
                                    "For a complete list of AWS SDK developer guides and code examples, see    Using Lambda with an AWS SDK.    This topic also includes information about getting started and details about previous SDK versions."
                                ]
                            },
                            {
                                "title": "DeleteFunction",
                                "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_lambda_DeleteFunction_section.html",
                                "sections": [
                                    "",
                                    "The following code examples show how to use DeleteFunction.",
                                    "Action examples are code excerpts from larger programs and must be run in context. You can see this action in            context in the following code example:            ",
                                    ".NETAWS SDK for .NETNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        AWS Code            Examples Repository.        /// <summary>    /// Delete an AWS Lambda function.    /// </summary>    /// <param name=\"functionName\">The name of the Lambda function to    /// delete.</param>    /// <returns>A Boolean value that indicates the success of the action.</returns>    public async Task<bool> DeleteFunctionAsync(string functionName)    {        var request = new DeleteFunctionRequest        {            FunctionName = functionName,        };        var response = await _lambdaService.DeleteFunctionAsync(request);        // A return value of NoContent means that the request was processed.        // In this case, the function was deleted, and the return value        // is intentionally blank.        return response.HttpStatusCode == System.Net.HttpStatusCode.NoContent;    }                        For API details, see                        DeleteFunction                        in AWS SDK for .NET API Reference.                    C++SDK for C++Note        There's more on GitHub. Find the complete example and learn how to set up and run in the        AWS Code            Examples Repository.            Aws::Client::ClientConfiguration clientConfig;        // Optional: Set to the AWS Region in which the bucket was created (overrides config file).        // clientConfig.region = \"us-east-1\";    Aws::Lambda::LambdaClient client(clientConfig);    Aws::Lambda::Model::DeleteFunctionRequest request;    request.SetFunctionName(LAMBDA_NAME);    Aws::Lambda::Model::DeleteFunctionOutcome outcome = client.DeleteFunction(            request);    if (outcome.IsSuccess()) {        std::cout << \"The lambda function was successfully deleted.\" << std::endl;    }    else {        std::cerr << \"Error with Lambda::DeleteFunction. \"                  << outcome.GetError().GetMessage()                  << std::endl;    }                        For API details, see                        DeleteFunction                        in AWS SDK for C++ API Reference.                    CLIAWS CLIExample 1: To delete a Lambda function by function nameThe following delete-function example deletes the Lambda function named my-function by specifying the function's name.aws lambda delete-function \\    --function-name my-functionThis command produces no output.Example 2: To delete a Lambda function by function ARNThe following delete-function example deletes the Lambda function named my-function by specifying the function's ARN.aws lambda delete-function \\    --function-name arn:aws:lambda:us-west-2:123456789012:function:my-functionThis command produces no output.Example 3: To delete a Lambda function by partial function ARNThe following delete-function example deletes the Lambda function named my-function by specifying the function's partial ARN.aws lambda delete-function \\    --function-name 123456789012:function:my-functionThis command produces no output.For more information, see AWS Lambda Function Configuration in the AWS Lambda Developer Guide.                        For API details, see                        DeleteFunction                        in AWS CLI Command Reference.                    GoSDK for Go V2Note        There's more on GitHub. Find the complete example and learn how to set up and run in the        AWS Code            Examples Repository.    import (\t\"bytes\"\t\"context\"\t\"encoding/json\"\t\"errors\"\t\"log\"\t\"time\"\t\"github.com/aws/aws-sdk-go-v2/aws\"\t\"github.com/aws/aws-sdk-go-v2/service/lambda\"\t\"github.com/aws/aws-sdk-go-v2/service/lambda/types\")// FunctionWrapper encapsulates function actions used in the examples.// It contains an AWS Lambda service client that is used to perform user actions.type FunctionWrapper struct {\tLambdaClient *lambda.Client}// DeleteFunction deletes the Lambda function specified by functionName.func (wrapper FunctionWrapper) DeleteFunction(ctx context.Context, functionName string) {\t_, err := wrapper.LambdaClient.DeleteFunction(ctx, &lambda.DeleteFunctionInput{\t\tFunctionName: aws.String(functionName),\t})\tif err != nil {\t\tlog.Panicf(\"Couldn't delete function %v. Here's why: %v\\n\", functionName, err)\t}}                        For API details, see                        DeleteFunction                        in AWS SDK for Go API Reference.                    JavaSDK for Java 2.xNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        AWS Code            Examples Repository.        /**     * Deletes an AWS Lambda function.     *     * @param awsLambda     an instance of the {@link LambdaClient} class, which is used to interact with the AWS Lambda service     * @param functionName  the name of the Lambda function to be deleted     *     * @throws LambdaException if an error occurs while deleting the Lambda function     */    public static void deleteLambdaFunction(LambdaClient awsLambda, String functionName) {        try {            DeleteFunctionRequest request = DeleteFunctionRequest.builder()                .functionName(functionName)                .build();            awsLambda.deleteFunction(request);            System.out.println(\"The \" + functionName + \" function was deleted\");        } catch (LambdaException e) {            System.err.println(e.getMessage());            System.exit(1);        }    }                        For API details, see                        DeleteFunction                        in AWS SDK for Java 2.x API Reference.                    JavaScriptSDK for JavaScript (v3)Note        There's more on GitHub. Find the complete example and learn how to set up and run in the        AWS Code            Examples Repository.    /** * @param {string} funcName */const deleteFunction = (funcName) => {  const client = new LambdaClient({});  const command = new DeleteFunctionCommand({ FunctionName: funcName });  return client.send(command);};                        For API details, see                        DeleteFunction                        in AWS SDK for JavaScript API Reference.                    KotlinSDK for KotlinNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        AWS Code            Examples Repository.    suspend fun delLambdaFunction(myFunctionName: String) {    val request =        DeleteFunctionRequest {            functionName = myFunctionName        }    LambdaClient { region = \"us-west-2\" }.use { awsLambda ->        awsLambda.deleteFunction(request)        println(\"$myFunctionName was deleted\")    }}                        For API details, see                        DeleteFunction                        in AWS SDK for Kotlin API reference.                    PHPSDK for PHPNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        AWS Code            Examples Repository.        public function deleteFunction($functionName)    {        return $this->lambdaClient->deleteFunction([            'FunctionName' => $functionName,        ]);    }                        For API details, see                        DeleteFunction                        in AWS SDK for PHP API Reference.                    PowerShellTools for PowerShellExample 1: This example deletes a specific version of a Lambda functionRemove-LMFunction -FunctionName \"MylambdaFunction123\" -Qualifier '3'                        For API details, see                        DeleteFunction                        in AWS Tools for PowerShell Cmdlet Reference.                    PythonSDK for Python (Boto3)Note        There's more on GitHub. Find the complete example and learn how to set up and run in the        AWS Code            Examples Repository.    class LambdaWrapper:    def __init__(self, lambda_client, iam_resource):        self.lambda_client = lambda_client        self.iam_resource = iam_resource    def delete_function(self, function_name):        \"\"\"        Deletes a Lambda function.        :param function_name: The name of the function to delete.        \"\"\"        try:            self.lambda_client.delete_function(FunctionName=function_name)        except ClientError:            logger.exception(\"Couldn't delete function %s.\", function_name)            raise                        For API details, see                        DeleteFunction                        in AWS SDK for Python (Boto3) API Reference.                    RubySDK for RubyNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        AWS Code            Examples Repository.    class LambdaWrapper  attr_accessor :lambda_client, :cloudwatch_client, :iam_client  def initialize    @lambda_client = Aws::Lambda::Client.new    @cloudwatch_client = Aws::CloudWatchLogs::Client.new(region: 'us-east-1')    @iam_client = Aws::IAM::Client.new(region: 'us-east-1')    @logger = Logger.new($stdout)    @logger.level = Logger::WARN  end  # Deletes a Lambda function.  # @param function_name: The name of the function to delete.  def delete_function(function_name)    print \"Deleting function: #{function_name}...\"    @lambda_client.delete_function(      function_name: function_name    )    print 'Done!'.green  rescue Aws::Lambda::Errors::ServiceException => e    @logger.error(\"There was an error deleting #{function_name}:\\n #{e.message}\")  end                        For API details, see                        DeleteFunction                        in AWS SDK for Ruby API Reference.                    RustSDK for RustNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        AWS Code            Examples Repository.        /** Delete a function and its role, and if possible or necessary, its associated code object and bucket. */    pub async fn delete_function(        &self,        location: Option<String>,    ) -> (        Result<DeleteFunctionOutput, anyhow::Error>,        Result<DeleteRoleOutput, anyhow::Error>,        Option<Result<DeleteObjectOutput, anyhow::Error>>,    ) {        info!(\"Deleting lambda function {}\", self.lambda_name);        let delete_function = self            .lambda_client            .delete_function()            .function_name(self.lambda_name.clone())            .send()            .await            .map_err(anyhow::Error::from);        info!(\"Deleting iam role {}\", self.role_name);        let delete_role = self            .iam_client            .delete_role()            .role_name(self.role_name.clone())            .send()            .await            .map_err(anyhow::Error::from);        let delete_object: Option<Result<DeleteObjectOutput, anyhow::Error>> =            if let Some(location) = location {                info!(\"Deleting object {location}\");                Some(                    self.s3_client                        .delete_object()                        .bucket(self.bucket.clone())                        .key(location)                        .send()                        .await                        .map_err(anyhow::Error::from),                )            } else {                info!(?location, \"Skipping delete object\");                None            };        (delete_function, delete_role, delete_object)    }                        For API details, see                        DeleteFunction                        in AWS SDK for Rust API reference.                    SAP ABAPSDK for SAP ABAPNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        AWS Code            Examples Repository.        TRY.        lo_lmd->deletefunction( iv_functionname = iv_function_name ).        MESSAGE 'Lambda function deleted.' TYPE 'I'.      CATCH /aws1/cx_lmdinvparamvalueex.        MESSAGE 'The request contains a non-valid parameter.' TYPE 'E'.      CATCH /aws1/cx_lmdresourceconflictex.        MESSAGE 'Resource already exists or another operation is in progress.' TYPE 'E'.      CATCH /aws1/cx_lmdresourcenotfoundex.        MESSAGE 'The requested resource does not exist.' TYPE 'E'.      CATCH /aws1/cx_lmdserviceexception.        MESSAGE 'An internal problem was encountered by the AWS Lambda service.' TYPE 'E'.      CATCH /aws1/cx_lmdtoomanyrequestsex.        MESSAGE 'The maximum request throughput was reached.' TYPE 'E'.    ENDTRY.                        For API details, see                        DeleteFunction                        in AWS SDK for SAP ABAP API reference.                    anchoranchoranchoranchoranchoranchoranchoranchoranchoranchoranchoranchoranchor.NETC++CLIGoJavaJavaScriptKotlinPHPPowerShellPythonRubyRustSAP ABAPAWS SDK for .NETNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        AWS Code            Examples Repository.        /// <summary>    /// Delete an AWS Lambda function.    /// </summary>    /// <param name=\"functionName\">The name of the Lambda function to    /// delete.</param>    /// <returns>A Boolean value that indicates the success of the action.</returns>    public async Task<bool> DeleteFunctionAsync(string functionName)    {        var request = new DeleteFunctionRequest        {            FunctionName = functionName,        };        var response = await _lambdaService.DeleteFunctionAsync(request);        // A return value of NoContent means that the request was processed.        // In this case, the function was deleted, and the return value        // is intentionally blank.        return response.HttpStatusCode == System.Net.HttpStatusCode.NoContent;    }                        For API details, see                        DeleteFunction                        in AWS SDK for .NET API Reference.                    ",
                                    "For a complete list of AWS SDK developer guides and code examples, see    Using Lambda with an AWS SDK.    This topic also includes information about getting started and details about previous SDK versions."
                                ]
                            },
                            {
                                "title": "DeleteFunctionConcurrency",
                                "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_lambda_DeleteFunctionConcurrency_section.html",
                                "sections": [
                                    "",
                                    "The following code examples show how to use DeleteFunctionConcurrency.",
                                    "CLIAWS CLITo remove the reserved concurrent execution limit from a functionThe following delete-function-concurrency example deletes the reserved concurrent execution limit from the my-function function.aws lambda delete-function-concurrency \\    --function-name  my-functionThis command produces no output.For more information, see Reserving Concurrency for a Lambda Function in the AWS Lambda Developer Guide.                        For API details, see                        DeleteFunctionConcurrency                        in AWS CLI Command Reference.                    PowerShellTools for PowerShellExample 1: This examples removes the Function Concurrency of the Lambda Function.Remove-LMFunctionConcurrency -FunctionName \"MylambdaFunction123\"                        For API details, see                        DeleteFunctionConcurrency                        in AWS Tools for PowerShell Cmdlet Reference.                    anchoranchorCLIPowerShellAWS CLITo remove the reserved concurrent execution limit from a functionThe following delete-function-concurrency example deletes the reserved concurrent execution limit from the my-function function.aws lambda delete-function-concurrency \\    --function-name  my-functionThis command produces no output.For more information, see Reserving Concurrency for a Lambda Function in the AWS Lambda Developer Guide.                        For API details, see                        DeleteFunctionConcurrency                        in AWS CLI Command Reference.                    ",
                                    "For a complete list of AWS SDK developer guides and code examples, see    Using Lambda with an AWS SDK.    This topic also includes information about getting started and details about previous SDK versions."
                                ]
                            },
                            {
                                "title": "DeleteProvisionedConcurrencyConfig",
                                "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_lambda_DeleteProvisionedConcurrencyConfig_section.html",
                                "sections": [
                                    "",
                                    "The following code examples show how to use DeleteProvisionedConcurrencyConfig.",
                                    "CLIAWS CLITo delete a provisioned concurrency configurationThe following delete-provisioned-concurrency-config example deletes the provisioned concurrency configuration for the GREEN alias of the specified function.aws lambda delete-provisioned-concurrency-config \\    --function-name my-function \\    --qualifier GREEN                        For API details, see                        DeleteProvisionedConcurrencyConfig                        in AWS CLI Command Reference.                    PowerShellTools for PowerShellExample 1: This example removes the Provisioned Concurrency Configuration for a specific Alias.Remove-LMProvisionedConcurrencyConfig -FunctionName \"MylambdaFunction123\" -Qualifier \"NewAlias1\"                        For API details, see                        DeleteProvisionedConcurrencyConfig                        in AWS Tools for PowerShell Cmdlet Reference.                    anchoranchorCLIPowerShellAWS CLITo delete a provisioned concurrency configurationThe following delete-provisioned-concurrency-config example deletes the provisioned concurrency configuration for the GREEN alias of the specified function.aws lambda delete-provisioned-concurrency-config \\    --function-name my-function \\    --qualifier GREEN                        For API details, see                        DeleteProvisionedConcurrencyConfig                        in AWS CLI Command Reference.                    ",
                                    "For a complete list of AWS SDK developer guides and code examples, see    Using Lambda with an AWS SDK.    This topic also includes information about getting started and details about previous SDK versions."
                                ]
                            },
                            {
                                "title": "GetAccountSettings",
                                "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_lambda_GetAccountSettings_section.html",
                                "sections": [
                                    "",
                                    "The following code examples show how to use GetAccountSettings.",
                                    "CLIAWS CLITo retrieve details about your account in an AWS RegionThe following get-account-settings example displays the Lambda limits and usage information for your account.aws lambda get-account-settingsOutput:{    \"AccountLimit\": {       \"CodeSizeUnzipped\": 262144000,       \"UnreservedConcurrentExecutions\": 1000,       \"ConcurrentExecutions\": 1000,       \"CodeSizeZipped\": 52428800,       \"TotalCodeSize\": 80530636800    },    \"AccountUsage\": {       \"FunctionCount\": 4,       \"TotalCodeSize\": 9426    }}For more information, see AWS Lambda Limits in the AWS Lambda Developer Guide.                        For API details, see                        GetAccountSettings                        in AWS CLI Command Reference.                    PowerShellTools for PowerShellExample 1: This sample displays to compare the Account Limit and Account UsageGet-LMAccountSetting | Select-Object @{Name=\"TotalCodeSizeLimit\";Expression={$_.AccountLimit.TotalCodeSize}}, @{Name=\"TotalCodeSizeUsed\";Expression={$_.AccountUsage.TotalCodeSize}}Output:TotalCodeSizeLimit TotalCodeSizeUsed------------------ -----------------       80530636800          15078795                        For API details, see                        GetAccountSettings                        in AWS Tools for PowerShell Cmdlet Reference.                    anchoranchorCLIPowerShellAWS CLITo retrieve details about your account in an AWS RegionThe following get-account-settings example displays the Lambda limits and usage information for your account.aws lambda get-account-settingsOutput:{    \"AccountLimit\": {       \"CodeSizeUnzipped\": 262144000,       \"UnreservedConcurrentExecutions\": 1000,       \"ConcurrentExecutions\": 1000,       \"CodeSizeZipped\": 52428800,       \"TotalCodeSize\": 80530636800    },    \"AccountUsage\": {       \"FunctionCount\": 4,       \"TotalCodeSize\": 9426    }}For more information, see AWS Lambda Limits in the AWS Lambda Developer Guide.                        For API details, see                        GetAccountSettings                        in AWS CLI Command Reference.                    ",
                                    "For a complete list of AWS SDK developer guides and code examples, see    Using Lambda with an AWS SDK.    This topic also includes information about getting started and details about previous SDK versions."
                                ]
                            },
                            {
                                "title": "GetAlias",
                                "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_lambda_GetAlias_section.html",
                                "sections": [
                                    "",
                                    "The following code examples show how to use GetAlias.",
                                    "CLIAWS CLITo retrieve details about a function aliasThe following get-alias example displays details for the alias named LIVE on the my-function Lambda function.aws lambda get-alias \\    --function-name my-function \\    --name LIVEOutput:{    \"FunctionVersion\": \"3\",    \"Name\": \"LIVE\",    \"AliasArn\": \"arn:aws:lambda:us-west-2:123456789012:function:my-function:LIVE\",    \"RevisionId\": \"594f41fb-b85f-4c20-95c7-6ca5f2a92c93\",    \"Description\": \"alias for live version of function\"}For more information, see Configuring AWS Lambda Function Aliases in the AWS Lambda Developer Guide.                        For API details, see                        GetAlias                        in AWS CLI Command Reference.                    PowerShellTools for PowerShellExample 1: This example retrieves the Routing Config weights for a specific Lambda Function Alias.Get-LMAlias -FunctionName \"MylambdaFunction123\" -Name \"newlabel1\" -Select RoutingConfigOutput:AdditionalVersionWeights------------------------{[1, 0.6]}                        For API details, see                        GetAlias                        in AWS Tools for PowerShell Cmdlet Reference.                    anchoranchorCLIPowerShellAWS CLITo retrieve details about a function aliasThe following get-alias example displays details for the alias named LIVE on the my-function Lambda function.aws lambda get-alias \\    --function-name my-function \\    --name LIVEOutput:{    \"FunctionVersion\": \"3\",    \"Name\": \"LIVE\",    \"AliasArn\": \"arn:aws:lambda:us-west-2:123456789012:function:my-function:LIVE\",    \"RevisionId\": \"594f41fb-b85f-4c20-95c7-6ca5f2a92c93\",    \"Description\": \"alias for live version of function\"}For more information, see Configuring AWS Lambda Function Aliases in the AWS Lambda Developer Guide.                        For API details, see                        GetAlias                        in AWS CLI Command Reference.                    ",
                                    "For a complete list of AWS SDK developer guides and code examples, see    Using Lambda with an AWS SDK.    This topic also includes information about getting started and details about previous SDK versions."
                                ]
                            },
                            {
                                "title": "GetFunction",
                                "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_lambda_GetFunction_section.html",
                                "sections": [
                                    "",
                                    "The following code examples show how to use GetFunction.",
                                    "Action examples are code excerpts from larger programs and must be run in context. You can see this action in            context in the following code example:            ",
                                    ".NETAWS SDK for .NETNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        AWS Code            Examples Repository.        /// <summary>    /// Gets information about a Lambda function.    /// </summary>    /// <param name=\"functionName\">The name of the Lambda function for    /// which to retrieve information.</param>    /// <returns>Async Task.</returns>    public async Task<FunctionConfiguration> GetFunctionAsync(string functionName)    {        var functionRequest = new GetFunctionRequest        {            FunctionName = functionName,        };        var response = await _lambdaService.GetFunctionAsync(functionRequest);        return response.Configuration;    }                        For API details, see                        GetFunction                        in AWS SDK for .NET API Reference.                    C++SDK for C++Note        There's more on GitHub. Find the complete example and learn how to set up and run in the        AWS Code            Examples Repository.            Aws::Client::ClientConfiguration clientConfig;        // Optional: Set to the AWS Region in which the bucket was created (overrides config file).        // clientConfig.region = \"us-east-1\";    Aws::Lambda::LambdaClient client(clientConfig);        Aws::Lambda::Model::GetFunctionRequest request;        request.SetFunctionName(functionName);        Aws::Lambda::Model::GetFunctionOutcome outcome = client.GetFunction(request);        if (outcome.IsSuccess()) {            std::cout << \"Function retrieve.\\n\" <<                      outcome.GetResult().GetConfiguration().Jsonize().View().WriteReadable()                      << std::endl;        }        else {            std::cerr << \"Error with Lambda::GetFunction. \"                      << outcome.GetError().GetMessage()                      << std::endl;        }                        For API details, see                        GetFunction                        in AWS SDK for C++ API Reference.                    CLIAWS CLITo retrieve information about a functionThe following get-function example displays information about the my-function function.aws lambda get-function \\    --function-name  my-functionOutput:{    \"Concurrency\": {        \"ReservedConcurrentExecutions\": 100    },    \"Code\": {        \"RepositoryType\": \"S3\",        \"Location\": \"https://awslambda-us-west-2-tasks.s3.us-west-2.amazonaws.com/snapshots/123456789012/my-function...\"    },    \"Configuration\": {        \"TracingConfig\": {            \"Mode\": \"PassThrough\"        },        \"Version\": \"$LATEST\",        \"CodeSha256\": \"5tT2qgzYUHoqwR616pZ2dpkn/0J1FrzJmlKidWaaCgk=\",        \"FunctionName\": \"my-function\",        \"VpcConfig\": {            \"SubnetIds\": [],            \"VpcId\": \"\",            \"SecurityGroupIds\": []        },        \"MemorySize\": 128,        \"RevisionId\": \"28f0fb31-5c5c-43d3-8955-03e76c5c1075\",        \"CodeSize\": 304,        \"FunctionArn\": \"arn:aws:lambda:us-west-2:123456789012:function:my-function\",        \"Handler\": \"index.handler\",        \"Role\": \"arn:aws:iam::123456789012:role/service-role/helloWorldPython-role-uy3l9qyq\",        \"Timeout\": 3,        \"LastModified\": \"2019-09-24T18:20:35.054+0000\",        \"Runtime\": \"nodejs10.x\",        \"Description\": \"\"    }}For more information, see AWS Lambda Function Configuration in the AWS Lambda Developer Guide.                        For API details, see                        GetFunction                        in AWS CLI Command Reference.                    GoSDK for Go V2Note        There's more on GitHub. Find the complete example and learn how to set up and run in the        AWS Code            Examples Repository.    import (\t\"bytes\"\t\"context\"\t\"encoding/json\"\t\"errors\"\t\"log\"\t\"time\"\t\"github.com/aws/aws-sdk-go-v2/aws\"\t\"github.com/aws/aws-sdk-go-v2/service/lambda\"\t\"github.com/aws/aws-sdk-go-v2/service/lambda/types\")// FunctionWrapper encapsulates function actions used in the examples.// It contains an AWS Lambda service client that is used to perform user actions.type FunctionWrapper struct {\tLambdaClient *lambda.Client}// GetFunction gets data about the Lambda function specified by functionName.func (wrapper FunctionWrapper) GetFunction(ctx context.Context, functionName string) types.State {\tvar state types.State\tfuncOutput, err := wrapper.LambdaClient.GetFunction(ctx, &lambda.GetFunctionInput{\t\tFunctionName: aws.String(functionName),\t})\tif err != nil {\t\tlog.Panicf(\"Couldn't get function %v. Here's why: %v\\n\", functionName, err)\t} else {\t\tstate = funcOutput.Configuration.State\t}\treturn state}                        For API details, see                        GetFunction                        in AWS SDK for Go API Reference.                    JavaSDK for Java 2.xNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        AWS Code            Examples Repository.        /**     * Retrieves information about an AWS Lambda function.     *     * @param awsLambda    an instance of the {@link LambdaClient} class, which is used to interact with the AWS Lambda service     * @param functionName the name of the AWS Lambda function to retrieve information about     */    public static void getFunction(LambdaClient awsLambda, String functionName) {        try {            GetFunctionRequest functionRequest = GetFunctionRequest.builder()                .functionName(functionName)                .build();            GetFunctionResponse response = awsLambda.getFunction(functionRequest);            System.out.println(\"The runtime of this Lambda function is \" + response.configuration().runtime());        } catch (LambdaException e) {            System.err.println(e.getMessage());            System.exit(1);        }    }                        For API details, see                        GetFunction                        in AWS SDK for Java 2.x API Reference.                    JavaScriptSDK for JavaScript (v3)Note        There's more on GitHub. Find the complete example and learn how to set up and run in the        AWS Code            Examples Repository.    const getFunction = (funcName) => {  const client = new LambdaClient({});  const command = new GetFunctionCommand({ FunctionName: funcName });  return client.send(command);};                        For API details, see                        GetFunction                        in AWS SDK for JavaScript API Reference.                    PHPSDK for PHPNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        AWS Code            Examples Repository.        public function getFunction($functionName)    {        return $this->lambdaClient->getFunction([            'FunctionName' => $functionName,        ]);    }                        For API details, see                        GetFunction                        in AWS SDK for PHP API Reference.                    PythonSDK for Python (Boto3)Note        There's more on GitHub. Find the complete example and learn how to set up and run in the        AWS Code            Examples Repository.    class LambdaWrapper:    def __init__(self, lambda_client, iam_resource):        self.lambda_client = lambda_client        self.iam_resource = iam_resource    def get_function(self, function_name):        \"\"\"        Gets data about a Lambda function.        :param function_name: The name of the function.        :return: The function data.        \"\"\"        response = None        try:            response = self.lambda_client.get_function(FunctionName=function_name)        except ClientError as err:            if err.response[\"Error\"][\"Code\"] == \"ResourceNotFoundException\":                logger.info(\"Function %s does not exist.\", function_name)            else:                logger.error(                    \"Couldn't get function %s. Here's why: %s: %s\",                    function_name,                    err.response[\"Error\"][\"Code\"],                    err.response[\"Error\"][\"Message\"],                )                raise        return response                        For API details, see                        GetFunction                        in AWS SDK for Python (Boto3) API Reference.                    RubySDK for RubyNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        AWS Code            Examples Repository.    class LambdaWrapper  attr_accessor :lambda_client, :cloudwatch_client, :iam_client  def initialize    @lambda_client = Aws::Lambda::Client.new    @cloudwatch_client = Aws::CloudWatchLogs::Client.new(region: 'us-east-1')    @iam_client = Aws::IAM::Client.new(region: 'us-east-1')    @logger = Logger.new($stdout)    @logger.level = Logger::WARN  end  # Gets data about a Lambda function.  #  # @param function_name: The name of the function.  # @return response: The function data, or nil if no such function exists.  def get_function(function_name)    @lambda_client.get_function(      {        function_name: function_name      }    )  rescue Aws::Lambda::Errors::ResourceNotFoundException => e    @logger.debug(\"Could not find function: #{function_name}:\\n #{e.message}\")    nil  end                        For API details, see                        GetFunction                        in AWS SDK for Ruby API Reference.                    RustSDK for RustNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        AWS Code            Examples Repository.        /** Get the Lambda function with this Manager's name. */    pub async fn get_function(&self) -> Result<GetFunctionOutput, anyhow::Error> {        info!(\"Getting lambda function\");        self.lambda_client            .get_function()            .function_name(self.lambda_name.clone())            .send()            .await            .map_err(anyhow::Error::from)    }                        For API details, see                        GetFunction                        in AWS SDK for Rust API reference.                    SAP ABAPSDK for SAP ABAPNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        AWS Code            Examples Repository.        TRY.        oo_result =  lo_lmd->getfunction( iv_functionname = iv_function_name ).       \" oo_result is returned for testing purposes. \"        MESSAGE 'Lambda function information retrieved.' TYPE 'I'.      CATCH /aws1/cx_lmdinvparamvalueex.        MESSAGE 'The request contains a non-valid parameter.' TYPE 'E'.      CATCH /aws1/cx_lmdserviceexception.        MESSAGE 'An internal problem was encountered by the AWS Lambda service.' TYPE 'E'.      CATCH /aws1/cx_lmdtoomanyrequestsex.        MESSAGE 'The maximum request throughput was reached.' TYPE 'E'.    ENDTRY.                        For API details, see                        GetFunction                        in AWS SDK for SAP ABAP API reference.                    anchoranchoranchoranchoranchoranchoranchoranchoranchoranchoranchor.NETC++CLIGoJavaJavaScriptPHPPythonRubyRustSAP ABAPAWS SDK for .NETNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        AWS Code            Examples Repository.        /// <summary>    /// Gets information about a Lambda function.    /// </summary>    /// <param name=\"functionName\">The name of the Lambda function for    /// which to retrieve information.</param>    /// <returns>Async Task.</returns>    public async Task<FunctionConfiguration> GetFunctionAsync(string functionName)    {        var functionRequest = new GetFunctionRequest        {            FunctionName = functionName,        };        var response = await _lambdaService.GetFunctionAsync(functionRequest);        return response.Configuration;    }                        For API details, see                        GetFunction                        in AWS SDK for .NET API Reference.                    ",
                                    "For a complete list of AWS SDK developer guides and code examples, see    Using Lambda with an AWS SDK.    This topic also includes information about getting started and details about previous SDK versions."
                                ]
                            },
                            {
                                "title": "GetFunctionConcurrency",
                                "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_lambda_GetFunctionConcurrency_section.html",
                                "sections": [
                                    "",
                                    "The following code examples show how to use GetFunctionConcurrency.",
                                    "CLIAWS CLITo view the reserved concurrency setting for a functionThe following get-function-concurrency example retrieves the reserved concurrency setting for the specified function.aws lambda get-function-concurrency \\    --function-name my-functionOutput:{    \"ReservedConcurrentExecutions\": 250}                        For API details, see                        GetFunctionConcurrency                        in AWS CLI Command Reference.                    PowerShellTools for PowerShellExample 1: This examples gets the Reserved concurrency for the Lambda FunctionGet-LMFunctionConcurrency -FunctionName \"MylambdaFunction123\" -Select *Output:ReservedConcurrentExecutions----------------------------100                        For API details, see                        GetFunctionConcurrency                        in AWS Tools for PowerShell Cmdlet Reference.                    anchoranchorCLIPowerShellAWS CLITo view the reserved concurrency setting for a functionThe following get-function-concurrency example retrieves the reserved concurrency setting for the specified function.aws lambda get-function-concurrency \\    --function-name my-functionOutput:{    \"ReservedConcurrentExecutions\": 250}                        For API details, see                        GetFunctionConcurrency                        in AWS CLI Command Reference.                    ",
                                    "For a complete list of AWS SDK developer guides and code examples, see    Using Lambda with an AWS SDK.    This topic also includes information about getting started and details about previous SDK versions."
                                ]
                            },
                            {
                                "title": "GetFunctionConfiguration",
                                "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_lambda_GetFunctionConfiguration_section.html",
                                "sections": [
                                    "",
                                    "The following code examples show how to use GetFunctionConfiguration.",
                                    "CLIAWS CLITo retrieve the version-specific settings of a Lambda functionThe following get-function-configuration example displays the settings for version 2 of the my-function function.aws lambda get-function-configuration \\    --function-name  my-function:2Output:{    \"FunctionName\": \"my-function\",    \"LastModified\": \"2019-09-26T20:28:40.438+0000\",    \"RevisionId\": \"e52502d4-9320-4688-9cd6-152a6ab7490d\",    \"MemorySize\": 256,    \"Version\": \"2\",    \"Role\": \"arn:aws:iam::123456789012:role/service-role/my-function-role-uy3l9qyq\",    \"Timeout\": 3,    \"Runtime\": \"nodejs10.x\",    \"TracingConfig\": {        \"Mode\": \"PassThrough\"    },    \"CodeSha256\": \"5tT2qgzYUHaqwR716pZ2dpkn/0J1FrzJmlKidWoaCgk=\",    \"Description\": \"\",    \"VpcConfig\": {        \"SubnetIds\": [],        \"VpcId\": \"\",        \"SecurityGroupIds\": []    },    \"CodeSize\": 304,    \"FunctionArn\": \"arn:aws:lambda:us-west-2:123456789012:function:my-function:2\",    \"Handler\": \"index.handler\"}For more information, see AWS Lambda Function Configuration in the AWS Lambda Developer Guide.                        For API details, see                        GetFunctionConfiguration                        in AWS CLI Command Reference.                    PowerShellTools for PowerShellExample 1: This example returns the version specific configuration of a Lambda Function.Get-LMFunctionConfiguration -FunctionName \"MylambdaFunction123\" -Qualifier \"PowershellAlias\"Output:CodeSha256                 : uWOW0R7z+f0VyLuUg7+/D08hkMFsq0SF4seuyUZJ/R8=CodeSize                   : 1426DeadLetterConfig           : Amazon.Lambda.Model.DeadLetterConfigDescription                : Verson 3 to test AliasesEnvironment                : Amazon.Lambda.Model.EnvironmentResponseFunctionArn                : arn:aws:lambda:us-east-1:123456789012:function:MylambdaFunction123                             :PowershellAliasFunctionName               : MylambdaFunction123Handler                    : lambda_function.launch_instanceKMSKeyArn                  : LastModified               : 2019-12-25T09:52:59.872+0000LastUpdateStatus           : SuccessfulLastUpdateStatusReason     : LastUpdateStatusReasonCode : Layers                     : {}MasterArn                  : MemorySize                 : 128RevisionId                 : 5d7de38b-87f2-4260-8f8a-e87280e10c33Role                       : arn:aws:iam::123456789012:role/service-role/lambdaRuntime                    : python3.8State                      : ActiveStateReason                : StateReasonCode            : Timeout                    : 600TracingConfig              : Amazon.Lambda.Model.TracingConfigResponseVersion                    : 4VpcConfig                  : Amazon.Lambda.Model.VpcConfigDetail                        For API details, see                        GetFunctionConfiguration                        in AWS Tools for PowerShell Cmdlet Reference.                    anchoranchorCLIPowerShellAWS CLITo retrieve the version-specific settings of a Lambda functionThe following get-function-configuration example displays the settings for version 2 of the my-function function.aws lambda get-function-configuration \\    --function-name  my-function:2Output:{    \"FunctionName\": \"my-function\",    \"LastModified\": \"2019-09-26T20:28:40.438+0000\",    \"RevisionId\": \"e52502d4-9320-4688-9cd6-152a6ab7490d\",    \"MemorySize\": 256,    \"Version\": \"2\",    \"Role\": \"arn:aws:iam::123456789012:role/service-role/my-function-role-uy3l9qyq\",    \"Timeout\": 3,    \"Runtime\": \"nodejs10.x\",    \"TracingConfig\": {        \"Mode\": \"PassThrough\"    },    \"CodeSha256\": \"5tT2qgzYUHaqwR716pZ2dpkn/0J1FrzJmlKidWoaCgk=\",    \"Description\": \"\",    \"VpcConfig\": {        \"SubnetIds\": [],        \"VpcId\": \"\",        \"SecurityGroupIds\": []    },    \"CodeSize\": 304,    \"FunctionArn\": \"arn:aws:lambda:us-west-2:123456789012:function:my-function:2\",    \"Handler\": \"index.handler\"}For more information, see AWS Lambda Function Configuration in the AWS Lambda Developer Guide.                        For API details, see                        GetFunctionConfiguration                        in AWS CLI Command Reference.                    ",
                                    "For a complete list of AWS SDK developer guides and code examples, see    Using Lambda with an AWS SDK.    This topic also includes information about getting started and details about previous SDK versions."
                                ]
                            },
                            {
                                "title": "GetPolicy",
                                "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_lambda_GetPolicy_section.html",
                                "sections": [
                                    "",
                                    "The following code examples show how to use GetPolicy.",
                                    "CLIAWS CLITo retrieve the resource-based IAM policy for a function, version, or aliasThe following get-policy example displays policy information about the my-function Lambda function.aws lambda get-policy \\    --function-name my-functionOutput:{    \"Policy\": {        \"Version\":\"2012-10-17\",        \"Id\":\"default\",        \"Statement\":        [            {                \"Sid\":\"iot-events\",                \"Effect\":\"Allow\",                \"Principal\": {\"Service\":\"iotevents.amazonaws.com\"},                \"Action\":\"lambda:InvokeFunction\",                \"Resource\":\"arn:aws:lambda:us-west-2:123456789012:function:my-function\"            }        ]    },    \"RevisionId\": \"93017fc9-59cb-41dc-901b-4845ce4bf668\"}For more information, see Using Resource-based Policies for AWS Lambda in the AWS Lambda Developer Guide.                        For API details, see                        GetPolicy                        in AWS CLI Command Reference.                    PowerShellTools for PowerShellExample 1: This sample displays the Function policy of the Lambda functionGet-LMPolicy -FunctionName test -Select PolicyOutput:{\"Version\":\"2012-10-17\",\"Id\":\"default\",\"Statement\":[{\"Sid\":\"xxxx\",\"Effect\":\"Allow\",\"Principal\":{\"Service\":\"sns.amazonaws.com\"},\"Action\":\"lambda:InvokeFunction\",\"Resource\":\"arn:aws:lambda:us-east-1:123456789102:function:test\"}]}                        For API details, see                        GetPolicy                        in AWS Tools for PowerShell Cmdlet Reference.                    anchoranchorCLIPowerShellAWS CLITo retrieve the resource-based IAM policy for a function, version, or aliasThe following get-policy example displays policy information about the my-function Lambda function.aws lambda get-policy \\    --function-name my-functionOutput:{    \"Policy\": {        \"Version\":\"2012-10-17\",        \"Id\":\"default\",        \"Statement\":        [            {                \"Sid\":\"iot-events\",                \"Effect\":\"Allow\",                \"Principal\": {\"Service\":\"iotevents.amazonaws.com\"},                \"Action\":\"lambda:InvokeFunction\",                \"Resource\":\"arn:aws:lambda:us-west-2:123456789012:function:my-function\"            }        ]    },    \"RevisionId\": \"93017fc9-59cb-41dc-901b-4845ce4bf668\"}For more information, see Using Resource-based Policies for AWS Lambda in the AWS Lambda Developer Guide.                        For API details, see                        GetPolicy                        in AWS CLI Command Reference.                    ",
                                    "For a complete list of AWS SDK developer guides and code examples, see    Using Lambda with an AWS SDK.    This topic also includes information about getting started and details about previous SDK versions."
                                ]
                            },
                            {
                                "title": "GetProvisionedConcurrencyConfig",
                                "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_lambda_GetProvisionedConcurrencyConfig_section.html",
                                "sections": [
                                    "",
                                    "The following code examples show how to use GetProvisionedConcurrencyConfig.",
                                    "CLIAWS CLITo view a provisioned concurrency configurationThe following get-provisioned-concurrency-config example displays details for the provisioned concurrency configuration for the BLUE alias of the specified function.aws lambda get-provisioned-concurrency-config \\    --function-name my-function \\    --qualifier BLUEOutput:{    \"RequestedProvisionedConcurrentExecutions\": 100,    \"AvailableProvisionedConcurrentExecutions\": 100,    \"AllocatedProvisionedConcurrentExecutions\": 100,    \"Status\": \"READY\",    \"LastModified\": \"2019-12-31T20:28:49+0000\"}                        For API details, see                        GetProvisionedConcurrencyConfig                        in AWS CLI Command Reference.                    PowerShellTools for PowerShellExample 1: This example gets the provisioned Concurrency Configuration for the specified Alias of the Lambda Function.C:\\>Get-LMProvisionedConcurrencyConfig -FunctionName \"MylambdaFunction123\" -Qualifier \"NewAlias1\"Output:AllocatedProvisionedConcurrentExecutions : 0AvailableProvisionedConcurrentExecutions : 0LastModified                             : 2020-01-15T03:21:26+0000RequestedProvisionedConcurrentExecutions : 70Status                                   : IN_PROGRESSStatusReason                             :                        For API details, see                        GetProvisionedConcurrencyConfig                        in AWS Tools for PowerShell Cmdlet Reference.                    anchoranchorCLIPowerShellAWS CLITo view a provisioned concurrency configurationThe following get-provisioned-concurrency-config example displays details for the provisioned concurrency configuration for the BLUE alias of the specified function.aws lambda get-provisioned-concurrency-config \\    --function-name my-function \\    --qualifier BLUEOutput:{    \"RequestedProvisionedConcurrentExecutions\": 100,    \"AvailableProvisionedConcurrentExecutions\": 100,    \"AllocatedProvisionedConcurrentExecutions\": 100,    \"Status\": \"READY\",    \"LastModified\": \"2019-12-31T20:28:49+0000\"}                        For API details, see                        GetProvisionedConcurrencyConfig                        in AWS CLI Command Reference.                    ",
                                    "For a complete list of AWS SDK developer guides and code examples, see    Using Lambda with an AWS SDK.    This topic also includes information about getting started and details about previous SDK versions."
                                ]
                            },
                            {
                                "title": "Invoke",
                                "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_lambda_Invoke_section.html",
                                "sections": [
                                    "",
                                    "The following code examples show how to use Invoke.",
                                    "Action examples are code excerpts from larger programs and must be run in context. You can see this action in            context in the following code example:            ",
                                    ".NETAWS SDK for .NETNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        AWS Code            Examples Repository.        /// <summary>    /// Invoke a Lambda function.    /// </summary>    /// <param name=\"functionName\">The name of the Lambda function to    /// invoke.</param    /// <param name=\"parameters\">The parameter values that will be passed to the function.</param>    /// <returns>A System Threading Task.</returns>    public async Task<string> InvokeFunctionAsync(        string functionName,        string parameters)    {        var payload = parameters;        var request = new InvokeRequest        {            FunctionName = functionName,            Payload = payload,        };        var response = await _lambdaService.InvokeAsync(request);        MemoryStream stream = response.Payload;        string returnValue = System.Text.Encoding.UTF8.GetString(stream.ToArray());        return returnValue;    }                        For API details, see                        Invoke                        in AWS SDK for .NET API Reference.                    C++SDK for C++Note        There's more on GitHub. Find the complete example and learn how to set up and run in the        AWS Code            Examples Repository.            Aws::Client::ClientConfiguration clientConfig;        // Optional: Set to the AWS Region in which the bucket was created (overrides config file).        // clientConfig.region = \"us-east-1\";    Aws::Lambda::LambdaClient client(clientConfig);        Aws::Lambda::Model::InvokeRequest request;        request.SetFunctionName(LAMBDA_NAME);        request.SetLogType(logType);        std::shared_ptr<Aws::IOStream> payload = Aws::MakeShared<Aws::StringStream>(                \"FunctionTest\");        *payload << jsonPayload.View().WriteReadable();        request.SetBody(payload);        request.SetContentType(\"application/json\");        Aws::Lambda::Model::InvokeOutcome outcome = client.Invoke(request);        if (outcome.IsSuccess()) {            invokeResult = std::move(outcome.GetResult());            result = true;            break;        }        else {            std::cerr << \"Error with Lambda::InvokeRequest. \"                      << outcome.GetError().GetMessage()                      << std::endl;            break;        }                        For API details, see                        Invoke                        in AWS SDK for C++ API Reference.                    CLIAWS CLIExample 1: To invoke a Lambda function synchronouslyThe following invoke example invokes the my-function function synchronously. The cli-binary-format option is required if you're using AWS CLI version 2. For more information, see AWS CLI supported global command line options in the AWS Command Line Interface User Guide.aws lambda invoke \\    --function-name my-function \\    --cli-binary-format raw-in-base64-out \\    --payload '{ \"name\": \"Bob\" }' \\    response.jsonOutput:{    \"ExecutedVersion\": \"$LATEST\",    \"StatusCode\": 200}For more information, see Synchronous Invocation in the AWS Lambda Developer Guide.Example 2: To invoke a Lambda function asynchronouslyThe following invoke example invokes the my-function function asynchronously. The cli-binary-format option is required if you're using AWS CLI version 2. For more information, see AWS CLI supported global command line options in the AWS Command Line Interface User Guide.aws lambda invoke \\    --function-name my-function \\    --invocation-type Event \\    --cli-binary-format raw-in-base64-out \\    --payload '{ \"name\": \"Bob\" }' \\    response.jsonOutput:{    \"StatusCode\": 202}For more information, see Asynchronous Invocation in the AWS Lambda Developer Guide.                        For API details, see                        Invoke                        in AWS CLI Command Reference.                    GoSDK for Go V2Note        There's more on GitHub. Find the complete example and learn how to set up and run in the        AWS Code            Examples Repository.    import (\t\"bytes\"\t\"context\"\t\"encoding/json\"\t\"errors\"\t\"log\"\t\"time\"\t\"github.com/aws/aws-sdk-go-v2/aws\"\t\"github.com/aws/aws-sdk-go-v2/service/lambda\"\t\"github.com/aws/aws-sdk-go-v2/service/lambda/types\")// FunctionWrapper encapsulates function actions used in the examples.// It contains an AWS Lambda service client that is used to perform user actions.type FunctionWrapper struct {\tLambdaClient *lambda.Client}// Invoke invokes the Lambda function specified by functionName, passing the parameters// as a JSON payload. When getLog is true, types.LogTypeTail is specified, which tells// Lambda to include the last few log lines in the returned result.func (wrapper FunctionWrapper) Invoke(ctx context.Context, functionName string, parameters any, getLog bool) *lambda.InvokeOutput {\tlogType := types.LogTypeNone\tif getLog {\t\tlogType = types.LogTypeTail\t}\tpayload, err := json.Marshal(parameters)\tif err != nil {\t\tlog.Panicf(\"Couldn't marshal parameters to JSON. Here's why %v\\n\", err)\t}\tinvokeOutput, err := wrapper.LambdaClient.Invoke(ctx, &lambda.InvokeInput{\t\tFunctionName: aws.String(functionName),\t\tLogType:      logType,\t\tPayload:      payload,\t})\tif err != nil {\t\tlog.Panicf(\"Couldn't invoke function %v. Here's why: %v\\n\", functionName, err)\t}\treturn invokeOutput}                        For API details, see                        Invoke                        in AWS SDK for Go API Reference.                    JavaSDK for Java 2.xNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        AWS Code            Examples Repository.        /**     * Invokes a specific AWS Lambda function.     *     * @param awsLambda    an instance of {@link LambdaClient} to interact with the AWS Lambda service     * @param functionName the name of the AWS Lambda function to be invoked     */    public static void invokeFunction(LambdaClient awsLambda, String functionName) {        InvokeResponse res;        try {            // Need a SdkBytes instance for the payload.            JSONObject jsonObj = new JSONObject();            jsonObj.put(\"inputValue\", \"2000\");            String json = jsonObj.toString();            SdkBytes payload = SdkBytes.fromUtf8String(json);            InvokeRequest request = InvokeRequest.builder()                .functionName(functionName)                .payload(payload)                .build();            res = awsLambda.invoke(request);            String value = res.payload().asUtf8String();            System.out.println(value);        } catch (LambdaException e) {            System.err.println(e.getMessage());            System.exit(1);        }    }                        For API details, see                        Invoke                        in AWS SDK for Java 2.x API Reference.                    JavaScriptSDK for JavaScript (v3)Note        There's more on GitHub. Find the complete example and learn how to set up and run in the        AWS Code            Examples Repository.    const invoke = async (funcName, payload) => {  const client = new LambdaClient({});  const command = new InvokeCommand({    FunctionName: funcName,    Payload: JSON.stringify(payload),    LogType: LogType.Tail,  });  const { Payload, LogResult } = await client.send(command);  const result = Buffer.from(Payload).toString();  const logs = Buffer.from(LogResult, \"base64\").toString();  return { logs, result };};                        For API details, see                        Invoke                        in AWS SDK for JavaScript API Reference.                    KotlinSDK for KotlinNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        AWS Code            Examples Repository.    suspend fun invokeFunction(functionNameVal: String) {    val json = \"\"\"{\"inputValue\":\"1000\"}\"\"\"    val byteArray = json.trimIndent().encodeToByteArray()    val request =        InvokeRequest {            functionName = functionNameVal            logType = LogType.Tail            payload = byteArray        }    LambdaClient { region = \"us-west-2\" }.use { awsLambda ->        val res = awsLambda.invoke(request)        println(\"${res.payload?.toString(Charsets.UTF_8)}\")        println(\"The log result is ${res.logResult}\")    }}                        For API details, see                        Invoke                        in AWS SDK for Kotlin API reference.                    PHPSDK for PHPNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        AWS Code            Examples Repository.        public function invoke($functionName, $params, $logType = 'None')    {        return $this->lambdaClient->invoke([            'FunctionName' => $functionName,            'Payload' => json_encode($params),            'LogType' => $logType,        ]);    }                        For API details, see                        Invoke                        in AWS SDK for PHP API Reference.                    PythonSDK for Python (Boto3)Note        There's more on GitHub. Find the complete example and learn how to set up and run in the        AWS Code            Examples Repository.    class LambdaWrapper:    def __init__(self, lambda_client, iam_resource):        self.lambda_client = lambda_client        self.iam_resource = iam_resource    def invoke_function(self, function_name, function_params, get_log=False):        \"\"\"        Invokes a Lambda function.        :param function_name: The name of the function to invoke.        :param function_params: The parameters of the function as a dict. This dict                                is serialized to JSON before it is sent to Lambda.        :param get_log: When true, the last 4 KB of the execution log are included in                        the response.        :return: The response from the function invocation.        \"\"\"        try:            response = self.lambda_client.invoke(                FunctionName=function_name,                Payload=json.dumps(function_params),                LogType=\"Tail\" if get_log else \"None\",            )            logger.info(\"Invoked function %s.\", function_name)        except ClientError:            logger.exception(\"Couldn't invoke function %s.\", function_name)            raise        return response                        For API details, see                        Invoke                        in AWS SDK for Python (Boto3) API Reference.                    RubySDK for RubyNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        AWS Code            Examples Repository.    class LambdaWrapper  attr_accessor :lambda_client, :cloudwatch_client, :iam_client  def initialize    @lambda_client = Aws::Lambda::Client.new    @cloudwatch_client = Aws::CloudWatchLogs::Client.new(region: 'us-east-1')    @iam_client = Aws::IAM::Client.new(region: 'us-east-1')    @logger = Logger.new($stdout)    @logger.level = Logger::WARN  end  # Invokes a Lambda function.  # @param function_name [String] The name of the function to invoke.  # @param payload [nil] Payload containing runtime parameters.  # @return [Object] The response from the function invocation.  def invoke_function(function_name, payload = nil)    params = { function_name: function_name }    params[:payload] = payload unless payload.nil?    @lambda_client.invoke(params)  rescue Aws::Lambda::Errors::ServiceException => e    @logger.error(\"There was an error executing #{function_name}:\\n #{e.message}\")  end                        For API details, see                        Invoke                        in AWS SDK for Ruby API Reference.                    RustSDK for RustNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        AWS Code            Examples Repository.        /** Invoke the lambda function using calculator InvokeArgs. */    pub async fn invoke(&self, args: InvokeArgs) -> Result<InvokeOutput, anyhow::Error> {        info!(?args, \"Invoking {}\", self.lambda_name);        let payload = serde_json::to_string(&args)?;        debug!(?payload, \"Sending payload\");        self.lambda_client            .invoke()            .function_name(self.lambda_name.clone())            .payload(Blob::new(payload))            .send()            .await            .map_err(anyhow::Error::from)    }fn log_invoke_output(invoke: &InvokeOutput, message: &str) {    if let Some(payload) = invoke.payload().cloned() {        let payload = String::from_utf8(payload.into_inner());        info!(?payload, message);    } else {        info!(\"Could not extract payload\")    }    if let Some(logs) = invoke.log_result() {        debug!(?logs, \"Invoked function logs\")    } else {        debug!(\"Invoked function had no logs\")    }}                        For API details, see                        Invoke                        in AWS SDK for Rust API reference.                    SAP ABAPSDK for SAP ABAPNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        AWS Code            Examples Repository.        TRY.        DATA(lv_json) = /aws1/cl_rt_util=>string_to_xstring(          `{`  &&            `\"action\": \"increment\",`  &&            `\"number\": 10` &&          `}`        ).        oo_result =  lo_lmd->invoke(                  \" oo_result is returned for testing purposes. \"                 iv_functionname = iv_function_name                 iv_payload = lv_json             ).        MESSAGE 'Lambda function invoked.' TYPE 'I'.      CATCH /aws1/cx_lmdinvparamvalueex.        MESSAGE 'The request contains a non-valid parameter.' TYPE 'E'.      CATCH /aws1/cx_lmdinvrequestcontex.        MESSAGE 'Unable to parse request body as JSON.' TYPE 'E'.      CATCH /aws1/cx_lmdinvalidzipfileex.        MESSAGE 'The deployment package could not be unzipped.' TYPE 'E'.      CATCH /aws1/cx_lmdrequesttoolargeex.        MESSAGE 'Invoke request body JSON input limit was exceeded by the request payload.' TYPE 'E'.      CATCH /aws1/cx_lmdresourceconflictex.        MESSAGE 'Resource already exists or another operation is in progress.' TYPE 'E'.      CATCH /aws1/cx_lmdresourcenotfoundex.        MESSAGE 'The requested resource does not exist.' TYPE 'E'.      CATCH /aws1/cx_lmdserviceexception.        MESSAGE 'An internal problem was encountered by the AWS Lambda service.' TYPE 'E'.      CATCH /aws1/cx_lmdtoomanyrequestsex.        MESSAGE 'The maximum request throughput was reached.' TYPE 'E'.      CATCH /aws1/cx_lmdunsuppedmediatyp00.        MESSAGE 'Invoke request body does not have JSON as its content type.' TYPE 'E'.    ENDTRY.                        For API details, see                        Invoke                        in AWS SDK for SAP ABAP API reference.                    anchoranchoranchoranchoranchoranchoranchoranchoranchoranchoranchoranchor.NETC++CLIGoJavaJavaScriptKotlinPHPPythonRubyRustSAP ABAPAWS SDK for .NETNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        AWS Code            Examples Repository.        /// <summary>    /// Invoke a Lambda function.    /// </summary>    /// <param name=\"functionName\">The name of the Lambda function to    /// invoke.</param    /// <param name=\"parameters\">The parameter values that will be passed to the function.</param>    /// <returns>A System Threading Task.</returns>    public async Task<string> InvokeFunctionAsync(        string functionName,        string parameters)    {        var payload = parameters;        var request = new InvokeRequest        {            FunctionName = functionName,            Payload = payload,        };        var response = await _lambdaService.InvokeAsync(request);        MemoryStream stream = response.Payload;        string returnValue = System.Text.Encoding.UTF8.GetString(stream.ToArray());        return returnValue;    }                        For API details, see                        Invoke                        in AWS SDK for .NET API Reference.                    ",
                                    "For a complete list of AWS SDK developer guides and code examples, see    Using Lambda with an AWS SDK.    This topic also includes information about getting started and details about previous SDK versions."
                                ]
                            },
                            {
                                "title": "ListFunctions",
                                "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_lambda_ListFunctions_section.html",
                                "sections": [
                                    "",
                                    "The following code examples show how to use ListFunctions.",
                                    "Action examples are code excerpts from larger programs and must be run in context. You can see this action in            context in the following code example:            ",
                                    ".NETAWS SDK for .NETNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        AWS Code            Examples Repository.        /// <summary>    /// Get a list of Lambda functions.    /// </summary>    /// <returns>A list of FunctionConfiguration objects.</returns>    public async Task<List<FunctionConfiguration>> ListFunctionsAsync()    {        var functionList = new List<FunctionConfiguration>();        var functionPaginator =            _lambdaService.Paginators.ListFunctions(new ListFunctionsRequest());        await foreach (var function in functionPaginator.Functions)        {            functionList.Add(function);        }        return functionList;    }                        For API details, see                        ListFunctions                        in AWS SDK for .NET API Reference.                    C++SDK for C++Note        There's more on GitHub. Find the complete example and learn how to set up and run in the        AWS Code            Examples Repository.            Aws::Client::ClientConfiguration clientConfig;        // Optional: Set to the AWS Region in which the bucket was created (overrides config file).        // clientConfig.region = \"us-east-1\";    Aws::Lambda::LambdaClient client(clientConfig);    std::vector<Aws::String> functions;    Aws::String marker;    do {        Aws::Lambda::Model::ListFunctionsRequest request;        if (!marker.empty()) {            request.SetMarker(marker);        }        Aws::Lambda::Model::ListFunctionsOutcome outcome = client.ListFunctions(                request);        if (outcome.IsSuccess()) {            const Aws::Lambda::Model::ListFunctionsResult &result = outcome.GetResult();            std::cout << result.GetFunctions().size()                      << \" lambda functions were retrieved.\" << std::endl;            for (const Aws::Lambda::Model::FunctionConfiguration &functionConfiguration: result.GetFunctions()) {                functions.push_back(functionConfiguration.GetFunctionName());                std::cout << functions.size() << \"  \"                          << functionConfiguration.GetDescription() << std::endl;                std::cout << \"   \"                          << Aws::Lambda::Model::RuntimeMapper::GetNameForRuntime(                                  functionConfiguration.GetRuntime()) << \": \"                          << functionConfiguration.GetHandler()                          << std::endl;            }            marker = result.GetNextMarker();        }        else {            std::cerr << \"Error with Lambda::ListFunctions. \"                      << outcome.GetError().GetMessage()                      << std::endl;        }    } while (!marker.empty());                        For API details, see                        ListFunctions                        in AWS SDK for C++ API Reference.                    CLIAWS CLITo retrieve a list of Lambda functionsThe following list-functions example displays a list of all of the functions for the current user.aws lambda list-functionsOutput:{    \"Functions\": [        {            \"TracingConfig\": {                \"Mode\": \"PassThrough\"            },            \"Version\": \"$LATEST\",            \"CodeSha256\": \"dBG9m8SGdmlEjw/JYXlhhvCrAv5TxvXsbL/RMr0fT/I=\",            \"FunctionName\": \"helloworld\",            \"MemorySize\": 128,            \"RevisionId\": \"1718e831-badf-4253-9518-d0644210af7b\",            \"CodeSize\": 294,            \"FunctionArn\": \"arn:aws:lambda:us-west-2:123456789012:function:helloworld\",            \"Handler\": \"helloworld.handler\",            \"Role\": \"arn:aws:iam::123456789012:role/service-role/MyTestFunction-role-zgur6bf4\",            \"Timeout\": 3,            \"LastModified\": \"2023-09-23T18:32:33.857+0000\",            \"Runtime\": \"nodejs18.x\",            \"Description\": \"\"        },        {            \"TracingConfig\": {                \"Mode\": \"PassThrough\"            },            \"Version\": \"$LATEST\",            \"CodeSha256\": \"sU0cJ2/hOZevwV/lTxCuQqK3gDZP3i8gUoqUUVRmY6E=\",            \"FunctionName\": \"my-function\",            \"VpcConfig\": {                \"SubnetIds\": [],                \"VpcId\": \"\",                \"SecurityGroupIds\": []            },            \"MemorySize\": 256,            \"RevisionId\": \"93017fc9-59cb-41dc-901b-4845ce4bf668\",            \"CodeSize\": 266,            \"FunctionArn\": \"arn:aws:lambda:us-west-2:123456789012:function:my-function\",            \"Handler\": \"index.handler\",            \"Role\": \"arn:aws:iam::123456789012:role/service-role/helloWorldPython-role-uy3l9qyq\",            \"Timeout\": 3,            \"LastModified\": \"2023-10-01T16:47:28.490+0000\",            \"Runtime\": \"nodejs18.x\",            \"Description\": \"\"        },        {            \"Layers\": [                {                    \"CodeSize\": 41784542,                    \"Arn\": \"arn:aws:lambda:us-west-2:420165488524:layer:AWSLambda-Python37-SciPy1x:2\"                },                {                    \"CodeSize\": 4121,                    \"Arn\": \"arn:aws:lambda:us-west-2:123456789012:layer:pythonLayer:1\"                }            ],            \"TracingConfig\": {                \"Mode\": \"PassThrough\"            },            \"Version\": \"$LATEST\",            \"CodeSha256\": \"ZQukCqxtkqFgyF2cU41Avj99TKQ/hNihPtDtRcc08mI=\",            \"FunctionName\": \"my-python-function\",            \"VpcConfig\": {                \"SubnetIds\": [],                \"VpcId\": \"\",                \"SecurityGroupIds\": []            },            \"MemorySize\": 128,            \"RevisionId\": \"80b4eabc-acf7-4ea8-919a-e874c213707d\",            \"CodeSize\": 299,            \"FunctionArn\": \"arn:aws:lambda:us-west-2:123456789012:function:my-python-function\",            \"Handler\": \"lambda_function.lambda_handler\",            \"Role\": \"arn:aws:iam::123456789012:role/service-role/my-python-function-role-z5g7dr6n\",            \"Timeout\": 3,            \"LastModified\": \"2023-10-01T19:40:41.643+0000\",            \"Runtime\": \"python3.11\",            \"Description\": \"\"        }    ]}For more information, see AWS Lambda Function Configuration in the AWS Lambda Developer Guide.                        For API details, see                        ListFunctions                        in AWS CLI Command Reference.                    GoSDK for Go V2Note        There's more on GitHub. Find the complete example and learn how to set up and run in the        AWS Code            Examples Repository.    import (\t\"bytes\"\t\"context\"\t\"encoding/json\"\t\"errors\"\t\"log\"\t\"time\"\t\"github.com/aws/aws-sdk-go-v2/aws\"\t\"github.com/aws/aws-sdk-go-v2/service/lambda\"\t\"github.com/aws/aws-sdk-go-v2/service/lambda/types\")// FunctionWrapper encapsulates function actions used in the examples.// It contains an AWS Lambda service client that is used to perform user actions.type FunctionWrapper struct {\tLambdaClient *lambda.Client}// ListFunctions lists up to maxItems functions for the account. This function uses a// lambda.ListFunctionsPaginator to paginate the results.func (wrapper FunctionWrapper) ListFunctions(ctx context.Context, maxItems int) []types.FunctionConfiguration {\tvar functions []types.FunctionConfiguration\tpaginator := lambda.NewListFunctionsPaginator(wrapper.LambdaClient, &lambda.ListFunctionsInput{\t\tMaxItems: aws.Int32(int32(maxItems)),\t})\tfor paginator.HasMorePages() && len(functions) < maxItems {\t\tpageOutput, err := paginator.NextPage(ctx)\t\tif err != nil {\t\t\tlog.Panicf(\"Couldn't list functions for your account. Here's why: %v\\n\", err)\t\t}\t\tfunctions = append(functions, pageOutput.Functions...)\t}\treturn functions}                        For API details, see                        ListFunctions                        in AWS SDK for Go API Reference.                    JavaScriptSDK for JavaScript (v3)Note        There's more on GitHub. Find the complete example and learn how to set up and run in the        AWS Code            Examples Repository.    const listFunctions = () => {  const client = new LambdaClient({});  const command = new ListFunctionsCommand({});  return client.send(command);};                        For API details, see                        ListFunctions                        in AWS SDK for JavaScript API Reference.                    PHPSDK for PHPNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        AWS Code            Examples Repository.        public function listFunctions($maxItems = 50, $marker = null)    {        if (is_null($marker)) {            return $this->lambdaClient->listFunctions([                'MaxItems' => $maxItems,            ]);        }        return $this->lambdaClient->listFunctions([            'Marker' => $marker,            'MaxItems' => $maxItems,        ]);    }                        For API details, see                        ListFunctions                        in AWS SDK for PHP API Reference.                    PowerShellTools for PowerShellExample 1: This sample displays all the Lambda functions with sorted code sizeGet-LMFunctionList | Sort-Object -Property CodeSize | Select-Object FunctionName, RunTime, Timeout, CodeSizeOutput:FunctionName                                                 Runtime   Timeout CodeSize------------                                                 -------   ------- --------test                                                         python2.7       3      243MylambdaFunction123                                          python3.8     600      659myfuncpython1                                                python3.8     303      675                        For API details, see                        ListFunctions                        in AWS Tools for PowerShell Cmdlet Reference.                    PythonSDK for Python (Boto3)Note        There's more on GitHub. Find the complete example and learn how to set up and run in the        AWS Code            Examples Repository.    class LambdaWrapper:    def __init__(self, lambda_client, iam_resource):        self.lambda_client = lambda_client        self.iam_resource = iam_resource    def list_functions(self):        \"\"\"        Lists the Lambda functions for the current account.        \"\"\"        try:            func_paginator = self.lambda_client.get_paginator(\"list_functions\")            for func_page in func_paginator.paginate():                for func in func_page[\"Functions\"]:                    print(func[\"FunctionName\"])                    desc = func.get(\"Description\")                    if desc:                        print(f\"\\t{desc}\")                    print(f\"\\t{func['Runtime']}: {func['Handler']}\")        except ClientError as err:            logger.error(                \"Couldn't list functions. Here's why: %s: %s\",                err.response[\"Error\"][\"Code\"],                err.response[\"Error\"][\"Message\"],            )            raise                        For API details, see                        ListFunctions                        in AWS SDK for Python (Boto3) API Reference.                    RubySDK for RubyNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        AWS Code            Examples Repository.    class LambdaWrapper  attr_accessor :lambda_client, :cloudwatch_client, :iam_client  def initialize    @lambda_client = Aws::Lambda::Client.new    @cloudwatch_client = Aws::CloudWatchLogs::Client.new(region: 'us-east-1')    @iam_client = Aws::IAM::Client.new(region: 'us-east-1')    @logger = Logger.new($stdout)    @logger.level = Logger::WARN  end  # Lists the Lambda functions for the current account.  def list_functions    functions = []    @lambda_client.list_functions.each do |response|      response['functions'].each do |function|        functions.append(function['function_name'])      end    end    functions  rescue Aws::Lambda::Errors::ServiceException => e    @logger.error(\"There was an error listing functions:\\n #{e.message}\")  end                        For API details, see                        ListFunctions                        in AWS SDK for Ruby API Reference.                    RustSDK for RustNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        AWS Code            Examples Repository.        /** List all Lambda functions in the current Region. */    pub async fn list_functions(&self) -> Result<ListFunctionsOutput, anyhow::Error> {        info!(\"Listing lambda functions\");        self.lambda_client            .list_functions()            .send()            .await            .map_err(anyhow::Error::from)    }                        For API details, see                        ListFunctions                        in AWS SDK for Rust API reference.                    SAP ABAPSDK for SAP ABAPNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        AWS Code            Examples Repository.        TRY.        oo_result =  lo_lmd->listfunctions( ).       \" oo_result is returned for testing purposes. \"        DATA(lt_functions) = oo_result->get_functions( ).        MESSAGE 'Retrieved list of Lambda functions.' TYPE 'I'.      CATCH /aws1/cx_lmdinvparamvalueex.        MESSAGE 'The request contains a non-valid parameter.' TYPE 'E'.      CATCH /aws1/cx_lmdserviceexception.        MESSAGE 'An internal problem was encountered by the AWS Lambda service.' TYPE 'E'.      CATCH /aws1/cx_lmdtoomanyrequestsex.        MESSAGE 'The maximum request throughput was reached.' TYPE 'E'.    ENDTRY.                        For API details, see                        ListFunctions                        in AWS SDK for SAP ABAP API reference.                    anchoranchoranchoranchoranchoranchoranchoranchoranchoranchoranchor.NETC++CLIGoJavaScriptPHPPowerShellPythonRubyRustSAP ABAPAWS SDK for .NETNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        AWS Code            Examples Repository.        /// <summary>    /// Get a list of Lambda functions.    /// </summary>    /// <returns>A list of FunctionConfiguration objects.</returns>    public async Task<List<FunctionConfiguration>> ListFunctionsAsync()    {        var functionList = new List<FunctionConfiguration>();        var functionPaginator =            _lambdaService.Paginators.ListFunctions(new ListFunctionsRequest());        await foreach (var function in functionPaginator.Functions)        {            functionList.Add(function);        }        return functionList;    }                        For API details, see                        ListFunctions                        in AWS SDK for .NET API Reference.                    ",
                                    "For a complete list of AWS SDK developer guides and code examples, see    Using Lambda with an AWS SDK.    This topic also includes information about getting started and details about previous SDK versions."
                                ]
                            },
                            {
                                "title": "ListProvisionedConcurrencyConfigs",
                                "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_lambda_ListProvisionedConcurrencyConfigs_section.html",
                                "sections": [
                                    "",
                                    "The following code examples show how to use ListProvisionedConcurrencyConfigs.",
                                    "CLIAWS CLITo get a list of provisioned concurrency configurationsThe following list-provisioned-concurrency-configs example lists the provisioned concurrency configurations for the specified function.aws lambda list-provisioned-concurrency-configs \\    --function-name my-functionOutput:{    \"ProvisionedConcurrencyConfigs\": [        {            \"FunctionArn\": \"arn:aws:lambda:us-east-2:123456789012:function:my-function:GREEN\",            \"RequestedProvisionedConcurrentExecutions\": 100,            \"AvailableProvisionedConcurrentExecutions\": 100,            \"AllocatedProvisionedConcurrentExecutions\": 100,            \"Status\": \"READY\",            \"LastModified\": \"2019-12-31T20:29:00+0000\"        },        {            \"FunctionArn\": \"arn:aws:lambda:us-east-2:123456789012:function:my-function:BLUE\",            \"RequestedProvisionedConcurrentExecutions\": 100,            \"AvailableProvisionedConcurrentExecutions\": 100,            \"AllocatedProvisionedConcurrentExecutions\": 100,            \"Status\": \"READY\",            \"LastModified\": \"2019-12-31T20:28:49+0000\"        }    ]}                        For API details, see                        ListProvisionedConcurrencyConfigs                        in AWS CLI Command Reference.                    PowerShellTools for PowerShellExample 1: This example retrieves the list of provisioned concurrency configurations for a Lambda function.Get-LMProvisionedConcurrencyConfigList -FunctionName \"MylambdaFunction123\"                        For API details, see                        ListProvisionedConcurrencyConfigs                        in AWS Tools for PowerShell Cmdlet Reference.                    anchoranchorCLIPowerShellAWS CLITo get a list of provisioned concurrency configurationsThe following list-provisioned-concurrency-configs example lists the provisioned concurrency configurations for the specified function.aws lambda list-provisioned-concurrency-configs \\    --function-name my-functionOutput:{    \"ProvisionedConcurrencyConfigs\": [        {            \"FunctionArn\": \"arn:aws:lambda:us-east-2:123456789012:function:my-function:GREEN\",            \"RequestedProvisionedConcurrentExecutions\": 100,            \"AvailableProvisionedConcurrentExecutions\": 100,            \"AllocatedProvisionedConcurrentExecutions\": 100,            \"Status\": \"READY\",            \"LastModified\": \"2019-12-31T20:29:00+0000\"        },        {            \"FunctionArn\": \"arn:aws:lambda:us-east-2:123456789012:function:my-function:BLUE\",            \"RequestedProvisionedConcurrentExecutions\": 100,            \"AvailableProvisionedConcurrentExecutions\": 100,            \"AllocatedProvisionedConcurrentExecutions\": 100,            \"Status\": \"READY\",            \"LastModified\": \"2019-12-31T20:28:49+0000\"        }    ]}                        For API details, see                        ListProvisionedConcurrencyConfigs                        in AWS CLI Command Reference.                    ",
                                    "For a complete list of AWS SDK developer guides and code examples, see    Using Lambda with an AWS SDK.    This topic also includes information about getting started and details about previous SDK versions."
                                ]
                            },
                            {
                                "title": "ListTags",
                                "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_lambda_ListTags_section.html",
                                "sections": [
                                    "",
                                    "The following code examples show how to use ListTags.",
                                    "CLIAWS CLITo retrieve the list of tags for a Lambda functionThe following list-tags example displays the tags attached to the my-function Lambda function.aws lambda list-tags \\    --resource arn:aws:lambda:us-west-2:123456789012:function:my-functionOutput:{    \"Tags\": {        \"Category\": \"Web Tools\",        \"Department\": \"Sales\"    }}For more information, see Tagging Lambda Functions in the AWS Lambda Developer Guide.                        For API details, see                        ListTags                        in AWS CLI Command Reference.                    PowerShellTools for PowerShellExample 1: Retrieves the tags and their values currently set on the specified function.Get-LMResourceTag -Resource \"arn:aws:lambda:us-west-2:123456789012:function:MyFunction\"Output:Key        Value---        -----California SacramentoOregon     SalemWashington Olympia                        For API details, see                        ListTags                        in AWS Tools for PowerShell Cmdlet Reference.                    anchoranchorCLIPowerShellAWS CLITo retrieve the list of tags for a Lambda functionThe following list-tags example displays the tags attached to the my-function Lambda function.aws lambda list-tags \\    --resource arn:aws:lambda:us-west-2:123456789012:function:my-functionOutput:{    \"Tags\": {        \"Category\": \"Web Tools\",        \"Department\": \"Sales\"    }}For more information, see Tagging Lambda Functions in the AWS Lambda Developer Guide.                        For API details, see                        ListTags                        in AWS CLI Command Reference.                    ",
                                    "For a complete list of AWS SDK developer guides and code examples, see    Using Lambda with an AWS SDK.    This topic also includes information about getting started and details about previous SDK versions."
                                ]
                            },
                            {
                                "title": "ListVersionsByFunction",
                                "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_lambda_ListVersionsByFunction_section.html",
                                "sections": [
                                    "",
                                    "The following code examples show how to use ListVersionsByFunction.",
                                    "CLIAWS CLITo retrieve a list of versions of a functionThe following list-versions-by-function example displays the list of versions for the my-function Lambda function.aws lambda list-versions-by-function \\    --function-name my-functionOutput:{    \"Versions\": [        {            \"TracingConfig\": {                \"Mode\": \"PassThrough\"            },            \"Version\": \"$LATEST\",            \"CodeSha256\": \"sU0cJ2/hOZevwV/lTxCuQqK3gDZP3i8gUoqUUVRmY6E=\",            \"FunctionName\": \"my-function\",            \"VpcConfig\": {                \"SubnetIds\": [],                \"VpcId\": \"\",                \"SecurityGroupIds\": []            },            \"MemorySize\": 256,            \"RevisionId\": \"93017fc9-59cb-41dc-901b-4845ce4bf668\",            \"CodeSize\": 266,            \"FunctionArn\": \"arn:aws:lambda:us-west-2:123456789012:function:my-function:$LATEST\",            \"Handler\": \"index.handler\",            \"Role\": \"arn:aws:iam::123456789012:role/service-role/helloWorldPython-role-uy3l9qyq\",            \"Timeout\": 3,            \"LastModified\": \"2019-10-01T16:47:28.490+0000\",            \"Runtime\": \"nodejs10.x\",            \"Description\": \"\"        },        {            \"TracingConfig\": {                \"Mode\": \"PassThrough\"            },            \"Version\": \"1\",            \"CodeSha256\": \"5tT2qgzYUHoqwR616pZ2dpkn/0J1FrzJmlKidWaaCgk=\",            \"FunctionName\": \"my-function\",            \"VpcConfig\": {                \"SubnetIds\": [],                \"VpcId\": \"\",                \"SecurityGroupIds\": []            },            \"MemorySize\": 256,            \"RevisionId\": \"949c8914-012e-4795-998c-e467121951b1\",            \"CodeSize\": 304,            \"FunctionArn\": \"arn:aws:lambda:us-west-2:123456789012:function:my-function:1\",            \"Handler\": \"index.handler\",            \"Role\": \"arn:aws:iam::123456789012:role/service-role/helloWorldPython-role-uy3l9qyq\",            \"Timeout\": 3,            \"LastModified\": \"2019-09-26T20:28:40.438+0000\",            \"Runtime\": \"nodejs10.x\",            \"Description\": \"new version\"        },        {            \"TracingConfig\": {                \"Mode\": \"PassThrough\"            },            \"Version\": \"2\",            \"CodeSha256\": \"sU0cJ2/hOZevwV/lTxCuQqK3gDZP3i8gUoqUUVRmY6E=\",            \"FunctionName\": \"my-function\",            \"VpcConfig\": {                \"SubnetIds\": [],                \"VpcId\": \"\",                \"SecurityGroupIds\": []            },            \"MemorySize\": 256,            \"RevisionId\": \"cd669f21-0f3d-4e1c-9566-948837f2e2ea\",            \"CodeSize\": 266,            \"FunctionArn\": \"arn:aws:lambda:us-west-2:123456789012:function:my-function:2\",            \"Handler\": \"index.handler\",            \"Role\": \"arn:aws:iam::123456789012:role/service-role/helloWorldPython-role-uy3l9qyq\",            \"Timeout\": 3,            \"LastModified\": \"2019-10-01T16:47:28.490+0000\",            \"Runtime\": \"nodejs10.x\",            \"Description\": \"newer version\"        }    ]}For more information, see Configuring AWS Lambda Function Aliases in the AWS Lambda Developer Guide.                        For API details, see                        ListVersionsByFunction                        in AWS CLI Command Reference.                    PowerShellTools for PowerShellExample 1: This example returns the list of version specific configurations for each version of the Lambda Function.Get-LMVersionsByFunction -FunctionName \"MylambdaFunction123\"Output:FunctionName        Runtime   MemorySize Timeout CodeSize LastModified                 RoleName------------        -------   ---------- ------- -------- ------------                 --------MylambdaFunction123 python3.8        128     600      659 2020-01-10T03:20:56.390+0000 lambdaMylambdaFunction123 python3.8        128       5     1426 2019-12-25T09:19:02.238+0000 lambdaMylambdaFunction123 python3.8        128       5     1426 2019-12-25T09:39:36.779+0000 lambdaMylambdaFunction123 python3.8        128     600     1426 2019-12-25T09:52:59.872+0000 lambda                        For API details, see                        ListVersionsByFunction                        in AWS Tools for PowerShell Cmdlet Reference.                    anchoranchorCLIPowerShellAWS CLITo retrieve a list of versions of a functionThe following list-versions-by-function example displays the list of versions for the my-function Lambda function.aws lambda list-versions-by-function \\    --function-name my-functionOutput:{    \"Versions\": [        {            \"TracingConfig\": {                \"Mode\": \"PassThrough\"            },            \"Version\": \"$LATEST\",            \"CodeSha256\": \"sU0cJ2/hOZevwV/lTxCuQqK3gDZP3i8gUoqUUVRmY6E=\",            \"FunctionName\": \"my-function\",            \"VpcConfig\": {                \"SubnetIds\": [],                \"VpcId\": \"\",                \"SecurityGroupIds\": []            },            \"MemorySize\": 256,            \"RevisionId\": \"93017fc9-59cb-41dc-901b-4845ce4bf668\",            \"CodeSize\": 266,            \"FunctionArn\": \"arn:aws:lambda:us-west-2:123456789012:function:my-function:$LATEST\",            \"Handler\": \"index.handler\",            \"Role\": \"arn:aws:iam::123456789012:role/service-role/helloWorldPython-role-uy3l9qyq\",            \"Timeout\": 3,            \"LastModified\": \"2019-10-01T16:47:28.490+0000\",            \"Runtime\": \"nodejs10.x\",            \"Description\": \"\"        },        {            \"TracingConfig\": {                \"Mode\": \"PassThrough\"            },            \"Version\": \"1\",            \"CodeSha256\": \"5tT2qgzYUHoqwR616pZ2dpkn/0J1FrzJmlKidWaaCgk=\",            \"FunctionName\": \"my-function\",            \"VpcConfig\": {                \"SubnetIds\": [],                \"VpcId\": \"\",                \"SecurityGroupIds\": []            },            \"MemorySize\": 256,            \"RevisionId\": \"949c8914-012e-4795-998c-e467121951b1\",            \"CodeSize\": 304,            \"FunctionArn\": \"arn:aws:lambda:us-west-2:123456789012:function:my-function:1\",            \"Handler\": \"index.handler\",            \"Role\": \"arn:aws:iam::123456789012:role/service-role/helloWorldPython-role-uy3l9qyq\",            \"Timeout\": 3,            \"LastModified\": \"2019-09-26T20:28:40.438+0000\",            \"Runtime\": \"nodejs10.x\",            \"Description\": \"new version\"        },        {            \"TracingConfig\": {                \"Mode\": \"PassThrough\"            },            \"Version\": \"2\",            \"CodeSha256\": \"sU0cJ2/hOZevwV/lTxCuQqK3gDZP3i8gUoqUUVRmY6E=\",            \"FunctionName\": \"my-function\",            \"VpcConfig\": {                \"SubnetIds\": [],                \"VpcId\": \"\",                \"SecurityGroupIds\": []            },            \"MemorySize\": 256,            \"RevisionId\": \"cd669f21-0f3d-4e1c-9566-948837f2e2ea\",            \"CodeSize\": 266,            \"FunctionArn\": \"arn:aws:lambda:us-west-2:123456789012:function:my-function:2\",            \"Handler\": \"index.handler\",            \"Role\": \"arn:aws:iam::123456789012:role/service-role/helloWorldPython-role-uy3l9qyq\",            \"Timeout\": 3,            \"LastModified\": \"2019-10-01T16:47:28.490+0000\",            \"Runtime\": \"nodejs10.x\",            \"Description\": \"newer version\"        }    ]}For more information, see Configuring AWS Lambda Function Aliases in the AWS Lambda Developer Guide.                        For API details, see                        ListVersionsByFunction                        in AWS CLI Command Reference.                    ",
                                    "For a complete list of AWS SDK developer guides and code examples, see    Using Lambda with an AWS SDK.    This topic also includes information about getting started and details about previous SDK versions."
                                ]
                            },
                            {
                                "title": "PublishVersion",
                                "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_lambda_PublishVersion_section.html",
                                "sections": [
                                    "",
                                    "The following code examples show how to use PublishVersion.",
                                    "CLIAWS CLITo publish a new version of a functionThe following publish-version example publishes a new version of the my-function Lambda function.aws lambda publish-version \\    --function-name my-functionOutput:{    \"TracingConfig\": {        \"Mode\": \"PassThrough\"    },    \"CodeSha256\": \"dBG9m8SGdmlEjw/JYXlhhvCrAv5TxvXsbL/RMr0fT/I=\",    \"FunctionName\": \"my-function\",    \"CodeSize\": 294,    \"RevisionId\": \"f31d3d39-cc63-4520-97d4-43cd44c94c20\",    \"MemorySize\": 128,    \"FunctionArn\": \"arn:aws:lambda:us-west-2:123456789012:function:my-function:3\",    \"Version\": \"2\",    \"Role\": \"arn:aws:iam::123456789012:role/service-role/MyTestFunction-role-zgur6bf4\",    \"Timeout\": 3,    \"LastModified\": \"2019-09-23T18:32:33.857+0000\",    \"Handler\": \"my-function.handler\",    \"Runtime\": \"nodejs10.x\",    \"Description\": \"\"}For more information, see Configuring AWS Lambda Function Aliases in the AWS Lambda Developer Guide.                        For API details, see                        PublishVersion                        in AWS CLI Command Reference.                    PowerShellTools for PowerShellExample 1: This example creates a version for the existing snapshot of Lambda Function CodePublish-LMVersion -FunctionName \"MylambdaFunction123\" -Description \"Publishing Existing Snapshot of function code as a  new version through Powershell\"                        For API details, see                        PublishVersion                        in AWS Tools for PowerShell Cmdlet Reference.                    anchoranchorCLIPowerShellAWS CLITo publish a new version of a functionThe following publish-version example publishes a new version of the my-function Lambda function.aws lambda publish-version \\    --function-name my-functionOutput:{    \"TracingConfig\": {        \"Mode\": \"PassThrough\"    },    \"CodeSha256\": \"dBG9m8SGdmlEjw/JYXlhhvCrAv5TxvXsbL/RMr0fT/I=\",    \"FunctionName\": \"my-function\",    \"CodeSize\": 294,    \"RevisionId\": \"f31d3d39-cc63-4520-97d4-43cd44c94c20\",    \"MemorySize\": 128,    \"FunctionArn\": \"arn:aws:lambda:us-west-2:123456789012:function:my-function:3\",    \"Version\": \"2\",    \"Role\": \"arn:aws:iam::123456789012:role/service-role/MyTestFunction-role-zgur6bf4\",    \"Timeout\": 3,    \"LastModified\": \"2019-09-23T18:32:33.857+0000\",    \"Handler\": \"my-function.handler\",    \"Runtime\": \"nodejs10.x\",    \"Description\": \"\"}For more information, see Configuring AWS Lambda Function Aliases in the AWS Lambda Developer Guide.                        For API details, see                        PublishVersion                        in AWS CLI Command Reference.                    ",
                                    "For a complete list of AWS SDK developer guides and code examples, see    Using Lambda with an AWS SDK.    This topic also includes information about getting started and details about previous SDK versions."
                                ]
                            },
                            {
                                "title": "PutFunctionConcurrency",
                                "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_lambda_PutFunctionConcurrency_section.html",
                                "sections": [
                                    "",
                                    "The following code examples show how to use PutFunctionConcurrency.",
                                    "CLIAWS CLITo configure a reserved concurrency limit for a functionThe following put-function-concurrency example configures 100 reserved concurrent executions for the my-function function.aws lambda put-function-concurrency \\    --function-name  my-function  \\    --reserved-concurrent-executions 100Output:{    \"ReservedConcurrentExecutions\": 100}For more information, see Reserving Concurrency for a Lambda Function in the AWS Lambda Developer Guide.                        For API details, see                        PutFunctionConcurrency                        in AWS CLI Command Reference.                    PowerShellTools for PowerShellExample 1: This example applies the concurrency settings for the Function as a whole.Write-LMFunctionConcurrency -FunctionName \"MylambdaFunction123\" -ReservedConcurrentExecution 100                        For API details, see                        PutFunctionConcurrency                        in AWS Tools for PowerShell Cmdlet Reference.                    anchoranchorCLIPowerShellAWS CLITo configure a reserved concurrency limit for a functionThe following put-function-concurrency example configures 100 reserved concurrent executions for the my-function function.aws lambda put-function-concurrency \\    --function-name  my-function  \\    --reserved-concurrent-executions 100Output:{    \"ReservedConcurrentExecutions\": 100}For more information, see Reserving Concurrency for a Lambda Function in the AWS Lambda Developer Guide.                        For API details, see                        PutFunctionConcurrency                        in AWS CLI Command Reference.                    ",
                                    "For a complete list of AWS SDK developer guides and code examples, see    Using Lambda with an AWS SDK.    This topic also includes information about getting started and details about previous SDK versions."
                                ]
                            },
                            {
                                "title": "PutProvisionedConcurrencyConfig",
                                "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_lambda_PutProvisionedConcurrencyConfig_section.html",
                                "sections": [
                                    "",
                                    "The following code examples show how to use PutProvisionedConcurrencyConfig.",
                                    "CLIAWS CLITo allocate provisioned concurrencyThe following put-provisioned-concurrency-config example allocates 100 provisioned concurrency for the BLUE alias of the specified function.aws lambda put-provisioned-concurrency-config \\    --function-name my-function \\    --qualifier BLUE \\    --provisioned-concurrent-executions 100Output:{    \"Requested ProvisionedConcurrentExecutions\": 100,    \"Allocated ProvisionedConcurrentExecutions\": 0,    \"Status\": \"IN_PROGRESS\",    \"LastModified\": \"2019-11-21T19:32:12+0000\"}                        For API details, see                        PutProvisionedConcurrencyConfig                        in AWS CLI Command Reference.                    PowerShellTools for PowerShellExample 1: This example adds a provisioned concurrency configuration to a Function's AliasWrite-LMProvisionedConcurrencyConfig -FunctionName \"MylambdaFunction123\" -ProvisionedConcurrentExecution 20 -Qualifier \"NewAlias1\"                        For API details, see                        PutProvisionedConcurrencyConfig                        in AWS Tools for PowerShell Cmdlet Reference.                    anchoranchorCLIPowerShellAWS CLITo allocate provisioned concurrencyThe following put-provisioned-concurrency-config example allocates 100 provisioned concurrency for the BLUE alias of the specified function.aws lambda put-provisioned-concurrency-config \\    --function-name my-function \\    --qualifier BLUE \\    --provisioned-concurrent-executions 100Output:{    \"Requested ProvisionedConcurrentExecutions\": 100,    \"Allocated ProvisionedConcurrentExecutions\": 0,    \"Status\": \"IN_PROGRESS\",    \"LastModified\": \"2019-11-21T19:32:12+0000\"}                        For API details, see                        PutProvisionedConcurrencyConfig                        in AWS CLI Command Reference.                    ",
                                    "For a complete list of AWS SDK developer guides and code examples, see    Using Lambda with an AWS SDK.    This topic also includes information about getting started and details about previous SDK versions."
                                ]
                            },
                            {
                                "title": "RemovePermission",
                                "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_lambda_RemovePermission_section.html",
                                "sections": [
                                    "",
                                    "The following code examples show how to use RemovePermission.",
                                    "CLIAWS CLITo remove permissions from an existing Lambda functionThe following remove-permission example removes permission to invoke a function named my-function.aws lambda remove-permission \\    --function-name my-function \\    --statement-id snsThis command produces no output.For more information, see Using Resource-based Policies for AWS Lambda in the AWS Lambda Developer Guide.                        For API details, see                        RemovePermission                        in AWS CLI Command Reference.                    PowerShellTools for PowerShellExample 1: This example removes the function policy for the specified StatementId of a Lambda Function.$policy =  Get-LMPolicy -FunctionName \"MylambdaFunction123\" -Select Policy | ConvertFrom-Json| Select-Object -ExpandProperty StatementRemove-LMPermission -FunctionName \"MylambdaFunction123\" -StatementId $policy[0].Sid                        For API details, see                        RemovePermission                        in AWS Tools for PowerShell Cmdlet Reference.                    anchoranchorCLIPowerShellAWS CLITo remove permissions from an existing Lambda functionThe following remove-permission example removes permission to invoke a function named my-function.aws lambda remove-permission \\    --function-name my-function \\    --statement-id snsThis command produces no output.For more information, see Using Resource-based Policies for AWS Lambda in the AWS Lambda Developer Guide.                        For API details, see                        RemovePermission                        in AWS CLI Command Reference.                    ",
                                    "For a complete list of AWS SDK developer guides and code examples, see    Using Lambda with an AWS SDK.    This topic also includes information about getting started and details about previous SDK versions."
                                ]
                            },
                            {
                                "title": "TagResource",
                                "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_lambda_TagResource_section.html",
                                "sections": [
                                    "",
                                    "The following code examples show how to use TagResource.",
                                    "CLIAWS CLITo add tags to an existing Lambda functionThe following tag-resource example adds a tag with the key name DEPARTMENT and a value of Department A to the specified Lambda function.aws lambda tag-resource \\    --resource arn:aws:lambda:us-west-2:123456789012:function:my-function \\    --tags \"DEPARTMENT=Department A\"This command produces no output.For more information, see Tagging Lambda Functions in the AWS Lambda Developer Guide.                        For API details, see                        TagResource                        in AWS CLI Command Reference.                    PowerShellTools for PowerShellExample 1: Adds the three tags (Washington, Oregon and California) and their associated values to the specified function identified by its ARN.Add-LMResourceTag -Resource \"arn:aws:lambda:us-west-2:123456789012:function:MyFunction\" -Tag @{ \"Washington\" = \"Olympia\"; \"Oregon\" = \"Salem\"; \"California\" = \"Sacramento\" }                        For API details, see                        TagResource                        in AWS Tools for PowerShell Cmdlet Reference.                    anchoranchorCLIPowerShellAWS CLITo add tags to an existing Lambda functionThe following tag-resource example adds a tag with the key name DEPARTMENT and a value of Department A to the specified Lambda function.aws lambda tag-resource \\    --resource arn:aws:lambda:us-west-2:123456789012:function:my-function \\    --tags \"DEPARTMENT=Department A\"This command produces no output.For more information, see Tagging Lambda Functions in the AWS Lambda Developer Guide.                        For API details, see                        TagResource                        in AWS CLI Command Reference.                    ",
                                    "For a complete list of AWS SDK developer guides and code examples, see    Using Lambda with an AWS SDK.    This topic also includes information about getting started and details about previous SDK versions."
                                ]
                            },
                            {
                                "title": "UntagResource",
                                "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_lambda_UntagResource_section.html",
                                "sections": [
                                    "",
                                    "The following code examples show how to use UntagResource.",
                                    "CLIAWS CLITo remove tags from an existing Lambda functionThe following untag-resource example removes the tag with the key name DEPARTMENT tag from the my-function Lambda function.aws lambda untag-resource \\    --resource arn:aws:lambda:us-west-2:123456789012:function:my-function \\    --tag-keys DEPARTMENTThis command produces no output.For more information, see Tagging Lambda Functions in the AWS Lambda Developer Guide.                        For API details, see                        UntagResource                        in AWS CLI Command Reference.                    PowerShellTools for PowerShellExample 1: Removes the supplied tags from a function. The cmdlet will prompt for confirmation before proceeding unless the -Force switch is specified. A single call is made to the service to remove the tags.Remove-LMResourceTag -Resource \"arn:aws:lambda:us-west-2:123456789012:function:MyFunction\" -TagKey \"Washington\",\"Oregon\",\"California\"Example 2: Removes the supplied tags from a function. The cmdlet will prompt for confirmation before proceeding unless the -Force switch is specified. Once call to the service is made per supplied tag.\"Washington\",\"Oregon\",\"California\" | Remove-LMResourceTag -Resource \"arn:aws:lambda:us-west-2:123456789012:function:MyFunction\"                        For API details, see                        UntagResource                        in AWS Tools for PowerShell Cmdlet Reference.                    anchoranchorCLIPowerShellAWS CLITo remove tags from an existing Lambda functionThe following untag-resource example removes the tag with the key name DEPARTMENT tag from the my-function Lambda function.aws lambda untag-resource \\    --resource arn:aws:lambda:us-west-2:123456789012:function:my-function \\    --tag-keys DEPARTMENTThis command produces no output.For more information, see Tagging Lambda Functions in the AWS Lambda Developer Guide.                        For API details, see                        UntagResource                        in AWS CLI Command Reference.                    ",
                                    "For a complete list of AWS SDK developer guides and code examples, see    Using Lambda with an AWS SDK.    This topic also includes information about getting started and details about previous SDK versions."
                                ]
                            },
                            {
                                "title": "UpdateAlias",
                                "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_lambda_UpdateAlias_section.html",
                                "sections": [
                                    "",
                                    "The following code examples show how to use UpdateAlias.",
                                    "CLIAWS CLITo update a function aliasThe following update-alias example updates the alias named LIVE to point to version 3 of the my-function Lambda function.aws lambda update-alias \\    --function-name my-function \\    --function-version 3 \\    --name LIVEOutput:{    \"FunctionVersion\": \"3\",    \"Name\": \"LIVE\",    \"AliasArn\": \"arn:aws:lambda:us-west-2:123456789012:function:my-function:LIVE\",    \"RevisionId\": \"594f41fb-b85f-4c20-95c7-6ca5f2a92c93\",    \"Description\": \"alias for live version of function\"}For more information, see Configuring AWS Lambda Function Aliases in the AWS Lambda Developer Guide.                        For API details, see                        UpdateAlias                        in AWS CLI Command Reference.                    PowerShellTools for PowerShellExample 1: This example updates the Configuration of an existing Lambda function Alias. It updates the RoutingConfiguration value to shift 60% (0.6) of traffic to version 1Update-LMAlias -FunctionName \"MylambdaFunction123\" -Description \" Alias for version 2\" -FunctionVersion 2 -Name \"newlabel1\" -RoutingConfig_AdditionalVersionWeight @{Name=\"1\";Value=\"0.6}                        For API details, see                        UpdateAlias                        in AWS Tools for PowerShell Cmdlet Reference.                    anchoranchorCLIPowerShellAWS CLITo update a function aliasThe following update-alias example updates the alias named LIVE to point to version 3 of the my-function Lambda function.aws lambda update-alias \\    --function-name my-function \\    --function-version 3 \\    --name LIVEOutput:{    \"FunctionVersion\": \"3\",    \"Name\": \"LIVE\",    \"AliasArn\": \"arn:aws:lambda:us-west-2:123456789012:function:my-function:LIVE\",    \"RevisionId\": \"594f41fb-b85f-4c20-95c7-6ca5f2a92c93\",    \"Description\": \"alias for live version of function\"}For more information, see Configuring AWS Lambda Function Aliases in the AWS Lambda Developer Guide.                        For API details, see                        UpdateAlias                        in AWS CLI Command Reference.                    ",
                                    "For a complete list of AWS SDK developer guides and code examples, see    Using Lambda with an AWS SDK.    This topic also includes information about getting started and details about previous SDK versions."
                                ]
                            },
                            {
                                "title": "UpdateFunctionCode",
                                "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_lambda_UpdateFunctionCode_section.html",
                                "sections": [
                                    "",
                                    "The following code examples show how to use UpdateFunctionCode.",
                                    "Action examples are code excerpts from larger programs and must be run in context. You can see this action in            context in the following code example:            ",
                                    ".NETAWS SDK for .NETNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        AWS Code            Examples Repository.        /// <summary>    /// Update an existing Lambda function.    /// </summary>    /// <param name=\"functionName\">The name of the Lambda function to update.</param>    /// <param name=\"bucketName\">The bucket where the zip file containing    /// the Lambda function code is stored.</param>    /// <param name=\"key\">The key name of the source code file.</param>    /// <returns>Async Task.</returns>    public async Task UpdateFunctionCodeAsync(        string functionName,        string bucketName,        string key)    {        var functionCodeRequest = new UpdateFunctionCodeRequest        {            FunctionName = functionName,            Publish = true,            S3Bucket = bucketName,            S3Key = key,        };        var response = await _lambdaService.UpdateFunctionCodeAsync(functionCodeRequest);        Console.WriteLine($\"The Function was last modified at {response.LastModified}.\");    }                        For API details, see                        UpdateFunctionCode                        in AWS SDK for .NET API Reference.                    C++SDK for C++Note        There's more on GitHub. Find the complete example and learn how to set up and run in the        AWS Code            Examples Repository.            Aws::Client::ClientConfiguration clientConfig;        // Optional: Set to the AWS Region in which the bucket was created (overrides config file).        // clientConfig.region = \"us-east-1\";    Aws::Lambda::LambdaClient client(clientConfig);        Aws::Lambda::Model::UpdateFunctionCodeRequest request;        request.SetFunctionName(LAMBDA_NAME);        std::ifstream ifstream(CALCULATOR_LAMBDA_CODE.c_str(),                               std::ios_base::in | std::ios_base::binary);        if (!ifstream.is_open()) {            std::cerr << \"Error opening file \" << INCREMENT_LAMBDA_CODE << \".\" << std::endl;#if USE_CPP_LAMBDA_FUNCTION            std::cerr                    << \"The cpp Lambda function must be built following the instructions in the cpp_lambda/README.md file. \"                    << std::endl;#endif            deleteLambdaFunction(client);            deleteIamRole(clientConfig);            return false;        }        Aws::StringStream buffer;        buffer << ifstream.rdbuf();        request.SetZipFile(                Aws::Utils::ByteBuffer((unsigned char *) buffer.str().c_str(),                                       buffer.str().length()));        request.SetPublish(true);        Aws::Lambda::Model::UpdateFunctionCodeOutcome outcome = client.UpdateFunctionCode(                request);        if (outcome.IsSuccess()) {            std::cout << \"The lambda code was successfully updated.\" << std::endl;        }        else {            std::cerr << \"Error with Lambda::UpdateFunctionCode. \"                      << outcome.GetError().GetMessage()                      << std::endl;        }                        For API details, see                        UpdateFunctionCode                        in AWS SDK for C++ API Reference.                    CLIAWS CLITo update the code of a Lambda functionThe following update-function-code example replaces the code of the unpublished ($LATEST) version of the my-function function with the contents of the specified zip file.aws lambda update-function-code \\    --function-name  my-function \\    --zip-file fileb://my-function.zipOutput:{    \"FunctionName\": \"my-function\",    \"LastModified\": \"2019-09-26T20:28:40.438+0000\",    \"RevisionId\": \"e52502d4-9320-4688-9cd6-152a6ab7490d\",    \"MemorySize\": 256,    \"Version\": \"$LATEST\",    \"Role\": \"arn:aws:iam::123456789012:role/service-role/my-function-role-uy3l9qyq\",    \"Timeout\": 3,    \"Runtime\": \"nodejs10.x\",    \"TracingConfig\": {        \"Mode\": \"PassThrough\"    },    \"CodeSha256\": \"5tT2qgzYUHaqwR716pZ2dpkn/0J1FrzJmlKidWoaCgk=\",    \"Description\": \"\",    \"VpcConfig\": {        \"SubnetIds\": [],        \"VpcId\": \"\",        \"SecurityGroupIds\": []    },    \"CodeSize\": 304,    \"FunctionArn\": \"arn:aws:lambda:us-west-2:123456789012:function:my-function\",    \"Handler\": \"index.handler\"}For more information, see AWS Lambda Function Configuration in the AWS Lambda Developer Guide.                        For API details, see                        UpdateFunctionCode                        in AWS CLI Command Reference.                    GoSDK for Go V2Note        There's more on GitHub. Find the complete example and learn how to set up and run in the        AWS Code            Examples Repository.    import (\t\"bytes\"\t\"context\"\t\"encoding/json\"\t\"errors\"\t\"log\"\t\"time\"\t\"github.com/aws/aws-sdk-go-v2/aws\"\t\"github.com/aws/aws-sdk-go-v2/service/lambda\"\t\"github.com/aws/aws-sdk-go-v2/service/lambda/types\")// FunctionWrapper encapsulates function actions used in the examples.// It contains an AWS Lambda service client that is used to perform user actions.type FunctionWrapper struct {\tLambdaClient *lambda.Client}// UpdateFunctionCode updates the code for the Lambda function specified by functionName.// The existing code for the Lambda function is entirely replaced by the code in the// zipPackage buffer. After the update action is called, a lambda.FunctionUpdatedV2Waiter// is used to wait until the update is successful.func (wrapper FunctionWrapper) UpdateFunctionCode(ctx context.Context, functionName string, zipPackage *bytes.Buffer) types.State {\tvar state types.State\t_, err := wrapper.LambdaClient.UpdateFunctionCode(ctx, &lambda.UpdateFunctionCodeInput{\t\tFunctionName: aws.String(functionName), ZipFile: zipPackage.Bytes(),\t})\tif err != nil {\t\tlog.Panicf(\"Couldn't update code for function %v. Here's why: %v\\n\", functionName, err)\t} else {\t\twaiter := lambda.NewFunctionUpdatedV2Waiter(wrapper.LambdaClient)\t\tfuncOutput, err := waiter.WaitForOutput(ctx, &lambda.GetFunctionInput{\t\t\tFunctionName: aws.String(functionName)}, 1*time.Minute)\t\tif err != nil {\t\t\tlog.Panicf(\"Couldn't wait for function %v to be active. Here's why: %v\\n\", functionName, err)\t\t} else {\t\t\tstate = funcOutput.Configuration.State\t\t}\t}\treturn state}                        For API details, see                        UpdateFunctionCode                        in AWS SDK for Go API Reference.                    JavaSDK for Java 2.xNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        AWS Code            Examples Repository.        /**     * Retrieves information about an AWS Lambda function.     *     * @param awsLambda    an instance of the {@link LambdaClient} class, which is used to interact with the AWS Lambda service     * @param functionName the name of the AWS Lambda function to retrieve information about     */    public static void getFunction(LambdaClient awsLambda, String functionName) {        try {            GetFunctionRequest functionRequest = GetFunctionRequest.builder()                .functionName(functionName)                .build();            GetFunctionResponse response = awsLambda.getFunction(functionRequest);            System.out.println(\"The runtime of this Lambda function is \" + response.configuration().runtime());        } catch (LambdaException e) {            System.err.println(e.getMessage());            System.exit(1);        }    }                        For API details, see                        UpdateFunctionCode                        in AWS SDK for Java 2.x API Reference.                    JavaScriptSDK for JavaScript (v3)Note        There's more on GitHub. Find the complete example and learn how to set up and run in the        AWS Code            Examples Repository.    const updateFunctionCode = async (funcName, newFunc) => {  const client = new LambdaClient({});  const code = await readFile(`${dirname}../functions/${newFunc}.zip`);  const command = new UpdateFunctionCodeCommand({    ZipFile: code,    FunctionName: funcName,    Architectures: [Architecture.arm64],    Handler: \"index.handler\", // Required when sending a .zip file    PackageType: PackageType.Zip, // Required when sending a .zip file    Runtime: Runtime.nodejs16x, // Required when sending a .zip file  });  return client.send(command);};                        For API details, see                        UpdateFunctionCode                        in AWS SDK for JavaScript API Reference.                    PHPSDK for PHPNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        AWS Code            Examples Repository.        public function updateFunctionCode($functionName, $s3Bucket, $s3Key)    {        return $this->lambdaClient->updateFunctionCode([            'FunctionName' => $functionName,            'S3Bucket' => $s3Bucket,            'S3Key' => $s3Key,        ]);    }                        For API details, see                        UpdateFunctionCode                        in AWS SDK for PHP API Reference.                    PowerShellTools for PowerShellExample 1: Updates the function named 'MyFunction' with new content contained in the specified zip file. For a C# .NET Core Lambda function the zip file should contain the compiled assembly.Update-LMFunctionCode -FunctionName MyFunction -ZipFilename .\\UpdatedCode.zipExample 2: This example is similar to the previous one but uses an Amazon S3 object containing the updated code to update the function.Update-LMFunctionCode -FunctionName MyFunction -BucketName amzn-s3-demo-bucket -Key UpdatedCode.zip                        For API details, see                        UpdateFunctionCode                        in AWS Tools for PowerShell Cmdlet Reference.                    PythonSDK for Python (Boto3)Note        There's more on GitHub. Find the complete example and learn how to set up and run in the        AWS Code            Examples Repository.    class LambdaWrapper:    def __init__(self, lambda_client, iam_resource):        self.lambda_client = lambda_client        self.iam_resource = iam_resource    def update_function_code(self, function_name, deployment_package):        \"\"\"        Updates the code for a Lambda function by submitting a .zip archive that contains        the code for the function.        :param function_name: The name of the function to update.        :param deployment_package: The function code to update, packaged as bytes in                                   .zip format.        :return: Data about the update, including the status.        \"\"\"        try:            response = self.lambda_client.update_function_code(                FunctionName=function_name, ZipFile=deployment_package            )        except ClientError as err:            logger.error(                \"Couldn't update function %s. Here's why: %s: %s\",                function_name,                err.response[\"Error\"][\"Code\"],                err.response[\"Error\"][\"Message\"],            )            raise        else:            return response                        For API details, see                        UpdateFunctionCode                        in AWS SDK for Python (Boto3) API Reference.                    RubySDK for RubyNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        AWS Code            Examples Repository.    class LambdaWrapper  attr_accessor :lambda_client, :cloudwatch_client, :iam_client  def initialize    @lambda_client = Aws::Lambda::Client.new    @cloudwatch_client = Aws::CloudWatchLogs::Client.new(region: 'us-east-1')    @iam_client = Aws::IAM::Client.new(region: 'us-east-1')    @logger = Logger.new($stdout)    @logger.level = Logger::WARN  end  # Updates the code for a Lambda function by submitting a .zip archive that contains  # the code for the function.  #  # @param function_name: The name of the function to update.  # @param deployment_package: The function code to update, packaged as bytes in  #                            .zip format.  # @return: Data about the update, including the status.  def update_function_code(function_name, deployment_package)    @lambda_client.update_function_code(      function_name: function_name,      zip_file: deployment_package    )    @lambda_client.wait_until(:function_updated_v2, { function_name: function_name }) do |w|      w.max_attempts = 5      w.delay = 5    end  rescue Aws::Lambda::Errors::ServiceException => e    @logger.error(\"There was an error updating function code for: #{function_name}:\\n #{e.message}\")    nil  rescue Aws::Waiters::Errors::WaiterFailed => e    @logger.error(\"Failed waiting for #{function_name} to update:\\n #{e.message}\")  end                        For API details, see                        UpdateFunctionCode                        in AWS SDK for Ruby API Reference.                    RustSDK for RustNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        AWS Code            Examples Repository.        /** Given a Path to a zip file, update the function's code and wait for the update to finish. */    pub async fn update_function_code(        &self,        zip_file: PathBuf,        key: String,    ) -> Result<UpdateFunctionCodeOutput, anyhow::Error> {        let function_code = self.prepare_function(zip_file, Some(key)).await?;        info!(\"Updating code for {}\", self.lambda_name);        let update = self            .lambda_client            .update_function_code()            .function_name(self.lambda_name.clone())            .s3_bucket(self.bucket.clone())            .s3_key(function_code.s3_key().unwrap().to_string())            .send()            .await            .map_err(anyhow::Error::from)?;        self.wait_for_function_ready().await?;        Ok(update)    }    /**     * Upload function code from a path to a zip file.     * The zip file must have an AL2 Linux-compatible binary called `bootstrap`.     * The easiest way to create such a zip is to use `cargo lambda build --output-format Zip`.     */    async fn prepare_function(        &self,        zip_file: PathBuf,        key: Option<String>,    ) -> Result<FunctionCode, anyhow::Error> {        let body = ByteStream::from_path(zip_file).await?;        let key = key.unwrap_or_else(|| format!(\"{}_code\", self.lambda_name));        info!(\"Uploading function code to s3://{}/{}\", self.bucket, key);        let _ = self            .s3_client            .put_object()            .bucket(self.bucket.clone())            .key(key.clone())            .body(body)            .send()            .await?;        Ok(FunctionCode::builder()            .s3_bucket(self.bucket.clone())            .s3_key(key)            .build())    }                        For API details, see                        UpdateFunctionCode                        in AWS SDK for Rust API reference.                    SAP ABAPSDK for SAP ABAPNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        AWS Code            Examples Repository.        TRY.        oo_result = lo_lmd->updatefunctioncode(     \" oo_result is returned for testing purposes. \"              iv_functionname = iv_function_name              iv_zipfile = io_zip_file          ).        MESSAGE 'Lambda function code updated.' TYPE 'I'.      CATCH /aws1/cx_lmdcodesigningcfgno00.        MESSAGE 'Code signing configuration does not exist.' TYPE 'E'.      CATCH /aws1/cx_lmdcodestorageexcdex.        MESSAGE 'Maximum total code size per account exceeded.' TYPE 'E'.      CATCH /aws1/cx_lmdcodeverification00.        MESSAGE 'Code signature failed one or more validation checks for signature mismatch or expiration.' TYPE 'E'.      CATCH /aws1/cx_lmdinvalidcodesigex.        MESSAGE 'Code signature failed the integrity check.' TYPE 'E'.      CATCH /aws1/cx_lmdinvparamvalueex.        MESSAGE 'The request contains a non-valid parameter.' TYPE 'E'.      CATCH /aws1/cx_lmdresourceconflictex.        MESSAGE 'Resource already exists or another operation is in progress.' TYPE 'E'.      CATCH /aws1/cx_lmdresourcenotfoundex.        MESSAGE 'The requested resource does not exist.' TYPE 'E'.      CATCH /aws1/cx_lmdserviceexception.        MESSAGE 'An internal problem was encountered by the AWS Lambda service.' TYPE 'E'.      CATCH /aws1/cx_lmdtoomanyrequestsex.        MESSAGE 'The maximum request throughput was reached.' TYPE 'E'.    ENDTRY.                        For API details, see                        UpdateFunctionCode                        in AWS SDK for SAP ABAP API reference.                    anchoranchoranchoranchoranchoranchoranchoranchoranchoranchoranchoranchor.NETC++CLIGoJavaJavaScriptPHPPowerShellPythonRubyRustSAP ABAPAWS SDK for .NETNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        AWS Code            Examples Repository.        /// <summary>    /// Update an existing Lambda function.    /// </summary>    /// <param name=\"functionName\">The name of the Lambda function to update.</param>    /// <param name=\"bucketName\">The bucket where the zip file containing    /// the Lambda function code is stored.</param>    /// <param name=\"key\">The key name of the source code file.</param>    /// <returns>Async Task.</returns>    public async Task UpdateFunctionCodeAsync(        string functionName,        string bucketName,        string key)    {        var functionCodeRequest = new UpdateFunctionCodeRequest        {            FunctionName = functionName,            Publish = true,            S3Bucket = bucketName,            S3Key = key,        };        var response = await _lambdaService.UpdateFunctionCodeAsync(functionCodeRequest);        Console.WriteLine($\"The Function was last modified at {response.LastModified}.\");    }                        For API details, see                        UpdateFunctionCode                        in AWS SDK for .NET API Reference.                    ",
                                    "For a complete list of AWS SDK developer guides and code examples, see    Using Lambda with an AWS SDK.    This topic also includes information about getting started and details about previous SDK versions."
                                ]
                            },
                            {
                                "title": "UpdateFunctionConfiguration",
                                "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_lambda_UpdateFunctionConfiguration_section.html",
                                "sections": [
                                    "",
                                    "The following code examples show how to use UpdateFunctionConfiguration.",
                                    "Action examples are code excerpts from larger programs and must be run in context. You can see this action in            context in the following code example:            ",
                                    ".NETAWS SDK for .NETNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        AWS Code            Examples Repository.        /// <summary>    /// Update the code of a Lambda function.    /// </summary>    /// <param name=\"functionName\">The name of the function to update.</param>    /// <param name=\"functionHandler\">The code that performs the function's actions.</param>    /// <param name=\"environmentVariables\">A dictionary of environment variables.</param>    /// <returns>A Boolean value indicating the success of the action.</returns>    public async Task<bool> UpdateFunctionConfigurationAsync(        string functionName,        string functionHandler,        Dictionary<string, string> environmentVariables)    {        var request = new UpdateFunctionConfigurationRequest        {            Handler = functionHandler,            FunctionName = functionName,            Environment = new Amazon.Lambda.Model.Environment { Variables = environmentVariables },        };        var response = await _lambdaService.UpdateFunctionConfigurationAsync(request);        Console.WriteLine(response.LastModified);        return response.HttpStatusCode == System.Net.HttpStatusCode.OK;    }                        For API details, see                        UpdateFunctionConfiguration                        in AWS SDK for .NET API Reference.                    C++SDK for C++Note        There's more on GitHub. Find the complete example and learn how to set up and run in the        AWS Code            Examples Repository.            Aws::Client::ClientConfiguration clientConfig;        // Optional: Set to the AWS Region in which the bucket was created (overrides config file).        // clientConfig.region = \"us-east-1\";    Aws::Lambda::LambdaClient client(clientConfig);        Aws::Lambda::Model::UpdateFunctionConfigurationRequest request;        request.SetFunctionName(LAMBDA_NAME);        Aws::Lambda::Model::Environment environment;        environment.AddVariables(\"LOG_LEVEL\", \"DEBUG\");        request.SetEnvironment(environment);        Aws::Lambda::Model::UpdateFunctionConfigurationOutcome outcome = client.UpdateFunctionConfiguration(                request);        if (outcome.IsSuccess()) {            std::cout << \"The lambda configuration was successfully updated.\"                      << std::endl;            break;        }        else {            std::cerr << \"Error with Lambda::UpdateFunctionConfiguration. \"                      << outcome.GetError().GetMessage()                      << std::endl;        }                        For API details, see                        UpdateFunctionConfiguration                        in AWS SDK for C++ API Reference.                    CLIAWS CLITo modify the configuration of a functionThe following update-function-configuration example modifies the memory size to be 256 MB for the unpublished ($LATEST) version of the my-function function.aws lambda update-function-configuration \\    --function-name  my-function \\    --memory-size 256Output:{    \"FunctionName\": \"my-function\",    \"LastModified\": \"2019-09-26T20:28:40.438+0000\",    \"RevisionId\": \"e52502d4-9320-4688-9cd6-152a6ab7490d\",    \"MemorySize\": 256,    \"Version\": \"$LATEST\",    \"Role\": \"arn:aws:iam::123456789012:role/service-role/my-function-role-uy3l9qyq\",    \"Timeout\": 3,    \"Runtime\": \"nodejs10.x\",    \"TracingConfig\": {        \"Mode\": \"PassThrough\"    },    \"CodeSha256\": \"5tT2qgzYUHaqwR716pZ2dpkn/0J1FrzJmlKidWoaCgk=\",    \"Description\": \"\",    \"VpcConfig\": {        \"SubnetIds\": [],        \"VpcId\": \"\",        \"SecurityGroupIds\": []    },    \"CodeSize\": 304,    \"FunctionArn\": \"arn:aws:lambda:us-west-2:123456789012:function:my-function\",    \"Handler\": \"index.handler\"}For more information, see AWS Lambda Function Configuration in the AWS Lambda Developer Guide.                        For API details, see                        UpdateFunctionConfiguration                        in AWS CLI Command Reference.                    GoSDK for Go V2Note        There's more on GitHub. Find the complete example and learn how to set up and run in the        AWS Code            Examples Repository.    import (\t\"bytes\"\t\"context\"\t\"encoding/json\"\t\"errors\"\t\"log\"\t\"time\"\t\"github.com/aws/aws-sdk-go-v2/aws\"\t\"github.com/aws/aws-sdk-go-v2/service/lambda\"\t\"github.com/aws/aws-sdk-go-v2/service/lambda/types\")// FunctionWrapper encapsulates function actions used in the examples.// It contains an AWS Lambda service client that is used to perform user actions.type FunctionWrapper struct {\tLambdaClient *lambda.Client}// UpdateFunctionConfiguration updates a map of environment variables configured for// the Lambda function specified by functionName.func (wrapper FunctionWrapper) UpdateFunctionConfiguration(ctx context.Context, functionName string, envVars map[string]string) {\t_, err := wrapper.LambdaClient.UpdateFunctionConfiguration(ctx, &lambda.UpdateFunctionConfigurationInput{\t\tFunctionName: aws.String(functionName),\t\tEnvironment:  &types.Environment{Variables: envVars},\t})\tif err != nil {\t\tlog.Panicf(\"Couldn't update configuration for %v. Here's why: %v\", functionName, err)\t}}                        For API details, see                        UpdateFunctionConfiguration                        in AWS SDK for Go API Reference.                    JavaSDK for Java 2.xNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        AWS Code            Examples Repository.        /**     * Updates the configuration of an AWS Lambda function.     *     * @param awsLambda     the {@link LambdaClient} instance to use for the AWS Lambda operation     * @param functionName  the name of the AWS Lambda function to update     * @param handler       the new handler for the AWS Lambda function     *     * @throws LambdaException if there is an error while updating the function configuration     */    public static void updateFunctionConfiguration(LambdaClient awsLambda, String functionName, String handler) {        try {            UpdateFunctionConfigurationRequest configurationRequest = UpdateFunctionConfigurationRequest.builder()                .functionName(functionName)                .handler(handler)                .runtime(Runtime.JAVA17)                .build();            awsLambda.updateFunctionConfiguration(configurationRequest);        } catch (LambdaException e) {            System.err.println(e.getMessage());            System.exit(1);        }    }                        For API details, see                        UpdateFunctionConfiguration                        in AWS SDK for Java 2.x API Reference.                    JavaScriptSDK for JavaScript (v3)Note        There's more on GitHub. Find the complete example and learn how to set up and run in the        AWS Code            Examples Repository.    const updateFunctionConfiguration = (funcName) => {  const client = new LambdaClient({});  const config = readFileSync(`${dirname}../functions/config.json`).toString();  const command = new UpdateFunctionConfigurationCommand({    ...JSON.parse(config),    FunctionName: funcName,  });  const result = client.send(command);  waitForFunctionUpdated({ FunctionName: funcName });  return result;};                        For API details, see                        UpdateFunctionConfiguration                        in AWS SDK for JavaScript API Reference.                    PHPSDK for PHPNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        AWS Code            Examples Repository.        public function updateFunctionConfiguration($functionName, $handler, $environment = '')    {        return $this->lambdaClient->updateFunctionConfiguration([            'FunctionName' => $functionName,            'Handler' => \"$handler.lambda_handler\",            'Environment' => $environment,        ]);    }                        For API details, see                        UpdateFunctionConfiguration                        in AWS SDK for PHP API Reference.                    PowerShellTools for PowerShellExample 1: This example updates the existing Lambda Function ConfigurationUpdate-LMFunctionConfiguration -FunctionName \"MylambdaFunction123\" -Handler \"lambda_function.launch_instance\" -Timeout 600 -Environment_Variable @{ \"envvar1\"=\"value\";\"envvar2\"=\"value\" } -Role arn:aws:iam::123456789101:role/service-role/lambda -DeadLetterConfig_TargetArn arn:aws:sns:us-east-1: 123456789101:MyfirstTopic                        For API details, see                        UpdateFunctionConfiguration                        in AWS Tools for PowerShell Cmdlet Reference.                    PythonSDK for Python (Boto3)Note        There's more on GitHub. Find the complete example and learn how to set up and run in the        AWS Code            Examples Repository.    class LambdaWrapper:    def __init__(self, lambda_client, iam_resource):        self.lambda_client = lambda_client        self.iam_resource = iam_resource    def update_function_configuration(self, function_name, env_vars):        \"\"\"        Updates the environment variables for a Lambda function.        :param function_name: The name of the function to update.        :param env_vars: A dict of environment variables to update.        :return: Data about the update, including the status.        \"\"\"        try:            response = self.lambda_client.update_function_configuration(                FunctionName=function_name, Environment={\"Variables\": env_vars}            )        except ClientError as err:            logger.error(                \"Couldn't update function configuration %s. Here's why: %s: %s\",                function_name,                err.response[\"Error\"][\"Code\"],                err.response[\"Error\"][\"Message\"],            )            raise        else:            return response                        For API details, see                        UpdateFunctionConfiguration                        in AWS SDK for Python (Boto3) API Reference.                    RubySDK for RubyNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        AWS Code            Examples Repository.    class LambdaWrapper  attr_accessor :lambda_client, :cloudwatch_client, :iam_client  def initialize    @lambda_client = Aws::Lambda::Client.new    @cloudwatch_client = Aws::CloudWatchLogs::Client.new(region: 'us-east-1')    @iam_client = Aws::IAM::Client.new(region: 'us-east-1')    @logger = Logger.new($stdout)    @logger.level = Logger::WARN  end  # Updates the environment variables for a Lambda function.  # @param function_name: The name of the function to update.  # @param log_level: The log level of the function.  # @return: Data about the update, including the status.  def update_function_configuration(function_name, log_level)    @lambda_client.update_function_configuration({                                                   function_name: function_name,                                                   environment: {                                                     variables: {                                                       'LOG_LEVEL' => log_level                                                     }                                                   }                                                 })    @lambda_client.wait_until(:function_updated_v2, { function_name: function_name }) do |w|      w.max_attempts = 5      w.delay = 5    end  rescue Aws::Lambda::Errors::ServiceException => e    @logger.error(\"There was an error updating configurations for #{function_name}:\\n #{e.message}\")  rescue Aws::Waiters::Errors::WaiterFailed => e    @logger.error(\"Failed waiting for #{function_name} to activate:\\n #{e.message}\")  end                        For API details, see                        UpdateFunctionConfiguration                        in AWS SDK for Ruby API Reference.                    RustSDK for RustNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        AWS Code            Examples Repository.        /** Update the environment for a function. */    pub async fn update_function_configuration(        &self,        environment: Environment,    ) -> Result<UpdateFunctionConfigurationOutput, anyhow::Error> {        info!(            ?environment,            \"Updating environment for {}\", self.lambda_name        );        let updated = self            .lambda_client            .update_function_configuration()            .function_name(self.lambda_name.clone())            .environment(environment)            .send()            .await            .map_err(anyhow::Error::from)?;        self.wait_for_function_ready().await?;        Ok(updated)    }                        For API details, see                        UpdateFunctionConfiguration                        in AWS SDK for Rust API reference.                    SAP ABAPSDK for SAP ABAPNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        AWS Code            Examples Repository.        TRY.        oo_result = lo_lmd->updatefunctionconfiguration(     \" oo_result is returned for testing purposes. \"              iv_functionname = iv_function_name              iv_runtime = iv_runtime              iv_description  = 'Updated Lambda function'              iv_memorysize  = iv_memory_size          ).        MESSAGE 'Lambda function configuration/settings updated.' TYPE 'I'.      CATCH /aws1/cx_lmdcodesigningcfgno00.        MESSAGE 'Code signing configuration does not exist.' TYPE 'E'.      CATCH /aws1/cx_lmdcodeverification00.        MESSAGE 'Code signature failed one or more validation checks for signature mismatch or expiration.' TYPE 'E'.      CATCH /aws1/cx_lmdinvalidcodesigex.        MESSAGE 'Code signature failed the integrity check.' TYPE 'E'.      CATCH /aws1/cx_lmdinvparamvalueex.        MESSAGE 'The request contains a non-valid parameter.' TYPE 'E'.      CATCH /aws1/cx_lmdresourceconflictex.        MESSAGE 'Resource already exists or another operation is in progress.' TYPE 'E'.      CATCH /aws1/cx_lmdresourcenotfoundex.        MESSAGE 'The requested resource does not exist.' TYPE 'E'.      CATCH /aws1/cx_lmdserviceexception.        MESSAGE 'An internal problem was encountered by the AWS Lambda service.' TYPE 'E'.      CATCH /aws1/cx_lmdtoomanyrequestsex.        MESSAGE 'The maximum request throughput was reached.' TYPE 'E'.    ENDTRY.                        For API details, see                        UpdateFunctionConfiguration                        in AWS SDK for SAP ABAP API reference.                    anchoranchoranchoranchoranchoranchoranchoranchoranchoranchoranchoranchor.NETC++CLIGoJavaJavaScriptPHPPowerShellPythonRubyRustSAP ABAPAWS SDK for .NETNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        AWS Code            Examples Repository.        /// <summary>    /// Update the code of a Lambda function.    /// </summary>    /// <param name=\"functionName\">The name of the function to update.</param>    /// <param name=\"functionHandler\">The code that performs the function's actions.</param>    /// <param name=\"environmentVariables\">A dictionary of environment variables.</param>    /// <returns>A Boolean value indicating the success of the action.</returns>    public async Task<bool> UpdateFunctionConfigurationAsync(        string functionName,        string functionHandler,        Dictionary<string, string> environmentVariables)    {        var request = new UpdateFunctionConfigurationRequest        {            Handler = functionHandler,            FunctionName = functionName,            Environment = new Amazon.Lambda.Model.Environment { Variables = environmentVariables },        };        var response = await _lambdaService.UpdateFunctionConfigurationAsync(request);        Console.WriteLine(response.LastModified);        return response.HttpStatusCode == System.Net.HttpStatusCode.OK;    }                        For API details, see                        UpdateFunctionConfiguration                        in AWS SDK for .NET API Reference.                    ",
                                    "For a complete list of AWS SDK developer guides and code examples, see    Using Lambda with an AWS SDK.    This topic also includes information about getting started and details about previous SDK versions."
                                ]
                            }
                        ],
                        "sections": [
                            "",
                            "The following code examples demonstrate how to perform individual Lambda        actions with AWS SDKs. Each example includes        a link to GitHub, where you can find instructions for setting up and running the code.    ",
                            "These excerpts call the Lambda API and        are code excerpts from larger programs that must be run in context.        You can see actions in context in        Scenarios for            Lambda using AWS SDKs        .    ",
                            "        The following examples include only the most commonly used actions.        For a complete list, see the        AWS Lambda API Reference.    ",
                            "  1.CreateAlias",
                            "  2.CreateFunction",
                            "  3.DeleteAlias",
                            "  4.DeleteFunction",
                            "  5.DeleteFunctionConcurrency",
                            "  6.DeleteProvisionedConcurrencyConfig",
                            "  7.GetAccountSettings",
                            "  8.GetAlias",
                            "  9.GetFunction",
                            "  10.GetFunctionConcurrency",
                            "  11.GetFunctionConfiguration",
                            "  12.GetPolicy",
                            "  13.GetProvisionedConcurrencyConfig",
                            "  14.Invoke",
                            "  15.ListFunctions",
                            "  16.ListProvisionedConcurrencyConfigs",
                            "  17.ListTags",
                            "  18.ListVersionsByFunction",
                            "  19.PublishVersion",
                            "  20.PutFunctionConcurrency",
                            "  21.PutProvisionedConcurrencyConfig",
                            "  22.RemovePermission",
                            "  23.TagResource",
                            "  24.UntagResource",
                            "  25.UpdateAlias",
                            "  26.UpdateFunctionCode",
                            "  27.UpdateFunctionConfiguration"
                        ]
                    }
                ],
                "sections": [
                    "",
                    "The following code examples show how to use the basics of AWS Lambda with AWS    SDKs.",
                    "  1.Hello Lambda",
                    "  2.Learn the basics",
                    "  3.ActionsCreateAliasCreateFunctionDeleteAliasDeleteFunctionDeleteFunctionConcurrencyDeleteProvisionedConcurrencyConfigGetAccountSettingsGetAliasGetFunctionGetFunctionConcurrencyGetFunctionConfigurationGetPolicyGetProvisionedConcurrencyConfigInvokeListFunctionsListProvisionedConcurrencyConfigsListTagsListVersionsByFunctionPublishVersionPutFunctionConcurrencyPutProvisionedConcurrencyConfigRemovePermissionTagResourceUntagResourceUpdateAliasUpdateFunctionCodeUpdateFunctionConfiguration",
                    "  4.CreateAlias",
                    "  5.CreateFunction",
                    "  6.DeleteAlias",
                    "  7.DeleteFunction",
                    "  8.DeleteFunctionConcurrency",
                    "  9.DeleteProvisionedConcurrencyConfig",
                    "  10.GetAccountSettings",
                    "  11.GetAlias",
                    "  12.GetFunction",
                    "  13.GetFunctionConcurrency",
                    "  14.GetFunctionConfiguration",
                    "  15.GetPolicy",
                    "  16.GetProvisionedConcurrencyConfig",
                    "  17.Invoke",
                    "  18.ListFunctions",
                    "  19.ListProvisionedConcurrencyConfigs",
                    "  20.ListTags",
                    "  21.ListVersionsByFunction",
                    "  22.PublishVersion",
                    "  23.PutFunctionConcurrency",
                    "  24.PutProvisionedConcurrencyConfig",
                    "  25.RemovePermission",
                    "  26.TagResource",
                    "  27.UntagResource",
                    "  28.UpdateAlias",
                    "  29.UpdateFunctionCode",
                    "  30.UpdateFunctionConfiguration"
                ]
            },
            {
                "title": "Scenarios",
                "href": "https://docs.aws.amazon.com/lambda/latest/dg/service_code_examples_scenarios.html",
                "contents": [
                    {
                        "title": "Automatically confirm known users with a Lambda function",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_cross_CognitoAutoConfirmUser_section.html",
                        "sections": [
                            "",
                            "The following code examples show how to automatically confirm known Amazon Cognito users with a Lambda function.",
                            {
                                "code_example": "PreSignUp"
                            },
                            "GoSDK for Go V2Note        There's more on GitHub. Find the complete example and learn how to set up and run in the        AWS Code            Examples Repository.    Run an interactive scenario at a command prompt.import (\t\"context\"\t\"errors\"\t\"log\"\t\"strings\"\t\"user_pools_and_lambda_triggers/actions\"\t\"github.com/aws/aws-sdk-go-v2/aws\"\t\"github.com/aws/aws-sdk-go-v2/service/cognitoidentityprovider\"\t\"github.com/aws/aws-sdk-go-v2/service/cognitoidentityprovider/types\"\t\"github.com/awsdocs/aws-doc-sdk-examples/gov2/demotools\")// AutoConfirm separates the steps of this scenario into individual functions so that// they are simpler to read and understand.type AutoConfirm struct {\thelper       IScenarioHelper\tquestioner   demotools.IQuestioner\tresources    Resources\tcognitoActor *actions.CognitoActions}// NewAutoConfirm constructs a new auto confirm runner.func NewAutoConfirm(sdkConfig aws.Config, questioner demotools.IQuestioner, helper IScenarioHelper) AutoConfirm {\tscenario := AutoConfirm{\t\thelper:       helper,\t\tquestioner:   questioner,\t\tresources:    Resources{},\t\tcognitoActor: &actions.CognitoActions{CognitoClient: cognitoidentityprovider.NewFromConfig(sdkConfig)},\t}\tscenario.resources.init(scenario.cognitoActor, questioner)\treturn scenario}// AddPreSignUpTrigger adds a Lambda handler as an invocation target for the PreSignUp trigger.func (runner *AutoConfirm) AddPreSignUpTrigger(ctx context.Context, userPoolId string, functionArn string) {\tlog.Printf(\"Let's add a Lambda function to handle the PreSignUp trigger from Cognito.\\n\" +\t\t\"This trigger happens when a user signs up, and lets your function take action before the main Cognito\\n\" +\t\t\"sign up processing occurs.\\n\")\terr := runner.cognitoActor.UpdateTriggers(\t\tctx, userPoolId,\t\tactions.TriggerInfo{Trigger: actions.PreSignUp, HandlerArn: aws.String(functionArn)})\tif err != nil {\t\tpanic(err)\t}\tlog.Printf(\"Lambda function %v added to user pool %v to handle the PreSignUp trigger.\\n\",\t\tfunctionArn, userPoolId)}// SignUpUser signs up a user from the known user table with a password you specify.func (runner *AutoConfirm) SignUpUser(ctx context.Context, clientId string, usersTable string) (string, string) {\tlog.Println(\"Let's sign up a user to your Cognito user pool. When the user's email matches an email in the\\n\" +\t\t\"DynamoDB known users table, it is automatically verified and the user is confirmed.\")\tknownUsers, err := runner.helper.GetKnownUsers(ctx, usersTable)\tif err != nil {\t\tpanic(err)\t}\tuserChoice := runner.questioner.AskChoice(\"Which user do you want to use?\\n\", knownUsers.UserNameList())\tuser := knownUsers.Users[userChoice]\tvar signedUp bool\tvar userConfirmed bool\tpassword := runner.questioner.AskPassword(\"Enter a password that has at least eight characters, uppercase, lowercase, numbers and symbols.\\n\"+\t\t\"(the password will not display as you type):\", 8)\tfor !signedUp {\t\tlog.Printf(\"Signing up user '%v' with email '%v' to Cognito.\\n\", user.UserName, user.UserEmail)\t\tuserConfirmed, err = runner.cognitoActor.SignUp(ctx, clientId, user.UserName, password, user.UserEmail)\t\tif err != nil {\t\t\tvar invalidPassword *types.InvalidPasswordException\t\t\tif errors.As(err, &invalidPassword) {\t\t\t\tpassword = runner.questioner.AskPassword(\"Enter another password:\", 8)\t\t\t} else {\t\t\t\tpanic(err)\t\t\t}\t\t} else {\t\t\tsignedUp = true\t\t}\t}\tlog.Printf(\"User %v signed up, confirmed = %v.\\n\", user.UserName, userConfirmed)\tlog.Println(strings.Repeat(\"-\", 88))\treturn user.UserName, password}// SignInUser signs in a user.func (runner *AutoConfirm) SignInUser(ctx context.Context, clientId string, userName string, password string) string {\trunner.questioner.Ask(\"Press Enter when you're ready to continue.\")\tlog.Printf(\"Let's sign in as %v...\\n\", userName)\tauthResult, err := runner.cognitoActor.SignIn(ctx, clientId, userName, password)\tif err != nil {\t\tpanic(err)\t}\tlog.Printf(\"Successfully signed in. Your access token starts with: %v...\\n\", (*authResult.AccessToken)[:10])\tlog.Println(strings.Repeat(\"-\", 88))\treturn *authResult.AccessToken}// Run runs the scenario.func (runner *AutoConfirm) Run(ctx context.Context, stackName string) {\tdefer func() {\t\tif r := recover(); r != nil {\t\t\tlog.Println(\"Something went wrong with the demo.\")\t\t\trunner.resources.Cleanup(ctx)\t\t}\t}()\tlog.Println(strings.Repeat(\"-\", 88))\tlog.Printf(\"Welcome\\n\")\tlog.Println(strings.Repeat(\"-\", 88))\tstackOutputs, err := runner.helper.GetStackOutputs(ctx, stackName)\tif err != nil {\t\tpanic(err)\t}\trunner.resources.userPoolId = stackOutputs[\"UserPoolId\"]\trunner.helper.PopulateUserTable(ctx, stackOutputs[\"TableName\"])\trunner.AddPreSignUpTrigger(ctx, stackOutputs[\"UserPoolId\"], stackOutputs[\"AutoConfirmFunctionArn\"])\trunner.resources.triggers = append(runner.resources.triggers, actions.PreSignUp)\tuserName, password := runner.SignUpUser(ctx, stackOutputs[\"UserPoolClientId\"], stackOutputs[\"TableName\"])\trunner.helper.ListRecentLogEvents(ctx, stackOutputs[\"AutoConfirmFunction\"])\trunner.resources.userAccessTokens = append(runner.resources.userAccessTokens,\t\trunner.SignInUser(ctx, stackOutputs[\"UserPoolClientId\"], userName, password))\trunner.resources.Cleanup(ctx)\tlog.Println(strings.Repeat(\"-\", 88))\tlog.Println(\"Thanks for watching!\")\tlog.Println(strings.Repeat(\"-\", 88))}Handle the PreSignUp trigger with a Lambda function.import (\t\"context\"\t\"log\"\t\"os\"\t\"github.com/aws/aws-lambda-go/events\"\t\"github.com/aws/aws-lambda-go/lambda\"\t\"github.com/aws/aws-sdk-go-v2/aws\"\t\"github.com/aws/aws-sdk-go-v2/config\"\t\"github.com/aws/aws-sdk-go-v2/feature/dynamodb/attributevalue\"\t\"github.com/aws/aws-sdk-go-v2/service/dynamodb\"\tdynamodbtypes \"github.com/aws/aws-sdk-go-v2/service/dynamodb/types\")const TABLE_NAME = \"TABLE_NAME\"// UserInfo defines structured user data that can be marshalled to a DynamoDB format.type UserInfo struct {\tUserName  string `dynamodbav:\"UserName\"`\tUserEmail string `dynamodbav:\"UserEmail\"`}// GetKey marshals the user email value to a DynamoDB key format.func (user UserInfo) GetKey() map[string]dynamodbtypes.AttributeValue {\tuserEmail, err := attributevalue.Marshal(user.UserEmail)\tif err != nil {\t\tpanic(err)\t}\treturn map[string]dynamodbtypes.AttributeValue{\"UserEmail\": userEmail}}type handler struct {\tdynamoClient *dynamodb.Client}// HandleRequest handles the PreSignUp event by looking up a user in an Amazon DynamoDB table and// specifying whether they should be confirmed and verified.func (h *handler) HandleRequest(ctx context.Context, event events.CognitoEventUserPoolsPreSignup) (events.CognitoEventUserPoolsPreSignup, error) {\tlog.Printf(\"Received presignup from %v for user '%v'\", event.TriggerSource, event.UserName)\tif event.TriggerSource != \"PreSignUp_SignUp\" {\t\t// Other trigger sources, such as PreSignUp_AdminInitiateAuth, ignore the response from this handler.\t\treturn event, nil\t}\ttableName := os.Getenv(TABLE_NAME)\tuser := UserInfo{\t\tUserEmail: event.Request.UserAttributes[\"email\"],\t}\tlog.Printf(\"Looking up email %v in table %v.\\n\", user.UserEmail, tableName)\toutput, err := h.dynamoClient.GetItem(ctx, &dynamodb.GetItemInput{\t\tKey:       user.GetKey(),\t\tTableName: aws.String(tableName),\t})\tif err != nil {\t\tlog.Printf(\"Error looking up email %v.\\n\", user.UserEmail)\t\treturn event, err\t}\tif output.Item == nil {\t\tlog.Printf(\"Email %v not found. Email verification is required.\\n\", user.UserEmail)\t\treturn event, err\t}\terr = attributevalue.UnmarshalMap(output.Item, &user)\tif err != nil {\t\tlog.Printf(\"Couldn't unmarshal DynamoDB item. Here's why: %v\\n\", err)\t\treturn event, err\t}\tif user.UserName != event.UserName {\t\tlog.Printf(\"UserEmail %v found, but stored UserName '%v' does not match supplied UserName '%v'. Verification is required.\\n\",\t\t\tuser.UserEmail, user.UserName, event.UserName)\t} else {\t\tlog.Printf(\"UserEmail %v found with matching UserName %v. User is confirmed.\\n\", user.UserEmail, user.UserName)\t\tevent.Response.AutoConfirmUser = true\t\tevent.Response.AutoVerifyEmail = true\t}\treturn event, err}func main() {\tctx := context.Background()\tsdkConfig, err := config.LoadDefaultConfig(ctx)\tif err != nil {\t\tlog.Panicln(err)\t}\th := handler{\t\tdynamoClient: dynamodb.NewFromConfig(sdkConfig),\t}\tlambda.Start(h.HandleRequest)}Create a struct that performs common tasks.import (\t\"context\"\t\"log\"\t\"strings\"\t\"time\"\t\"user_pools_and_lambda_triggers/actions\"\t\"github.com/aws/aws-sdk-go-v2/aws\"\t\"github.com/aws/aws-sdk-go-v2/service/cloudformation\"\t\"github.com/aws/aws-sdk-go-v2/service/cloudwatchlogs\"\t\"github.com/aws/aws-sdk-go-v2/service/dynamodb\"\t\"github.com/awsdocs/aws-doc-sdk-examples/gov2/demotools\")// IScenarioHelper defines common functions used by the workflows in this example.type IScenarioHelper interface {\tPause(secs int)\tGetStackOutputs(ctx context.Context, stackName string) (actions.StackOutputs, error)\tPopulateUserTable(ctx context.Context, tableName string)\tGetKnownUsers(ctx context.Context, tableName string) (actions.UserList, error)\tAddKnownUser(ctx context.Context, tableName string, user actions.User)\tListRecentLogEvents(ctx context.Context, functionName string)}// ScenarioHelper contains AWS wrapper structs used by the workflows in this example.type ScenarioHelper struct {\tquestioner  demotools.IQuestioner\tdynamoActor *actions.DynamoActions\tcfnActor    *actions.CloudFormationActions\tcwlActor    *actions.CloudWatchLogsActions\tisTestRun   bool}// NewScenarioHelper constructs a new scenario helper.func NewScenarioHelper(sdkConfig aws.Config, questioner demotools.IQuestioner) ScenarioHelper {\tscenario := ScenarioHelper{\t\tquestioner:  questioner,\t\tdynamoActor: &actions.DynamoActions{DynamoClient: dynamodb.NewFromConfig(sdkConfig)},\t\tcfnActor:    &actions.CloudFormationActions{CfnClient: cloudformation.NewFromConfig(sdkConfig)},\t\tcwlActor:    &actions.CloudWatchLogsActions{CwlClient: cloudwatchlogs.NewFromConfig(sdkConfig)},\t}\treturn scenario}// Pause waits for the specified number of seconds.func (helper ScenarioHelper) Pause(secs int) {\tif !helper.isTestRun {\t\ttime.Sleep(time.Duration(secs) * time.Second)\t}}// GetStackOutputs gets the outputs from the specified CloudFormation stack in a structured format.func (helper ScenarioHelper) GetStackOutputs(ctx context.Context, stackName string) (actions.StackOutputs, error) {\treturn helper.cfnActor.GetOutputs(ctx, stackName), nil}// PopulateUserTable fills the known user table with example data.func (helper ScenarioHelper) PopulateUserTable(ctx context.Context, tableName string) {\tlog.Printf(\"First, let's add some users to the DynamoDB %v table we'll use for this example.\\n\", tableName)\terr := helper.dynamoActor.PopulateTable(ctx, tableName)\tif err != nil {\t\tpanic(err)\t}}// GetKnownUsers gets the users from the known users table in a structured format.func (helper ScenarioHelper) GetKnownUsers(ctx context.Context, tableName string) (actions.UserList, error) {\tknownUsers, err := helper.dynamoActor.Scan(ctx, tableName)\tif err != nil {\t\tlog.Printf(\"Couldn't get known users from table %v. Here's why: %v\\n\", tableName, err)\t}\treturn knownUsers, err}// AddKnownUser adds a user to the known users table.func (helper ScenarioHelper) AddKnownUser(ctx context.Context, tableName string, user actions.User) {\tlog.Printf(\"Adding user '%v' with email '%v' to the DynamoDB known users table...\\n\",\t\tuser.UserName, user.UserEmail)\terr := helper.dynamoActor.AddUser(ctx, tableName, user)\tif err != nil {\t\tpanic(err)\t}}// ListRecentLogEvents gets the most recent log stream and events for the specified Lambda function and displays them.func (helper ScenarioHelper) ListRecentLogEvents(ctx context.Context, functionName string) {\tlog.Println(\"Waiting a few seconds to let Lambda write to CloudWatch Logs...\")\thelper.Pause(10)\tlog.Println(\"Okay, let's check the logs to find what's happened recently with your Lambda function.\")\tlogStream, err := helper.cwlActor.GetLatestLogStream(ctx, functionName)\tif err != nil {\t\tpanic(err)\t}\tlog.Printf(\"Getting some recent events from log stream %v\\n\", *logStream.LogStreamName)\tevents, err := helper.cwlActor.GetLogEvents(ctx, functionName, *logStream.LogStreamName, 10)\tif err != nil {\t\tpanic(err)\t}\tfor _, event := range events {\t\tlog.Printf(\"\\t%v\", *event.Message)\t}\tlog.Println(strings.Repeat(\"-\", 88))}Create a struct that wraps Amazon Cognito actions.import (\t\"context\"\t\"errors\"\t\"log\"\t\"github.com/aws/aws-sdk-go-v2/aws\"\t\"github.com/aws/aws-sdk-go-v2/service/cognitoidentityprovider\"\t\"github.com/aws/aws-sdk-go-v2/service/cognitoidentityprovider/types\")type CognitoActions struct {\tCognitoClient *cognitoidentityprovider.Client}// Trigger and TriggerInfo define typed data for updating an Amazon Cognito trigger.type Trigger intconst (\tPreSignUp Trigger = iota\tUserMigration\tPostAuthentication)type TriggerInfo struct {\tTrigger    Trigger\tHandlerArn *string}// UpdateTriggers adds or removes Lambda triggers for a user pool. When a trigger is specified with a `nil` value,// it is removed from the user pool.func (actor CognitoActions) UpdateTriggers(ctx context.Context, userPoolId string, triggers ...TriggerInfo) error {\toutput, err := actor.CognitoClient.DescribeUserPool(ctx, &cognitoidentityprovider.DescribeUserPoolInput{\t\tUserPoolId: aws.String(userPoolId),\t})\tif err != nil {\t\tlog.Printf(\"Couldn't get info about user pool %v. Here's why: %v\\n\", userPoolId, err)\t\treturn err\t}\tlambdaConfig := output.UserPool.LambdaConfig\tfor _, trigger := range triggers {\t\tswitch trigger.Trigger {\t\tcase PreSignUp:\t\t\tlambdaConfig.PreSignUp = trigger.HandlerArn\t\tcase UserMigration:\t\t\tlambdaConfig.UserMigration = trigger.HandlerArn\t\tcase PostAuthentication:\t\t\tlambdaConfig.PostAuthentication = trigger.HandlerArn\t\t}\t}\t_, err = actor.CognitoClient.UpdateUserPool(ctx, &cognitoidentityprovider.UpdateUserPoolInput{\t\tUserPoolId:   aws.String(userPoolId),\t\tLambdaConfig: lambdaConfig,\t})\tif err != nil {\t\tlog.Printf(\"Couldn't update user pool %v. Here's why: %v\\n\", userPoolId, err)\t}\treturn err}// SignUp signs up a user with Amazon Cognito.func (actor CognitoActions) SignUp(ctx context.Context, clientId string, userName string, password string, userEmail string) (bool, error) {\tconfirmed := false\toutput, err := actor.CognitoClient.SignUp(ctx, &cognitoidentityprovider.SignUpInput{\t\tClientId: aws.String(clientId),\t\tPassword: aws.String(password),\t\tUsername: aws.String(userName),\t\tUserAttributes: []types.AttributeType{\t\t\t{Name: aws.String(\"email\"), Value: aws.String(userEmail)},\t\t},\t})\tif err != nil {\t\tvar invalidPassword *types.InvalidPasswordException\t\tif errors.As(err, &invalidPassword) {\t\t\tlog.Println(*invalidPassword.Message)\t\t} else {\t\t\tlog.Printf(\"Couldn't sign up user %v. Here's why: %v\\n\", userName, err)\t\t}\t} else {\t\tconfirmed = output.UserConfirmed\t}\treturn confirmed, err}// SignIn signs in a user to Amazon Cognito using a username and password authentication flow.func (actor CognitoActions) SignIn(ctx context.Context, clientId string, userName string, password string) (*types.AuthenticationResultType, error) {\tvar authResult *types.AuthenticationResultType\toutput, err := actor.CognitoClient.InitiateAuth(ctx, &cognitoidentityprovider.InitiateAuthInput{\t\tAuthFlow:       \"USER_PASSWORD_AUTH\",\t\tClientId:       aws.String(clientId),\t\tAuthParameters: map[string]string{\"USERNAME\": userName, \"PASSWORD\": password},\t})\tif err != nil {\t\tvar resetRequired *types.PasswordResetRequiredException\t\tif errors.As(err, &resetRequired) {\t\t\tlog.Println(*resetRequired.Message)\t\t} else {\t\t\tlog.Printf(\"Couldn't sign in user %v. Here's why: %v\\n\", userName, err)\t\t}\t} else {\t\tauthResult = output.AuthenticationResult\t}\treturn authResult, err}// ForgotPassword starts a password recovery flow for a user. This flow typically sends a confirmation code// to the user's configured notification destination, such as email.func (actor CognitoActions) ForgotPassword(ctx context.Context, clientId string, userName string) (*types.CodeDeliveryDetailsType, error) {\toutput, err := actor.CognitoClient.ForgotPassword(ctx, &cognitoidentityprovider.ForgotPasswordInput{\t\tClientId: aws.String(clientId),\t\tUsername: aws.String(userName),\t})\tif err != nil {\t\tlog.Printf(\"Couldn't start password reset for user '%v'. Here;s why: %v\\n\", userName, err)\t}\treturn output.CodeDeliveryDetails, err}// ConfirmForgotPassword confirms a user with a confirmation code and a new password.func (actor CognitoActions) ConfirmForgotPassword(ctx context.Context, clientId string, code string, userName string, password string) error {\t_, err := actor.CognitoClient.ConfirmForgotPassword(ctx, &cognitoidentityprovider.ConfirmForgotPasswordInput{\t\tClientId:         aws.String(clientId),\t\tConfirmationCode: aws.String(code),\t\tPassword:         aws.String(password),\t\tUsername:         aws.String(userName),\t})\tif err != nil {\t\tvar invalidPassword *types.InvalidPasswordException\t\tif errors.As(err, &invalidPassword) {\t\t\tlog.Println(*invalidPassword.Message)\t\t} else {\t\t\tlog.Printf(\"Couldn't confirm user %v. Here's why: %v\", userName, err)\t\t}\t}\treturn err}// DeleteUser removes a user from the user pool.func (actor CognitoActions) DeleteUser(ctx context.Context, userAccessToken string) error {\t_, err := actor.CognitoClient.DeleteUser(ctx, &cognitoidentityprovider.DeleteUserInput{\t\tAccessToken: aws.String(userAccessToken),\t})\tif err != nil {\t\tlog.Printf(\"Couldn't delete user. Here's why: %v\\n\", err)\t}\treturn err}// AdminCreateUser uses administrator credentials to add a user to a user pool. This method leaves the user// in a state that requires they enter a new password next time they sign in.func (actor CognitoActions) AdminCreateUser(ctx context.Context, userPoolId string, userName string, userEmail string) error {\t_, err := actor.CognitoClient.AdminCreateUser(ctx, &cognitoidentityprovider.AdminCreateUserInput{\t\tUserPoolId:     aws.String(userPoolId),\t\tUsername:       aws.String(userName),\t\tMessageAction:  types.MessageActionTypeSuppress,\t\tUserAttributes: []types.AttributeType{{Name: aws.String(\"email\"), Value: aws.String(userEmail)}},\t})\tif err != nil {\t\tvar userExists *types.UsernameExistsException\t\tif errors.As(err, &userExists) {\t\t\tlog.Printf(\"User %v already exists in the user pool.\", userName)\t\t\terr = nil\t\t} else {\t\t\tlog.Printf(\"Couldn't create user %v. Here's why: %v\\n\", userName, err)\t\t}\t}\treturn err}// AdminSetUserPassword uses administrator credentials to set a password for a user without requiring a// temporary password.func (actor CognitoActions) AdminSetUserPassword(ctx context.Context, userPoolId string, userName string, password string) error {\t_, err := actor.CognitoClient.AdminSetUserPassword(ctx, &cognitoidentityprovider.AdminSetUserPasswordInput{\t\tPassword:   aws.String(password),\t\tUserPoolId: aws.String(userPoolId),\t\tUsername:   aws.String(userName),\t\tPermanent:  true,\t})\tif err != nil {\t\tvar invalidPassword *types.InvalidPasswordException\t\tif errors.As(err, &invalidPassword) {\t\t\tlog.Println(*invalidPassword.Message)\t\t} else {\t\t\tlog.Printf(\"Couldn't set password for user %v. Here's why: %v\\n\", userName, err)\t\t}\t}\treturn err}Create a struct that wraps DynamoDB actions.import (\t\"context\"\t\"fmt\"\t\"log\"\t\"github.com/aws/aws-sdk-go-v2/aws\"\t\"github.com/aws/aws-sdk-go-v2/feature/dynamodb/attributevalue\"\t\"github.com/aws/aws-sdk-go-v2/service/dynamodb\"\t\"github.com/aws/aws-sdk-go-v2/service/dynamodb/types\")// DynamoActions encapsulates the Amazon Simple Notification Service (Amazon SNS) actions// used in the examples.type DynamoActions struct {\tDynamoClient *dynamodb.Client}// User defines structured user data.type User struct {\tUserName  string\tUserEmail string\tLastLogin *LoginInfo `dynamodbav:\",omitempty\"`}// LoginInfo defines structured custom login data.type LoginInfo struct {\tUserPoolId string\tClientId   string\tTime       string}// UserList defines a list of users.type UserList struct {\tUsers []User}// UserNameList returns the usernames contained in a UserList as a list of strings.func (users *UserList) UserNameList() []string {\tnames := make([]string, len(users.Users))\tfor i := 0; i < len(users.Users); i++ {\t\tnames[i] = users.Users[i].UserName\t}\treturn names}// PopulateTable adds a set of test users to the table.func (actor DynamoActions) PopulateTable(ctx context.Context, tableName string) error {\tvar err error\tvar item map[string]types.AttributeValue\tvar writeReqs []types.WriteRequest\tfor i := 1; i < 4; i++ {\t\titem, err = attributevalue.MarshalMap(User{UserName: fmt.Sprintf(\"test_user_%v\", i), UserEmail: fmt.Sprintf(\"test_email_%v@example.com\", i)})\t\tif err != nil {\t\t\tlog.Printf(\"Couldn't marshall user into DynamoDB format. Here's why: %v\\n\", err)\t\t\treturn err\t\t}\t\twriteReqs = append(writeReqs, types.WriteRequest{PutRequest: &types.PutRequest{Item: item}})\t}\t_, err = actor.DynamoClient.BatchWriteItem(ctx, &dynamodb.BatchWriteItemInput{\t\tRequestItems: map[string][]types.WriteRequest{tableName: writeReqs},\t})\tif err != nil {\t\tlog.Printf(\"Couldn't populate table %v with users. Here's why: %v\\n\", tableName, err)\t}\treturn err}// Scan scans the table for all items.func (actor DynamoActions) Scan(ctx context.Context, tableName string) (UserList, error) {\tvar userList UserList\toutput, err := actor.DynamoClient.Scan(ctx, &dynamodb.ScanInput{\t\tTableName: aws.String(tableName),\t})\tif err != nil {\t\tlog.Printf(\"Couldn't scan table %v for items. Here's why: %v\\n\", tableName, err)\t} else {\t\terr = attributevalue.UnmarshalListOfMaps(output.Items, &userList.Users)\t\tif err != nil {\t\t\tlog.Printf(\"Couldn't unmarshal items into users. Here's why: %v\\n\", err)\t\t}\t}\treturn userList, err}// AddUser adds a user item to a table.func (actor DynamoActions) AddUser(ctx context.Context, tableName string, user User) error {\tuserItem, err := attributevalue.MarshalMap(user)\tif err != nil {\t\tlog.Printf(\"Couldn't marshall user to item. Here's why: %v\\n\", err)\t}\t_, err = actor.DynamoClient.PutItem(ctx, &dynamodb.PutItemInput{\t\tItem:      userItem,\t\tTableName: aws.String(tableName),\t})\tif err != nil {\t\tlog.Printf(\"Couldn't put item in table %v. Here's why: %v\", tableName, err)\t}\treturn err}Create a struct that wraps CloudWatch Logs actions.import (\t\"context\"\t\"fmt\"\t\"log\"\t\"github.com/aws/aws-sdk-go-v2/aws\"\t\"github.com/aws/aws-sdk-go-v2/service/cloudwatchlogs\"\t\"github.com/aws/aws-sdk-go-v2/service/cloudwatchlogs/types\")type CloudWatchLogsActions struct {\tCwlClient *cloudwatchlogs.Client}// GetLatestLogStream gets the most recent log stream for a Lambda function.func (actor CloudWatchLogsActions) GetLatestLogStream(ctx context.Context, functionName string) (types.LogStream, error) {\tvar logStream types.LogStream\tlogGroupName := fmt.Sprintf(\"/aws/lambda/%s\", functionName)\toutput, err := actor.CwlClient.DescribeLogStreams(ctx, &cloudwatchlogs.DescribeLogStreamsInput{\t\tDescending:   aws.Bool(true),\t\tLimit:        aws.Int32(1),\t\tLogGroupName: aws.String(logGroupName),\t\tOrderBy:      types.OrderByLastEventTime,\t})\tif err != nil {\t\tlog.Printf(\"Couldn't get log streams for log group %v. Here's why: %v\\n\", logGroupName, err)\t} else {\t\tlogStream = output.LogStreams[0]\t}\treturn logStream, err}// GetLogEvents gets the most recent eventCount events from the specified log stream.func (actor CloudWatchLogsActions) GetLogEvents(ctx context.Context, functionName string, logStreamName string, eventCount int32) (\t[]types.OutputLogEvent, error) {\tvar events []types.OutputLogEvent\tlogGroupName := fmt.Sprintf(\"/aws/lambda/%s\", functionName)\toutput, err := actor.CwlClient.GetLogEvents(ctx, &cloudwatchlogs.GetLogEventsInput{\t\tLogStreamName: aws.String(logStreamName),\t\tLimit:         aws.Int32(eventCount),\t\tLogGroupName:  aws.String(logGroupName),\t})\tif err != nil {\t\tlog.Printf(\"Couldn't get log event for log stream %v. Here's why: %v\\n\", logStreamName, err)\t} else {\t\tevents = output.Events\t}\treturn events, err}Create a struct that wraps AWS CloudFormation actions.import (\t\"context\"\t\"log\"\t\"github.com/aws/aws-sdk-go-v2/aws\"\t\"github.com/aws/aws-sdk-go-v2/service/cloudformation\")// StackOutputs defines a map of outputs from a specific stack.type StackOutputs map[string]stringtype CloudFormationActions struct {\tCfnClient *cloudformation.Client}// GetOutputs gets the outputs from a CloudFormation stack and puts them into a structured format.func (actor CloudFormationActions) GetOutputs(ctx context.Context, stackName string) StackOutputs {\toutput, err := actor.CfnClient.DescribeStacks(ctx, &cloudformation.DescribeStacksInput{\t\tStackName: aws.String(stackName),\t})\tif err != nil || len(output.Stacks) == 0 {\t\tlog.Panicf(\"Couldn't find a CloudFormation stack named %v. Here's why: %v\\n\", stackName, err)\t}\tstackOutputs := StackOutputs{}\tfor _, out := range output.Stacks[0].Outputs {\t\tstackOutputs[*out.OutputKey] = *out.OutputValue\t}\treturn stackOutputs}Clean up resources.import (\t\"context\"\t\"log\"\t\"user_pools_and_lambda_triggers/actions\"\t\"github.com/awsdocs/aws-doc-sdk-examples/gov2/demotools\")// Resources keeps track of AWS resources created during an example and handles// cleanup when the example finishes.type Resources struct {\tuserPoolId       string\tuserAccessTokens []string\ttriggers         []actions.Trigger\tcognitoActor *actions.CognitoActions\tquestioner   demotools.IQuestioner}func (resources *Resources) init(cognitoActor *actions.CognitoActions, questioner demotools.IQuestioner) {\tresources.userAccessTokens = []string{}\tresources.triggers = []actions.Trigger{}\tresources.cognitoActor = cognitoActor\tresources.questioner = questioner}// Cleanup deletes all AWS resources created during an example.func (resources *Resources) Cleanup(ctx context.Context) {\tdefer func() {\t\tif r := recover(); r != nil {\t\t\tlog.Printf(\"Something went wrong during cleanup.\\n%v\\n\", r)\t\t\tlog.Println(\"Use the AWS Management Console to remove any remaining resources \\n\" +\t\t\t\t\"that were created for this scenario.\")\t\t}\t}()\twantDelete := resources.questioner.AskBool(\"Do you want to remove all of the AWS resources that were created \"+\t\t\"during this demo (y/n)?\", \"y\")\tif wantDelete {\t\tfor _, accessToken := range resources.userAccessTokens {\t\t\terr := resources.cognitoActor.DeleteUser(ctx, accessToken)\t\t\tif err != nil {\t\t\t\tlog.Println(\"Couldn't delete user during cleanup.\")\t\t\t\tpanic(err)\t\t\t}\t\t\tlog.Println(\"Deleted user.\")\t\t}\t\ttriggerList := make([]actions.TriggerInfo, len(resources.triggers))\t\tfor i := 0; i < len(resources.triggers); i++ {\t\t\ttriggerList[i] = actions.TriggerInfo{Trigger: resources.triggers[i], HandlerArn: nil}\t\t}\t\terr := resources.cognitoActor.UpdateTriggers(ctx, resources.userPoolId, triggerList...)\t\tif err != nil {\t\t\tlog.Println(\"Couldn't update Cognito triggers during cleanup.\")\t\t\tpanic(err)\t\t}\t\tlog.Println(\"Removed Cognito triggers from user pool.\")\t} else {\t\tlog.Println(\"Be sure to remove resources when you're done with them to avoid unexpected charges!\")\t}}For API details, see the following topics in AWS SDK for Go API Reference.DeleteUserInitiateAuthSignUpUpdateUserPoolJavaScriptSDK for JavaScript (v3)Note        There's more on GitHub. Find the complete example and learn how to set up and run in the        AWS Code            Examples Repository.    Configure an interactive \"Scenario\" run. The JavaScript (v3) examplesshare a Scenario runner to streamline complex examples. The completesource code is on GitHub.import { AutoConfirm } from \"./scenario-auto-confirm.js\";/** * The context is passed to every scenario. Scenario steps * will modify the context. */const context = {  errors: [],  users: [    {      UserName: \"test_user_1\",      UserEmail: \"test_email_1@example.com\",    },    {      UserName: \"test_user_2\",      UserEmail: \"test_email_2@example.com\",    },    {      UserName: \"test_user_3\",      UserEmail: \"test_email_3@example.com\",    },  ],};/** * Three Scenarios are created for the workflow. A Scenario is an orchestration class * that simplifies running a series of steps. */export const scenarios = {  // Demonstrate automatically confirming known users in a database.  \"auto-confirm\": AutoConfirm(context),};// Call function if run directlyimport { fileURLToPath } from \"node:url\";import { parseScenarioArgs } from \"@aws-doc-sdk-examples/lib/scenario/index.js\";if (process.argv[1] === fileURLToPath(import.meta.url)) {  parseScenarioArgs(scenarios, {    name: \"Cognito user pools and triggers\",    description:      \"Demonstrate how to use the AWS SDKs to customize Amazon Cognito authentication behavior.\",  });}This Scenario demonstrates auto-confirming a known user.It orchestrates the example steps.import { wait } from \"@aws-doc-sdk-examples/lib/utils/util-timers.js\";import {  Scenario,  ScenarioAction,  ScenarioInput,  ScenarioOutput,} from \"@aws-doc-sdk-examples/lib/scenario/scenario.js\";import {  getStackOutputs,  logCleanUpReminder,  promptForStackName,  promptForStackRegion,  skipWhenErrors,} from \"./steps-common.js\";import { populateTable } from \"./actions/dynamodb-actions.js\";import {  addPreSignUpHandler,  deleteUser,  getUser,  signIn,  signUpUser,} from \"./actions/cognito-actions.js\";import {  getLatestLogStreamForLambda,  getLogEvents,} from \"./actions/cloudwatch-logs-actions.js\";/** * @typedef {{ *   errors: Error[], *   password: string, *   users: { UserName: string, UserEmail: string }[], *   selectedUser?: string, *   stackName?: string, *   stackRegion?: string, *   token?: string, *   confirmDeleteSignedInUser?: boolean, *   TableName?: string, *   UserPoolClientId?: string, *   UserPoolId?: string, *   UserPoolArn?: string, *   AutoConfirmHandlerArn?: string, *   AutoConfirmHandlerName?: string * }} State */const greeting = new ScenarioOutput(  \"greeting\",  (/** @type {State} */ state) => `This demo will populate some users into the \\database created as part of the \"${state.stackName}\" stack. \\Then the autoConfirmHandler will be linked to the PreSignUp \\trigger from Cognito. Finally, you will choose a user to sign up.`,  { skipWhen: skipWhenErrors },);const logPopulatingUsers = new ScenarioOutput(  \"logPopulatingUsers\",  \"Populating the DynamoDB table with some users.\",  { skipWhenErrors: skipWhenErrors },);const logPopulatingUsersComplete = new ScenarioOutput(  \"logPopulatingUsersComplete\",  \"Done populating users.\",  { skipWhen: skipWhenErrors },);const populateUsers = new ScenarioAction(  \"populateUsers\",  async (/** @type {State} */ state) => {    const [_, err] = await populateTable({      region: state.stackRegion,      tableName: state.TableName,      items: state.users,    });    if (err) {      state.errors.push(err);    }  },  {    skipWhen: skipWhenErrors,  },);const logSetupSignUpTrigger = new ScenarioOutput(  \"logSetupSignUpTrigger\",  \"Setting up the PreSignUp trigger for the Cognito User Pool.\",  { skipWhen: skipWhenErrors },);const setupSignUpTrigger = new ScenarioAction(  \"setupSignUpTrigger\",  async (/** @type {State} */ state) => {    const [_, err] = await addPreSignUpHandler({      region: state.stackRegion,      userPoolId: state.UserPoolId,      handlerArn: state.AutoConfirmHandlerArn,    });    if (err) {      state.errors.push(err);    }  },  {    skipWhen: skipWhenErrors,  },);const logSetupSignUpTriggerComplete = new ScenarioOutput(  \"logSetupSignUpTriggerComplete\",  (    /** @type {State} */ state,  ) => `The lambda function \"${state.AutoConfirmHandlerName}\" \\has been configured as the PreSignUp trigger handler for the user pool \"${state.UserPoolId}\".`,  { skipWhen: skipWhenErrors },);const selectUser = new ScenarioInput(  \"selectedUser\",  \"Select a user to sign up.\",  {    type: \"select\",    choices: (/** @type {State} */ state) => state.users.map((u) => u.UserName),    skipWhen: skipWhenErrors,    default: (/** @type {State} */ state) => state.users[0].UserName,  },);const checkIfUserAlreadyExists = new ScenarioAction(  \"checkIfUserAlreadyExists\",  async (/** @type {State} */ state) => {    const [user, err] = await getUser({      region: state.stackRegion,      userPoolId: state.UserPoolId,      username: state.selectedUser,    });    if (err?.name === \"UserNotFoundException\") {      // Do nothing. We're not expecting the user to exist before      // sign up is complete.      return;    }    if (err) {      state.errors.push(err);      return;    }    if (user) {      state.errors.push(        new Error(          `The user \"${state.selectedUser}\" already exists in the user pool \"${state.UserPoolId}\".`,        ),      );    }  },  {    skipWhen: skipWhenErrors,  },);const createPassword = new ScenarioInput(  \"password\",  \"Enter a password that has at least eight characters, uppercase, lowercase, numbers and symbols.\",  { type: \"password\", skipWhen: skipWhenErrors, default: \"Abcd1234!\" },);const logSignUpExistingUser = new ScenarioOutput(  \"logSignUpExistingUser\",  (/** @type {State} */ state) => `Signing up user \"${state.selectedUser}\".`,  { skipWhen: skipWhenErrors },);const signUpExistingUser = new ScenarioAction(  \"signUpExistingUser\",  async (/** @type {State} */ state) => {    const signUp = (password) =>      signUpUser({        region: state.stackRegion,        userPoolClientId: state.UserPoolClientId,        username: state.selectedUser,        email: state.users.find((u) => u.UserName === state.selectedUser)          .UserEmail,        password,      });    let [_, err] = await signUp(state.password);    while (err?.name === \"InvalidPasswordException\") {      console.warn(\"The password you entered was invalid.\");      await createPassword.handle(state);      [_, err] = await signUp(state.password);    }    if (err) {      state.errors.push(err);    }  },  { skipWhen: skipWhenErrors },);const logSignUpExistingUserComplete = new ScenarioOutput(  \"logSignUpExistingUserComplete\",  (/** @type {State} */ state) =>    `\"${state.selectedUser} was signed up successfully.`,  { skipWhen: skipWhenErrors },);const logLambdaLogs = new ScenarioAction(  \"logLambdaLogs\",  async (/** @type {State} */ state) => {    console.log(      \"Waiting a few seconds to let Lambda write to CloudWatch Logs...\\n\",    );    await wait(10);    const [logStream, logStreamErr] = await getLatestLogStreamForLambda({      functionName: state.AutoConfirmHandlerName,      region: state.stackRegion,    });    if (logStreamErr) {      state.errors.push(logStreamErr);      return;    }    console.log(      `Getting some recent events from log stream \"${logStream.logStreamName}\"`,    );    const [logEvents, logEventsErr] = await getLogEvents({      functionName: state.AutoConfirmHandlerName,      region: state.stackRegion,      eventCount: 10,      logStreamName: logStream.logStreamName,    });    if (logEventsErr) {      state.errors.push(logEventsErr);      return;    }    console.log(logEvents.map((ev) => `\\t${ev.message}`).join(\"\"));  },  { skipWhen: skipWhenErrors },);const logSignInUser = new ScenarioOutput(  \"logSignInUser\",  (/** @type {State} */ state) => `Let's sign in as ${state.selectedUser}`,  { skipWhen: skipWhenErrors },);const signInUser = new ScenarioAction(  \"signInUser\",  async (/** @type {State} */ state) => {    const [response, err] = await signIn({      region: state.stackRegion,      clientId: state.UserPoolClientId,      username: state.selectedUser,      password: state.password,    });    if (err?.name === \"PasswordResetRequiredException\") {      state.errors.push(new Error(\"Please reset your password.\"));      return;    }    if (err) {      state.errors.push(err);      return;    }    state.token = response?.AuthenticationResult?.AccessToken;  },  { skipWhen: skipWhenErrors },);const logSignInUserComplete = new ScenarioOutput(  \"logSignInUserComplete\",  (/** @type {State} */ state) =>    `Successfully signed in. Your access token starts with: ${state.token.slice(0, 11)}`,  { skipWhen: skipWhenErrors },);const confirmDeleteSignedInUser = new ScenarioInput(  \"confirmDeleteSignedInUser\",  \"Do you want to delete the currently signed in user?\",  { type: \"confirm\", skipWhen: skipWhenErrors },);const deleteSignedInUser = new ScenarioAction(  \"deleteSignedInUser\",  async (/** @type {State} */ state) => {    const [_, err] = await deleteUser({      region: state.stackRegion,      accessToken: state.token,    });    if (err) {      state.errors.push(err);    }  },  {    skipWhen: (/** @type {State} */ state) =>      skipWhenErrors(state) || !state.confirmDeleteSignedInUser,  },);const logErrors = new ScenarioOutput(  \"logErrors\",  (/** @type {State}*/ state) => {    const errorList = state.errors      .map((err) => ` - ${err.name}: ${err.message}`)      .join(\"\\n\");    return `Scenario errors found:\\n${errorList}`;  },  {    // Don't log errors when there aren't any!    skipWhen: (/** @type {State} */ state) => state.errors.length === 0,  },);export const AutoConfirm = (context) =>  new Scenario(    \"AutoConfirm\",    [      promptForStackName,      promptForStackRegion,      getStackOutputs,      greeting,      logPopulatingUsers,      populateUsers,      logPopulatingUsersComplete,      logSetupSignUpTrigger,      setupSignUpTrigger,      logSetupSignUpTriggerComplete,      selectUser,      checkIfUserAlreadyExists,      createPassword,      logSignUpExistingUser,      signUpExistingUser,      logSignUpExistingUserComplete,      logLambdaLogs,      logSignInUser,      signInUser,      logSignInUserComplete,      confirmDeleteSignedInUser,      deleteSignedInUser,      logCleanUpReminder,      logErrors,    ],    context,  );These are steps that are shared with other Scenarios.import {  ScenarioAction,  ScenarioInput,  ScenarioOutput,} from \"@aws-doc-sdk-examples/lib/scenario/scenario.js\";import { getCfnOutputs } from \"@aws-doc-sdk-examples/lib/sdk/cfn-outputs.js\";export const skipWhenErrors = (state) => state.errors.length > 0;export const getStackOutputs = new ScenarioAction(  \"getStackOutputs\",  async (state) => {    if (!state.stackName || !state.stackRegion) {      state.errors.push(        new Error(          \"No stack name or region provided. The stack name and \\region are required to fetch CFN outputs relevant to this example.\",        ),      );      return;    }    const outputs = await getCfnOutputs(state.stackName, state.stackRegion);    Object.assign(state, outputs);  },);export const promptForStackName = new ScenarioInput(  \"stackName\",  \"Enter the name of the stack you deployed earlier.\",  { type: \"input\", default: \"PoolsAndTriggersStack\" },);export const promptForStackRegion = new ScenarioInput(  \"stackRegion\",  \"Enter the region of the stack you deployed earlier.\",  { type: \"input\", default: \"us-east-1\" },);export const logCleanUpReminder = new ScenarioOutput(  \"logCleanUpReminder\",  \"All done. Remember to run 'cdk destroy' to teardown the stack.\",  { skipWhen: skipWhenErrors },);A handler for the PreSignUp trigger with a Lambda function.import type { PreSignUpTriggerEvent, Handler } from \"aws-lambda\";import type { UserRepository } from \"./user-repository\";import { DynamoDBUserRepository } from \"./user-repository\";export class PreSignUpHandler {  private userRepository: UserRepository;  constructor(userRepository: UserRepository) {    this.userRepository = userRepository;  }  private isPreSignUpTriggerSource(event: PreSignUpTriggerEvent): boolean {    return event.triggerSource === \"PreSignUp_SignUp\";  }  private getEventUserEmail(event: PreSignUpTriggerEvent): string {    return event.request.userAttributes.email;  }  async handlePreSignUpTriggerEvent(    event: PreSignUpTriggerEvent,  ): Promise<PreSignUpTriggerEvent> {    console.log(      `Received presignup from ${event.triggerSource} for user '${event.userName}'`,    );    if (!this.isPreSignUpTriggerSource(event)) {      return event;    }    const eventEmail = this.getEventUserEmail(event);    console.log(`Looking up email ${eventEmail}.`);    const storedUserInfo =      await this.userRepository.getUserInfoByEmail(eventEmail);    if (!storedUserInfo) {      console.log(        `Email ${eventEmail} not found. Email verification is required.`,      );      return event;    }    if (storedUserInfo.UserName !== event.userName) {      console.log(        `UserEmail ${eventEmail} found, but stored UserName '${storedUserInfo.UserName}' does not match supplied UserName '${event.userName}'. Verification is required.`,      );    } else {      console.log(        `UserEmail ${eventEmail} found with matching UserName ${storedUserInfo.UserName}. User is confirmed.`,      );      event.response.autoConfirmUser = true;      event.response.autoVerifyEmail = true;    }    return event;  }}const createPreSignUpHandler = (): PreSignUpHandler => {  const tableName = process.env.TABLE_NAME;  if (!tableName) {    throw new Error(\"TABLE_NAME environment variable is not set\");  }  const userRepository = new DynamoDBUserRepository(tableName);  return new PreSignUpHandler(userRepository);};export const handler: Handler = async (event: PreSignUpTriggerEvent) => {  const preSignUpHandler = createPreSignUpHandler();  return preSignUpHandler.handlePreSignUpTriggerEvent(event);};Module of CloudWatch Logs actions.import {  CloudWatchLogsClient,  GetLogEventsCommand,  OrderBy,  paginateDescribeLogStreams,} from \"@aws-sdk/client-cloudwatch-logs\";/** * Get the latest log stream for a Lambda function. * @param {{ functionName: string, region: string }} config * @returns {Promise<[import(\"@aws-sdk/client-cloudwatch-logs\").LogStream | null, unknown]>} */export const getLatestLogStreamForLambda = async ({ functionName, region }) => {  try {    const logGroupName = `/aws/lambda/${functionName}`;    const cwlClient = new CloudWatchLogsClient({ region });    const paginator = paginateDescribeLogStreams(      { client: cwlClient },      {        descending: true,        limit: 1,        orderBy: OrderBy.LastEventTime,        logGroupName,      },    );    for await (const page of paginator) {      return [page.logStreams[0], null];    }  } catch (err) {    return [null, err];  }};/** * Get the log events for a Lambda function's log stream. * @param {{ *   functionName: string, *   logStreamName: string, *   eventCount: number, *   region: string * }} config * @returns {Promise<[import(\"@aws-sdk/client-cloudwatch-logs\").OutputLogEvent[] | null, unknown]>} */export const getLogEvents = async ({  functionName,  logStreamName,  eventCount,  region,}) => {  try {    const cwlClient = new CloudWatchLogsClient({ region });    const logGroupName = `/aws/lambda/${functionName}`;    const response = await cwlClient.send(      new GetLogEventsCommand({        logStreamName: logStreamName,        limit: eventCount,        logGroupName: logGroupName,      }),    );    return [response.events, null];  } catch (err) {    return [null, err];  }};Module of Amazon Cognito actions.import {  AdminGetUserCommand,  CognitoIdentityProviderClient,  DeleteUserCommand,  InitiateAuthCommand,  SignUpCommand,  UpdateUserPoolCommand,} from \"@aws-sdk/client-cognito-identity-provider\";/** * Connect a Lambda function to the PreSignUp trigger for a Cognito user pool * @param {{ region: string, userPoolId: string, handlerArn: string }} config * @returns {Promise<[import(\"@aws-sdk/client-cognito-identity-provider\").UpdateUserPoolCommandOutput | null, unknown]>} */export const addPreSignUpHandler = async ({  region,  userPoolId,  handlerArn,}) => {  try {    const cognitoClient = new CognitoIdentityProviderClient({      region,    });    const command = new UpdateUserPoolCommand({      UserPoolId: userPoolId,      LambdaConfig: {        PreSignUp: handlerArn,      },    });    const response = await cognitoClient.send(command);    return [response, null];  } catch (err) {    return [null, err];  }};/** * Attempt to register a user to a user pool with a given username and password. * @param {{ *   region: string, *   userPoolClientId: string, *   username: string, *   email: string, *   password: string * }} config * @returns {Promise<[import(\"@aws-sdk/client-cognito-identity-provider\").SignUpCommandOutput | null, unknown]>} */export const signUpUser = async ({  region,  userPoolClientId,  username,  email,  password,}) => {  try {    const cognitoClient = new CognitoIdentityProviderClient({      region,    });    const response = await cognitoClient.send(      new SignUpCommand({        ClientId: userPoolClientId,        Username: username,        Password: password,        UserAttributes: [{ Name: \"email\", Value: email }],      }),    );    return [response, null];  } catch (err) {    return [null, err];  }};/** * Sign in a user to Amazon Cognito using a username and password authentication flow. * @param {{ region: string, clientId: string, username: string, password: string }} config * @returns {Promise<[import(\"@aws-sdk/client-cognito-identity-provider\").InitiateAuthCommandOutput | null, unknown]>} */export const signIn = async ({ region, clientId, username, password }) => {  try {    const cognitoClient = new CognitoIdentityProviderClient({ region });    const response = await cognitoClient.send(      new InitiateAuthCommand({        AuthFlow: \"USER_PASSWORD_AUTH\",        ClientId: clientId,        AuthParameters: { USERNAME: username, PASSWORD: password },      }),    );    return [response, null];  } catch (err) {    return [null, err];  }};/** * Retrieve an existing user from a user pool. * @param {{ region: string, userPoolId: string, username: string }} config * @returns {Promise<[import(\"@aws-sdk/client-cognito-identity-provider\").AdminGetUserCommandOutput | null, unknown]>} */export const getUser = async ({ region, userPoolId, username }) => {  try {    const cognitoClient = new CognitoIdentityProviderClient({ region });    const response = await cognitoClient.send(      new AdminGetUserCommand({        UserPoolId: userPoolId,        Username: username,      }),    );    return [response, null];  } catch (err) {    return [null, err];  }};/** * Delete the signed-in user. Useful for allowing a user to delete their * own profile. * @param {{ region: string, accessToken: string }} config * @returns {Promise<[import(\"@aws-sdk/client-cognito-identity-provider\").DeleteUserCommandOutput | null, unknown]>} */export const deleteUser = async ({ region, accessToken }) => {  try {    const client = new CognitoIdentityProviderClient({ region });    const response = await client.send(      new DeleteUserCommand({ AccessToken: accessToken }),    );    return [response, null];  } catch (err) {    return [null, err];  }};Module of DynamoDB actions.import { DynamoDBClient } from \"@aws-sdk/client-dynamodb\";import {  BatchWriteCommand,  DynamoDBDocumentClient,} from \"@aws-sdk/lib-dynamodb\";/** * Populate a DynamoDB table with provide items. * @param {{ region: string, tableName: string, items: Record<string, unknown>[] }} config * @returns {Promise<[import(\"@aws-sdk/lib-dynamodb\").BatchWriteCommandOutput | null, unknown]>} */export const populateTable = async ({ region, tableName, items }) => {  try {    const ddbClient = new DynamoDBClient({ region });    const docClient = DynamoDBDocumentClient.from(ddbClient);    const response = await docClient.send(      new BatchWriteCommand({        RequestItems: {          [tableName]: items.map((item) => ({            PutRequest: {              Item: item,            },          })),        },      }),    );    return [response, null];  } catch (err) {    return [null, err];  }};For API details, see the following topics in AWS SDK for JavaScript API Reference.DeleteUserInitiateAuthSignUpUpdateUserPoolanchoranchorGoJavaScriptSDK for Go V2Note        There's more on GitHub. Find the complete example and learn how to set up and run in the        AWS Code            Examples Repository.    Run an interactive scenario at a command prompt.import (\t\"context\"\t\"errors\"\t\"log\"\t\"strings\"\t\"user_pools_and_lambda_triggers/actions\"\t\"github.com/aws/aws-sdk-go-v2/aws\"\t\"github.com/aws/aws-sdk-go-v2/service/cognitoidentityprovider\"\t\"github.com/aws/aws-sdk-go-v2/service/cognitoidentityprovider/types\"\t\"github.com/awsdocs/aws-doc-sdk-examples/gov2/demotools\")// AutoConfirm separates the steps of this scenario into individual functions so that// they are simpler to read and understand.type AutoConfirm struct {\thelper       IScenarioHelper\tquestioner   demotools.IQuestioner\tresources    Resources\tcognitoActor *actions.CognitoActions}// NewAutoConfirm constructs a new auto confirm runner.func NewAutoConfirm(sdkConfig aws.Config, questioner demotools.IQuestioner, helper IScenarioHelper) AutoConfirm {\tscenario := AutoConfirm{\t\thelper:       helper,\t\tquestioner:   questioner,\t\tresources:    Resources{},\t\tcognitoActor: &actions.CognitoActions{CognitoClient: cognitoidentityprovider.NewFromConfig(sdkConfig)},\t}\tscenario.resources.init(scenario.cognitoActor, questioner)\treturn scenario}// AddPreSignUpTrigger adds a Lambda handler as an invocation target for the PreSignUp trigger.func (runner *AutoConfirm) AddPreSignUpTrigger(ctx context.Context, userPoolId string, functionArn string) {\tlog.Printf(\"Let's add a Lambda function to handle the PreSignUp trigger from Cognito.\\n\" +\t\t\"This trigger happens when a user signs up, and lets your function take action before the main Cognito\\n\" +\t\t\"sign up processing occurs.\\n\")\terr := runner.cognitoActor.UpdateTriggers(\t\tctx, userPoolId,\t\tactions.TriggerInfo{Trigger: actions.PreSignUp, HandlerArn: aws.String(functionArn)})\tif err != nil {\t\tpanic(err)\t}\tlog.Printf(\"Lambda function %v added to user pool %v to handle the PreSignUp trigger.\\n\",\t\tfunctionArn, userPoolId)}// SignUpUser signs up a user from the known user table with a password you specify.func (runner *AutoConfirm) SignUpUser(ctx context.Context, clientId string, usersTable string) (string, string) {\tlog.Println(\"Let's sign up a user to your Cognito user pool. When the user's email matches an email in the\\n\" +\t\t\"DynamoDB known users table, it is automatically verified and the user is confirmed.\")\tknownUsers, err := runner.helper.GetKnownUsers(ctx, usersTable)\tif err != nil {\t\tpanic(err)\t}\tuserChoice := runner.questioner.AskChoice(\"Which user do you want to use?\\n\", knownUsers.UserNameList())\tuser := knownUsers.Users[userChoice]\tvar signedUp bool\tvar userConfirmed bool\tpassword := runner.questioner.AskPassword(\"Enter a password that has at least eight characters, uppercase, lowercase, numbers and symbols.\\n\"+\t\t\"(the password will not display as you type):\", 8)\tfor !signedUp {\t\tlog.Printf(\"Signing up user '%v' with email '%v' to Cognito.\\n\", user.UserName, user.UserEmail)\t\tuserConfirmed, err = runner.cognitoActor.SignUp(ctx, clientId, user.UserName, password, user.UserEmail)\t\tif err != nil {\t\t\tvar invalidPassword *types.InvalidPasswordException\t\t\tif errors.As(err, &invalidPassword) {\t\t\t\tpassword = runner.questioner.AskPassword(\"Enter another password:\", 8)\t\t\t} else {\t\t\t\tpanic(err)\t\t\t}\t\t} else {\t\t\tsignedUp = true\t\t}\t}\tlog.Printf(\"User %v signed up, confirmed = %v.\\n\", user.UserName, userConfirmed)\tlog.Println(strings.Repeat(\"-\", 88))\treturn user.UserName, password}// SignInUser signs in a user.func (runner *AutoConfirm) SignInUser(ctx context.Context, clientId string, userName string, password string) string {\trunner.questioner.Ask(\"Press Enter when you're ready to continue.\")\tlog.Printf(\"Let's sign in as %v...\\n\", userName)\tauthResult, err := runner.cognitoActor.SignIn(ctx, clientId, userName, password)\tif err != nil {\t\tpanic(err)\t}\tlog.Printf(\"Successfully signed in. Your access token starts with: %v...\\n\", (*authResult.AccessToken)[:10])\tlog.Println(strings.Repeat(\"-\", 88))\treturn *authResult.AccessToken}// Run runs the scenario.func (runner *AutoConfirm) Run(ctx context.Context, stackName string) {\tdefer func() {\t\tif r := recover(); r != nil {\t\t\tlog.Println(\"Something went wrong with the demo.\")\t\t\trunner.resources.Cleanup(ctx)\t\t}\t}()\tlog.Println(strings.Repeat(\"-\", 88))\tlog.Printf(\"Welcome\\n\")\tlog.Println(strings.Repeat(\"-\", 88))\tstackOutputs, err := runner.helper.GetStackOutputs(ctx, stackName)\tif err != nil {\t\tpanic(err)\t}\trunner.resources.userPoolId = stackOutputs[\"UserPoolId\"]\trunner.helper.PopulateUserTable(ctx, stackOutputs[\"TableName\"])\trunner.AddPreSignUpTrigger(ctx, stackOutputs[\"UserPoolId\"], stackOutputs[\"AutoConfirmFunctionArn\"])\trunner.resources.triggers = append(runner.resources.triggers, actions.PreSignUp)\tuserName, password := runner.SignUpUser(ctx, stackOutputs[\"UserPoolClientId\"], stackOutputs[\"TableName\"])\trunner.helper.ListRecentLogEvents(ctx, stackOutputs[\"AutoConfirmFunction\"])\trunner.resources.userAccessTokens = append(runner.resources.userAccessTokens,\t\trunner.SignInUser(ctx, stackOutputs[\"UserPoolClientId\"], userName, password))\trunner.resources.Cleanup(ctx)\tlog.Println(strings.Repeat(\"-\", 88))\tlog.Println(\"Thanks for watching!\")\tlog.Println(strings.Repeat(\"-\", 88))}Handle the PreSignUp trigger with a Lambda function.import (\t\"context\"\t\"log\"\t\"os\"\t\"github.com/aws/aws-lambda-go/events\"\t\"github.com/aws/aws-lambda-go/lambda\"\t\"github.com/aws/aws-sdk-go-v2/aws\"\t\"github.com/aws/aws-sdk-go-v2/config\"\t\"github.com/aws/aws-sdk-go-v2/feature/dynamodb/attributevalue\"\t\"github.com/aws/aws-sdk-go-v2/service/dynamodb\"\tdynamodbtypes \"github.com/aws/aws-sdk-go-v2/service/dynamodb/types\")const TABLE_NAME = \"TABLE_NAME\"// UserInfo defines structured user data that can be marshalled to a DynamoDB format.type UserInfo struct {\tUserName  string `dynamodbav:\"UserName\"`\tUserEmail string `dynamodbav:\"UserEmail\"`}// GetKey marshals the user email value to a DynamoDB key format.func (user UserInfo) GetKey() map[string]dynamodbtypes.AttributeValue {\tuserEmail, err := attributevalue.Marshal(user.UserEmail)\tif err != nil {\t\tpanic(err)\t}\treturn map[string]dynamodbtypes.AttributeValue{\"UserEmail\": userEmail}}type handler struct {\tdynamoClient *dynamodb.Client}// HandleRequest handles the PreSignUp event by looking up a user in an Amazon DynamoDB table and// specifying whether they should be confirmed and verified.func (h *handler) HandleRequest(ctx context.Context, event events.CognitoEventUserPoolsPreSignup) (events.CognitoEventUserPoolsPreSignup, error) {\tlog.Printf(\"Received presignup from %v for user '%v'\", event.TriggerSource, event.UserName)\tif event.TriggerSource != \"PreSignUp_SignUp\" {\t\t// Other trigger sources, such as PreSignUp_AdminInitiateAuth, ignore the response from this handler.\t\treturn event, nil\t}\ttableName := os.Getenv(TABLE_NAME)\tuser := UserInfo{\t\tUserEmail: event.Request.UserAttributes[\"email\"],\t}\tlog.Printf(\"Looking up email %v in table %v.\\n\", user.UserEmail, tableName)\toutput, err := h.dynamoClient.GetItem(ctx, &dynamodb.GetItemInput{\t\tKey:       user.GetKey(),\t\tTableName: aws.String(tableName),\t})\tif err != nil {\t\tlog.Printf(\"Error looking up email %v.\\n\", user.UserEmail)\t\treturn event, err\t}\tif output.Item == nil {\t\tlog.Printf(\"Email %v not found. Email verification is required.\\n\", user.UserEmail)\t\treturn event, err\t}\terr = attributevalue.UnmarshalMap(output.Item, &user)\tif err != nil {\t\tlog.Printf(\"Couldn't unmarshal DynamoDB item. Here's why: %v\\n\", err)\t\treturn event, err\t}\tif user.UserName != event.UserName {\t\tlog.Printf(\"UserEmail %v found, but stored UserName '%v' does not match supplied UserName '%v'. Verification is required.\\n\",\t\t\tuser.UserEmail, user.UserName, event.UserName)\t} else {\t\tlog.Printf(\"UserEmail %v found with matching UserName %v. User is confirmed.\\n\", user.UserEmail, user.UserName)\t\tevent.Response.AutoConfirmUser = true\t\tevent.Response.AutoVerifyEmail = true\t}\treturn event, err}func main() {\tctx := context.Background()\tsdkConfig, err := config.LoadDefaultConfig(ctx)\tif err != nil {\t\tlog.Panicln(err)\t}\th := handler{\t\tdynamoClient: dynamodb.NewFromConfig(sdkConfig),\t}\tlambda.Start(h.HandleRequest)}Create a struct that performs common tasks.import (\t\"context\"\t\"log\"\t\"strings\"\t\"time\"\t\"user_pools_and_lambda_triggers/actions\"\t\"github.com/aws/aws-sdk-go-v2/aws\"\t\"github.com/aws/aws-sdk-go-v2/service/cloudformation\"\t\"github.com/aws/aws-sdk-go-v2/service/cloudwatchlogs\"\t\"github.com/aws/aws-sdk-go-v2/service/dynamodb\"\t\"github.com/awsdocs/aws-doc-sdk-examples/gov2/demotools\")// IScenarioHelper defines common functions used by the workflows in this example.type IScenarioHelper interface {\tPause(secs int)\tGetStackOutputs(ctx context.Context, stackName string) (actions.StackOutputs, error)\tPopulateUserTable(ctx context.Context, tableName string)\tGetKnownUsers(ctx context.Context, tableName string) (actions.UserList, error)\tAddKnownUser(ctx context.Context, tableName string, user actions.User)\tListRecentLogEvents(ctx context.Context, functionName string)}// ScenarioHelper contains AWS wrapper structs used by the workflows in this example.type ScenarioHelper struct {\tquestioner  demotools.IQuestioner\tdynamoActor *actions.DynamoActions\tcfnActor    *actions.CloudFormationActions\tcwlActor    *actions.CloudWatchLogsActions\tisTestRun   bool}// NewScenarioHelper constructs a new scenario helper.func NewScenarioHelper(sdkConfig aws.Config, questioner demotools.IQuestioner) ScenarioHelper {\tscenario := ScenarioHelper{\t\tquestioner:  questioner,\t\tdynamoActor: &actions.DynamoActions{DynamoClient: dynamodb.NewFromConfig(sdkConfig)},\t\tcfnActor:    &actions.CloudFormationActions{CfnClient: cloudformation.NewFromConfig(sdkConfig)},\t\tcwlActor:    &actions.CloudWatchLogsActions{CwlClient: cloudwatchlogs.NewFromConfig(sdkConfig)},\t}\treturn scenario}// Pause waits for the specified number of seconds.func (helper ScenarioHelper) Pause(secs int) {\tif !helper.isTestRun {\t\ttime.Sleep(time.Duration(secs) * time.Second)\t}}// GetStackOutputs gets the outputs from the specified CloudFormation stack in a structured format.func (helper ScenarioHelper) GetStackOutputs(ctx context.Context, stackName string) (actions.StackOutputs, error) {\treturn helper.cfnActor.GetOutputs(ctx, stackName), nil}// PopulateUserTable fills the known user table with example data.func (helper ScenarioHelper) PopulateUserTable(ctx context.Context, tableName string) {\tlog.Printf(\"First, let's add some users to the DynamoDB %v table we'll use for this example.\\n\", tableName)\terr := helper.dynamoActor.PopulateTable(ctx, tableName)\tif err != nil {\t\tpanic(err)\t}}// GetKnownUsers gets the users from the known users table in a structured format.func (helper ScenarioHelper) GetKnownUsers(ctx context.Context, tableName string) (actions.UserList, error) {\tknownUsers, err := helper.dynamoActor.Scan(ctx, tableName)\tif err != nil {\t\tlog.Printf(\"Couldn't get known users from table %v. Here's why: %v\\n\", tableName, err)\t}\treturn knownUsers, err}// AddKnownUser adds a user to the known users table.func (helper ScenarioHelper) AddKnownUser(ctx context.Context, tableName string, user actions.User) {\tlog.Printf(\"Adding user '%v' with email '%v' to the DynamoDB known users table...\\n\",\t\tuser.UserName, user.UserEmail)\terr := helper.dynamoActor.AddUser(ctx, tableName, user)\tif err != nil {\t\tpanic(err)\t}}// ListRecentLogEvents gets the most recent log stream and events for the specified Lambda function and displays them.func (helper ScenarioHelper) ListRecentLogEvents(ctx context.Context, functionName string) {\tlog.Println(\"Waiting a few seconds to let Lambda write to CloudWatch Logs...\")\thelper.Pause(10)\tlog.Println(\"Okay, let's check the logs to find what's happened recently with your Lambda function.\")\tlogStream, err := helper.cwlActor.GetLatestLogStream(ctx, functionName)\tif err != nil {\t\tpanic(err)\t}\tlog.Printf(\"Getting some recent events from log stream %v\\n\", *logStream.LogStreamName)\tevents, err := helper.cwlActor.GetLogEvents(ctx, functionName, *logStream.LogStreamName, 10)\tif err != nil {\t\tpanic(err)\t}\tfor _, event := range events {\t\tlog.Printf(\"\\t%v\", *event.Message)\t}\tlog.Println(strings.Repeat(\"-\", 88))}Create a struct that wraps Amazon Cognito actions.import (\t\"context\"\t\"errors\"\t\"log\"\t\"github.com/aws/aws-sdk-go-v2/aws\"\t\"github.com/aws/aws-sdk-go-v2/service/cognitoidentityprovider\"\t\"github.com/aws/aws-sdk-go-v2/service/cognitoidentityprovider/types\")type CognitoActions struct {\tCognitoClient *cognitoidentityprovider.Client}// Trigger and TriggerInfo define typed data for updating an Amazon Cognito trigger.type Trigger intconst (\tPreSignUp Trigger = iota\tUserMigration\tPostAuthentication)type TriggerInfo struct {\tTrigger    Trigger\tHandlerArn *string}// UpdateTriggers adds or removes Lambda triggers for a user pool. When a trigger is specified with a `nil` value,// it is removed from the user pool.func (actor CognitoActions) UpdateTriggers(ctx context.Context, userPoolId string, triggers ...TriggerInfo) error {\toutput, err := actor.CognitoClient.DescribeUserPool(ctx, &cognitoidentityprovider.DescribeUserPoolInput{\t\tUserPoolId: aws.String(userPoolId),\t})\tif err != nil {\t\tlog.Printf(\"Couldn't get info about user pool %v. Here's why: %v\\n\", userPoolId, err)\t\treturn err\t}\tlambdaConfig := output.UserPool.LambdaConfig\tfor _, trigger := range triggers {\t\tswitch trigger.Trigger {\t\tcase PreSignUp:\t\t\tlambdaConfig.PreSignUp = trigger.HandlerArn\t\tcase UserMigration:\t\t\tlambdaConfig.UserMigration = trigger.HandlerArn\t\tcase PostAuthentication:\t\t\tlambdaConfig.PostAuthentication = trigger.HandlerArn\t\t}\t}\t_, err = actor.CognitoClient.UpdateUserPool(ctx, &cognitoidentityprovider.UpdateUserPoolInput{\t\tUserPoolId:   aws.String(userPoolId),\t\tLambdaConfig: lambdaConfig,\t})\tif err != nil {\t\tlog.Printf(\"Couldn't update user pool %v. Here's why: %v\\n\", userPoolId, err)\t}\treturn err}// SignUp signs up a user with Amazon Cognito.func (actor CognitoActions) SignUp(ctx context.Context, clientId string, userName string, password string, userEmail string) (bool, error) {\tconfirmed := false\toutput, err := actor.CognitoClient.SignUp(ctx, &cognitoidentityprovider.SignUpInput{\t\tClientId: aws.String(clientId),\t\tPassword: aws.String(password),\t\tUsername: aws.String(userName),\t\tUserAttributes: []types.AttributeType{\t\t\t{Name: aws.String(\"email\"), Value: aws.String(userEmail)},\t\t},\t})\tif err != nil {\t\tvar invalidPassword *types.InvalidPasswordException\t\tif errors.As(err, &invalidPassword) {\t\t\tlog.Println(*invalidPassword.Message)\t\t} else {\t\t\tlog.Printf(\"Couldn't sign up user %v. Here's why: %v\\n\", userName, err)\t\t}\t} else {\t\tconfirmed = output.UserConfirmed\t}\treturn confirmed, err}// SignIn signs in a user to Amazon Cognito using a username and password authentication flow.func (actor CognitoActions) SignIn(ctx context.Context, clientId string, userName string, password string) (*types.AuthenticationResultType, error) {\tvar authResult *types.AuthenticationResultType\toutput, err := actor.CognitoClient.InitiateAuth(ctx, &cognitoidentityprovider.InitiateAuthInput{\t\tAuthFlow:       \"USER_PASSWORD_AUTH\",\t\tClientId:       aws.String(clientId),\t\tAuthParameters: map[string]string{\"USERNAME\": userName, \"PASSWORD\": password},\t})\tif err != nil {\t\tvar resetRequired *types.PasswordResetRequiredException\t\tif errors.As(err, &resetRequired) {\t\t\tlog.Println(*resetRequired.Message)\t\t} else {\t\t\tlog.Printf(\"Couldn't sign in user %v. Here's why: %v\\n\", userName, err)\t\t}\t} else {\t\tauthResult = output.AuthenticationResult\t}\treturn authResult, err}// ForgotPassword starts a password recovery flow for a user. This flow typically sends a confirmation code// to the user's configured notification destination, such as email.func (actor CognitoActions) ForgotPassword(ctx context.Context, clientId string, userName string) (*types.CodeDeliveryDetailsType, error) {\toutput, err := actor.CognitoClient.ForgotPassword(ctx, &cognitoidentityprovider.ForgotPasswordInput{\t\tClientId: aws.String(clientId),\t\tUsername: aws.String(userName),\t})\tif err != nil {\t\tlog.Printf(\"Couldn't start password reset for user '%v'. Here;s why: %v\\n\", userName, err)\t}\treturn output.CodeDeliveryDetails, err}// ConfirmForgotPassword confirms a user with a confirmation code and a new password.func (actor CognitoActions) ConfirmForgotPassword(ctx context.Context, clientId string, code string, userName string, password string) error {\t_, err := actor.CognitoClient.ConfirmForgotPassword(ctx, &cognitoidentityprovider.ConfirmForgotPasswordInput{\t\tClientId:         aws.String(clientId),\t\tConfirmationCode: aws.String(code),\t\tPassword:         aws.String(password),\t\tUsername:         aws.String(userName),\t})\tif err != nil {\t\tvar invalidPassword *types.InvalidPasswordException\t\tif errors.As(err, &invalidPassword) {\t\t\tlog.Println(*invalidPassword.Message)\t\t} else {\t\t\tlog.Printf(\"Couldn't confirm user %v. Here's why: %v\", userName, err)\t\t}\t}\treturn err}// DeleteUser removes a user from the user pool.func (actor CognitoActions) DeleteUser(ctx context.Context, userAccessToken string) error {\t_, err := actor.CognitoClient.DeleteUser(ctx, &cognitoidentityprovider.DeleteUserInput{\t\tAccessToken: aws.String(userAccessToken),\t})\tif err != nil {\t\tlog.Printf(\"Couldn't delete user. Here's why: %v\\n\", err)\t}\treturn err}// AdminCreateUser uses administrator credentials to add a user to a user pool. This method leaves the user// in a state that requires they enter a new password next time they sign in.func (actor CognitoActions) AdminCreateUser(ctx context.Context, userPoolId string, userName string, userEmail string) error {\t_, err := actor.CognitoClient.AdminCreateUser(ctx, &cognitoidentityprovider.AdminCreateUserInput{\t\tUserPoolId:     aws.String(userPoolId),\t\tUsername:       aws.String(userName),\t\tMessageAction:  types.MessageActionTypeSuppress,\t\tUserAttributes: []types.AttributeType{{Name: aws.String(\"email\"), Value: aws.String(userEmail)}},\t})\tif err != nil {\t\tvar userExists *types.UsernameExistsException\t\tif errors.As(err, &userExists) {\t\t\tlog.Printf(\"User %v already exists in the user pool.\", userName)\t\t\terr = nil\t\t} else {\t\t\tlog.Printf(\"Couldn't create user %v. Here's why: %v\\n\", userName, err)\t\t}\t}\treturn err}// AdminSetUserPassword uses administrator credentials to set a password for a user without requiring a// temporary password.func (actor CognitoActions) AdminSetUserPassword(ctx context.Context, userPoolId string, userName string, password string) error {\t_, err := actor.CognitoClient.AdminSetUserPassword(ctx, &cognitoidentityprovider.AdminSetUserPasswordInput{\t\tPassword:   aws.String(password),\t\tUserPoolId: aws.String(userPoolId),\t\tUsername:   aws.String(userName),\t\tPermanent:  true,\t})\tif err != nil {\t\tvar invalidPassword *types.InvalidPasswordException\t\tif errors.As(err, &invalidPassword) {\t\t\tlog.Println(*invalidPassword.Message)\t\t} else {\t\t\tlog.Printf(\"Couldn't set password for user %v. Here's why: %v\\n\", userName, err)\t\t}\t}\treturn err}Create a struct that wraps DynamoDB actions.import (\t\"context\"\t\"fmt\"\t\"log\"\t\"github.com/aws/aws-sdk-go-v2/aws\"\t\"github.com/aws/aws-sdk-go-v2/feature/dynamodb/attributevalue\"\t\"github.com/aws/aws-sdk-go-v2/service/dynamodb\"\t\"github.com/aws/aws-sdk-go-v2/service/dynamodb/types\")// DynamoActions encapsulates the Amazon Simple Notification Service (Amazon SNS) actions// used in the examples.type DynamoActions struct {\tDynamoClient *dynamodb.Client}// User defines structured user data.type User struct {\tUserName  string\tUserEmail string\tLastLogin *LoginInfo `dynamodbav:\",omitempty\"`}// LoginInfo defines structured custom login data.type LoginInfo struct {\tUserPoolId string\tClientId   string\tTime       string}// UserList defines a list of users.type UserList struct {\tUsers []User}// UserNameList returns the usernames contained in a UserList as a list of strings.func (users *UserList) UserNameList() []string {\tnames := make([]string, len(users.Users))\tfor i := 0; i < len(users.Users); i++ {\t\tnames[i] = users.Users[i].UserName\t}\treturn names}// PopulateTable adds a set of test users to the table.func (actor DynamoActions) PopulateTable(ctx context.Context, tableName string) error {\tvar err error\tvar item map[string]types.AttributeValue\tvar writeReqs []types.WriteRequest\tfor i := 1; i < 4; i++ {\t\titem, err = attributevalue.MarshalMap(User{UserName: fmt.Sprintf(\"test_user_%v\", i), UserEmail: fmt.Sprintf(\"test_email_%v@example.com\", i)})\t\tif err != nil {\t\t\tlog.Printf(\"Couldn't marshall user into DynamoDB format. Here's why: %v\\n\", err)\t\t\treturn err\t\t}\t\twriteReqs = append(writeReqs, types.WriteRequest{PutRequest: &types.PutRequest{Item: item}})\t}\t_, err = actor.DynamoClient.BatchWriteItem(ctx, &dynamodb.BatchWriteItemInput{\t\tRequestItems: map[string][]types.WriteRequest{tableName: writeReqs},\t})\tif err != nil {\t\tlog.Printf(\"Couldn't populate table %v with users. Here's why: %v\\n\", tableName, err)\t}\treturn err}// Scan scans the table for all items.func (actor DynamoActions) Scan(ctx context.Context, tableName string) (UserList, error) {\tvar userList UserList\toutput, err := actor.DynamoClient.Scan(ctx, &dynamodb.ScanInput{\t\tTableName: aws.String(tableName),\t})\tif err != nil {\t\tlog.Printf(\"Couldn't scan table %v for items. Here's why: %v\\n\", tableName, err)\t} else {\t\terr = attributevalue.UnmarshalListOfMaps(output.Items, &userList.Users)\t\tif err != nil {\t\t\tlog.Printf(\"Couldn't unmarshal items into users. Here's why: %v\\n\", err)\t\t}\t}\treturn userList, err}// AddUser adds a user item to a table.func (actor DynamoActions) AddUser(ctx context.Context, tableName string, user User) error {\tuserItem, err := attributevalue.MarshalMap(user)\tif err != nil {\t\tlog.Printf(\"Couldn't marshall user to item. Here's why: %v\\n\", err)\t}\t_, err = actor.DynamoClient.PutItem(ctx, &dynamodb.PutItemInput{\t\tItem:      userItem,\t\tTableName: aws.String(tableName),\t})\tif err != nil {\t\tlog.Printf(\"Couldn't put item in table %v. Here's why: %v\", tableName, err)\t}\treturn err}Create a struct that wraps CloudWatch Logs actions.import (\t\"context\"\t\"fmt\"\t\"log\"\t\"github.com/aws/aws-sdk-go-v2/aws\"\t\"github.com/aws/aws-sdk-go-v2/service/cloudwatchlogs\"\t\"github.com/aws/aws-sdk-go-v2/service/cloudwatchlogs/types\")type CloudWatchLogsActions struct {\tCwlClient *cloudwatchlogs.Client}// GetLatestLogStream gets the most recent log stream for a Lambda function.func (actor CloudWatchLogsActions) GetLatestLogStream(ctx context.Context, functionName string) (types.LogStream, error) {\tvar logStream types.LogStream\tlogGroupName := fmt.Sprintf(\"/aws/lambda/%s\", functionName)\toutput, err := actor.CwlClient.DescribeLogStreams(ctx, &cloudwatchlogs.DescribeLogStreamsInput{\t\tDescending:   aws.Bool(true),\t\tLimit:        aws.Int32(1),\t\tLogGroupName: aws.String(logGroupName),\t\tOrderBy:      types.OrderByLastEventTime,\t})\tif err != nil {\t\tlog.Printf(\"Couldn't get log streams for log group %v. Here's why: %v\\n\", logGroupName, err)\t} else {\t\tlogStream = output.LogStreams[0]\t}\treturn logStream, err}// GetLogEvents gets the most recent eventCount events from the specified log stream.func (actor CloudWatchLogsActions) GetLogEvents(ctx context.Context, functionName string, logStreamName string, eventCount int32) (\t[]types.OutputLogEvent, error) {\tvar events []types.OutputLogEvent\tlogGroupName := fmt.Sprintf(\"/aws/lambda/%s\", functionName)\toutput, err := actor.CwlClient.GetLogEvents(ctx, &cloudwatchlogs.GetLogEventsInput{\t\tLogStreamName: aws.String(logStreamName),\t\tLimit:         aws.Int32(eventCount),\t\tLogGroupName:  aws.String(logGroupName),\t})\tif err != nil {\t\tlog.Printf(\"Couldn't get log event for log stream %v. Here's why: %v\\n\", logStreamName, err)\t} else {\t\tevents = output.Events\t}\treturn events, err}Create a struct that wraps AWS CloudFormation actions.import (\t\"context\"\t\"log\"\t\"github.com/aws/aws-sdk-go-v2/aws\"\t\"github.com/aws/aws-sdk-go-v2/service/cloudformation\")// StackOutputs defines a map of outputs from a specific stack.type StackOutputs map[string]stringtype CloudFormationActions struct {\tCfnClient *cloudformation.Client}// GetOutputs gets the outputs from a CloudFormation stack and puts them into a structured format.func (actor CloudFormationActions) GetOutputs(ctx context.Context, stackName string) StackOutputs {\toutput, err := actor.CfnClient.DescribeStacks(ctx, &cloudformation.DescribeStacksInput{\t\tStackName: aws.String(stackName),\t})\tif err != nil || len(output.Stacks) == 0 {\t\tlog.Panicf(\"Couldn't find a CloudFormation stack named %v. Here's why: %v\\n\", stackName, err)\t}\tstackOutputs := StackOutputs{}\tfor _, out := range output.Stacks[0].Outputs {\t\tstackOutputs[*out.OutputKey] = *out.OutputValue\t}\treturn stackOutputs}Clean up resources.import (\t\"context\"\t\"log\"\t\"user_pools_and_lambda_triggers/actions\"\t\"github.com/awsdocs/aws-doc-sdk-examples/gov2/demotools\")// Resources keeps track of AWS resources created during an example and handles// cleanup when the example finishes.type Resources struct {\tuserPoolId       string\tuserAccessTokens []string\ttriggers         []actions.Trigger\tcognitoActor *actions.CognitoActions\tquestioner   demotools.IQuestioner}func (resources *Resources) init(cognitoActor *actions.CognitoActions, questioner demotools.IQuestioner) {\tresources.userAccessTokens = []string{}\tresources.triggers = []actions.Trigger{}\tresources.cognitoActor = cognitoActor\tresources.questioner = questioner}// Cleanup deletes all AWS resources created during an example.func (resources *Resources) Cleanup(ctx context.Context) {\tdefer func() {\t\tif r := recover(); r != nil {\t\t\tlog.Printf(\"Something went wrong during cleanup.\\n%v\\n\", r)\t\t\tlog.Println(\"Use the AWS Management Console to remove any remaining resources \\n\" +\t\t\t\t\"that were created for this scenario.\")\t\t}\t}()\twantDelete := resources.questioner.AskBool(\"Do you want to remove all of the AWS resources that were created \"+\t\t\"during this demo (y/n)?\", \"y\")\tif wantDelete {\t\tfor _, accessToken := range resources.userAccessTokens {\t\t\terr := resources.cognitoActor.DeleteUser(ctx, accessToken)\t\t\tif err != nil {\t\t\t\tlog.Println(\"Couldn't delete user during cleanup.\")\t\t\t\tpanic(err)\t\t\t}\t\t\tlog.Println(\"Deleted user.\")\t\t}\t\ttriggerList := make([]actions.TriggerInfo, len(resources.triggers))\t\tfor i := 0; i < len(resources.triggers); i++ {\t\t\ttriggerList[i] = actions.TriggerInfo{Trigger: resources.triggers[i], HandlerArn: nil}\t\t}\t\terr := resources.cognitoActor.UpdateTriggers(ctx, resources.userPoolId, triggerList...)\t\tif err != nil {\t\t\tlog.Println(\"Couldn't update Cognito triggers during cleanup.\")\t\t\tpanic(err)\t\t}\t\tlog.Println(\"Removed Cognito triggers from user pool.\")\t} else {\t\tlog.Println(\"Be sure to remove resources when you're done with them to avoid unexpected charges!\")\t}}For API details, see the following topics in AWS SDK for Go API Reference.DeleteUserInitiateAuthSignUpUpdateUserPool",
                            "For a complete list of AWS SDK developer guides and code examples, see    Using Lambda with an AWS SDK.    This topic also includes information about getting started and details about previous SDK versions."
                        ]
                    },
                    {
                        "title": "Automatically migrate known users with a Lambda function",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_cross_CognitoAutoMigrateUser_section.html",
                        "sections": [
                            "",
                            "The following code example shows how to automatically migrate known Amazon Cognito users with a Lambda function.",
                            {
                                "code_example": "MigrateUser"
                            },
                            "GoSDK for Go V2Note        There's more on GitHub. Find the complete example and learn how to set up and run in the        AWS Code            Examples Repository.    Run an interactive scenario at a command prompt.import (\t\"context\"\t\"errors\"\t\"fmt\"\t\"log\"\t\"strings\"\t\"user_pools_and_lambda_triggers/actions\"\t\"github.com/aws/aws-sdk-go-v2/aws\"\t\"github.com/aws/aws-sdk-go-v2/service/cognitoidentityprovider\"\t\"github.com/aws/aws-sdk-go-v2/service/cognitoidentityprovider/types\"\t\"github.com/awsdocs/aws-doc-sdk-examples/gov2/demotools\")// MigrateUser separates the steps of this scenario into individual functions so that// they are simpler to read and understand.type MigrateUser struct {\thelper       IScenarioHelper\tquestioner   demotools.IQuestioner\tresources    Resources\tcognitoActor *actions.CognitoActions}// NewMigrateUser constructs a new migrate user runner.func NewMigrateUser(sdkConfig aws.Config, questioner demotools.IQuestioner, helper IScenarioHelper) MigrateUser {\tscenario := MigrateUser{\t\thelper:       helper,\t\tquestioner:   questioner,\t\tresources:    Resources{},\t\tcognitoActor: &actions.CognitoActions{CognitoClient: cognitoidentityprovider.NewFromConfig(sdkConfig)},\t}\tscenario.resources.init(scenario.cognitoActor, questioner)\treturn scenario}// AddMigrateUserTrigger adds a Lambda handler as an invocation target for the MigrateUser trigger.func (runner *MigrateUser) AddMigrateUserTrigger(ctx context.Context, userPoolId string, functionArn string) {\tlog.Printf(\"Let's add a Lambda function to handle the MigrateUser trigger from Cognito.\\n\" +\t\t\"This trigger happens when an unknown user signs in, and lets your function take action before Cognito\\n\" +\t\t\"rejects the user.\\n\\n\")\terr := runner.cognitoActor.UpdateTriggers(\t\tctx, userPoolId,\t\tactions.TriggerInfo{Trigger: actions.UserMigration, HandlerArn: aws.String(functionArn)})\tif err != nil {\t\tpanic(err)\t}\tlog.Printf(\"Lambda function %v added to user pool %v to handle the MigrateUser trigger.\\n\",\t\tfunctionArn, userPoolId)\tlog.Println(strings.Repeat(\"-\", 88))}// SignInUser adds a new user to the known users table and signs that user in to Amazon Cognito.func (runner *MigrateUser) SignInUser(ctx context.Context, usersTable string, clientId string) (bool, actions.User) {\tlog.Println(\"Let's sign in a user to your Cognito user pool. When the username and email matches an entry in the\\n\" +\t\t\"DynamoDB known users table, the email is automatically verified and the user is migrated to the Cognito user pool.\")\tuser := actions.User{}\tuser.UserName = runner.questioner.Ask(\"\\nEnter a username:\")\tuser.UserEmail = runner.questioner.Ask(\"\\nEnter an email that you own. This email will be used to confirm user migration\\n\" +\t\t\"during this example:\")\trunner.helper.AddKnownUser(ctx, usersTable, user)\tvar err error\tvar resetRequired *types.PasswordResetRequiredException\tvar authResult *types.AuthenticationResultType\tsignedIn := false\tfor !signedIn && resetRequired == nil {\t\tlog.Printf(\"Signing in to Cognito as user '%v'. The expected result is a PasswordResetRequiredException.\\n\\n\", user.UserName)\t\tauthResult, err = runner.cognitoActor.SignIn(ctx, clientId, user.UserName, \"_\")\t\tif err != nil {\t\t\tif errors.As(err, &resetRequired) {\t\t\t\tlog.Printf(\"\\nUser '%v' is not in the Cognito user pool but was found in the DynamoDB known users table.\\n\"+\t\t\t\t\t\"User migration is started and a password reset is required.\", user.UserName)\t\t\t} else {\t\t\t\tpanic(err)\t\t\t}\t\t} else {\t\t\tlog.Printf(\"User '%v' successfully signed in. This is unexpected and probably means you have not\\n\"+\t\t\t\t\"cleaned up a previous run of this scenario, so the user exist in the Cognito user pool.\\n\"+\t\t\t\t\"You can continue this example and select to clean up resources, or manually remove\\n\"+\t\t\t\t\"the user from your user pool and try again.\", user.UserName)\t\t\trunner.resources.userAccessTokens = append(runner.resources.userAccessTokens, *authResult.AccessToken)\t\t\tsignedIn = true\t\t}\t}\tlog.Println(strings.Repeat(\"-\", 88))\treturn resetRequired != nil, user}// ResetPassword starts a password recovery flow.func (runner *MigrateUser) ResetPassword(ctx context.Context, clientId string, user actions.User) {\twantCode := runner.questioner.AskBool(fmt.Sprintf(\"In order to migrate the user to Cognito, you must be able to receive a confirmation\\n\"+\t\t\"code by email at %v. Do you want to send a code (y/n)?\", user.UserEmail), \"y\")\tif !wantCode {\t\tlog.Println(\"To complete this example and successfully migrate a user to Cognito, you must enter an email\\n\" +\t\t\t\"you own that can receive a confirmation code.\")\t\treturn\t}\tcodeDelivery, err := runner.cognitoActor.ForgotPassword(ctx, clientId, user.UserName)\tif err != nil {\t\tpanic(err)\t}\tlog.Printf(\"\\nA confirmation code has been sent to %v.\", *codeDelivery.Destination)\tcode := runner.questioner.Ask(\"Check your email and enter it here:\")\tconfirmed := false\tpassword := runner.questioner.AskPassword(\"\\nEnter a password that has at least eight characters, uppercase, lowercase, numbers and symbols.\\n\"+\t\t\"(the password will not display as you type):\", 8)\tfor !confirmed {\t\tlog.Printf(\"\\nConfirming password reset for user '%v'.\\n\", user.UserName)\t\terr = runner.cognitoActor.ConfirmForgotPassword(ctx, clientId, code, user.UserName, password)\t\tif err != nil {\t\t\tvar invalidPassword *types.InvalidPasswordException\t\t\tif errors.As(err, &invalidPassword) {\t\t\t\tpassword = runner.questioner.AskPassword(\"\\nEnter another password:\", 8)\t\t\t} else {\t\t\t\tpanic(err)\t\t\t}\t\t} else {\t\t\tconfirmed = true\t\t}\t}\tlog.Printf(\"User '%v' successfully confirmed and migrated.\\n\", user.UserName)\tlog.Println(\"Signing in with your username and password...\")\tauthResult, err := runner.cognitoActor.SignIn(ctx, clientId, user.UserName, password)\tif err != nil {\t\tpanic(err)\t}\tlog.Printf(\"Successfully signed in. Your access token starts with: %v...\\n\", (*authResult.AccessToken)[:10])\trunner.resources.userAccessTokens = append(runner.resources.userAccessTokens, *authResult.AccessToken)\tlog.Println(strings.Repeat(\"-\", 88))}// Run runs the scenario.func (runner *MigrateUser) Run(ctx context.Context, stackName string) {\tdefer func() {\t\tif r := recover(); r != nil {\t\t\tlog.Println(\"Something went wrong with the demo.\")\t\t\trunner.resources.Cleanup(ctx)\t\t}\t}()\tlog.Println(strings.Repeat(\"-\", 88))\tlog.Printf(\"Welcome\\n\")\tlog.Println(strings.Repeat(\"-\", 88))\tstackOutputs, err := runner.helper.GetStackOutputs(ctx, stackName)\tif err != nil {\t\tpanic(err)\t}\trunner.resources.userPoolId = stackOutputs[\"UserPoolId\"]\trunner.AddMigrateUserTrigger(ctx, stackOutputs[\"UserPoolId\"], stackOutputs[\"MigrateUserFunctionArn\"])\trunner.resources.triggers = append(runner.resources.triggers, actions.UserMigration)\tresetNeeded, user := runner.SignInUser(ctx, stackOutputs[\"TableName\"], stackOutputs[\"UserPoolClientId\"])\tif resetNeeded {\t\trunner.helper.ListRecentLogEvents(ctx, stackOutputs[\"MigrateUserFunction\"])\t\trunner.ResetPassword(ctx, stackOutputs[\"UserPoolClientId\"], user)\t}\trunner.resources.Cleanup(ctx)\tlog.Println(strings.Repeat(\"-\", 88))\tlog.Println(\"Thanks for watching!\")\tlog.Println(strings.Repeat(\"-\", 88))}Handle the MigrateUser trigger with a Lambda function.import (\t\"context\"\t\"log\"\t\"os\"\t\"github.com/aws/aws-lambda-go/events\"\t\"github.com/aws/aws-lambda-go/lambda\"\t\"github.com/aws/aws-sdk-go-v2/aws\"\t\"github.com/aws/aws-sdk-go-v2/config\"\t\"github.com/aws/aws-sdk-go-v2/feature/dynamodb/attributevalue\"\t\"github.com/aws/aws-sdk-go-v2/feature/dynamodb/expression\"\t\"github.com/aws/aws-sdk-go-v2/service/dynamodb\")const TABLE_NAME = \"TABLE_NAME\"// UserInfo defines structured user data that can be marshalled to a DynamoDB format.type UserInfo struct {\tUserName  string `dynamodbav:\"UserName\"`\tUserEmail string `dynamodbav:\"UserEmail\"`}type handler struct {\tdynamoClient *dynamodb.Client}// HandleRequest handles the MigrateUser event by looking up a user in an Amazon DynamoDB table and// specifying whether they should be migrated to the user pool.func (h *handler) HandleRequest(ctx context.Context, event events.CognitoEventUserPoolsMigrateUser) (events.CognitoEventUserPoolsMigrateUser, error) {\tlog.Printf(\"Received migrate trigger from %v for user '%v'\", event.TriggerSource, event.UserName)\tif event.TriggerSource != \"UserMigration_Authentication\" {\t\treturn event, nil\t}\ttableName := os.Getenv(TABLE_NAME)\tuser := UserInfo{\t\tUserName: event.UserName,\t}\tlog.Printf(\"Looking up user '%v' in table %v.\\n\", user.UserName, tableName)\tfilterEx := expression.Name(\"UserName\").Equal(expression.Value(user.UserName))\texpr, err := expression.NewBuilder().WithFilter(filterEx).Build()\tif err != nil {\t\tlog.Printf(\"Error building expression to query for user '%v'.\\n\", user.UserName)\t\treturn event, err\t}\toutput, err := h.dynamoClient.Scan(ctx, &dynamodb.ScanInput{\t\tTableName:                 aws.String(tableName),\t\tFilterExpression:          expr.Filter(),\t\tExpressionAttributeNames:  expr.Names(),\t\tExpressionAttributeValues: expr.Values(),\t})\tif err != nil {\t\tlog.Printf(\"Error looking up user '%v'.\\n\", user.UserName)\t\treturn event, err\t}\tif len(output.Items) == 0 {\t\tlog.Printf(\"User '%v' not found, not migrating user.\\n\", user.UserName)\t\treturn event, err\t}\tvar users []UserInfo\terr = attributevalue.UnmarshalListOfMaps(output.Items, &users)\tif err != nil {\t\tlog.Printf(\"Couldn't unmarshal DynamoDB items. Here's why: %v\\n\", err)\t\treturn event, err\t}\tuser = users[0]\tlog.Printf(\"UserName '%v' found with email %v. User is migrated and must reset password.\\n\", user.UserName, user.UserEmail)\tevent.CognitoEventUserPoolsMigrateUserResponse.UserAttributes = map[string]string{\t\t\"email\":          user.UserEmail,\t\t\"email_verified\": \"true\", // email_verified is required for the forgot password flow.\t}\tevent.CognitoEventUserPoolsMigrateUserResponse.FinalUserStatus = \"RESET_REQUIRED\"\tevent.CognitoEventUserPoolsMigrateUserResponse.MessageAction = \"SUPPRESS\"\treturn event, err}func main() {\tctx := context.Background()\tsdkConfig, err := config.LoadDefaultConfig(ctx)\tif err != nil {\t\tlog.Panicln(err)\t}\th := handler{\t\tdynamoClient: dynamodb.NewFromConfig(sdkConfig),\t}\tlambda.Start(h.HandleRequest)}Create a struct that performs common tasks.import (\t\"context\"\t\"log\"\t\"strings\"\t\"time\"\t\"user_pools_and_lambda_triggers/actions\"\t\"github.com/aws/aws-sdk-go-v2/aws\"\t\"github.com/aws/aws-sdk-go-v2/service/cloudformation\"\t\"github.com/aws/aws-sdk-go-v2/service/cloudwatchlogs\"\t\"github.com/aws/aws-sdk-go-v2/service/dynamodb\"\t\"github.com/awsdocs/aws-doc-sdk-examples/gov2/demotools\")// IScenarioHelper defines common functions used by the workflows in this example.type IScenarioHelper interface {\tPause(secs int)\tGetStackOutputs(ctx context.Context, stackName string) (actions.StackOutputs, error)\tPopulateUserTable(ctx context.Context, tableName string)\tGetKnownUsers(ctx context.Context, tableName string) (actions.UserList, error)\tAddKnownUser(ctx context.Context, tableName string, user actions.User)\tListRecentLogEvents(ctx context.Context, functionName string)}// ScenarioHelper contains AWS wrapper structs used by the workflows in this example.type ScenarioHelper struct {\tquestioner  demotools.IQuestioner\tdynamoActor *actions.DynamoActions\tcfnActor    *actions.CloudFormationActions\tcwlActor    *actions.CloudWatchLogsActions\tisTestRun   bool}// NewScenarioHelper constructs a new scenario helper.func NewScenarioHelper(sdkConfig aws.Config, questioner demotools.IQuestioner) ScenarioHelper {\tscenario := ScenarioHelper{\t\tquestioner:  questioner,\t\tdynamoActor: &actions.DynamoActions{DynamoClient: dynamodb.NewFromConfig(sdkConfig)},\t\tcfnActor:    &actions.CloudFormationActions{CfnClient: cloudformation.NewFromConfig(sdkConfig)},\t\tcwlActor:    &actions.CloudWatchLogsActions{CwlClient: cloudwatchlogs.NewFromConfig(sdkConfig)},\t}\treturn scenario}// Pause waits for the specified number of seconds.func (helper ScenarioHelper) Pause(secs int) {\tif !helper.isTestRun {\t\ttime.Sleep(time.Duration(secs) * time.Second)\t}}// GetStackOutputs gets the outputs from the specified CloudFormation stack in a structured format.func (helper ScenarioHelper) GetStackOutputs(ctx context.Context, stackName string) (actions.StackOutputs, error) {\treturn helper.cfnActor.GetOutputs(ctx, stackName), nil}// PopulateUserTable fills the known user table with example data.func (helper ScenarioHelper) PopulateUserTable(ctx context.Context, tableName string) {\tlog.Printf(\"First, let's add some users to the DynamoDB %v table we'll use for this example.\\n\", tableName)\terr := helper.dynamoActor.PopulateTable(ctx, tableName)\tif err != nil {\t\tpanic(err)\t}}// GetKnownUsers gets the users from the known users table in a structured format.func (helper ScenarioHelper) GetKnownUsers(ctx context.Context, tableName string) (actions.UserList, error) {\tknownUsers, err := helper.dynamoActor.Scan(ctx, tableName)\tif err != nil {\t\tlog.Printf(\"Couldn't get known users from table %v. Here's why: %v\\n\", tableName, err)\t}\treturn knownUsers, err}// AddKnownUser adds a user to the known users table.func (helper ScenarioHelper) AddKnownUser(ctx context.Context, tableName string, user actions.User) {\tlog.Printf(\"Adding user '%v' with email '%v' to the DynamoDB known users table...\\n\",\t\tuser.UserName, user.UserEmail)\terr := helper.dynamoActor.AddUser(ctx, tableName, user)\tif err != nil {\t\tpanic(err)\t}}// ListRecentLogEvents gets the most recent log stream and events for the specified Lambda function and displays them.func (helper ScenarioHelper) ListRecentLogEvents(ctx context.Context, functionName string) {\tlog.Println(\"Waiting a few seconds to let Lambda write to CloudWatch Logs...\")\thelper.Pause(10)\tlog.Println(\"Okay, let's check the logs to find what's happened recently with your Lambda function.\")\tlogStream, err := helper.cwlActor.GetLatestLogStream(ctx, functionName)\tif err != nil {\t\tpanic(err)\t}\tlog.Printf(\"Getting some recent events from log stream %v\\n\", *logStream.LogStreamName)\tevents, err := helper.cwlActor.GetLogEvents(ctx, functionName, *logStream.LogStreamName, 10)\tif err != nil {\t\tpanic(err)\t}\tfor _, event := range events {\t\tlog.Printf(\"\\t%v\", *event.Message)\t}\tlog.Println(strings.Repeat(\"-\", 88))}Create a struct that wraps Amazon Cognito actions.import (\t\"context\"\t\"errors\"\t\"log\"\t\"github.com/aws/aws-sdk-go-v2/aws\"\t\"github.com/aws/aws-sdk-go-v2/service/cognitoidentityprovider\"\t\"github.com/aws/aws-sdk-go-v2/service/cognitoidentityprovider/types\")type CognitoActions struct {\tCognitoClient *cognitoidentityprovider.Client}// Trigger and TriggerInfo define typed data for updating an Amazon Cognito trigger.type Trigger intconst (\tPreSignUp Trigger = iota\tUserMigration\tPostAuthentication)type TriggerInfo struct {\tTrigger    Trigger\tHandlerArn *string}// UpdateTriggers adds or removes Lambda triggers for a user pool. When a trigger is specified with a `nil` value,// it is removed from the user pool.func (actor CognitoActions) UpdateTriggers(ctx context.Context, userPoolId string, triggers ...TriggerInfo) error {\toutput, err := actor.CognitoClient.DescribeUserPool(ctx, &cognitoidentityprovider.DescribeUserPoolInput{\t\tUserPoolId: aws.String(userPoolId),\t})\tif err != nil {\t\tlog.Printf(\"Couldn't get info about user pool %v. Here's why: %v\\n\", userPoolId, err)\t\treturn err\t}\tlambdaConfig := output.UserPool.LambdaConfig\tfor _, trigger := range triggers {\t\tswitch trigger.Trigger {\t\tcase PreSignUp:\t\t\tlambdaConfig.PreSignUp = trigger.HandlerArn\t\tcase UserMigration:\t\t\tlambdaConfig.UserMigration = trigger.HandlerArn\t\tcase PostAuthentication:\t\t\tlambdaConfig.PostAuthentication = trigger.HandlerArn\t\t}\t}\t_, err = actor.CognitoClient.UpdateUserPool(ctx, &cognitoidentityprovider.UpdateUserPoolInput{\t\tUserPoolId:   aws.String(userPoolId),\t\tLambdaConfig: lambdaConfig,\t})\tif err != nil {\t\tlog.Printf(\"Couldn't update user pool %v. Here's why: %v\\n\", userPoolId, err)\t}\treturn err}// SignUp signs up a user with Amazon Cognito.func (actor CognitoActions) SignUp(ctx context.Context, clientId string, userName string, password string, userEmail string) (bool, error) {\tconfirmed := false\toutput, err := actor.CognitoClient.SignUp(ctx, &cognitoidentityprovider.SignUpInput{\t\tClientId: aws.String(clientId),\t\tPassword: aws.String(password),\t\tUsername: aws.String(userName),\t\tUserAttributes: []types.AttributeType{\t\t\t{Name: aws.String(\"email\"), Value: aws.String(userEmail)},\t\t},\t})\tif err != nil {\t\tvar invalidPassword *types.InvalidPasswordException\t\tif errors.As(err, &invalidPassword) {\t\t\tlog.Println(*invalidPassword.Message)\t\t} else {\t\t\tlog.Printf(\"Couldn't sign up user %v. Here's why: %v\\n\", userName, err)\t\t}\t} else {\t\tconfirmed = output.UserConfirmed\t}\treturn confirmed, err}// SignIn signs in a user to Amazon Cognito using a username and password authentication flow.func (actor CognitoActions) SignIn(ctx context.Context, clientId string, userName string, password string) (*types.AuthenticationResultType, error) {\tvar authResult *types.AuthenticationResultType\toutput, err := actor.CognitoClient.InitiateAuth(ctx, &cognitoidentityprovider.InitiateAuthInput{\t\tAuthFlow:       \"USER_PASSWORD_AUTH\",\t\tClientId:       aws.String(clientId),\t\tAuthParameters: map[string]string{\"USERNAME\": userName, \"PASSWORD\": password},\t})\tif err != nil {\t\tvar resetRequired *types.PasswordResetRequiredException\t\tif errors.As(err, &resetRequired) {\t\t\tlog.Println(*resetRequired.Message)\t\t} else {\t\t\tlog.Printf(\"Couldn't sign in user %v. Here's why: %v\\n\", userName, err)\t\t}\t} else {\t\tauthResult = output.AuthenticationResult\t}\treturn authResult, err}// ForgotPassword starts a password recovery flow for a user. This flow typically sends a confirmation code// to the user's configured notification destination, such as email.func (actor CognitoActions) ForgotPassword(ctx context.Context, clientId string, userName string) (*types.CodeDeliveryDetailsType, error) {\toutput, err := actor.CognitoClient.ForgotPassword(ctx, &cognitoidentityprovider.ForgotPasswordInput{\t\tClientId: aws.String(clientId),\t\tUsername: aws.String(userName),\t})\tif err != nil {\t\tlog.Printf(\"Couldn't start password reset for user '%v'. Here;s why: %v\\n\", userName, err)\t}\treturn output.CodeDeliveryDetails, err}// ConfirmForgotPassword confirms a user with a confirmation code and a new password.func (actor CognitoActions) ConfirmForgotPassword(ctx context.Context, clientId string, code string, userName string, password string) error {\t_, err := actor.CognitoClient.ConfirmForgotPassword(ctx, &cognitoidentityprovider.ConfirmForgotPasswordInput{\t\tClientId:         aws.String(clientId),\t\tConfirmationCode: aws.String(code),\t\tPassword:         aws.String(password),\t\tUsername:         aws.String(userName),\t})\tif err != nil {\t\tvar invalidPassword *types.InvalidPasswordException\t\tif errors.As(err, &invalidPassword) {\t\t\tlog.Println(*invalidPassword.Message)\t\t} else {\t\t\tlog.Printf(\"Couldn't confirm user %v. Here's why: %v\", userName, err)\t\t}\t}\treturn err}// DeleteUser removes a user from the user pool.func (actor CognitoActions) DeleteUser(ctx context.Context, userAccessToken string) error {\t_, err := actor.CognitoClient.DeleteUser(ctx, &cognitoidentityprovider.DeleteUserInput{\t\tAccessToken: aws.String(userAccessToken),\t})\tif err != nil {\t\tlog.Printf(\"Couldn't delete user. Here's why: %v\\n\", err)\t}\treturn err}// AdminCreateUser uses administrator credentials to add a user to a user pool. This method leaves the user// in a state that requires they enter a new password next time they sign in.func (actor CognitoActions) AdminCreateUser(ctx context.Context, userPoolId string, userName string, userEmail string) error {\t_, err := actor.CognitoClient.AdminCreateUser(ctx, &cognitoidentityprovider.AdminCreateUserInput{\t\tUserPoolId:     aws.String(userPoolId),\t\tUsername:       aws.String(userName),\t\tMessageAction:  types.MessageActionTypeSuppress,\t\tUserAttributes: []types.AttributeType{{Name: aws.String(\"email\"), Value: aws.String(userEmail)}},\t})\tif err != nil {\t\tvar userExists *types.UsernameExistsException\t\tif errors.As(err, &userExists) {\t\t\tlog.Printf(\"User %v already exists in the user pool.\", userName)\t\t\terr = nil\t\t} else {\t\t\tlog.Printf(\"Couldn't create user %v. Here's why: %v\\n\", userName, err)\t\t}\t}\treturn err}// AdminSetUserPassword uses administrator credentials to set a password for a user without requiring a// temporary password.func (actor CognitoActions) AdminSetUserPassword(ctx context.Context, userPoolId string, userName string, password string) error {\t_, err := actor.CognitoClient.AdminSetUserPassword(ctx, &cognitoidentityprovider.AdminSetUserPasswordInput{\t\tPassword:   aws.String(password),\t\tUserPoolId: aws.String(userPoolId),\t\tUsername:   aws.String(userName),\t\tPermanent:  true,\t})\tif err != nil {\t\tvar invalidPassword *types.InvalidPasswordException\t\tif errors.As(err, &invalidPassword) {\t\t\tlog.Println(*invalidPassword.Message)\t\t} else {\t\t\tlog.Printf(\"Couldn't set password for user %v. Here's why: %v\\n\", userName, err)\t\t}\t}\treturn err}Create a struct that wraps DynamoDB actions.import (\t\"context\"\t\"fmt\"\t\"log\"\t\"github.com/aws/aws-sdk-go-v2/aws\"\t\"github.com/aws/aws-sdk-go-v2/feature/dynamodb/attributevalue\"\t\"github.com/aws/aws-sdk-go-v2/service/dynamodb\"\t\"github.com/aws/aws-sdk-go-v2/service/dynamodb/types\")// DynamoActions encapsulates the Amazon Simple Notification Service (Amazon SNS) actions// used in the examples.type DynamoActions struct {\tDynamoClient *dynamodb.Client}// User defines structured user data.type User struct {\tUserName  string\tUserEmail string\tLastLogin *LoginInfo `dynamodbav:\",omitempty\"`}// LoginInfo defines structured custom login data.type LoginInfo struct {\tUserPoolId string\tClientId   string\tTime       string}// UserList defines a list of users.type UserList struct {\tUsers []User}// UserNameList returns the usernames contained in a UserList as a list of strings.func (users *UserList) UserNameList() []string {\tnames := make([]string, len(users.Users))\tfor i := 0; i < len(users.Users); i++ {\t\tnames[i] = users.Users[i].UserName\t}\treturn names}// PopulateTable adds a set of test users to the table.func (actor DynamoActions) PopulateTable(ctx context.Context, tableName string) error {\tvar err error\tvar item map[string]types.AttributeValue\tvar writeReqs []types.WriteRequest\tfor i := 1; i < 4; i++ {\t\titem, err = attributevalue.MarshalMap(User{UserName: fmt.Sprintf(\"test_user_%v\", i), UserEmail: fmt.Sprintf(\"test_email_%v@example.com\", i)})\t\tif err != nil {\t\t\tlog.Printf(\"Couldn't marshall user into DynamoDB format. Here's why: %v\\n\", err)\t\t\treturn err\t\t}\t\twriteReqs = append(writeReqs, types.WriteRequest{PutRequest: &types.PutRequest{Item: item}})\t}\t_, err = actor.DynamoClient.BatchWriteItem(ctx, &dynamodb.BatchWriteItemInput{\t\tRequestItems: map[string][]types.WriteRequest{tableName: writeReqs},\t})\tif err != nil {\t\tlog.Printf(\"Couldn't populate table %v with users. Here's why: %v\\n\", tableName, err)\t}\treturn err}// Scan scans the table for all items.func (actor DynamoActions) Scan(ctx context.Context, tableName string) (UserList, error) {\tvar userList UserList\toutput, err := actor.DynamoClient.Scan(ctx, &dynamodb.ScanInput{\t\tTableName: aws.String(tableName),\t})\tif err != nil {\t\tlog.Printf(\"Couldn't scan table %v for items. Here's why: %v\\n\", tableName, err)\t} else {\t\terr = attributevalue.UnmarshalListOfMaps(output.Items, &userList.Users)\t\tif err != nil {\t\t\tlog.Printf(\"Couldn't unmarshal items into users. Here's why: %v\\n\", err)\t\t}\t}\treturn userList, err}// AddUser adds a user item to a table.func (actor DynamoActions) AddUser(ctx context.Context, tableName string, user User) error {\tuserItem, err := attributevalue.MarshalMap(user)\tif err != nil {\t\tlog.Printf(\"Couldn't marshall user to item. Here's why: %v\\n\", err)\t}\t_, err = actor.DynamoClient.PutItem(ctx, &dynamodb.PutItemInput{\t\tItem:      userItem,\t\tTableName: aws.String(tableName),\t})\tif err != nil {\t\tlog.Printf(\"Couldn't put item in table %v. Here's why: %v\", tableName, err)\t}\treturn err}Create a struct that wraps CloudWatch Logs actions.import (\t\"context\"\t\"fmt\"\t\"log\"\t\"github.com/aws/aws-sdk-go-v2/aws\"\t\"github.com/aws/aws-sdk-go-v2/service/cloudwatchlogs\"\t\"github.com/aws/aws-sdk-go-v2/service/cloudwatchlogs/types\")type CloudWatchLogsActions struct {\tCwlClient *cloudwatchlogs.Client}// GetLatestLogStream gets the most recent log stream for a Lambda function.func (actor CloudWatchLogsActions) GetLatestLogStream(ctx context.Context, functionName string) (types.LogStream, error) {\tvar logStream types.LogStream\tlogGroupName := fmt.Sprintf(\"/aws/lambda/%s\", functionName)\toutput, err := actor.CwlClient.DescribeLogStreams(ctx, &cloudwatchlogs.DescribeLogStreamsInput{\t\tDescending:   aws.Bool(true),\t\tLimit:        aws.Int32(1),\t\tLogGroupName: aws.String(logGroupName),\t\tOrderBy:      types.OrderByLastEventTime,\t})\tif err != nil {\t\tlog.Printf(\"Couldn't get log streams for log group %v. Here's why: %v\\n\", logGroupName, err)\t} else {\t\tlogStream = output.LogStreams[0]\t}\treturn logStream, err}// GetLogEvents gets the most recent eventCount events from the specified log stream.func (actor CloudWatchLogsActions) GetLogEvents(ctx context.Context, functionName string, logStreamName string, eventCount int32) (\t[]types.OutputLogEvent, error) {\tvar events []types.OutputLogEvent\tlogGroupName := fmt.Sprintf(\"/aws/lambda/%s\", functionName)\toutput, err := actor.CwlClient.GetLogEvents(ctx, &cloudwatchlogs.GetLogEventsInput{\t\tLogStreamName: aws.String(logStreamName),\t\tLimit:         aws.Int32(eventCount),\t\tLogGroupName:  aws.String(logGroupName),\t})\tif err != nil {\t\tlog.Printf(\"Couldn't get log event for log stream %v. Here's why: %v\\n\", logStreamName, err)\t} else {\t\tevents = output.Events\t}\treturn events, err}Create a struct that wraps AWS CloudFormation actions.import (\t\"context\"\t\"log\"\t\"github.com/aws/aws-sdk-go-v2/aws\"\t\"github.com/aws/aws-sdk-go-v2/service/cloudformation\")// StackOutputs defines a map of outputs from a specific stack.type StackOutputs map[string]stringtype CloudFormationActions struct {\tCfnClient *cloudformation.Client}// GetOutputs gets the outputs from a CloudFormation stack and puts them into a structured format.func (actor CloudFormationActions) GetOutputs(ctx context.Context, stackName string) StackOutputs {\toutput, err := actor.CfnClient.DescribeStacks(ctx, &cloudformation.DescribeStacksInput{\t\tStackName: aws.String(stackName),\t})\tif err != nil || len(output.Stacks) == 0 {\t\tlog.Panicf(\"Couldn't find a CloudFormation stack named %v. Here's why: %v\\n\", stackName, err)\t}\tstackOutputs := StackOutputs{}\tfor _, out := range output.Stacks[0].Outputs {\t\tstackOutputs[*out.OutputKey] = *out.OutputValue\t}\treturn stackOutputs}Clean up resources.import (\t\"context\"\t\"log\"\t\"user_pools_and_lambda_triggers/actions\"\t\"github.com/awsdocs/aws-doc-sdk-examples/gov2/demotools\")// Resources keeps track of AWS resources created during an example and handles// cleanup when the example finishes.type Resources struct {\tuserPoolId       string\tuserAccessTokens []string\ttriggers         []actions.Trigger\tcognitoActor *actions.CognitoActions\tquestioner   demotools.IQuestioner}func (resources *Resources) init(cognitoActor *actions.CognitoActions, questioner demotools.IQuestioner) {\tresources.userAccessTokens = []string{}\tresources.triggers = []actions.Trigger{}\tresources.cognitoActor = cognitoActor\tresources.questioner = questioner}// Cleanup deletes all AWS resources created during an example.func (resources *Resources) Cleanup(ctx context.Context) {\tdefer func() {\t\tif r := recover(); r != nil {\t\t\tlog.Printf(\"Something went wrong during cleanup.\\n%v\\n\", r)\t\t\tlog.Println(\"Use the AWS Management Console to remove any remaining resources \\n\" +\t\t\t\t\"that were created for this scenario.\")\t\t}\t}()\twantDelete := resources.questioner.AskBool(\"Do you want to remove all of the AWS resources that were created \"+\t\t\"during this demo (y/n)?\", \"y\")\tif wantDelete {\t\tfor _, accessToken := range resources.userAccessTokens {\t\t\terr := resources.cognitoActor.DeleteUser(ctx, accessToken)\t\t\tif err != nil {\t\t\t\tlog.Println(\"Couldn't delete user during cleanup.\")\t\t\t\tpanic(err)\t\t\t}\t\t\tlog.Println(\"Deleted user.\")\t\t}\t\ttriggerList := make([]actions.TriggerInfo, len(resources.triggers))\t\tfor i := 0; i < len(resources.triggers); i++ {\t\t\ttriggerList[i] = actions.TriggerInfo{Trigger: resources.triggers[i], HandlerArn: nil}\t\t}\t\terr := resources.cognitoActor.UpdateTriggers(ctx, resources.userPoolId, triggerList...)\t\tif err != nil {\t\t\tlog.Println(\"Couldn't update Cognito triggers during cleanup.\")\t\t\tpanic(err)\t\t}\t\tlog.Println(\"Removed Cognito triggers from user pool.\")\t} else {\t\tlog.Println(\"Be sure to remove resources when you're done with them to avoid unexpected charges!\")\t}}For API details, see the following topics in AWS SDK for Go API Reference.ConfirmForgotPasswordDeleteUserForgotPasswordInitiateAuthSignUpUpdateUserPoolanchorGoSDK for Go V2Note        There's more on GitHub. Find the complete example and learn how to set up and run in the        AWS Code            Examples Repository.    Run an interactive scenario at a command prompt.import (\t\"context\"\t\"errors\"\t\"fmt\"\t\"log\"\t\"strings\"\t\"user_pools_and_lambda_triggers/actions\"\t\"github.com/aws/aws-sdk-go-v2/aws\"\t\"github.com/aws/aws-sdk-go-v2/service/cognitoidentityprovider\"\t\"github.com/aws/aws-sdk-go-v2/service/cognitoidentityprovider/types\"\t\"github.com/awsdocs/aws-doc-sdk-examples/gov2/demotools\")// MigrateUser separates the steps of this scenario into individual functions so that// they are simpler to read and understand.type MigrateUser struct {\thelper       IScenarioHelper\tquestioner   demotools.IQuestioner\tresources    Resources\tcognitoActor *actions.CognitoActions}// NewMigrateUser constructs a new migrate user runner.func NewMigrateUser(sdkConfig aws.Config, questioner demotools.IQuestioner, helper IScenarioHelper) MigrateUser {\tscenario := MigrateUser{\t\thelper:       helper,\t\tquestioner:   questioner,\t\tresources:    Resources{},\t\tcognitoActor: &actions.CognitoActions{CognitoClient: cognitoidentityprovider.NewFromConfig(sdkConfig)},\t}\tscenario.resources.init(scenario.cognitoActor, questioner)\treturn scenario}// AddMigrateUserTrigger adds a Lambda handler as an invocation target for the MigrateUser trigger.func (runner *MigrateUser) AddMigrateUserTrigger(ctx context.Context, userPoolId string, functionArn string) {\tlog.Printf(\"Let's add a Lambda function to handle the MigrateUser trigger from Cognito.\\n\" +\t\t\"This trigger happens when an unknown user signs in, and lets your function take action before Cognito\\n\" +\t\t\"rejects the user.\\n\\n\")\terr := runner.cognitoActor.UpdateTriggers(\t\tctx, userPoolId,\t\tactions.TriggerInfo{Trigger: actions.UserMigration, HandlerArn: aws.String(functionArn)})\tif err != nil {\t\tpanic(err)\t}\tlog.Printf(\"Lambda function %v added to user pool %v to handle the MigrateUser trigger.\\n\",\t\tfunctionArn, userPoolId)\tlog.Println(strings.Repeat(\"-\", 88))}// SignInUser adds a new user to the known users table and signs that user in to Amazon Cognito.func (runner *MigrateUser) SignInUser(ctx context.Context, usersTable string, clientId string) (bool, actions.User) {\tlog.Println(\"Let's sign in a user to your Cognito user pool. When the username and email matches an entry in the\\n\" +\t\t\"DynamoDB known users table, the email is automatically verified and the user is migrated to the Cognito user pool.\")\tuser := actions.User{}\tuser.UserName = runner.questioner.Ask(\"\\nEnter a username:\")\tuser.UserEmail = runner.questioner.Ask(\"\\nEnter an email that you own. This email will be used to confirm user migration\\n\" +\t\t\"during this example:\")\trunner.helper.AddKnownUser(ctx, usersTable, user)\tvar err error\tvar resetRequired *types.PasswordResetRequiredException\tvar authResult *types.AuthenticationResultType\tsignedIn := false\tfor !signedIn && resetRequired == nil {\t\tlog.Printf(\"Signing in to Cognito as user '%v'. The expected result is a PasswordResetRequiredException.\\n\\n\", user.UserName)\t\tauthResult, err = runner.cognitoActor.SignIn(ctx, clientId, user.UserName, \"_\")\t\tif err != nil {\t\t\tif errors.As(err, &resetRequired) {\t\t\t\tlog.Printf(\"\\nUser '%v' is not in the Cognito user pool but was found in the DynamoDB known users table.\\n\"+\t\t\t\t\t\"User migration is started and a password reset is required.\", user.UserName)\t\t\t} else {\t\t\t\tpanic(err)\t\t\t}\t\t} else {\t\t\tlog.Printf(\"User '%v' successfully signed in. This is unexpected and probably means you have not\\n\"+\t\t\t\t\"cleaned up a previous run of this scenario, so the user exist in the Cognito user pool.\\n\"+\t\t\t\t\"You can continue this example and select to clean up resources, or manually remove\\n\"+\t\t\t\t\"the user from your user pool and try again.\", user.UserName)\t\t\trunner.resources.userAccessTokens = append(runner.resources.userAccessTokens, *authResult.AccessToken)\t\t\tsignedIn = true\t\t}\t}\tlog.Println(strings.Repeat(\"-\", 88))\treturn resetRequired != nil, user}// ResetPassword starts a password recovery flow.func (runner *MigrateUser) ResetPassword(ctx context.Context, clientId string, user actions.User) {\twantCode := runner.questioner.AskBool(fmt.Sprintf(\"In order to migrate the user to Cognito, you must be able to receive a confirmation\\n\"+\t\t\"code by email at %v. Do you want to send a code (y/n)?\", user.UserEmail), \"y\")\tif !wantCode {\t\tlog.Println(\"To complete this example and successfully migrate a user to Cognito, you must enter an email\\n\" +\t\t\t\"you own that can receive a confirmation code.\")\t\treturn\t}\tcodeDelivery, err := runner.cognitoActor.ForgotPassword(ctx, clientId, user.UserName)\tif err != nil {\t\tpanic(err)\t}\tlog.Printf(\"\\nA confirmation code has been sent to %v.\", *codeDelivery.Destination)\tcode := runner.questioner.Ask(\"Check your email and enter it here:\")\tconfirmed := false\tpassword := runner.questioner.AskPassword(\"\\nEnter a password that has at least eight characters, uppercase, lowercase, numbers and symbols.\\n\"+\t\t\"(the password will not display as you type):\", 8)\tfor !confirmed {\t\tlog.Printf(\"\\nConfirming password reset for user '%v'.\\n\", user.UserName)\t\terr = runner.cognitoActor.ConfirmForgotPassword(ctx, clientId, code, user.UserName, password)\t\tif err != nil {\t\t\tvar invalidPassword *types.InvalidPasswordException\t\t\tif errors.As(err, &invalidPassword) {\t\t\t\tpassword = runner.questioner.AskPassword(\"\\nEnter another password:\", 8)\t\t\t} else {\t\t\t\tpanic(err)\t\t\t}\t\t} else {\t\t\tconfirmed = true\t\t}\t}\tlog.Printf(\"User '%v' successfully confirmed and migrated.\\n\", user.UserName)\tlog.Println(\"Signing in with your username and password...\")\tauthResult, err := runner.cognitoActor.SignIn(ctx, clientId, user.UserName, password)\tif err != nil {\t\tpanic(err)\t}\tlog.Printf(\"Successfully signed in. Your access token starts with: %v...\\n\", (*authResult.AccessToken)[:10])\trunner.resources.userAccessTokens = append(runner.resources.userAccessTokens, *authResult.AccessToken)\tlog.Println(strings.Repeat(\"-\", 88))}// Run runs the scenario.func (runner *MigrateUser) Run(ctx context.Context, stackName string) {\tdefer func() {\t\tif r := recover(); r != nil {\t\t\tlog.Println(\"Something went wrong with the demo.\")\t\t\trunner.resources.Cleanup(ctx)\t\t}\t}()\tlog.Println(strings.Repeat(\"-\", 88))\tlog.Printf(\"Welcome\\n\")\tlog.Println(strings.Repeat(\"-\", 88))\tstackOutputs, err := runner.helper.GetStackOutputs(ctx, stackName)\tif err != nil {\t\tpanic(err)\t}\trunner.resources.userPoolId = stackOutputs[\"UserPoolId\"]\trunner.AddMigrateUserTrigger(ctx, stackOutputs[\"UserPoolId\"], stackOutputs[\"MigrateUserFunctionArn\"])\trunner.resources.triggers = append(runner.resources.triggers, actions.UserMigration)\tresetNeeded, user := runner.SignInUser(ctx, stackOutputs[\"TableName\"], stackOutputs[\"UserPoolClientId\"])\tif resetNeeded {\t\trunner.helper.ListRecentLogEvents(ctx, stackOutputs[\"MigrateUserFunction\"])\t\trunner.ResetPassword(ctx, stackOutputs[\"UserPoolClientId\"], user)\t}\trunner.resources.Cleanup(ctx)\tlog.Println(strings.Repeat(\"-\", 88))\tlog.Println(\"Thanks for watching!\")\tlog.Println(strings.Repeat(\"-\", 88))}Handle the MigrateUser trigger with a Lambda function.import (\t\"context\"\t\"log\"\t\"os\"\t\"github.com/aws/aws-lambda-go/events\"\t\"github.com/aws/aws-lambda-go/lambda\"\t\"github.com/aws/aws-sdk-go-v2/aws\"\t\"github.com/aws/aws-sdk-go-v2/config\"\t\"github.com/aws/aws-sdk-go-v2/feature/dynamodb/attributevalue\"\t\"github.com/aws/aws-sdk-go-v2/feature/dynamodb/expression\"\t\"github.com/aws/aws-sdk-go-v2/service/dynamodb\")const TABLE_NAME = \"TABLE_NAME\"// UserInfo defines structured user data that can be marshalled to a DynamoDB format.type UserInfo struct {\tUserName  string `dynamodbav:\"UserName\"`\tUserEmail string `dynamodbav:\"UserEmail\"`}type handler struct {\tdynamoClient *dynamodb.Client}// HandleRequest handles the MigrateUser event by looking up a user in an Amazon DynamoDB table and// specifying whether they should be migrated to the user pool.func (h *handler) HandleRequest(ctx context.Context, event events.CognitoEventUserPoolsMigrateUser) (events.CognitoEventUserPoolsMigrateUser, error) {\tlog.Printf(\"Received migrate trigger from %v for user '%v'\", event.TriggerSource, event.UserName)\tif event.TriggerSource != \"UserMigration_Authentication\" {\t\treturn event, nil\t}\ttableName := os.Getenv(TABLE_NAME)\tuser := UserInfo{\t\tUserName: event.UserName,\t}\tlog.Printf(\"Looking up user '%v' in table %v.\\n\", user.UserName, tableName)\tfilterEx := expression.Name(\"UserName\").Equal(expression.Value(user.UserName))\texpr, err := expression.NewBuilder().WithFilter(filterEx).Build()\tif err != nil {\t\tlog.Printf(\"Error building expression to query for user '%v'.\\n\", user.UserName)\t\treturn event, err\t}\toutput, err := h.dynamoClient.Scan(ctx, &dynamodb.ScanInput{\t\tTableName:                 aws.String(tableName),\t\tFilterExpression:          expr.Filter(),\t\tExpressionAttributeNames:  expr.Names(),\t\tExpressionAttributeValues: expr.Values(),\t})\tif err != nil {\t\tlog.Printf(\"Error looking up user '%v'.\\n\", user.UserName)\t\treturn event, err\t}\tif len(output.Items) == 0 {\t\tlog.Printf(\"User '%v' not found, not migrating user.\\n\", user.UserName)\t\treturn event, err\t}\tvar users []UserInfo\terr = attributevalue.UnmarshalListOfMaps(output.Items, &users)\tif err != nil {\t\tlog.Printf(\"Couldn't unmarshal DynamoDB items. Here's why: %v\\n\", err)\t\treturn event, err\t}\tuser = users[0]\tlog.Printf(\"UserName '%v' found with email %v. User is migrated and must reset password.\\n\", user.UserName, user.UserEmail)\tevent.CognitoEventUserPoolsMigrateUserResponse.UserAttributes = map[string]string{\t\t\"email\":          user.UserEmail,\t\t\"email_verified\": \"true\", // email_verified is required for the forgot password flow.\t}\tevent.CognitoEventUserPoolsMigrateUserResponse.FinalUserStatus = \"RESET_REQUIRED\"\tevent.CognitoEventUserPoolsMigrateUserResponse.MessageAction = \"SUPPRESS\"\treturn event, err}func main() {\tctx := context.Background()\tsdkConfig, err := config.LoadDefaultConfig(ctx)\tif err != nil {\t\tlog.Panicln(err)\t}\th := handler{\t\tdynamoClient: dynamodb.NewFromConfig(sdkConfig),\t}\tlambda.Start(h.HandleRequest)}Create a struct that performs common tasks.import (\t\"context\"\t\"log\"\t\"strings\"\t\"time\"\t\"user_pools_and_lambda_triggers/actions\"\t\"github.com/aws/aws-sdk-go-v2/aws\"\t\"github.com/aws/aws-sdk-go-v2/service/cloudformation\"\t\"github.com/aws/aws-sdk-go-v2/service/cloudwatchlogs\"\t\"github.com/aws/aws-sdk-go-v2/service/dynamodb\"\t\"github.com/awsdocs/aws-doc-sdk-examples/gov2/demotools\")// IScenarioHelper defines common functions used by the workflows in this example.type IScenarioHelper interface {\tPause(secs int)\tGetStackOutputs(ctx context.Context, stackName string) (actions.StackOutputs, error)\tPopulateUserTable(ctx context.Context, tableName string)\tGetKnownUsers(ctx context.Context, tableName string) (actions.UserList, error)\tAddKnownUser(ctx context.Context, tableName string, user actions.User)\tListRecentLogEvents(ctx context.Context, functionName string)}// ScenarioHelper contains AWS wrapper structs used by the workflows in this example.type ScenarioHelper struct {\tquestioner  demotools.IQuestioner\tdynamoActor *actions.DynamoActions\tcfnActor    *actions.CloudFormationActions\tcwlActor    *actions.CloudWatchLogsActions\tisTestRun   bool}// NewScenarioHelper constructs a new scenario helper.func NewScenarioHelper(sdkConfig aws.Config, questioner demotools.IQuestioner) ScenarioHelper {\tscenario := ScenarioHelper{\t\tquestioner:  questioner,\t\tdynamoActor: &actions.DynamoActions{DynamoClient: dynamodb.NewFromConfig(sdkConfig)},\t\tcfnActor:    &actions.CloudFormationActions{CfnClient: cloudformation.NewFromConfig(sdkConfig)},\t\tcwlActor:    &actions.CloudWatchLogsActions{CwlClient: cloudwatchlogs.NewFromConfig(sdkConfig)},\t}\treturn scenario}// Pause waits for the specified number of seconds.func (helper ScenarioHelper) Pause(secs int) {\tif !helper.isTestRun {\t\ttime.Sleep(time.Duration(secs) * time.Second)\t}}// GetStackOutputs gets the outputs from the specified CloudFormation stack in a structured format.func (helper ScenarioHelper) GetStackOutputs(ctx context.Context, stackName string) (actions.StackOutputs, error) {\treturn helper.cfnActor.GetOutputs(ctx, stackName), nil}// PopulateUserTable fills the known user table with example data.func (helper ScenarioHelper) PopulateUserTable(ctx context.Context, tableName string) {\tlog.Printf(\"First, let's add some users to the DynamoDB %v table we'll use for this example.\\n\", tableName)\terr := helper.dynamoActor.PopulateTable(ctx, tableName)\tif err != nil {\t\tpanic(err)\t}}// GetKnownUsers gets the users from the known users table in a structured format.func (helper ScenarioHelper) GetKnownUsers(ctx context.Context, tableName string) (actions.UserList, error) {\tknownUsers, err := helper.dynamoActor.Scan(ctx, tableName)\tif err != nil {\t\tlog.Printf(\"Couldn't get known users from table %v. Here's why: %v\\n\", tableName, err)\t}\treturn knownUsers, err}// AddKnownUser adds a user to the known users table.func (helper ScenarioHelper) AddKnownUser(ctx context.Context, tableName string, user actions.User) {\tlog.Printf(\"Adding user '%v' with email '%v' to the DynamoDB known users table...\\n\",\t\tuser.UserName, user.UserEmail)\terr := helper.dynamoActor.AddUser(ctx, tableName, user)\tif err != nil {\t\tpanic(err)\t}}// ListRecentLogEvents gets the most recent log stream and events for the specified Lambda function and displays them.func (helper ScenarioHelper) ListRecentLogEvents(ctx context.Context, functionName string) {\tlog.Println(\"Waiting a few seconds to let Lambda write to CloudWatch Logs...\")\thelper.Pause(10)\tlog.Println(\"Okay, let's check the logs to find what's happened recently with your Lambda function.\")\tlogStream, err := helper.cwlActor.GetLatestLogStream(ctx, functionName)\tif err != nil {\t\tpanic(err)\t}\tlog.Printf(\"Getting some recent events from log stream %v\\n\", *logStream.LogStreamName)\tevents, err := helper.cwlActor.GetLogEvents(ctx, functionName, *logStream.LogStreamName, 10)\tif err != nil {\t\tpanic(err)\t}\tfor _, event := range events {\t\tlog.Printf(\"\\t%v\", *event.Message)\t}\tlog.Println(strings.Repeat(\"-\", 88))}Create a struct that wraps Amazon Cognito actions.import (\t\"context\"\t\"errors\"\t\"log\"\t\"github.com/aws/aws-sdk-go-v2/aws\"\t\"github.com/aws/aws-sdk-go-v2/service/cognitoidentityprovider\"\t\"github.com/aws/aws-sdk-go-v2/service/cognitoidentityprovider/types\")type CognitoActions struct {\tCognitoClient *cognitoidentityprovider.Client}// Trigger and TriggerInfo define typed data for updating an Amazon Cognito trigger.type Trigger intconst (\tPreSignUp Trigger = iota\tUserMigration\tPostAuthentication)type TriggerInfo struct {\tTrigger    Trigger\tHandlerArn *string}// UpdateTriggers adds or removes Lambda triggers for a user pool. When a trigger is specified with a `nil` value,// it is removed from the user pool.func (actor CognitoActions) UpdateTriggers(ctx context.Context, userPoolId string, triggers ...TriggerInfo) error {\toutput, err := actor.CognitoClient.DescribeUserPool(ctx, &cognitoidentityprovider.DescribeUserPoolInput{\t\tUserPoolId: aws.String(userPoolId),\t})\tif err != nil {\t\tlog.Printf(\"Couldn't get info about user pool %v. Here's why: %v\\n\", userPoolId, err)\t\treturn err\t}\tlambdaConfig := output.UserPool.LambdaConfig\tfor _, trigger := range triggers {\t\tswitch trigger.Trigger {\t\tcase PreSignUp:\t\t\tlambdaConfig.PreSignUp = trigger.HandlerArn\t\tcase UserMigration:\t\t\tlambdaConfig.UserMigration = trigger.HandlerArn\t\tcase PostAuthentication:\t\t\tlambdaConfig.PostAuthentication = trigger.HandlerArn\t\t}\t}\t_, err = actor.CognitoClient.UpdateUserPool(ctx, &cognitoidentityprovider.UpdateUserPoolInput{\t\tUserPoolId:   aws.String(userPoolId),\t\tLambdaConfig: lambdaConfig,\t})\tif err != nil {\t\tlog.Printf(\"Couldn't update user pool %v. Here's why: %v\\n\", userPoolId, err)\t}\treturn err}// SignUp signs up a user with Amazon Cognito.func (actor CognitoActions) SignUp(ctx context.Context, clientId string, userName string, password string, userEmail string) (bool, error) {\tconfirmed := false\toutput, err := actor.CognitoClient.SignUp(ctx, &cognitoidentityprovider.SignUpInput{\t\tClientId: aws.String(clientId),\t\tPassword: aws.String(password),\t\tUsername: aws.String(userName),\t\tUserAttributes: []types.AttributeType{\t\t\t{Name: aws.String(\"email\"), Value: aws.String(userEmail)},\t\t},\t})\tif err != nil {\t\tvar invalidPassword *types.InvalidPasswordException\t\tif errors.As(err, &invalidPassword) {\t\t\tlog.Println(*invalidPassword.Message)\t\t} else {\t\t\tlog.Printf(\"Couldn't sign up user %v. Here's why: %v\\n\", userName, err)\t\t}\t} else {\t\tconfirmed = output.UserConfirmed\t}\treturn confirmed, err}// SignIn signs in a user to Amazon Cognito using a username and password authentication flow.func (actor CognitoActions) SignIn(ctx context.Context, clientId string, userName string, password string) (*types.AuthenticationResultType, error) {\tvar authResult *types.AuthenticationResultType\toutput, err := actor.CognitoClient.InitiateAuth(ctx, &cognitoidentityprovider.InitiateAuthInput{\t\tAuthFlow:       \"USER_PASSWORD_AUTH\",\t\tClientId:       aws.String(clientId),\t\tAuthParameters: map[string]string{\"USERNAME\": userName, \"PASSWORD\": password},\t})\tif err != nil {\t\tvar resetRequired *types.PasswordResetRequiredException\t\tif errors.As(err, &resetRequired) {\t\t\tlog.Println(*resetRequired.Message)\t\t} else {\t\t\tlog.Printf(\"Couldn't sign in user %v. Here's why: %v\\n\", userName, err)\t\t}\t} else {\t\tauthResult = output.AuthenticationResult\t}\treturn authResult, err}// ForgotPassword starts a password recovery flow for a user. This flow typically sends a confirmation code// to the user's configured notification destination, such as email.func (actor CognitoActions) ForgotPassword(ctx context.Context, clientId string, userName string) (*types.CodeDeliveryDetailsType, error) {\toutput, err := actor.CognitoClient.ForgotPassword(ctx, &cognitoidentityprovider.ForgotPasswordInput{\t\tClientId: aws.String(clientId),\t\tUsername: aws.String(userName),\t})\tif err != nil {\t\tlog.Printf(\"Couldn't start password reset for user '%v'. Here;s why: %v\\n\", userName, err)\t}\treturn output.CodeDeliveryDetails, err}// ConfirmForgotPassword confirms a user with a confirmation code and a new password.func (actor CognitoActions) ConfirmForgotPassword(ctx context.Context, clientId string, code string, userName string, password string) error {\t_, err := actor.CognitoClient.ConfirmForgotPassword(ctx, &cognitoidentityprovider.ConfirmForgotPasswordInput{\t\tClientId:         aws.String(clientId),\t\tConfirmationCode: aws.String(code),\t\tPassword:         aws.String(password),\t\tUsername:         aws.String(userName),\t})\tif err != nil {\t\tvar invalidPassword *types.InvalidPasswordException\t\tif errors.As(err, &invalidPassword) {\t\t\tlog.Println(*invalidPassword.Message)\t\t} else {\t\t\tlog.Printf(\"Couldn't confirm user %v. Here's why: %v\", userName, err)\t\t}\t}\treturn err}// DeleteUser removes a user from the user pool.func (actor CognitoActions) DeleteUser(ctx context.Context, userAccessToken string) error {\t_, err := actor.CognitoClient.DeleteUser(ctx, &cognitoidentityprovider.DeleteUserInput{\t\tAccessToken: aws.String(userAccessToken),\t})\tif err != nil {\t\tlog.Printf(\"Couldn't delete user. Here's why: %v\\n\", err)\t}\treturn err}// AdminCreateUser uses administrator credentials to add a user to a user pool. This method leaves the user// in a state that requires they enter a new password next time they sign in.func (actor CognitoActions) AdminCreateUser(ctx context.Context, userPoolId string, userName string, userEmail string) error {\t_, err := actor.CognitoClient.AdminCreateUser(ctx, &cognitoidentityprovider.AdminCreateUserInput{\t\tUserPoolId:     aws.String(userPoolId),\t\tUsername:       aws.String(userName),\t\tMessageAction:  types.MessageActionTypeSuppress,\t\tUserAttributes: []types.AttributeType{{Name: aws.String(\"email\"), Value: aws.String(userEmail)}},\t})\tif err != nil {\t\tvar userExists *types.UsernameExistsException\t\tif errors.As(err, &userExists) {\t\t\tlog.Printf(\"User %v already exists in the user pool.\", userName)\t\t\terr = nil\t\t} else {\t\t\tlog.Printf(\"Couldn't create user %v. Here's why: %v\\n\", userName, err)\t\t}\t}\treturn err}// AdminSetUserPassword uses administrator credentials to set a password for a user without requiring a// temporary password.func (actor CognitoActions) AdminSetUserPassword(ctx context.Context, userPoolId string, userName string, password string) error {\t_, err := actor.CognitoClient.AdminSetUserPassword(ctx, &cognitoidentityprovider.AdminSetUserPasswordInput{\t\tPassword:   aws.String(password),\t\tUserPoolId: aws.String(userPoolId),\t\tUsername:   aws.String(userName),\t\tPermanent:  true,\t})\tif err != nil {\t\tvar invalidPassword *types.InvalidPasswordException\t\tif errors.As(err, &invalidPassword) {\t\t\tlog.Println(*invalidPassword.Message)\t\t} else {\t\t\tlog.Printf(\"Couldn't set password for user %v. Here's why: %v\\n\", userName, err)\t\t}\t}\treturn err}Create a struct that wraps DynamoDB actions.import (\t\"context\"\t\"fmt\"\t\"log\"\t\"github.com/aws/aws-sdk-go-v2/aws\"\t\"github.com/aws/aws-sdk-go-v2/feature/dynamodb/attributevalue\"\t\"github.com/aws/aws-sdk-go-v2/service/dynamodb\"\t\"github.com/aws/aws-sdk-go-v2/service/dynamodb/types\")// DynamoActions encapsulates the Amazon Simple Notification Service (Amazon SNS) actions// used in the examples.type DynamoActions struct {\tDynamoClient *dynamodb.Client}// User defines structured user data.type User struct {\tUserName  string\tUserEmail string\tLastLogin *LoginInfo `dynamodbav:\",omitempty\"`}// LoginInfo defines structured custom login data.type LoginInfo struct {\tUserPoolId string\tClientId   string\tTime       string}// UserList defines a list of users.type UserList struct {\tUsers []User}// UserNameList returns the usernames contained in a UserList as a list of strings.func (users *UserList) UserNameList() []string {\tnames := make([]string, len(users.Users))\tfor i := 0; i < len(users.Users); i++ {\t\tnames[i] = users.Users[i].UserName\t}\treturn names}// PopulateTable adds a set of test users to the table.func (actor DynamoActions) PopulateTable(ctx context.Context, tableName string) error {\tvar err error\tvar item map[string]types.AttributeValue\tvar writeReqs []types.WriteRequest\tfor i := 1; i < 4; i++ {\t\titem, err = attributevalue.MarshalMap(User{UserName: fmt.Sprintf(\"test_user_%v\", i), UserEmail: fmt.Sprintf(\"test_email_%v@example.com\", i)})\t\tif err != nil {\t\t\tlog.Printf(\"Couldn't marshall user into DynamoDB format. Here's why: %v\\n\", err)\t\t\treturn err\t\t}\t\twriteReqs = append(writeReqs, types.WriteRequest{PutRequest: &types.PutRequest{Item: item}})\t}\t_, err = actor.DynamoClient.BatchWriteItem(ctx, &dynamodb.BatchWriteItemInput{\t\tRequestItems: map[string][]types.WriteRequest{tableName: writeReqs},\t})\tif err != nil {\t\tlog.Printf(\"Couldn't populate table %v with users. Here's why: %v\\n\", tableName, err)\t}\treturn err}// Scan scans the table for all items.func (actor DynamoActions) Scan(ctx context.Context, tableName string) (UserList, error) {\tvar userList UserList\toutput, err := actor.DynamoClient.Scan(ctx, &dynamodb.ScanInput{\t\tTableName: aws.String(tableName),\t})\tif err != nil {\t\tlog.Printf(\"Couldn't scan table %v for items. Here's why: %v\\n\", tableName, err)\t} else {\t\terr = attributevalue.UnmarshalListOfMaps(output.Items, &userList.Users)\t\tif err != nil {\t\t\tlog.Printf(\"Couldn't unmarshal items into users. Here's why: %v\\n\", err)\t\t}\t}\treturn userList, err}// AddUser adds a user item to a table.func (actor DynamoActions) AddUser(ctx context.Context, tableName string, user User) error {\tuserItem, err := attributevalue.MarshalMap(user)\tif err != nil {\t\tlog.Printf(\"Couldn't marshall user to item. Here's why: %v\\n\", err)\t}\t_, err = actor.DynamoClient.PutItem(ctx, &dynamodb.PutItemInput{\t\tItem:      userItem,\t\tTableName: aws.String(tableName),\t})\tif err != nil {\t\tlog.Printf(\"Couldn't put item in table %v. Here's why: %v\", tableName, err)\t}\treturn err}Create a struct that wraps CloudWatch Logs actions.import (\t\"context\"\t\"fmt\"\t\"log\"\t\"github.com/aws/aws-sdk-go-v2/aws\"\t\"github.com/aws/aws-sdk-go-v2/service/cloudwatchlogs\"\t\"github.com/aws/aws-sdk-go-v2/service/cloudwatchlogs/types\")type CloudWatchLogsActions struct {\tCwlClient *cloudwatchlogs.Client}// GetLatestLogStream gets the most recent log stream for a Lambda function.func (actor CloudWatchLogsActions) GetLatestLogStream(ctx context.Context, functionName string) (types.LogStream, error) {\tvar logStream types.LogStream\tlogGroupName := fmt.Sprintf(\"/aws/lambda/%s\", functionName)\toutput, err := actor.CwlClient.DescribeLogStreams(ctx, &cloudwatchlogs.DescribeLogStreamsInput{\t\tDescending:   aws.Bool(true),\t\tLimit:        aws.Int32(1),\t\tLogGroupName: aws.String(logGroupName),\t\tOrderBy:      types.OrderByLastEventTime,\t})\tif err != nil {\t\tlog.Printf(\"Couldn't get log streams for log group %v. Here's why: %v\\n\", logGroupName, err)\t} else {\t\tlogStream = output.LogStreams[0]\t}\treturn logStream, err}// GetLogEvents gets the most recent eventCount events from the specified log stream.func (actor CloudWatchLogsActions) GetLogEvents(ctx context.Context, functionName string, logStreamName string, eventCount int32) (\t[]types.OutputLogEvent, error) {\tvar events []types.OutputLogEvent\tlogGroupName := fmt.Sprintf(\"/aws/lambda/%s\", functionName)\toutput, err := actor.CwlClient.GetLogEvents(ctx, &cloudwatchlogs.GetLogEventsInput{\t\tLogStreamName: aws.String(logStreamName),\t\tLimit:         aws.Int32(eventCount),\t\tLogGroupName:  aws.String(logGroupName),\t})\tif err != nil {\t\tlog.Printf(\"Couldn't get log event for log stream %v. Here's why: %v\\n\", logStreamName, err)\t} else {\t\tevents = output.Events\t}\treturn events, err}Create a struct that wraps AWS CloudFormation actions.import (\t\"context\"\t\"log\"\t\"github.com/aws/aws-sdk-go-v2/aws\"\t\"github.com/aws/aws-sdk-go-v2/service/cloudformation\")// StackOutputs defines a map of outputs from a specific stack.type StackOutputs map[string]stringtype CloudFormationActions struct {\tCfnClient *cloudformation.Client}// GetOutputs gets the outputs from a CloudFormation stack and puts them into a structured format.func (actor CloudFormationActions) GetOutputs(ctx context.Context, stackName string) StackOutputs {\toutput, err := actor.CfnClient.DescribeStacks(ctx, &cloudformation.DescribeStacksInput{\t\tStackName: aws.String(stackName),\t})\tif err != nil || len(output.Stacks) == 0 {\t\tlog.Panicf(\"Couldn't find a CloudFormation stack named %v. Here's why: %v\\n\", stackName, err)\t}\tstackOutputs := StackOutputs{}\tfor _, out := range output.Stacks[0].Outputs {\t\tstackOutputs[*out.OutputKey] = *out.OutputValue\t}\treturn stackOutputs}Clean up resources.import (\t\"context\"\t\"log\"\t\"user_pools_and_lambda_triggers/actions\"\t\"github.com/awsdocs/aws-doc-sdk-examples/gov2/demotools\")// Resources keeps track of AWS resources created during an example and handles// cleanup when the example finishes.type Resources struct {\tuserPoolId       string\tuserAccessTokens []string\ttriggers         []actions.Trigger\tcognitoActor *actions.CognitoActions\tquestioner   demotools.IQuestioner}func (resources *Resources) init(cognitoActor *actions.CognitoActions, questioner demotools.IQuestioner) {\tresources.userAccessTokens = []string{}\tresources.triggers = []actions.Trigger{}\tresources.cognitoActor = cognitoActor\tresources.questioner = questioner}// Cleanup deletes all AWS resources created during an example.func (resources *Resources) Cleanup(ctx context.Context) {\tdefer func() {\t\tif r := recover(); r != nil {\t\t\tlog.Printf(\"Something went wrong during cleanup.\\n%v\\n\", r)\t\t\tlog.Println(\"Use the AWS Management Console to remove any remaining resources \\n\" +\t\t\t\t\"that were created for this scenario.\")\t\t}\t}()\twantDelete := resources.questioner.AskBool(\"Do you want to remove all of the AWS resources that were created \"+\t\t\"during this demo (y/n)?\", \"y\")\tif wantDelete {\t\tfor _, accessToken := range resources.userAccessTokens {\t\t\terr := resources.cognitoActor.DeleteUser(ctx, accessToken)\t\t\tif err != nil {\t\t\t\tlog.Println(\"Couldn't delete user during cleanup.\")\t\t\t\tpanic(err)\t\t\t}\t\t\tlog.Println(\"Deleted user.\")\t\t}\t\ttriggerList := make([]actions.TriggerInfo, len(resources.triggers))\t\tfor i := 0; i < len(resources.triggers); i++ {\t\t\ttriggerList[i] = actions.TriggerInfo{Trigger: resources.triggers[i], HandlerArn: nil}\t\t}\t\terr := resources.cognitoActor.UpdateTriggers(ctx, resources.userPoolId, triggerList...)\t\tif err != nil {\t\t\tlog.Println(\"Couldn't update Cognito triggers during cleanup.\")\t\t\tpanic(err)\t\t}\t\tlog.Println(\"Removed Cognito triggers from user pool.\")\t} else {\t\tlog.Println(\"Be sure to remove resources when you're done with them to avoid unexpected charges!\")\t}}For API details, see the following topics in AWS SDK for Go API Reference.ConfirmForgotPasswordDeleteUserForgotPasswordInitiateAuthSignUpUpdateUserPool",
                            "For a complete list of AWS SDK developer guides and code examples, see    Using Lambda with an AWS SDK.    This topic also includes information about getting started and details about previous SDK versions."
                        ]
                    },
                    {
                        "title": "Create a REST API to track COVID-19 data",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_cross_ApiGatewayDataTracker_section.html",
                        "sections": [
                            "",
                            "The following code example shows how to create a REST API that simulates a system to track daily cases of COVID-19 in the United States, using fictional data.",
                            "PythonSDK for Python (Boto3)        Shows how to use AWS Chalice with the AWS SDK for Python (Boto3) to        create a serverless REST API that uses Amazon API Gateway, AWS Lambda, and        Amazon DynamoDB. The REST API simulates a system that tracks daily cases        of COVID-19 in the United States, using fictional data. Learn how to:    Use AWS Chalice to define routes in Lambda functions that        are called to handle REST requests that come through API Gateway.Use Lambda functions to retrieve and store data in a DynamoDB        table to serve REST requests.Define table structure and security role resources in an AWS CloudFormation template.Use AWS Chalice and CloudFormation to package and deploy all necessary resources.Use CloudFormation to clean up all created resources.        For complete source code and instructions on how to set up and run, see the full example on        GitHub.    Services used in this exampleAPI GatewayAWS CloudFormationDynamoDBLambdaanchorPythonSDK for Python (Boto3)        Shows how to use AWS Chalice with the AWS SDK for Python (Boto3) to        create a serverless REST API that uses Amazon API Gateway, AWS Lambda, and        Amazon DynamoDB. The REST API simulates a system that tracks daily cases        of COVID-19 in the United States, using fictional data. Learn how to:    Use AWS Chalice to define routes in Lambda functions that        are called to handle REST requests that come through API Gateway.Use Lambda functions to retrieve and store data in a DynamoDB        table to serve REST requests.Define table structure and security role resources in an AWS CloudFormation template.Use AWS Chalice and CloudFormation to package and deploy all necessary resources.Use CloudFormation to clean up all created resources.        For complete source code and instructions on how to set up and run, see the full example on        GitHub.    Services used in this exampleAPI GatewayAWS CloudFormationDynamoDBLambda",
                            "For a complete list of AWS SDK developer guides and code examples, see    Using Lambda with an AWS SDK.    This topic also includes information about getting started and details about previous SDK versions."
                        ]
                    },
                    {
                        "title": "Create a lending library REST API",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_cross_AuroraRestLendingLibrary_section.html",
                        "sections": [
                            "",
                            "The following code example shows how to create a lending library where patrons can borrow and return books by using a REST API backed by an Amazon Aurora database.",
                            "PythonSDK for Python (Boto3)        Shows how to use the AWS SDK for Python (Boto3) with the Amazon Relational Database Service (Amazon RDS) API and AWS Chalice to create a REST API        backed by an Amazon Aurora database. The web service is fully serverless and represents        a simple lending library where patrons can borrow and return books. Learn how to:    Create and manage a serverless Aurora database cluster.Use AWS Secrets Manager to manage database credentials.Implement a data storage layer that uses Amazon RDS to move data into            and out of the database.Use AWS Chalice to deploy a serverless REST API to Amazon API Gateway and AWS Lambda.Use the Requests package to send requests to the web service.        For complete source code and instructions on how to set up and run, see the full example on        GitHub.    Services used in this exampleAPI GatewayAuroraLambdaSecrets ManageranchorPythonSDK for Python (Boto3)        Shows how to use the AWS SDK for Python (Boto3) with the Amazon Relational Database Service (Amazon RDS) API and AWS Chalice to create a REST API        backed by an Amazon Aurora database. The web service is fully serverless and represents        a simple lending library where patrons can borrow and return books. Learn how to:    Create and manage a serverless Aurora database cluster.Use AWS Secrets Manager to manage database credentials.Implement a data storage layer that uses Amazon RDS to move data into            and out of the database.Use AWS Chalice to deploy a serverless REST API to Amazon API Gateway and AWS Lambda.Use the Requests package to send requests to the web service.        For complete source code and instructions on how to set up and run, see the full example on        GitHub.    Services used in this exampleAPI GatewayAuroraLambdaSecrets Manager",
                            "For a complete list of AWS SDK developer guides and code examples, see    Using Lambda with an AWS SDK.    This topic also includes information about getting started and details about previous SDK versions."
                        ]
                    },
                    {
                        "title": "Create a messenger application",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_cross_StepFunctionsMessenger_section.html",
                        "sections": [
                            "",
                            "The following code example shows how to create an AWS Step Functions messenger application that retrieves message records from a database table.",
                            "PythonSDK for Python (Boto3)        Shows how to use the AWS SDK for Python (Boto3) with AWS Step Functions to create a messenger application that        retrieves message records from an Amazon DynamoDB table and sends them with Amazon Simple Queue Service (Amazon SQS).        The state machine integrates with an AWS Lambda function to scan the database for unsent messages.    Create a state machine that retrieves and updates message records from an Amazon DynamoDB table.Update the state machine definition to also send messages to Amazon Simple Queue Service (Amazon SQS).Start and stop state machine runs.Connect to Lambda, DynamoDB, and Amazon SQS from a state machine by using service integrations.        For complete source code and instructions on how to set up and run, see the full example on        GitHub.    Services used in this exampleDynamoDBLambdaAmazon SQSStep FunctionsanchorPythonSDK for Python (Boto3)        Shows how to use the AWS SDK for Python (Boto3) with AWS Step Functions to create a messenger application that        retrieves message records from an Amazon DynamoDB table and sends them with Amazon Simple Queue Service (Amazon SQS).        The state machine integrates with an AWS Lambda function to scan the database for unsent messages.    Create a state machine that retrieves and updates message records from an Amazon DynamoDB table.Update the state machine definition to also send messages to Amazon Simple Queue Service (Amazon SQS).Start and stop state machine runs.Connect to Lambda, DynamoDB, and Amazon SQS from a state machine by using service integrations.        For complete source code and instructions on how to set up and run, see the full example on        GitHub.    Services used in this exampleDynamoDBLambdaAmazon SQSStep Functions",
                            "For a complete list of AWS SDK developer guides and code examples, see    Using Lambda with an AWS SDK.    This topic also includes information about getting started and details about previous SDK versions."
                        ]
                    },
                    {
                        "title": "Create a serverless application to manage photos",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_cross_PAM_section.html",
                        "sections": [
                            "",
                            "The following code examples show how to create a serverless application that lets users manage photos using labels.",
                            ".NETAWS SDK for .NET        Shows how to develop a photo asset management application that detects labels in images        using Amazon Rekognition and stores them for later retrieval.    For complete source code and instructions on how to set up and run, see the full example        on         GitHub.For a deep dive into the origin of this example see the post on AWS Community.Services used in this exampleAPI GatewayDynamoDBLambdaAmazon RekognitionAmazon S3Amazon SNSC++SDK for C++        Shows how to develop a photo asset management application that detects labels in images        using Amazon Rekognition and stores them for later retrieval.    For complete source code and instructions on how to set up and run, see the full example        on         GitHub.For a deep dive into the origin of this example see the post on AWS Community.Services used in this exampleAPI GatewayDynamoDBLambdaAmazon RekognitionAmazon S3Amazon SNSJavaSDK for Java 2.x        Shows how to develop a photo asset management application that detects labels in images        using Amazon Rekognition and stores them for later retrieval.    For complete source code and instructions on how to set up and run, see the full example        on         GitHub.For a deep dive into the origin of this example see the post on AWS Community.Services used in this exampleAPI GatewayDynamoDBLambdaAmazon RekognitionAmazon S3Amazon SNSJavaScriptSDK for JavaScript (v3)        Shows how to develop a photo asset management application that detects labels in images        using Amazon Rekognition and stores them for later retrieval.    For complete source code and instructions on how to set up and run, see the full example        on         GitHub.For a deep dive into the origin of this example see the post on AWS Community.Services used in this exampleAPI GatewayDynamoDBLambdaAmazon RekognitionAmazon S3Amazon SNSKotlinSDK for Kotlin        Shows how to develop a photo asset management application that detects labels in images        using Amazon Rekognition and stores them for later retrieval.    For complete source code and instructions on how to set up and run, see the full example        on         GitHub.For a deep dive into the origin of this example see the post on AWS Community.Services used in this exampleAPI GatewayDynamoDBLambdaAmazon RekognitionAmazon S3Amazon SNSPHPSDK for PHP        Shows how to develop a photo asset management application that detects labels in images        using Amazon Rekognition and stores them for later retrieval.    For complete source code and instructions on how to set up and run, see the full example        on         GitHub.For a deep dive into the origin of this example see the post on AWS Community.Services used in this exampleAPI GatewayDynamoDBLambdaAmazon RekognitionAmazon S3Amazon SNSRustSDK for Rust        Shows how to develop a photo asset management application that detects labels in images        using Amazon Rekognition and stores them for later retrieval.    For complete source code and instructions on how to set up and run, see the full example        on         GitHub.For a deep dive into the origin of this example see the post on AWS Community.Services used in this exampleAPI GatewayDynamoDBLambdaAmazon RekognitionAmazon S3Amazon SNSanchoranchoranchoranchoranchoranchoranchor.NETC++JavaJavaScriptKotlinPHPRustAWS SDK for .NET        Shows how to develop a photo asset management application that detects labels in images        using Amazon Rekognition and stores them for later retrieval.    For complete source code and instructions on how to set up and run, see the full example        on         GitHub.For a deep dive into the origin of this example see the post on AWS Community.Services used in this exampleAPI GatewayDynamoDBLambdaAmazon RekognitionAmazon S3Amazon SNS",
                            "For a complete list of AWS SDK developer guides and code examples, see    Using Lambda with an AWS SDK.    This topic also includes information about getting started and details about previous SDK versions."
                        ]
                    },
                    {
                        "title": "Create a websocket chat application",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_cross_ApiGatewayWebsocketChat_section.html",
                        "sections": [
                            "",
                            "The following code example shows how to create a chat application that is served by a websocket API built on Amazon API Gateway.",
                            "PythonSDK for Python (Boto3)        Shows how to use the AWS SDK for Python (Boto3) with Amazon API Gateway V2 to        create a websocket API that integrates with AWS Lambda and Amazon DynamoDB.    Create a websocket API served by API Gateway.Define a Lambda handler that stores connections in DynamoDB and posts messages to        other chat participants.Connect to the websocket chat application and send messages with the Websockets        package.        For complete source code and instructions on how to set up and run, see the full example on        GitHub.    Services used in this exampleAPI GatewayDynamoDBLambdaanchorPythonSDK for Python (Boto3)        Shows how to use the AWS SDK for Python (Boto3) with Amazon API Gateway V2 to        create a websocket API that integrates with AWS Lambda and Amazon DynamoDB.    Create a websocket API served by API Gateway.Define a Lambda handler that stores connections in DynamoDB and posts messages to        other chat participants.Connect to the websocket chat application and send messages with the Websockets        package.        For complete source code and instructions on how to set up and run, see the full example on        GitHub.    Services used in this exampleAPI GatewayDynamoDBLambda",
                            "For a complete list of AWS SDK developer guides and code examples, see    Using Lambda with an AWS SDK.    This topic also includes information about getting started and details about previous SDK versions."
                        ]
                    },
                    {
                        "title": "Create an application to analyze customer feedback",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_cross_FSA_section.html",
                        "sections": [
                            "",
                            "The following code examples show how to create an application that analyzes customer comment cards, translates them from their original language, determines their sentiment, and generates an audio file from the translated text.",
                            ".NETAWS SDK for .NET    This example application analyzes and stores customer feedback cards. Specifically,    it fulfills the need of a fictitious hotel in New York City. The hotel receives feedback    from guests in various languages in the form of physical comment cards. That feedback    is uploaded into the app through a web client.    After an image of a comment card is uploaded, the following steps occur:  Text is extracted from the image using Amazon Textract.Amazon Comprehend determines the sentiment of the extracted text and its language.The extracted text is translated to English using Amazon Translate.Amazon Polly synthesizes an audio file from the extracted text. The full app can be deployed with the AWS CDK. For source code and deployment    instructions, see the project in     GitHub. Services used in this exampleAmazon ComprehendLambdaAmazon PollyAmazon TextractAmazon TranslateJavaSDK for Java 2.x    This example application analyzes and stores customer feedback cards. Specifically,    it fulfills the need of a fictitious hotel in New York City. The hotel receives feedback    from guests in various languages in the form of physical comment cards. That feedback    is uploaded into the app through a web client.    After an image of a comment card is uploaded, the following steps occur:  Text is extracted from the image using Amazon Textract.Amazon Comprehend determines the sentiment of the extracted text and its language.The extracted text is translated to English using Amazon Translate.Amazon Polly synthesizes an audio file from the extracted text. The full app can be deployed with the AWS CDK. For source code and deployment    instructions, see the project in     GitHub. Services used in this exampleAmazon ComprehendLambdaAmazon PollyAmazon TextractAmazon TranslateJavaScriptSDK for JavaScript (v3)    This example application analyzes and stores customer feedback cards. Specifically,    it fulfills the need of a fictitious hotel in New York City. The hotel receives feedback    from guests in various languages in the form of physical comment cards. That feedback    is uploaded into the app through a web client.    After an image of a comment card is uploaded, the following steps occur:  Text is extracted from the image using Amazon Textract.Amazon Comprehend determines the sentiment of the extracted text and its language.The extracted text is translated to English using Amazon Translate.Amazon Polly synthesizes an audio file from the extracted text. The full app can be deployed with the AWS CDK. For source code and deployment    instructions, see the project in     GitHub. The following excerpts show how the AWS SDK for JavaScript is used inside of Lambda functions. import {  ComprehendClient,  DetectDominantLanguageCommand,  DetectSentimentCommand,} from \"@aws-sdk/client-comprehend\";/** * Determine the language and sentiment of the extracted text. * * @param {{ source_text: string}} extractTextOutput */export const handler = async (extractTextOutput) => {  const comprehendClient = new ComprehendClient({});  const detectDominantLanguageCommand = new DetectDominantLanguageCommand({    Text: extractTextOutput.source_text,  });  // The source language is required for sentiment analysis and  // translation in the next step.  const { Languages } = await comprehendClient.send(    detectDominantLanguageCommand,  );  const languageCode = Languages[0].LanguageCode;  const detectSentimentCommand = new DetectSentimentCommand({    Text: extractTextOutput.source_text,    LanguageCode: languageCode,  });  const { Sentiment } = await comprehendClient.send(detectSentimentCommand);  return {    sentiment: Sentiment,    language_code: languageCode,  };};import {  DetectDocumentTextCommand,  TextractClient,} from \"@aws-sdk/client-textract\";/** * Fetch the S3 object from the event and analyze it using Amazon Textract. * * @param {import(\"@types/aws-lambda\").EventBridgeEvent<\"Object Created\">} eventBridgeS3Event */export const handler = async (eventBridgeS3Event) => {  const textractClient = new TextractClient();  const detectDocumentTextCommand = new DetectDocumentTextCommand({    Document: {      S3Object: {        Bucket: eventBridgeS3Event.bucket,        Name: eventBridgeS3Event.object,      },    },  });  // Textract returns a list of blocks. A block can be a line, a page, word, etc.  // Each block also contains geometry of the detected text.  // For more information on the Block type, see https://docs.aws.amazon.com/textract/latest/dg/API_Block.html.  const { Blocks } = await textractClient.send(detectDocumentTextCommand);  // For the purpose of this example, we are only interested in words.  const extractedWords = Blocks.filter((b) => b.BlockType === \"WORD\").map(    (b) => b.Text,  );  return extractedWords.join(\" \");};import { PollyClient, SynthesizeSpeechCommand } from \"@aws-sdk/client-polly\";import { S3Client } from \"@aws-sdk/client-s3\";import { Upload } from \"@aws-sdk/lib-storage\";/** * Synthesize an audio file from text. * * @param {{ bucket: string, translated_text: string, object: string}} sourceDestinationConfig */export const handler = async (sourceDestinationConfig) => {  const pollyClient = new PollyClient({});  const synthesizeSpeechCommand = new SynthesizeSpeechCommand({    Engine: \"neural\",    Text: sourceDestinationConfig.translated_text,    VoiceId: \"Ruth\",    OutputFormat: \"mp3\",  });  const { AudioStream } = await pollyClient.send(synthesizeSpeechCommand);  const audioKey = `${sourceDestinationConfig.object}.mp3`;  // Store the audio file in S3.  const s3Client = new S3Client();  const upload = new Upload({    client: s3Client,    params: {      Bucket: sourceDestinationConfig.bucket,      Key: audioKey,      Body: AudioStream,      ContentType: \"audio/mp3\",    },  });  await upload.done();  return audioKey;};import {  TranslateClient,  TranslateTextCommand,} from \"@aws-sdk/client-translate\";/** * Translate the extracted text to English. * * @param {{ extracted_text: string, source_language_code: string}} textAndSourceLanguage */export const handler = async (textAndSourceLanguage) => {  const translateClient = new TranslateClient({});  const translateCommand = new TranslateTextCommand({    SourceLanguageCode: textAndSourceLanguage.source_language_code,    TargetLanguageCode: \"en\",    Text: textAndSourceLanguage.extracted_text,  });  const { TranslatedText } = await translateClient.send(translateCommand);  return { translated_text: TranslatedText };};Services used in this exampleAmazon ComprehendLambdaAmazon PollyAmazon TextractAmazon TranslateRubySDK for Ruby    This example application analyzes and stores customer feedback cards. Specifically,    it fulfills the need of a fictitious hotel in New York City. The hotel receives feedback    from guests in various languages in the form of physical comment cards. That feedback    is uploaded into the app through a web client.    After an image of a comment card is uploaded, the following steps occur:  Text is extracted from the image using Amazon Textract.Amazon Comprehend determines the sentiment of the extracted text and its language.The extracted text is translated to English using Amazon Translate.Amazon Polly synthesizes an audio file from the extracted text. The full app can be deployed with the AWS CDK. For source code and deployment    instructions, see the project in     GitHub. Services used in this exampleAmazon ComprehendLambdaAmazon PollyAmazon TextractAmazon Translateanchoranchoranchoranchor.NETJavaJavaScriptRubyAWS SDK for .NET    This example application analyzes and stores customer feedback cards. Specifically,    it fulfills the need of a fictitious hotel in New York City. The hotel receives feedback    from guests in various languages in the form of physical comment cards. That feedback    is uploaded into the app through a web client.    After an image of a comment card is uploaded, the following steps occur:  Text is extracted from the image using Amazon Textract.Amazon Comprehend determines the sentiment of the extracted text and its language.The extracted text is translated to English using Amazon Translate.Amazon Polly synthesizes an audio file from the extracted text. The full app can be deployed with the AWS CDK. For source code and deployment    instructions, see the project in     GitHub. Services used in this exampleAmazon ComprehendLambdaAmazon PollyAmazon TextractAmazon Translate",
                            "For a complete list of AWS SDK developer guides and code examples, see    Using Lambda with an AWS SDK.    This topic also includes information about getting started and details about previous SDK versions."
                        ]
                    },
                    {
                        "title": "Invoke a Lambda function from a browser",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_cross_LambdaForBrowser_section.html",
                        "sections": [
                            "",
                            "The following code example shows how to invoke an AWS Lambda function from a browser.",
                            "JavaScriptSDK for JavaScript (v2)        You can create a browser-based application that uses an AWS Lambda function to update an Amazon DynamoDB        table with user selections.            For complete source code and instructions on how to set up and run, see the full example on        GitHub.    Services used in this exampleDynamoDBLambdaSDK for JavaScript (v3)        You can create a browser-based application that uses an AWS Lambda function to update an Amazon DynamoDB        table with user selections. This app uses AWS SDK for JavaScript v3.            For complete source code and instructions on how to set up and run, see the full example on        GitHub.    Services used in this exampleDynamoDBLambdaanchorJavaScriptSDK for JavaScript (v2)        You can create a browser-based application that uses an AWS Lambda function to update an Amazon DynamoDB        table with user selections.            For complete source code and instructions on how to set up and run, see the full example on        GitHub.    Services used in this exampleDynamoDBLambdaSDK for JavaScript (v3)        You can create a browser-based application that uses an AWS Lambda function to update an Amazon DynamoDB        table with user selections. This app uses AWS SDK for JavaScript v3.            For complete source code and instructions on how to set up and run, see the full example on        GitHub.    Services used in this exampleDynamoDBLambda",
                            "For a complete list of AWS SDK developer guides and code examples, see    Using Lambda with an AWS SDK.    This topic also includes information about getting started and details about previous SDK versions."
                        ]
                    },
                    {
                        "title": "Transform data with S3 Object Lambda",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_cross_ServerlessS3DataTransformation_section.html",
                        "sections": [
                            "",
                            "The following code example shows how to transform data for your application with S3 Object Lambda.",
                            ".NETAWS SDK for .NET        Shows how to add custom code to standard S3 GET requests to modify the requested object retrieved from S3 so that the object suit the needs of the requesting client or application.            For complete source code and instructions on how to set up and run, see the full example on        GitHub.    Services used in this exampleLambdaAmazon S3anchor.NETAWS SDK for .NET        Shows how to add custom code to standard S3 GET requests to modify the requested object retrieved from S3 so that the object suit the needs of the requesting client or application.            For complete source code and instructions on how to set up and run, see the full example on        GitHub.    Services used in this exampleLambdaAmazon S3",
                            "For a complete list of AWS SDK developer guides and code examples, see    Using Lambda with an AWS SDK.    This topic also includes information about getting started and details about previous SDK versions."
                        ]
                    },
                    {
                        "title": "Use API Gateway to invoke a Lambda function",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_cross_LambdaAPIGateway_section.html",
                        "sections": [
                            "",
                            "The following code examples show how to create an AWS Lambda function invoked by Amazon API Gateway.",
                            "JavaSDK for Java 2.x        Shows how to create an AWS Lambda function by using the Lambda Java runtime API.        This example invokes different AWS services to perform a specific use case. This example demonstrates how to        create a Lambda function invoked by Amazon API Gateway that scans an Amazon DynamoDB table for work anniversaries        and uses Amazon Simple Notification Service (Amazon SNS) to send a text message to your employees that congratulates        them at their one year anniversary date.            For complete source code and instructions on how to set up and run, see the full example on        GitHub.    Services used in this exampleAPI GatewayDynamoDBLambdaAmazon SNSJavaScriptSDK for JavaScript (v3)        Shows how to create an AWS Lambda function by using the Lambda JavaScript runtime API.        This example invokes different AWS services to perform a specific use case. This example demonstrates how to        create a Lambda function invoked by Amazon API Gateway that scans an Amazon DynamoDB table for work anniversaries        and uses Amazon Simple Notification Service (Amazon SNS) to send a text message to your employees that congratulates        them at their one year anniversary date.            For complete source code and instructions on how to set up and run, see the full example on        GitHub.    This example is also available in the        AWS SDK for JavaScript v3 developer guide.Services used in this exampleAPI GatewayDynamoDBLambdaAmazon SNSPythonSDK for Python (Boto3)        This example shows how to create and use an Amazon API Gateway REST API that targets an        AWS Lambda function. The Lambda handler demonstrates how to route based on HTTP        methods; how to get data from the query string, header, and body; and how to        return a JSON response.    Deploy a Lambda function.Create an API Gateway REST API.Create a REST resource that targets the Lambda function.Grant permission to let API Gateway invoke the Lambda function.Use the Requests package to send requests to the REST API.Clean up all resources created during the demo.        This example is best viewed on GitHub. For complete source code and        instructions on how to set up and run, see the full example on        GitHub.    Services used in this exampleAPI GatewayLambdaanchoranchoranchorJavaJavaScriptPythonSDK for Java 2.x        Shows how to create an AWS Lambda function by using the Lambda Java runtime API.        This example invokes different AWS services to perform a specific use case. This example demonstrates how to        create a Lambda function invoked by Amazon API Gateway that scans an Amazon DynamoDB table for work anniversaries        and uses Amazon Simple Notification Service (Amazon SNS) to send a text message to your employees that congratulates        them at their one year anniversary date.            For complete source code and instructions on how to set up and run, see the full example on        GitHub.    Services used in this exampleAPI GatewayDynamoDBLambdaAmazon SNS",
                            "For a complete list of AWS SDK developer guides and code examples, see    Using Lambda with an AWS SDK.    This topic also includes information about getting started and details about previous SDK versions."
                        ]
                    },
                    {
                        "title": "Use Step Functions to invoke Lambda functions",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_cross_ServerlessWorkflows_section.html",
                        "sections": [
                            "",
                            "The following code example shows how to create an AWS Step Functions state machine that invokes AWS Lambda functions in sequence.",
                            "JavaSDK for Java 2.x        Shows how to create an AWS serverless workflow by using AWS Step Functions and the AWS SDK for Java 2.x.        Each workflow step is implemented using an AWS Lambda function.            For complete source code and instructions on how to set up and run, see the full example on        GitHub.    Services used in this exampleDynamoDBLambdaAmazon SESStep FunctionsanchorJavaSDK for Java 2.x        Shows how to create an AWS serverless workflow by using AWS Step Functions and the AWS SDK for Java 2.x.        Each workflow step is implemented using an AWS Lambda function.            For complete source code and instructions on how to set up and run, see the full example on        GitHub.    Services used in this exampleDynamoDBLambdaAmazon SESStep Functions",
                            "For a complete list of AWS SDK developer guides and code examples, see    Using Lambda with an AWS SDK.    This topic also includes information about getting started and details about previous SDK versions."
                        ]
                    },
                    {
                        "title": "Use scheduled events to invoke a Lambda function",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_cross_LambdaScheduledEvents_section.html",
                        "sections": [
                            "",
                            "The following code examples show how to create an AWS Lambda function invoked by an Amazon EventBridge scheduled event.",
                            "JavaSDK for Java 2.x        Shows how to create an Amazon EventBridge scheduled event that invokes an AWS Lambda function.        Configure EventBridge to use a cron expression to schedule when the Lambda function is invoked.        In this example, you create a Lambda function by using the Lambda Java runtime API.        This example invokes different AWS services to perform a specific use case. This example demonstrates how to        create an app that sends a mobile text message to your employees that congratulates         them at the one year anniversary date.             For complete source code and instructions on how to set up and run, see the full example on        GitHub.    Services used in this exampleDynamoDBEventBridgeLambdaAmazon SNSJavaScriptSDK for JavaScript (v3)        Shows how to create an Amazon EventBridge scheduled event that invokes an AWS Lambda function.        Configure EventBridge to use a cron expression to schedule when the Lambda function is invoked.        In this example, you create a Lambda function by using the Lambda JavaScript runtime API.        This example invokes different AWS services to perform a specific use case. This example demonstrates how to        create an app that sends a mobile text message to your employees that congratulates         them at the one year anniversary date.             For complete source code and instructions on how to set up and run, see the full example on        GitHub.    This example is also available in the        AWS SDK for JavaScript v3 developer guide.Services used in this exampleDynamoDBEventBridgeLambdaAmazon SNSPythonSDK for Python (Boto3)        This example shows how to register an AWS Lambda function as the target of a        scheduled Amazon EventBridge event. The Lambda handler writes a friendly message and the        full event data to Amazon CloudWatch Logs for later retrieval.    Deploys a Lambda function.Creates an EventBridge scheduled event and makes the Lambda function the target.Grants permission to let EventBridge invoke the Lambda function.Prints the latest data from CloudWatch Logs to show the result of the scheduled invocations.Cleans up all resources created during the demo.        This example is best viewed on GitHub. For complete source code and        instructions on how to set up and run, see the full example on        GitHub.    Services used in this exampleCloudWatch LogsEventBridgeLambdaanchoranchoranchorJavaJavaScriptPythonSDK for Java 2.x        Shows how to create an Amazon EventBridge scheduled event that invokes an AWS Lambda function.        Configure EventBridge to use a cron expression to schedule when the Lambda function is invoked.        In this example, you create a Lambda function by using the Lambda Java runtime API.        This example invokes different AWS services to perform a specific use case. This example demonstrates how to        create an app that sends a mobile text message to your employees that congratulates         them at the one year anniversary date.             For complete source code and instructions on how to set up and run, see the full example on        GitHub.    Services used in this exampleDynamoDBEventBridgeLambdaAmazon SNS",
                            "For a complete list of AWS SDK developer guides and code examples, see    Using Lambda with an AWS SDK.    This topic also includes information about getting started and details about previous SDK versions."
                        ]
                    },
                    {
                        "title": "Write custom activity data with a Lambda function after Amazon Cognito user authentication",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_cross_CognitoCustomActivityLog_section.html",
                        "sections": [
                            "",
                            "The following code example shows how to write custom activity data with a Lambda function after Amazon Cognito user authentication.",
                            {
                                "code_example": "PostAuthentication"
                            },
                            "GoSDK for Go V2Note        There's more on GitHub. Find the complete example and learn how to set up and run in the        AWS Code            Examples Repository.    Run an interactive scenario at a command prompt.import (\t\"context\"\t\"errors\"\t\"log\"\t\"strings\"\t\"user_pools_and_lambda_triggers/actions\"\t\"github.com/aws/aws-sdk-go-v2/aws\"\t\"github.com/aws/aws-sdk-go-v2/service/cognitoidentityprovider\"\t\"github.com/aws/aws-sdk-go-v2/service/cognitoidentityprovider/types\"\t\"github.com/awsdocs/aws-doc-sdk-examples/gov2/demotools\")// ActivityLog separates the steps of this scenario into individual functions so that// they are simpler to read and understand.type ActivityLog struct {\thelper       IScenarioHelper\tquestioner   demotools.IQuestioner\tresources    Resources\tcognitoActor *actions.CognitoActions}// NewActivityLog constructs a new activity log runner.func NewActivityLog(sdkConfig aws.Config, questioner demotools.IQuestioner, helper IScenarioHelper) ActivityLog {\tscenario := ActivityLog{\t\thelper:       helper,\t\tquestioner:   questioner,\t\tresources:    Resources{},\t\tcognitoActor: &actions.CognitoActions{CognitoClient: cognitoidentityprovider.NewFromConfig(sdkConfig)},\t}\tscenario.resources.init(scenario.cognitoActor, questioner)\treturn scenario}// AddUserToPool selects a user from the known users table and uses administrator credentials to add the user to the user pool.func (runner *ActivityLog) AddUserToPool(ctx context.Context, userPoolId string, tableName string) (string, string) {\tlog.Println(\"To facilitate this example, let's add a user to the user pool using administrator privileges.\")\tusers, err := runner.helper.GetKnownUsers(ctx, tableName)\tif err != nil {\t\tpanic(err)\t}\tuser := users.Users[0]\tlog.Printf(\"Adding known user %v to the user pool.\\n\", user.UserName)\terr = runner.cognitoActor.AdminCreateUser(ctx, userPoolId, user.UserName, user.UserEmail)\tif err != nil {\t\tpanic(err)\t}\tpwSet := false\tpassword := runner.questioner.AskPassword(\"\\nEnter a password that has at least eight characters, uppercase, lowercase, numbers and symbols.\\n\"+\t\t\"(the password will not display as you type):\", 8)\tfor !pwSet {\t\tlog.Printf(\"\\nSetting password for user '%v'.\\n\", user.UserName)\t\terr = runner.cognitoActor.AdminSetUserPassword(ctx, userPoolId, user.UserName, password)\t\tif err != nil {\t\t\tvar invalidPassword *types.InvalidPasswordException\t\t\tif errors.As(err, &invalidPassword) {\t\t\t\tpassword = runner.questioner.AskPassword(\"\\nEnter another password:\", 8)\t\t\t} else {\t\t\t\tpanic(err)\t\t\t}\t\t} else {\t\t\tpwSet = true\t\t}\t}\tlog.Println(strings.Repeat(\"-\", 88))\treturn user.UserName, password}// AddActivityLogTrigger adds a Lambda handler as an invocation target for the PostAuthentication trigger.func (runner *ActivityLog) AddActivityLogTrigger(ctx context.Context, userPoolId string, activityLogArn string) {\tlog.Println(\"Let's add a Lambda function to handle the PostAuthentication trigger from Cognito.\\n\" +\t\t\"This trigger happens after a user is authenticated, and lets your function take action, such as logging\\n\" +\t\t\"the outcome.\")\terr := runner.cognitoActor.UpdateTriggers(\t\tctx, userPoolId,\t\tactions.TriggerInfo{Trigger: actions.PostAuthentication, HandlerArn: aws.String(activityLogArn)})\tif err != nil {\t\tpanic(err)\t}\trunner.resources.triggers = append(runner.resources.triggers, actions.PostAuthentication)\tlog.Printf(\"Lambda function %v added to user pool %v to handle PostAuthentication Cognito trigger.\\n\",\t\tactivityLogArn, userPoolId)\tlog.Println(strings.Repeat(\"-\", 88))}// SignInUser signs in as the specified user.func (runner *ActivityLog) SignInUser(ctx context.Context, clientId string, userName string, password string) {\tlog.Printf(\"Now we'll sign in user %v and check the results in the logs and the DynamoDB table.\", userName)\trunner.questioner.Ask(\"Press Enter when you're ready.\")\tauthResult, err := runner.cognitoActor.SignIn(ctx, clientId, userName, password)\tif err != nil {\t\tpanic(err)\t}\tlog.Println(\"Sign in successful.\",\t\t\"The PostAuthentication Lambda handler writes custom information to CloudWatch Logs.\")\trunner.resources.userAccessTokens = append(runner.resources.userAccessTokens, *authResult.AccessToken)}// GetKnownUserLastLogin gets the login info for a user from the Amazon DynamoDB table and displays it.func (runner *ActivityLog) GetKnownUserLastLogin(ctx context.Context, tableName string, userName string) {\tlog.Println(\"The PostAuthentication handler also writes login data to the DynamoDB table.\")\trunner.questioner.Ask(\"Press Enter when you're ready to continue.\")\tusers, err := runner.helper.GetKnownUsers(ctx, tableName)\tif err != nil {\t\tpanic(err)\t}\tfor _, user := range users.Users {\t\tif user.UserName == userName {\t\t\tlog.Println(\"The last login info for the user in the known users table is:\")\t\t\tlog.Printf(\"\\t%+v\", *user.LastLogin)\t\t}\t}\tlog.Println(strings.Repeat(\"-\", 88))}// Run runs the scenario.func (runner *ActivityLog) Run(ctx context.Context, stackName string) {\tdefer func() {\t\tif r := recover(); r != nil {\t\t\tlog.Println(\"Something went wrong with the demo.\")\t\t\trunner.resources.Cleanup(ctx)\t\t}\t}()\tlog.Println(strings.Repeat(\"-\", 88))\tlog.Printf(\"Welcome\\n\")\tlog.Println(strings.Repeat(\"-\", 88))\tstackOutputs, err := runner.helper.GetStackOutputs(ctx, stackName)\tif err != nil {\t\tpanic(err)\t}\trunner.resources.userPoolId = stackOutputs[\"UserPoolId\"]\trunner.helper.PopulateUserTable(ctx, stackOutputs[\"TableName\"])\tuserName, password := runner.AddUserToPool(ctx, stackOutputs[\"UserPoolId\"], stackOutputs[\"TableName\"])\trunner.AddActivityLogTrigger(ctx, stackOutputs[\"UserPoolId\"], stackOutputs[\"ActivityLogFunctionArn\"])\trunner.SignInUser(ctx, stackOutputs[\"UserPoolClientId\"], userName, password)\trunner.helper.ListRecentLogEvents(ctx, stackOutputs[\"ActivityLogFunction\"])\trunner.GetKnownUserLastLogin(ctx, stackOutputs[\"TableName\"], userName)\trunner.resources.Cleanup(ctx)\tlog.Println(strings.Repeat(\"-\", 88))\tlog.Println(\"Thanks for watching!\")\tlog.Println(strings.Repeat(\"-\", 88))}Handle the PostAuthentication trigger with a Lambda function.import (\t\"context\"\t\"fmt\"\t\"log\"\t\"os\"\t\"time\"\t\"github.com/aws/aws-lambda-go/events\"\t\"github.com/aws/aws-lambda-go/lambda\"\t\"github.com/aws/aws-sdk-go-v2/aws\"\t\"github.com/aws/aws-sdk-go-v2/config\"\t\"github.com/aws/aws-sdk-go-v2/feature/dynamodb/attributevalue\"\t\"github.com/aws/aws-sdk-go-v2/service/dynamodb\"\tdynamodbtypes \"github.com/aws/aws-sdk-go-v2/service/dynamodb/types\")const TABLE_NAME = \"TABLE_NAME\"// LoginInfo defines structured login data that can be marshalled to a DynamoDB format.type LoginInfo struct {\tUserPoolId string `dynamodbav:\"UserPoolId\"`\tClientId   string `dynamodbav:\"ClientId\"`\tTime       string `dynamodbav:\"Time\"`}// UserInfo defines structured user data that can be marshalled to a DynamoDB format.type UserInfo struct {\tUserName  string    `dynamodbav:\"UserName\"`\tUserEmail string    `dynamodbav:\"UserEmail\"`\tLastLogin LoginInfo `dynamodbav:\"LastLogin\"`}// GetKey marshals the user email value to a DynamoDB key format.func (user UserInfo) GetKey() map[string]dynamodbtypes.AttributeValue {\tuserEmail, err := attributevalue.Marshal(user.UserEmail)\tif err != nil {\t\tpanic(err)\t}\treturn map[string]dynamodbtypes.AttributeValue{\"UserEmail\": userEmail}}type handler struct {\tdynamoClient *dynamodb.Client}// HandleRequest handles the PostAuthentication event by writing custom data to the logs and// to an Amazon DynamoDB table.func (h *handler) HandleRequest(ctx context.Context, event events.CognitoEventUserPoolsPostAuthentication) (events.CognitoEventUserPoolsPostAuthentication, error) {\tlog.Printf(\"Received post authentication trigger from %v for user '%v'\", event.TriggerSource, event.UserName)\ttableName := os.Getenv(TABLE_NAME)\tuser := UserInfo{\t\tUserName:  event.UserName,\t\tUserEmail: event.Request.UserAttributes[\"email\"],\t\tLastLogin: LoginInfo{\t\t\tUserPoolId: event.UserPoolID,\t\t\tClientId:   event.CallerContext.ClientID,\t\t\tTime:       time.Now().Format(time.UnixDate),\t\t},\t}\t// Write to CloudWatch Logs.\tfmt.Printf(\"%#v\", user)\t// Also write to an external system. This examples uses DynamoDB to demonstrate.\tuserMap, err := attributevalue.MarshalMap(user)\tif err != nil {\t\tlog.Printf(\"Couldn't marshal to DynamoDB map. Here's why: %v\\n\", err)\t} else if len(userMap) == 0 {\t\tlog.Printf(\"User info marshaled to an empty map.\")\t} else {\t\t_, err := h.dynamoClient.PutItem(ctx, &dynamodb.PutItemInput{\t\t\tItem:      userMap,\t\t\tTableName: aws.String(tableName),\t\t})\t\tif err != nil {\t\t\tlog.Printf(\"Couldn't write to DynamoDB. Here's why: %v\\n\", err)\t\t} else {\t\t\tlog.Printf(\"Wrote user info to DynamoDB table %v.\\n\", tableName)\t\t}\t}\treturn event, nil}func main() {\tctx := context.Background()\tsdkConfig, err := config.LoadDefaultConfig(ctx)\tif err != nil {\t\tlog.Panicln(err)\t}\th := handler{\t\tdynamoClient: dynamodb.NewFromConfig(sdkConfig),\t}\tlambda.Start(h.HandleRequest)}Create a struct that performs common tasks.import (\t\"context\"\t\"log\"\t\"strings\"\t\"time\"\t\"user_pools_and_lambda_triggers/actions\"\t\"github.com/aws/aws-sdk-go-v2/aws\"\t\"github.com/aws/aws-sdk-go-v2/service/cloudformation\"\t\"github.com/aws/aws-sdk-go-v2/service/cloudwatchlogs\"\t\"github.com/aws/aws-sdk-go-v2/service/dynamodb\"\t\"github.com/awsdocs/aws-doc-sdk-examples/gov2/demotools\")// IScenarioHelper defines common functions used by the workflows in this example.type IScenarioHelper interface {\tPause(secs int)\tGetStackOutputs(ctx context.Context, stackName string) (actions.StackOutputs, error)\tPopulateUserTable(ctx context.Context, tableName string)\tGetKnownUsers(ctx context.Context, tableName string) (actions.UserList, error)\tAddKnownUser(ctx context.Context, tableName string, user actions.User)\tListRecentLogEvents(ctx context.Context, functionName string)}// ScenarioHelper contains AWS wrapper structs used by the workflows in this example.type ScenarioHelper struct {\tquestioner  demotools.IQuestioner\tdynamoActor *actions.DynamoActions\tcfnActor    *actions.CloudFormationActions\tcwlActor    *actions.CloudWatchLogsActions\tisTestRun   bool}// NewScenarioHelper constructs a new scenario helper.func NewScenarioHelper(sdkConfig aws.Config, questioner demotools.IQuestioner) ScenarioHelper {\tscenario := ScenarioHelper{\t\tquestioner:  questioner,\t\tdynamoActor: &actions.DynamoActions{DynamoClient: dynamodb.NewFromConfig(sdkConfig)},\t\tcfnActor:    &actions.CloudFormationActions{CfnClient: cloudformation.NewFromConfig(sdkConfig)},\t\tcwlActor:    &actions.CloudWatchLogsActions{CwlClient: cloudwatchlogs.NewFromConfig(sdkConfig)},\t}\treturn scenario}// Pause waits for the specified number of seconds.func (helper ScenarioHelper) Pause(secs int) {\tif !helper.isTestRun {\t\ttime.Sleep(time.Duration(secs) * time.Second)\t}}// GetStackOutputs gets the outputs from the specified CloudFormation stack in a structured format.func (helper ScenarioHelper) GetStackOutputs(ctx context.Context, stackName string) (actions.StackOutputs, error) {\treturn helper.cfnActor.GetOutputs(ctx, stackName), nil}// PopulateUserTable fills the known user table with example data.func (helper ScenarioHelper) PopulateUserTable(ctx context.Context, tableName string) {\tlog.Printf(\"First, let's add some users to the DynamoDB %v table we'll use for this example.\\n\", tableName)\terr := helper.dynamoActor.PopulateTable(ctx, tableName)\tif err != nil {\t\tpanic(err)\t}}// GetKnownUsers gets the users from the known users table in a structured format.func (helper ScenarioHelper) GetKnownUsers(ctx context.Context, tableName string) (actions.UserList, error) {\tknownUsers, err := helper.dynamoActor.Scan(ctx, tableName)\tif err != nil {\t\tlog.Printf(\"Couldn't get known users from table %v. Here's why: %v\\n\", tableName, err)\t}\treturn knownUsers, err}// AddKnownUser adds a user to the known users table.func (helper ScenarioHelper) AddKnownUser(ctx context.Context, tableName string, user actions.User) {\tlog.Printf(\"Adding user '%v' with email '%v' to the DynamoDB known users table...\\n\",\t\tuser.UserName, user.UserEmail)\terr := helper.dynamoActor.AddUser(ctx, tableName, user)\tif err != nil {\t\tpanic(err)\t}}// ListRecentLogEvents gets the most recent log stream and events for the specified Lambda function and displays them.func (helper ScenarioHelper) ListRecentLogEvents(ctx context.Context, functionName string) {\tlog.Println(\"Waiting a few seconds to let Lambda write to CloudWatch Logs...\")\thelper.Pause(10)\tlog.Println(\"Okay, let's check the logs to find what's happened recently with your Lambda function.\")\tlogStream, err := helper.cwlActor.GetLatestLogStream(ctx, functionName)\tif err != nil {\t\tpanic(err)\t}\tlog.Printf(\"Getting some recent events from log stream %v\\n\", *logStream.LogStreamName)\tevents, err := helper.cwlActor.GetLogEvents(ctx, functionName, *logStream.LogStreamName, 10)\tif err != nil {\t\tpanic(err)\t}\tfor _, event := range events {\t\tlog.Printf(\"\\t%v\", *event.Message)\t}\tlog.Println(strings.Repeat(\"-\", 88))}Create a struct that wraps Amazon Cognito actions.import (\t\"context\"\t\"errors\"\t\"log\"\t\"github.com/aws/aws-sdk-go-v2/aws\"\t\"github.com/aws/aws-sdk-go-v2/service/cognitoidentityprovider\"\t\"github.com/aws/aws-sdk-go-v2/service/cognitoidentityprovider/types\")type CognitoActions struct {\tCognitoClient *cognitoidentityprovider.Client}// Trigger and TriggerInfo define typed data for updating an Amazon Cognito trigger.type Trigger intconst (\tPreSignUp Trigger = iota\tUserMigration\tPostAuthentication)type TriggerInfo struct {\tTrigger    Trigger\tHandlerArn *string}// UpdateTriggers adds or removes Lambda triggers for a user pool. When a trigger is specified with a `nil` value,// it is removed from the user pool.func (actor CognitoActions) UpdateTriggers(ctx context.Context, userPoolId string, triggers ...TriggerInfo) error {\toutput, err := actor.CognitoClient.DescribeUserPool(ctx, &cognitoidentityprovider.DescribeUserPoolInput{\t\tUserPoolId: aws.String(userPoolId),\t})\tif err != nil {\t\tlog.Printf(\"Couldn't get info about user pool %v. Here's why: %v\\n\", userPoolId, err)\t\treturn err\t}\tlambdaConfig := output.UserPool.LambdaConfig\tfor _, trigger := range triggers {\t\tswitch trigger.Trigger {\t\tcase PreSignUp:\t\t\tlambdaConfig.PreSignUp = trigger.HandlerArn\t\tcase UserMigration:\t\t\tlambdaConfig.UserMigration = trigger.HandlerArn\t\tcase PostAuthentication:\t\t\tlambdaConfig.PostAuthentication = trigger.HandlerArn\t\t}\t}\t_, err = actor.CognitoClient.UpdateUserPool(ctx, &cognitoidentityprovider.UpdateUserPoolInput{\t\tUserPoolId:   aws.String(userPoolId),\t\tLambdaConfig: lambdaConfig,\t})\tif err != nil {\t\tlog.Printf(\"Couldn't update user pool %v. Here's why: %v\\n\", userPoolId, err)\t}\treturn err}// SignUp signs up a user with Amazon Cognito.func (actor CognitoActions) SignUp(ctx context.Context, clientId string, userName string, password string, userEmail string) (bool, error) {\tconfirmed := false\toutput, err := actor.CognitoClient.SignUp(ctx, &cognitoidentityprovider.SignUpInput{\t\tClientId: aws.String(clientId),\t\tPassword: aws.String(password),\t\tUsername: aws.String(userName),\t\tUserAttributes: []types.AttributeType{\t\t\t{Name: aws.String(\"email\"), Value: aws.String(userEmail)},\t\t},\t})\tif err != nil {\t\tvar invalidPassword *types.InvalidPasswordException\t\tif errors.As(err, &invalidPassword) {\t\t\tlog.Println(*invalidPassword.Message)\t\t} else {\t\t\tlog.Printf(\"Couldn't sign up user %v. Here's why: %v\\n\", userName, err)\t\t}\t} else {\t\tconfirmed = output.UserConfirmed\t}\treturn confirmed, err}// SignIn signs in a user to Amazon Cognito using a username and password authentication flow.func (actor CognitoActions) SignIn(ctx context.Context, clientId string, userName string, password string) (*types.AuthenticationResultType, error) {\tvar authResult *types.AuthenticationResultType\toutput, err := actor.CognitoClient.InitiateAuth(ctx, &cognitoidentityprovider.InitiateAuthInput{\t\tAuthFlow:       \"USER_PASSWORD_AUTH\",\t\tClientId:       aws.String(clientId),\t\tAuthParameters: map[string]string{\"USERNAME\": userName, \"PASSWORD\": password},\t})\tif err != nil {\t\tvar resetRequired *types.PasswordResetRequiredException\t\tif errors.As(err, &resetRequired) {\t\t\tlog.Println(*resetRequired.Message)\t\t} else {\t\t\tlog.Printf(\"Couldn't sign in user %v. Here's why: %v\\n\", userName, err)\t\t}\t} else {\t\tauthResult = output.AuthenticationResult\t}\treturn authResult, err}// ForgotPassword starts a password recovery flow for a user. This flow typically sends a confirmation code// to the user's configured notification destination, such as email.func (actor CognitoActions) ForgotPassword(ctx context.Context, clientId string, userName string) (*types.CodeDeliveryDetailsType, error) {\toutput, err := actor.CognitoClient.ForgotPassword(ctx, &cognitoidentityprovider.ForgotPasswordInput{\t\tClientId: aws.String(clientId),\t\tUsername: aws.String(userName),\t})\tif err != nil {\t\tlog.Printf(\"Couldn't start password reset for user '%v'. Here;s why: %v\\n\", userName, err)\t}\treturn output.CodeDeliveryDetails, err}// ConfirmForgotPassword confirms a user with a confirmation code and a new password.func (actor CognitoActions) ConfirmForgotPassword(ctx context.Context, clientId string, code string, userName string, password string) error {\t_, err := actor.CognitoClient.ConfirmForgotPassword(ctx, &cognitoidentityprovider.ConfirmForgotPasswordInput{\t\tClientId:         aws.String(clientId),\t\tConfirmationCode: aws.String(code),\t\tPassword:         aws.String(password),\t\tUsername:         aws.String(userName),\t})\tif err != nil {\t\tvar invalidPassword *types.InvalidPasswordException\t\tif errors.As(err, &invalidPassword) {\t\t\tlog.Println(*invalidPassword.Message)\t\t} else {\t\t\tlog.Printf(\"Couldn't confirm user %v. Here's why: %v\", userName, err)\t\t}\t}\treturn err}// DeleteUser removes a user from the user pool.func (actor CognitoActions) DeleteUser(ctx context.Context, userAccessToken string) error {\t_, err := actor.CognitoClient.DeleteUser(ctx, &cognitoidentityprovider.DeleteUserInput{\t\tAccessToken: aws.String(userAccessToken),\t})\tif err != nil {\t\tlog.Printf(\"Couldn't delete user. Here's why: %v\\n\", err)\t}\treturn err}// AdminCreateUser uses administrator credentials to add a user to a user pool. This method leaves the user// in a state that requires they enter a new password next time they sign in.func (actor CognitoActions) AdminCreateUser(ctx context.Context, userPoolId string, userName string, userEmail string) error {\t_, err := actor.CognitoClient.AdminCreateUser(ctx, &cognitoidentityprovider.AdminCreateUserInput{\t\tUserPoolId:     aws.String(userPoolId),\t\tUsername:       aws.String(userName),\t\tMessageAction:  types.MessageActionTypeSuppress,\t\tUserAttributes: []types.AttributeType{{Name: aws.String(\"email\"), Value: aws.String(userEmail)}},\t})\tif err != nil {\t\tvar userExists *types.UsernameExistsException\t\tif errors.As(err, &userExists) {\t\t\tlog.Printf(\"User %v already exists in the user pool.\", userName)\t\t\terr = nil\t\t} else {\t\t\tlog.Printf(\"Couldn't create user %v. Here's why: %v\\n\", userName, err)\t\t}\t}\treturn err}// AdminSetUserPassword uses administrator credentials to set a password for a user without requiring a// temporary password.func (actor CognitoActions) AdminSetUserPassword(ctx context.Context, userPoolId string, userName string, password string) error {\t_, err := actor.CognitoClient.AdminSetUserPassword(ctx, &cognitoidentityprovider.AdminSetUserPasswordInput{\t\tPassword:   aws.String(password),\t\tUserPoolId: aws.String(userPoolId),\t\tUsername:   aws.String(userName),\t\tPermanent:  true,\t})\tif err != nil {\t\tvar invalidPassword *types.InvalidPasswordException\t\tif errors.As(err, &invalidPassword) {\t\t\tlog.Println(*invalidPassword.Message)\t\t} else {\t\t\tlog.Printf(\"Couldn't set password for user %v. Here's why: %v\\n\", userName, err)\t\t}\t}\treturn err}Create a struct that wraps DynamoDB actions.import (\t\"context\"\t\"fmt\"\t\"log\"\t\"github.com/aws/aws-sdk-go-v2/aws\"\t\"github.com/aws/aws-sdk-go-v2/feature/dynamodb/attributevalue\"\t\"github.com/aws/aws-sdk-go-v2/service/dynamodb\"\t\"github.com/aws/aws-sdk-go-v2/service/dynamodb/types\")// DynamoActions encapsulates the Amazon Simple Notification Service (Amazon SNS) actions// used in the examples.type DynamoActions struct {\tDynamoClient *dynamodb.Client}// User defines structured user data.type User struct {\tUserName  string\tUserEmail string\tLastLogin *LoginInfo `dynamodbav:\",omitempty\"`}// LoginInfo defines structured custom login data.type LoginInfo struct {\tUserPoolId string\tClientId   string\tTime       string}// UserList defines a list of users.type UserList struct {\tUsers []User}// UserNameList returns the usernames contained in a UserList as a list of strings.func (users *UserList) UserNameList() []string {\tnames := make([]string, len(users.Users))\tfor i := 0; i < len(users.Users); i++ {\t\tnames[i] = users.Users[i].UserName\t}\treturn names}// PopulateTable adds a set of test users to the table.func (actor DynamoActions) PopulateTable(ctx context.Context, tableName string) error {\tvar err error\tvar item map[string]types.AttributeValue\tvar writeReqs []types.WriteRequest\tfor i := 1; i < 4; i++ {\t\titem, err = attributevalue.MarshalMap(User{UserName: fmt.Sprintf(\"test_user_%v\", i), UserEmail: fmt.Sprintf(\"test_email_%v@example.com\", i)})\t\tif err != nil {\t\t\tlog.Printf(\"Couldn't marshall user into DynamoDB format. Here's why: %v\\n\", err)\t\t\treturn err\t\t}\t\twriteReqs = append(writeReqs, types.WriteRequest{PutRequest: &types.PutRequest{Item: item}})\t}\t_, err = actor.DynamoClient.BatchWriteItem(ctx, &dynamodb.BatchWriteItemInput{\t\tRequestItems: map[string][]types.WriteRequest{tableName: writeReqs},\t})\tif err != nil {\t\tlog.Printf(\"Couldn't populate table %v with users. Here's why: %v\\n\", tableName, err)\t}\treturn err}// Scan scans the table for all items.func (actor DynamoActions) Scan(ctx context.Context, tableName string) (UserList, error) {\tvar userList UserList\toutput, err := actor.DynamoClient.Scan(ctx, &dynamodb.ScanInput{\t\tTableName: aws.String(tableName),\t})\tif err != nil {\t\tlog.Printf(\"Couldn't scan table %v for items. Here's why: %v\\n\", tableName, err)\t} else {\t\terr = attributevalue.UnmarshalListOfMaps(output.Items, &userList.Users)\t\tif err != nil {\t\t\tlog.Printf(\"Couldn't unmarshal items into users. Here's why: %v\\n\", err)\t\t}\t}\treturn userList, err}// AddUser adds a user item to a table.func (actor DynamoActions) AddUser(ctx context.Context, tableName string, user User) error {\tuserItem, err := attributevalue.MarshalMap(user)\tif err != nil {\t\tlog.Printf(\"Couldn't marshall user to item. Here's why: %v\\n\", err)\t}\t_, err = actor.DynamoClient.PutItem(ctx, &dynamodb.PutItemInput{\t\tItem:      userItem,\t\tTableName: aws.String(tableName),\t})\tif err != nil {\t\tlog.Printf(\"Couldn't put item in table %v. Here's why: %v\", tableName, err)\t}\treturn err}Create a struct that wraps CloudWatch Logs actions.import (\t\"context\"\t\"fmt\"\t\"log\"\t\"github.com/aws/aws-sdk-go-v2/aws\"\t\"github.com/aws/aws-sdk-go-v2/service/cloudwatchlogs\"\t\"github.com/aws/aws-sdk-go-v2/service/cloudwatchlogs/types\")type CloudWatchLogsActions struct {\tCwlClient *cloudwatchlogs.Client}// GetLatestLogStream gets the most recent log stream for a Lambda function.func (actor CloudWatchLogsActions) GetLatestLogStream(ctx context.Context, functionName string) (types.LogStream, error) {\tvar logStream types.LogStream\tlogGroupName := fmt.Sprintf(\"/aws/lambda/%s\", functionName)\toutput, err := actor.CwlClient.DescribeLogStreams(ctx, &cloudwatchlogs.DescribeLogStreamsInput{\t\tDescending:   aws.Bool(true),\t\tLimit:        aws.Int32(1),\t\tLogGroupName: aws.String(logGroupName),\t\tOrderBy:      types.OrderByLastEventTime,\t})\tif err != nil {\t\tlog.Printf(\"Couldn't get log streams for log group %v. Here's why: %v\\n\", logGroupName, err)\t} else {\t\tlogStream = output.LogStreams[0]\t}\treturn logStream, err}// GetLogEvents gets the most recent eventCount events from the specified log stream.func (actor CloudWatchLogsActions) GetLogEvents(ctx context.Context, functionName string, logStreamName string, eventCount int32) (\t[]types.OutputLogEvent, error) {\tvar events []types.OutputLogEvent\tlogGroupName := fmt.Sprintf(\"/aws/lambda/%s\", functionName)\toutput, err := actor.CwlClient.GetLogEvents(ctx, &cloudwatchlogs.GetLogEventsInput{\t\tLogStreamName: aws.String(logStreamName),\t\tLimit:         aws.Int32(eventCount),\t\tLogGroupName:  aws.String(logGroupName),\t})\tif err != nil {\t\tlog.Printf(\"Couldn't get log event for log stream %v. Here's why: %v\\n\", logStreamName, err)\t} else {\t\tevents = output.Events\t}\treturn events, err}Create a struct that wraps AWS CloudFormation actions.import (\t\"context\"\t\"log\"\t\"github.com/aws/aws-sdk-go-v2/aws\"\t\"github.com/aws/aws-sdk-go-v2/service/cloudformation\")// StackOutputs defines a map of outputs from a specific stack.type StackOutputs map[string]stringtype CloudFormationActions struct {\tCfnClient *cloudformation.Client}// GetOutputs gets the outputs from a CloudFormation stack and puts them into a structured format.func (actor CloudFormationActions) GetOutputs(ctx context.Context, stackName string) StackOutputs {\toutput, err := actor.CfnClient.DescribeStacks(ctx, &cloudformation.DescribeStacksInput{\t\tStackName: aws.String(stackName),\t})\tif err != nil || len(output.Stacks) == 0 {\t\tlog.Panicf(\"Couldn't find a CloudFormation stack named %v. Here's why: %v\\n\", stackName, err)\t}\tstackOutputs := StackOutputs{}\tfor _, out := range output.Stacks[0].Outputs {\t\tstackOutputs[*out.OutputKey] = *out.OutputValue\t}\treturn stackOutputs}Clean up resources.import (\t\"context\"\t\"log\"\t\"user_pools_and_lambda_triggers/actions\"\t\"github.com/awsdocs/aws-doc-sdk-examples/gov2/demotools\")// Resources keeps track of AWS resources created during an example and handles// cleanup when the example finishes.type Resources struct {\tuserPoolId       string\tuserAccessTokens []string\ttriggers         []actions.Trigger\tcognitoActor *actions.CognitoActions\tquestioner   demotools.IQuestioner}func (resources *Resources) init(cognitoActor *actions.CognitoActions, questioner demotools.IQuestioner) {\tresources.userAccessTokens = []string{}\tresources.triggers = []actions.Trigger{}\tresources.cognitoActor = cognitoActor\tresources.questioner = questioner}// Cleanup deletes all AWS resources created during an example.func (resources *Resources) Cleanup(ctx context.Context) {\tdefer func() {\t\tif r := recover(); r != nil {\t\t\tlog.Printf(\"Something went wrong during cleanup.\\n%v\\n\", r)\t\t\tlog.Println(\"Use the AWS Management Console to remove any remaining resources \\n\" +\t\t\t\t\"that were created for this scenario.\")\t\t}\t}()\twantDelete := resources.questioner.AskBool(\"Do you want to remove all of the AWS resources that were created \"+\t\t\"during this demo (y/n)?\", \"y\")\tif wantDelete {\t\tfor _, accessToken := range resources.userAccessTokens {\t\t\terr := resources.cognitoActor.DeleteUser(ctx, accessToken)\t\t\tif err != nil {\t\t\t\tlog.Println(\"Couldn't delete user during cleanup.\")\t\t\t\tpanic(err)\t\t\t}\t\t\tlog.Println(\"Deleted user.\")\t\t}\t\ttriggerList := make([]actions.TriggerInfo, len(resources.triggers))\t\tfor i := 0; i < len(resources.triggers); i++ {\t\t\ttriggerList[i] = actions.TriggerInfo{Trigger: resources.triggers[i], HandlerArn: nil}\t\t}\t\terr := resources.cognitoActor.UpdateTriggers(ctx, resources.userPoolId, triggerList...)\t\tif err != nil {\t\t\tlog.Println(\"Couldn't update Cognito triggers during cleanup.\")\t\t\tpanic(err)\t\t}\t\tlog.Println(\"Removed Cognito triggers from user pool.\")\t} else {\t\tlog.Println(\"Be sure to remove resources when you're done with them to avoid unexpected charges!\")\t}}For API details, see the following topics in AWS SDK for Go API Reference.AdminCreateUserAdminSetUserPasswordDeleteUserInitiateAuthUpdateUserPoolanchorGoSDK for Go V2Note        There's more on GitHub. Find the complete example and learn how to set up and run in the        AWS Code            Examples Repository.    Run an interactive scenario at a command prompt.import (\t\"context\"\t\"errors\"\t\"log\"\t\"strings\"\t\"user_pools_and_lambda_triggers/actions\"\t\"github.com/aws/aws-sdk-go-v2/aws\"\t\"github.com/aws/aws-sdk-go-v2/service/cognitoidentityprovider\"\t\"github.com/aws/aws-sdk-go-v2/service/cognitoidentityprovider/types\"\t\"github.com/awsdocs/aws-doc-sdk-examples/gov2/demotools\")// ActivityLog separates the steps of this scenario into individual functions so that// they are simpler to read and understand.type ActivityLog struct {\thelper       IScenarioHelper\tquestioner   demotools.IQuestioner\tresources    Resources\tcognitoActor *actions.CognitoActions}// NewActivityLog constructs a new activity log runner.func NewActivityLog(sdkConfig aws.Config, questioner demotools.IQuestioner, helper IScenarioHelper) ActivityLog {\tscenario := ActivityLog{\t\thelper:       helper,\t\tquestioner:   questioner,\t\tresources:    Resources{},\t\tcognitoActor: &actions.CognitoActions{CognitoClient: cognitoidentityprovider.NewFromConfig(sdkConfig)},\t}\tscenario.resources.init(scenario.cognitoActor, questioner)\treturn scenario}// AddUserToPool selects a user from the known users table and uses administrator credentials to add the user to the user pool.func (runner *ActivityLog) AddUserToPool(ctx context.Context, userPoolId string, tableName string) (string, string) {\tlog.Println(\"To facilitate this example, let's add a user to the user pool using administrator privileges.\")\tusers, err := runner.helper.GetKnownUsers(ctx, tableName)\tif err != nil {\t\tpanic(err)\t}\tuser := users.Users[0]\tlog.Printf(\"Adding known user %v to the user pool.\\n\", user.UserName)\terr = runner.cognitoActor.AdminCreateUser(ctx, userPoolId, user.UserName, user.UserEmail)\tif err != nil {\t\tpanic(err)\t}\tpwSet := false\tpassword := runner.questioner.AskPassword(\"\\nEnter a password that has at least eight characters, uppercase, lowercase, numbers and symbols.\\n\"+\t\t\"(the password will not display as you type):\", 8)\tfor !pwSet {\t\tlog.Printf(\"\\nSetting password for user '%v'.\\n\", user.UserName)\t\terr = runner.cognitoActor.AdminSetUserPassword(ctx, userPoolId, user.UserName, password)\t\tif err != nil {\t\t\tvar invalidPassword *types.InvalidPasswordException\t\t\tif errors.As(err, &invalidPassword) {\t\t\t\tpassword = runner.questioner.AskPassword(\"\\nEnter another password:\", 8)\t\t\t} else {\t\t\t\tpanic(err)\t\t\t}\t\t} else {\t\t\tpwSet = true\t\t}\t}\tlog.Println(strings.Repeat(\"-\", 88))\treturn user.UserName, password}// AddActivityLogTrigger adds a Lambda handler as an invocation target for the PostAuthentication trigger.func (runner *ActivityLog) AddActivityLogTrigger(ctx context.Context, userPoolId string, activityLogArn string) {\tlog.Println(\"Let's add a Lambda function to handle the PostAuthentication trigger from Cognito.\\n\" +\t\t\"This trigger happens after a user is authenticated, and lets your function take action, such as logging\\n\" +\t\t\"the outcome.\")\terr := runner.cognitoActor.UpdateTriggers(\t\tctx, userPoolId,\t\tactions.TriggerInfo{Trigger: actions.PostAuthentication, HandlerArn: aws.String(activityLogArn)})\tif err != nil {\t\tpanic(err)\t}\trunner.resources.triggers = append(runner.resources.triggers, actions.PostAuthentication)\tlog.Printf(\"Lambda function %v added to user pool %v to handle PostAuthentication Cognito trigger.\\n\",\t\tactivityLogArn, userPoolId)\tlog.Println(strings.Repeat(\"-\", 88))}// SignInUser signs in as the specified user.func (runner *ActivityLog) SignInUser(ctx context.Context, clientId string, userName string, password string) {\tlog.Printf(\"Now we'll sign in user %v and check the results in the logs and the DynamoDB table.\", userName)\trunner.questioner.Ask(\"Press Enter when you're ready.\")\tauthResult, err := runner.cognitoActor.SignIn(ctx, clientId, userName, password)\tif err != nil {\t\tpanic(err)\t}\tlog.Println(\"Sign in successful.\",\t\t\"The PostAuthentication Lambda handler writes custom information to CloudWatch Logs.\")\trunner.resources.userAccessTokens = append(runner.resources.userAccessTokens, *authResult.AccessToken)}// GetKnownUserLastLogin gets the login info for a user from the Amazon DynamoDB table and displays it.func (runner *ActivityLog) GetKnownUserLastLogin(ctx context.Context, tableName string, userName string) {\tlog.Println(\"The PostAuthentication handler also writes login data to the DynamoDB table.\")\trunner.questioner.Ask(\"Press Enter when you're ready to continue.\")\tusers, err := runner.helper.GetKnownUsers(ctx, tableName)\tif err != nil {\t\tpanic(err)\t}\tfor _, user := range users.Users {\t\tif user.UserName == userName {\t\t\tlog.Println(\"The last login info for the user in the known users table is:\")\t\t\tlog.Printf(\"\\t%+v\", *user.LastLogin)\t\t}\t}\tlog.Println(strings.Repeat(\"-\", 88))}// Run runs the scenario.func (runner *ActivityLog) Run(ctx context.Context, stackName string) {\tdefer func() {\t\tif r := recover(); r != nil {\t\t\tlog.Println(\"Something went wrong with the demo.\")\t\t\trunner.resources.Cleanup(ctx)\t\t}\t}()\tlog.Println(strings.Repeat(\"-\", 88))\tlog.Printf(\"Welcome\\n\")\tlog.Println(strings.Repeat(\"-\", 88))\tstackOutputs, err := runner.helper.GetStackOutputs(ctx, stackName)\tif err != nil {\t\tpanic(err)\t}\trunner.resources.userPoolId = stackOutputs[\"UserPoolId\"]\trunner.helper.PopulateUserTable(ctx, stackOutputs[\"TableName\"])\tuserName, password := runner.AddUserToPool(ctx, stackOutputs[\"UserPoolId\"], stackOutputs[\"TableName\"])\trunner.AddActivityLogTrigger(ctx, stackOutputs[\"UserPoolId\"], stackOutputs[\"ActivityLogFunctionArn\"])\trunner.SignInUser(ctx, stackOutputs[\"UserPoolClientId\"], userName, password)\trunner.helper.ListRecentLogEvents(ctx, stackOutputs[\"ActivityLogFunction\"])\trunner.GetKnownUserLastLogin(ctx, stackOutputs[\"TableName\"], userName)\trunner.resources.Cleanup(ctx)\tlog.Println(strings.Repeat(\"-\", 88))\tlog.Println(\"Thanks for watching!\")\tlog.Println(strings.Repeat(\"-\", 88))}Handle the PostAuthentication trigger with a Lambda function.import (\t\"context\"\t\"fmt\"\t\"log\"\t\"os\"\t\"time\"\t\"github.com/aws/aws-lambda-go/events\"\t\"github.com/aws/aws-lambda-go/lambda\"\t\"github.com/aws/aws-sdk-go-v2/aws\"\t\"github.com/aws/aws-sdk-go-v2/config\"\t\"github.com/aws/aws-sdk-go-v2/feature/dynamodb/attributevalue\"\t\"github.com/aws/aws-sdk-go-v2/service/dynamodb\"\tdynamodbtypes \"github.com/aws/aws-sdk-go-v2/service/dynamodb/types\")const TABLE_NAME = \"TABLE_NAME\"// LoginInfo defines structured login data that can be marshalled to a DynamoDB format.type LoginInfo struct {\tUserPoolId string `dynamodbav:\"UserPoolId\"`\tClientId   string `dynamodbav:\"ClientId\"`\tTime       string `dynamodbav:\"Time\"`}// UserInfo defines structured user data that can be marshalled to a DynamoDB format.type UserInfo struct {\tUserName  string    `dynamodbav:\"UserName\"`\tUserEmail string    `dynamodbav:\"UserEmail\"`\tLastLogin LoginInfo `dynamodbav:\"LastLogin\"`}// GetKey marshals the user email value to a DynamoDB key format.func (user UserInfo) GetKey() map[string]dynamodbtypes.AttributeValue {\tuserEmail, err := attributevalue.Marshal(user.UserEmail)\tif err != nil {\t\tpanic(err)\t}\treturn map[string]dynamodbtypes.AttributeValue{\"UserEmail\": userEmail}}type handler struct {\tdynamoClient *dynamodb.Client}// HandleRequest handles the PostAuthentication event by writing custom data to the logs and// to an Amazon DynamoDB table.func (h *handler) HandleRequest(ctx context.Context, event events.CognitoEventUserPoolsPostAuthentication) (events.CognitoEventUserPoolsPostAuthentication, error) {\tlog.Printf(\"Received post authentication trigger from %v for user '%v'\", event.TriggerSource, event.UserName)\ttableName := os.Getenv(TABLE_NAME)\tuser := UserInfo{\t\tUserName:  event.UserName,\t\tUserEmail: event.Request.UserAttributes[\"email\"],\t\tLastLogin: LoginInfo{\t\t\tUserPoolId: event.UserPoolID,\t\t\tClientId:   event.CallerContext.ClientID,\t\t\tTime:       time.Now().Format(time.UnixDate),\t\t},\t}\t// Write to CloudWatch Logs.\tfmt.Printf(\"%#v\", user)\t// Also write to an external system. This examples uses DynamoDB to demonstrate.\tuserMap, err := attributevalue.MarshalMap(user)\tif err != nil {\t\tlog.Printf(\"Couldn't marshal to DynamoDB map. Here's why: %v\\n\", err)\t} else if len(userMap) == 0 {\t\tlog.Printf(\"User info marshaled to an empty map.\")\t} else {\t\t_, err := h.dynamoClient.PutItem(ctx, &dynamodb.PutItemInput{\t\t\tItem:      userMap,\t\t\tTableName: aws.String(tableName),\t\t})\t\tif err != nil {\t\t\tlog.Printf(\"Couldn't write to DynamoDB. Here's why: %v\\n\", err)\t\t} else {\t\t\tlog.Printf(\"Wrote user info to DynamoDB table %v.\\n\", tableName)\t\t}\t}\treturn event, nil}func main() {\tctx := context.Background()\tsdkConfig, err := config.LoadDefaultConfig(ctx)\tif err != nil {\t\tlog.Panicln(err)\t}\th := handler{\t\tdynamoClient: dynamodb.NewFromConfig(sdkConfig),\t}\tlambda.Start(h.HandleRequest)}Create a struct that performs common tasks.import (\t\"context\"\t\"log\"\t\"strings\"\t\"time\"\t\"user_pools_and_lambda_triggers/actions\"\t\"github.com/aws/aws-sdk-go-v2/aws\"\t\"github.com/aws/aws-sdk-go-v2/service/cloudformation\"\t\"github.com/aws/aws-sdk-go-v2/service/cloudwatchlogs\"\t\"github.com/aws/aws-sdk-go-v2/service/dynamodb\"\t\"github.com/awsdocs/aws-doc-sdk-examples/gov2/demotools\")// IScenarioHelper defines common functions used by the workflows in this example.type IScenarioHelper interface {\tPause(secs int)\tGetStackOutputs(ctx context.Context, stackName string) (actions.StackOutputs, error)\tPopulateUserTable(ctx context.Context, tableName string)\tGetKnownUsers(ctx context.Context, tableName string) (actions.UserList, error)\tAddKnownUser(ctx context.Context, tableName string, user actions.User)\tListRecentLogEvents(ctx context.Context, functionName string)}// ScenarioHelper contains AWS wrapper structs used by the workflows in this example.type ScenarioHelper struct {\tquestioner  demotools.IQuestioner\tdynamoActor *actions.DynamoActions\tcfnActor    *actions.CloudFormationActions\tcwlActor    *actions.CloudWatchLogsActions\tisTestRun   bool}// NewScenarioHelper constructs a new scenario helper.func NewScenarioHelper(sdkConfig aws.Config, questioner demotools.IQuestioner) ScenarioHelper {\tscenario := ScenarioHelper{\t\tquestioner:  questioner,\t\tdynamoActor: &actions.DynamoActions{DynamoClient: dynamodb.NewFromConfig(sdkConfig)},\t\tcfnActor:    &actions.CloudFormationActions{CfnClient: cloudformation.NewFromConfig(sdkConfig)},\t\tcwlActor:    &actions.CloudWatchLogsActions{CwlClient: cloudwatchlogs.NewFromConfig(sdkConfig)},\t}\treturn scenario}// Pause waits for the specified number of seconds.func (helper ScenarioHelper) Pause(secs int) {\tif !helper.isTestRun {\t\ttime.Sleep(time.Duration(secs) * time.Second)\t}}// GetStackOutputs gets the outputs from the specified CloudFormation stack in a structured format.func (helper ScenarioHelper) GetStackOutputs(ctx context.Context, stackName string) (actions.StackOutputs, error) {\treturn helper.cfnActor.GetOutputs(ctx, stackName), nil}// PopulateUserTable fills the known user table with example data.func (helper ScenarioHelper) PopulateUserTable(ctx context.Context, tableName string) {\tlog.Printf(\"First, let's add some users to the DynamoDB %v table we'll use for this example.\\n\", tableName)\terr := helper.dynamoActor.PopulateTable(ctx, tableName)\tif err != nil {\t\tpanic(err)\t}}// GetKnownUsers gets the users from the known users table in a structured format.func (helper ScenarioHelper) GetKnownUsers(ctx context.Context, tableName string) (actions.UserList, error) {\tknownUsers, err := helper.dynamoActor.Scan(ctx, tableName)\tif err != nil {\t\tlog.Printf(\"Couldn't get known users from table %v. Here's why: %v\\n\", tableName, err)\t}\treturn knownUsers, err}// AddKnownUser adds a user to the known users table.func (helper ScenarioHelper) AddKnownUser(ctx context.Context, tableName string, user actions.User) {\tlog.Printf(\"Adding user '%v' with email '%v' to the DynamoDB known users table...\\n\",\t\tuser.UserName, user.UserEmail)\terr := helper.dynamoActor.AddUser(ctx, tableName, user)\tif err != nil {\t\tpanic(err)\t}}// ListRecentLogEvents gets the most recent log stream and events for the specified Lambda function and displays them.func (helper ScenarioHelper) ListRecentLogEvents(ctx context.Context, functionName string) {\tlog.Println(\"Waiting a few seconds to let Lambda write to CloudWatch Logs...\")\thelper.Pause(10)\tlog.Println(\"Okay, let's check the logs to find what's happened recently with your Lambda function.\")\tlogStream, err := helper.cwlActor.GetLatestLogStream(ctx, functionName)\tif err != nil {\t\tpanic(err)\t}\tlog.Printf(\"Getting some recent events from log stream %v\\n\", *logStream.LogStreamName)\tevents, err := helper.cwlActor.GetLogEvents(ctx, functionName, *logStream.LogStreamName, 10)\tif err != nil {\t\tpanic(err)\t}\tfor _, event := range events {\t\tlog.Printf(\"\\t%v\", *event.Message)\t}\tlog.Println(strings.Repeat(\"-\", 88))}Create a struct that wraps Amazon Cognito actions.import (\t\"context\"\t\"errors\"\t\"log\"\t\"github.com/aws/aws-sdk-go-v2/aws\"\t\"github.com/aws/aws-sdk-go-v2/service/cognitoidentityprovider\"\t\"github.com/aws/aws-sdk-go-v2/service/cognitoidentityprovider/types\")type CognitoActions struct {\tCognitoClient *cognitoidentityprovider.Client}// Trigger and TriggerInfo define typed data for updating an Amazon Cognito trigger.type Trigger intconst (\tPreSignUp Trigger = iota\tUserMigration\tPostAuthentication)type TriggerInfo struct {\tTrigger    Trigger\tHandlerArn *string}// UpdateTriggers adds or removes Lambda triggers for a user pool. When a trigger is specified with a `nil` value,// it is removed from the user pool.func (actor CognitoActions) UpdateTriggers(ctx context.Context, userPoolId string, triggers ...TriggerInfo) error {\toutput, err := actor.CognitoClient.DescribeUserPool(ctx, &cognitoidentityprovider.DescribeUserPoolInput{\t\tUserPoolId: aws.String(userPoolId),\t})\tif err != nil {\t\tlog.Printf(\"Couldn't get info about user pool %v. Here's why: %v\\n\", userPoolId, err)\t\treturn err\t}\tlambdaConfig := output.UserPool.LambdaConfig\tfor _, trigger := range triggers {\t\tswitch trigger.Trigger {\t\tcase PreSignUp:\t\t\tlambdaConfig.PreSignUp = trigger.HandlerArn\t\tcase UserMigration:\t\t\tlambdaConfig.UserMigration = trigger.HandlerArn\t\tcase PostAuthentication:\t\t\tlambdaConfig.PostAuthentication = trigger.HandlerArn\t\t}\t}\t_, err = actor.CognitoClient.UpdateUserPool(ctx, &cognitoidentityprovider.UpdateUserPoolInput{\t\tUserPoolId:   aws.String(userPoolId),\t\tLambdaConfig: lambdaConfig,\t})\tif err != nil {\t\tlog.Printf(\"Couldn't update user pool %v. Here's why: %v\\n\", userPoolId, err)\t}\treturn err}// SignUp signs up a user with Amazon Cognito.func (actor CognitoActions) SignUp(ctx context.Context, clientId string, userName string, password string, userEmail string) (bool, error) {\tconfirmed := false\toutput, err := actor.CognitoClient.SignUp(ctx, &cognitoidentityprovider.SignUpInput{\t\tClientId: aws.String(clientId),\t\tPassword: aws.String(password),\t\tUsername: aws.String(userName),\t\tUserAttributes: []types.AttributeType{\t\t\t{Name: aws.String(\"email\"), Value: aws.String(userEmail)},\t\t},\t})\tif err != nil {\t\tvar invalidPassword *types.InvalidPasswordException\t\tif errors.As(err, &invalidPassword) {\t\t\tlog.Println(*invalidPassword.Message)\t\t} else {\t\t\tlog.Printf(\"Couldn't sign up user %v. Here's why: %v\\n\", userName, err)\t\t}\t} else {\t\tconfirmed = output.UserConfirmed\t}\treturn confirmed, err}// SignIn signs in a user to Amazon Cognito using a username and password authentication flow.func (actor CognitoActions) SignIn(ctx context.Context, clientId string, userName string, password string) (*types.AuthenticationResultType, error) {\tvar authResult *types.AuthenticationResultType\toutput, err := actor.CognitoClient.InitiateAuth(ctx, &cognitoidentityprovider.InitiateAuthInput{\t\tAuthFlow:       \"USER_PASSWORD_AUTH\",\t\tClientId:       aws.String(clientId),\t\tAuthParameters: map[string]string{\"USERNAME\": userName, \"PASSWORD\": password},\t})\tif err != nil {\t\tvar resetRequired *types.PasswordResetRequiredException\t\tif errors.As(err, &resetRequired) {\t\t\tlog.Println(*resetRequired.Message)\t\t} else {\t\t\tlog.Printf(\"Couldn't sign in user %v. Here's why: %v\\n\", userName, err)\t\t}\t} else {\t\tauthResult = output.AuthenticationResult\t}\treturn authResult, err}// ForgotPassword starts a password recovery flow for a user. This flow typically sends a confirmation code// to the user's configured notification destination, such as email.func (actor CognitoActions) ForgotPassword(ctx context.Context, clientId string, userName string) (*types.CodeDeliveryDetailsType, error) {\toutput, err := actor.CognitoClient.ForgotPassword(ctx, &cognitoidentityprovider.ForgotPasswordInput{\t\tClientId: aws.String(clientId),\t\tUsername: aws.String(userName),\t})\tif err != nil {\t\tlog.Printf(\"Couldn't start password reset for user '%v'. Here;s why: %v\\n\", userName, err)\t}\treturn output.CodeDeliveryDetails, err}// ConfirmForgotPassword confirms a user with a confirmation code and a new password.func (actor CognitoActions) ConfirmForgotPassword(ctx context.Context, clientId string, code string, userName string, password string) error {\t_, err := actor.CognitoClient.ConfirmForgotPassword(ctx, &cognitoidentityprovider.ConfirmForgotPasswordInput{\t\tClientId:         aws.String(clientId),\t\tConfirmationCode: aws.String(code),\t\tPassword:         aws.String(password),\t\tUsername:         aws.String(userName),\t})\tif err != nil {\t\tvar invalidPassword *types.InvalidPasswordException\t\tif errors.As(err, &invalidPassword) {\t\t\tlog.Println(*invalidPassword.Message)\t\t} else {\t\t\tlog.Printf(\"Couldn't confirm user %v. Here's why: %v\", userName, err)\t\t}\t}\treturn err}// DeleteUser removes a user from the user pool.func (actor CognitoActions) DeleteUser(ctx context.Context, userAccessToken string) error {\t_, err := actor.CognitoClient.DeleteUser(ctx, &cognitoidentityprovider.DeleteUserInput{\t\tAccessToken: aws.String(userAccessToken),\t})\tif err != nil {\t\tlog.Printf(\"Couldn't delete user. Here's why: %v\\n\", err)\t}\treturn err}// AdminCreateUser uses administrator credentials to add a user to a user pool. This method leaves the user// in a state that requires they enter a new password next time they sign in.func (actor CognitoActions) AdminCreateUser(ctx context.Context, userPoolId string, userName string, userEmail string) error {\t_, err := actor.CognitoClient.AdminCreateUser(ctx, &cognitoidentityprovider.AdminCreateUserInput{\t\tUserPoolId:     aws.String(userPoolId),\t\tUsername:       aws.String(userName),\t\tMessageAction:  types.MessageActionTypeSuppress,\t\tUserAttributes: []types.AttributeType{{Name: aws.String(\"email\"), Value: aws.String(userEmail)}},\t})\tif err != nil {\t\tvar userExists *types.UsernameExistsException\t\tif errors.As(err, &userExists) {\t\t\tlog.Printf(\"User %v already exists in the user pool.\", userName)\t\t\terr = nil\t\t} else {\t\t\tlog.Printf(\"Couldn't create user %v. Here's why: %v\\n\", userName, err)\t\t}\t}\treturn err}// AdminSetUserPassword uses administrator credentials to set a password for a user without requiring a// temporary password.func (actor CognitoActions) AdminSetUserPassword(ctx context.Context, userPoolId string, userName string, password string) error {\t_, err := actor.CognitoClient.AdminSetUserPassword(ctx, &cognitoidentityprovider.AdminSetUserPasswordInput{\t\tPassword:   aws.String(password),\t\tUserPoolId: aws.String(userPoolId),\t\tUsername:   aws.String(userName),\t\tPermanent:  true,\t})\tif err != nil {\t\tvar invalidPassword *types.InvalidPasswordException\t\tif errors.As(err, &invalidPassword) {\t\t\tlog.Println(*invalidPassword.Message)\t\t} else {\t\t\tlog.Printf(\"Couldn't set password for user %v. Here's why: %v\\n\", userName, err)\t\t}\t}\treturn err}Create a struct that wraps DynamoDB actions.import (\t\"context\"\t\"fmt\"\t\"log\"\t\"github.com/aws/aws-sdk-go-v2/aws\"\t\"github.com/aws/aws-sdk-go-v2/feature/dynamodb/attributevalue\"\t\"github.com/aws/aws-sdk-go-v2/service/dynamodb\"\t\"github.com/aws/aws-sdk-go-v2/service/dynamodb/types\")// DynamoActions encapsulates the Amazon Simple Notification Service (Amazon SNS) actions// used in the examples.type DynamoActions struct {\tDynamoClient *dynamodb.Client}// User defines structured user data.type User struct {\tUserName  string\tUserEmail string\tLastLogin *LoginInfo `dynamodbav:\",omitempty\"`}// LoginInfo defines structured custom login data.type LoginInfo struct {\tUserPoolId string\tClientId   string\tTime       string}// UserList defines a list of users.type UserList struct {\tUsers []User}// UserNameList returns the usernames contained in a UserList as a list of strings.func (users *UserList) UserNameList() []string {\tnames := make([]string, len(users.Users))\tfor i := 0; i < len(users.Users); i++ {\t\tnames[i] = users.Users[i].UserName\t}\treturn names}// PopulateTable adds a set of test users to the table.func (actor DynamoActions) PopulateTable(ctx context.Context, tableName string) error {\tvar err error\tvar item map[string]types.AttributeValue\tvar writeReqs []types.WriteRequest\tfor i := 1; i < 4; i++ {\t\titem, err = attributevalue.MarshalMap(User{UserName: fmt.Sprintf(\"test_user_%v\", i), UserEmail: fmt.Sprintf(\"test_email_%v@example.com\", i)})\t\tif err != nil {\t\t\tlog.Printf(\"Couldn't marshall user into DynamoDB format. Here's why: %v\\n\", err)\t\t\treturn err\t\t}\t\twriteReqs = append(writeReqs, types.WriteRequest{PutRequest: &types.PutRequest{Item: item}})\t}\t_, err = actor.DynamoClient.BatchWriteItem(ctx, &dynamodb.BatchWriteItemInput{\t\tRequestItems: map[string][]types.WriteRequest{tableName: writeReqs},\t})\tif err != nil {\t\tlog.Printf(\"Couldn't populate table %v with users. Here's why: %v\\n\", tableName, err)\t}\treturn err}// Scan scans the table for all items.func (actor DynamoActions) Scan(ctx context.Context, tableName string) (UserList, error) {\tvar userList UserList\toutput, err := actor.DynamoClient.Scan(ctx, &dynamodb.ScanInput{\t\tTableName: aws.String(tableName),\t})\tif err != nil {\t\tlog.Printf(\"Couldn't scan table %v for items. Here's why: %v\\n\", tableName, err)\t} else {\t\terr = attributevalue.UnmarshalListOfMaps(output.Items, &userList.Users)\t\tif err != nil {\t\t\tlog.Printf(\"Couldn't unmarshal items into users. Here's why: %v\\n\", err)\t\t}\t}\treturn userList, err}// AddUser adds a user item to a table.func (actor DynamoActions) AddUser(ctx context.Context, tableName string, user User) error {\tuserItem, err := attributevalue.MarshalMap(user)\tif err != nil {\t\tlog.Printf(\"Couldn't marshall user to item. Here's why: %v\\n\", err)\t}\t_, err = actor.DynamoClient.PutItem(ctx, &dynamodb.PutItemInput{\t\tItem:      userItem,\t\tTableName: aws.String(tableName),\t})\tif err != nil {\t\tlog.Printf(\"Couldn't put item in table %v. Here's why: %v\", tableName, err)\t}\treturn err}Create a struct that wraps CloudWatch Logs actions.import (\t\"context\"\t\"fmt\"\t\"log\"\t\"github.com/aws/aws-sdk-go-v2/aws\"\t\"github.com/aws/aws-sdk-go-v2/service/cloudwatchlogs\"\t\"github.com/aws/aws-sdk-go-v2/service/cloudwatchlogs/types\")type CloudWatchLogsActions struct {\tCwlClient *cloudwatchlogs.Client}// GetLatestLogStream gets the most recent log stream for a Lambda function.func (actor CloudWatchLogsActions) GetLatestLogStream(ctx context.Context, functionName string) (types.LogStream, error) {\tvar logStream types.LogStream\tlogGroupName := fmt.Sprintf(\"/aws/lambda/%s\", functionName)\toutput, err := actor.CwlClient.DescribeLogStreams(ctx, &cloudwatchlogs.DescribeLogStreamsInput{\t\tDescending:   aws.Bool(true),\t\tLimit:        aws.Int32(1),\t\tLogGroupName: aws.String(logGroupName),\t\tOrderBy:      types.OrderByLastEventTime,\t})\tif err != nil {\t\tlog.Printf(\"Couldn't get log streams for log group %v. Here's why: %v\\n\", logGroupName, err)\t} else {\t\tlogStream = output.LogStreams[0]\t}\treturn logStream, err}// GetLogEvents gets the most recent eventCount events from the specified log stream.func (actor CloudWatchLogsActions) GetLogEvents(ctx context.Context, functionName string, logStreamName string, eventCount int32) (\t[]types.OutputLogEvent, error) {\tvar events []types.OutputLogEvent\tlogGroupName := fmt.Sprintf(\"/aws/lambda/%s\", functionName)\toutput, err := actor.CwlClient.GetLogEvents(ctx, &cloudwatchlogs.GetLogEventsInput{\t\tLogStreamName: aws.String(logStreamName),\t\tLimit:         aws.Int32(eventCount),\t\tLogGroupName:  aws.String(logGroupName),\t})\tif err != nil {\t\tlog.Printf(\"Couldn't get log event for log stream %v. Here's why: %v\\n\", logStreamName, err)\t} else {\t\tevents = output.Events\t}\treturn events, err}Create a struct that wraps AWS CloudFormation actions.import (\t\"context\"\t\"log\"\t\"github.com/aws/aws-sdk-go-v2/aws\"\t\"github.com/aws/aws-sdk-go-v2/service/cloudformation\")// StackOutputs defines a map of outputs from a specific stack.type StackOutputs map[string]stringtype CloudFormationActions struct {\tCfnClient *cloudformation.Client}// GetOutputs gets the outputs from a CloudFormation stack and puts them into a structured format.func (actor CloudFormationActions) GetOutputs(ctx context.Context, stackName string) StackOutputs {\toutput, err := actor.CfnClient.DescribeStacks(ctx, &cloudformation.DescribeStacksInput{\t\tStackName: aws.String(stackName),\t})\tif err != nil || len(output.Stacks) == 0 {\t\tlog.Panicf(\"Couldn't find a CloudFormation stack named %v. Here's why: %v\\n\", stackName, err)\t}\tstackOutputs := StackOutputs{}\tfor _, out := range output.Stacks[0].Outputs {\t\tstackOutputs[*out.OutputKey] = *out.OutputValue\t}\treturn stackOutputs}Clean up resources.import (\t\"context\"\t\"log\"\t\"user_pools_and_lambda_triggers/actions\"\t\"github.com/awsdocs/aws-doc-sdk-examples/gov2/demotools\")// Resources keeps track of AWS resources created during an example and handles// cleanup when the example finishes.type Resources struct {\tuserPoolId       string\tuserAccessTokens []string\ttriggers         []actions.Trigger\tcognitoActor *actions.CognitoActions\tquestioner   demotools.IQuestioner}func (resources *Resources) init(cognitoActor *actions.CognitoActions, questioner demotools.IQuestioner) {\tresources.userAccessTokens = []string{}\tresources.triggers = []actions.Trigger{}\tresources.cognitoActor = cognitoActor\tresources.questioner = questioner}// Cleanup deletes all AWS resources created during an example.func (resources *Resources) Cleanup(ctx context.Context) {\tdefer func() {\t\tif r := recover(); r != nil {\t\t\tlog.Printf(\"Something went wrong during cleanup.\\n%v\\n\", r)\t\t\tlog.Println(\"Use the AWS Management Console to remove any remaining resources \\n\" +\t\t\t\t\"that were created for this scenario.\")\t\t}\t}()\twantDelete := resources.questioner.AskBool(\"Do you want to remove all of the AWS resources that were created \"+\t\t\"during this demo (y/n)?\", \"y\")\tif wantDelete {\t\tfor _, accessToken := range resources.userAccessTokens {\t\t\terr := resources.cognitoActor.DeleteUser(ctx, accessToken)\t\t\tif err != nil {\t\t\t\tlog.Println(\"Couldn't delete user during cleanup.\")\t\t\t\tpanic(err)\t\t\t}\t\t\tlog.Println(\"Deleted user.\")\t\t}\t\ttriggerList := make([]actions.TriggerInfo, len(resources.triggers))\t\tfor i := 0; i < len(resources.triggers); i++ {\t\t\ttriggerList[i] = actions.TriggerInfo{Trigger: resources.triggers[i], HandlerArn: nil}\t\t}\t\terr := resources.cognitoActor.UpdateTriggers(ctx, resources.userPoolId, triggerList...)\t\tif err != nil {\t\t\tlog.Println(\"Couldn't update Cognito triggers during cleanup.\")\t\t\tpanic(err)\t\t}\t\tlog.Println(\"Removed Cognito triggers from user pool.\")\t} else {\t\tlog.Println(\"Be sure to remove resources when you're done with them to avoid unexpected charges!\")\t}}For API details, see the following topics in AWS SDK for Go API Reference.AdminCreateUserAdminSetUserPasswordDeleteUserInitiateAuthUpdateUserPool",
                            "For a complete list of AWS SDK developer guides and code examples, see    Using Lambda with an AWS SDK.    This topic also includes information about getting started and details about previous SDK versions."
                        ]
                    }
                ],
                "sections": [
                    "",
                    "The following code examples show you how to implement common scenarios in Lambda        with AWS SDKs. These scenarios show you how to accomplish specific tasks by calling multiple functions        within Lambda or combined with other AWS services.        Each scenario includes a link to the complete source code, where you can find instructions on how to set up and run the code.    ",
                    "Scenarios target an intermediate level of experience to help you understand service actions in context.",
                    "  1.Automatically confirm known users with a Lambda function",
                    "  2.Automatically migrate known users with a Lambda function",
                    "  3.Create a REST API to track COVID-19 data",
                    "  4.Create a lending library REST API",
                    "  5.Create a messenger application",
                    "  6.Create a serverless application to manage photos",
                    "  7.Create a websocket chat application",
                    "  8.Create an application to analyze customer feedback",
                    "  9.Invoke a Lambda function from a browser",
                    "  10.Transform data with S3 Object Lambda",
                    "  11.Use API Gateway to invoke a Lambda function",
                    "  12.Use Step Functions to invoke Lambda functions",
                    "  13.Use scheduled events to invoke a Lambda function",
                    "  14.Write custom activity data with a Lambda function after Amazon Cognito user authentication"
                ]
            },
            {
                "title": "Serverless examples",
                "href": "https://docs.aws.amazon.com/lambda/latest/dg/service_code_examples_serverless_examples.html",
                "contents": [
                    {
                        "title": "Connecting to an Amazon RDS database in a Lambda function",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_serverless_connect_RDS_Lambda_section.html",
                        "sections": [
                            "",
                            "The following code examples show how to implement a Lambda function that connects to an RDS database. The function makes a simple database request and returns the result.",
                            "GoSDK for Go V2Note        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Connecting to an Amazon RDS database in a Lambda function using Go./*Golang v2 code here.*/package mainimport (\t\"context\"\t\"database/sql\"\t\"encoding/json\"\t\"fmt\"\t\"os\"\t\"github.com/aws/aws-lambda-go/lambda\"\t\"github.com/aws/aws-sdk-go-v2/config\"\t\"github.com/aws/aws-sdk-go-v2/feature/rds/auth\"\t_ \"github.com/go-sql-driver/mysql\")type MyEvent struct {\tName string `json:\"name\"`}func HandleRequest(event *MyEvent) (map[string]interface{}, error) {\tvar dbName string = os.Getenv(\"DatabaseName\")\tvar dbUser string = os.Getenv(\"DatabaseUser\")\tvar dbHost string = os.Getenv(\"DBHost\") // Add hostname without https\tvar dbPort int = os.Getenv(\"Port\")      // Add port number\tvar dbEndpoint string = fmt.Sprintf(\"%s:%d\", dbHost, dbPort)\tvar region string = os.Getenv(\"AWS_REGION\")\tcfg, err := config.LoadDefaultConfig(context.TODO())\tif err != nil {\t\tpanic(\"configuration error: \" + err.Error())\t}\tauthenticationToken, err := auth.BuildAuthToken(\t\tcontext.TODO(), dbEndpoint, region, dbUser, cfg.Credentials)\tif err != nil {\t\tpanic(\"failed to create authentication token: \" + err.Error())\t}\tdsn := fmt.Sprintf(\"%s:%s@tcp(%s)/%s?tls=true&allowCleartextPasswords=true\",\t\tdbUser, authenticationToken, dbEndpoint, dbName,\t)\tdb, err := sql.Open(\"mysql\", dsn)\tif err != nil {\t\tpanic(err)\t}\tdefer db.Close()\tvar sum int\terr = db.QueryRow(\"SELECT ?+? AS sum\", 3, 2).Scan(&sum)\tif err != nil {\t\tpanic(err)\t}\ts := fmt.Sprint(sum)\tmessage := fmt.Sprintf(\"The selected sum is: %s\", s)\tmessageBytes, err := json.Marshal(message)\tif err != nil {\t\treturn nil, err\t}\tmessageString := string(messageBytes)\treturn map[string]interface{}{\t\t\"statusCode\": 200,\t\t\"headers\":    map[string]string{\"Content-Type\": \"application/json\"},\t\t\"body\":       messageString,\t}, nil}func main() {\tlambda.Start(HandleRequest)}JavaSDK for Java 2.xNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Connecting to an Amazon RDS database in a Lambda function using Java.import com.amazonaws.services.lambda.runtime.Context;import com.amazonaws.services.lambda.runtime.RequestHandler;import com.amazonaws.services.lambda.runtime.events.APIGatewayProxyRequestEvent;import com.amazonaws.services.lambda.runtime.events.APIGatewayProxyResponseEvent;import software.amazon.awssdk.auth.credentials.DefaultCredentialsProvider;import software.amazon.awssdk.regions.Region;import software.amazon.awssdk.services.rdsdata.RdsDataClient;import software.amazon.awssdk.services.rdsdata.model.ExecuteStatementRequest;import software.amazon.awssdk.services.rdsdata.model.ExecuteStatementResponse;import software.amazon.awssdk.services.rdsdata.model.Field;import java.sql.Connection;import java.sql.DriverManager;import java.sql.PreparedStatement;import java.sql.ResultSet;public class RdsLambdaHandler implements RequestHandler<APIGatewayProxyRequestEvent, APIGatewayProxyResponseEvent> {    @Override    public APIGatewayProxyResponseEvent handleRequest(APIGatewayProxyRequestEvent event, Context context) {        APIGatewayProxyResponseEvent response = new APIGatewayProxyResponseEvent();        try {            // Obtain auth token            String token = createAuthToken();            // Define connection configuration            String connectionString = String.format(\"jdbc:mysql://%s:%s/%s?useSSL=true&requireSSL=true\",                    System.getenv(\"ProxyHostName\"),                    System.getenv(\"Port\"),                    System.getenv(\"DBName\"));            // Establish a connection to the database            try (Connection connection = DriverManager.getConnection(connectionString, System.getenv(\"DBUserName\"), token);                 PreparedStatement statement = connection.prepareStatement(\"SELECT ? + ? AS sum\")) {                statement.setInt(1, 3);                statement.setInt(2, 2);                try (ResultSet resultSet = statement.executeQuery()) {                    if (resultSet.next()) {                        int sum = resultSet.getInt(\"sum\");                        response.setStatusCode(200);                        response.setBody(\"The selected sum is: \" + sum);                    }                }            }        } catch (Exception e) {            response.setStatusCode(500);            response.setBody(\"Error: \" + e.getMessage());        }        return response;    }    private String createAuthToken() {        // Create RDS Data Service client        RdsDataClient rdsDataClient = RdsDataClient.builder()                .region(Region.of(System.getenv(\"AWS_REGION\")))                .credentialsProvider(DefaultCredentialsProvider.create())                .build();        // Define authentication request        ExecuteStatementRequest request = ExecuteStatementRequest.builder()                .resourceArn(System.getenv(\"ProxyHostName\"))                .secretArn(System.getenv(\"DBUserName\"))                .database(System.getenv(\"DBName\"))                .sql(\"SELECT 'RDS IAM Authentication'\")                .build();        // Execute request and obtain authentication token        ExecuteStatementResponse response = rdsDataClient.executeStatement(request);        Field tokenField = response.records().get(0).get(0);        return tokenField.stringValue();    }}JavaScriptSDK for JavaScript (v3)Note        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Connecting to an Amazon RDS database in a Lambda function using JavaScript.// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.// SPDX-License-Identifier: Apache-2.0/* Node.js code here.*/// ES6+ exampleimport { Signer } from \"@aws-sdk/rds-signer\";import mysql from 'mysql2/promise';async function createAuthToken() {  // Define connection authentication parameters  const dbinfo = {    hostname: process.env.ProxyHostName,    port: process.env.Port,    username: process.env.DBUserName,    region: process.env.AWS_REGION,  }  // Create RDS Signer object  const signer = new Signer(dbinfo);  // Request authorization token from RDS, specifying the username  const token = await signer.getAuthToken();  return token;}async function dbOps() {  // Obtain auth token  const token = await createAuthToken();  // Define connection configuration  let connectionConfig = {    host: process.env.ProxyHostName,    user: process.env.DBUserName,    password: token,    database: process.env.DBName,    ssl: 'Amazon RDS'  }  // Create the connection to the DB  const conn = await mysql.createConnection(connectionConfig);  // Obtain the result of the query  const [res,] = await conn.execute('select ?+? as sum', [3, 2]);  return res;}export const handler = async (event) => {  // Execute database flow  const result = await dbOps();  // Return result  return {    statusCode: 200,    body: JSON.stringify(\"The selected sum is: \" + result[0].sum)  }};Connecting to an Amazon RDS database in a Lambda function using TypeScript.import { Signer } from \"@aws-sdk/rds-signer\";import mysql from 'mysql2/promise';// RDS settings// Using '!' (non-null assertion operator) to tell the TypeScript compiler that the DB settings are not null or undefined,const proxy_host_name = process.env.PROXY_HOST_NAME!const port = parseInt(process.env.PORT!)const db_name = process.env.DB_NAME!const db_user_name = process.env.DB_USER_NAME!const aws_region = process.env.AWS_REGION!async function createAuthToken(): Promise<string> {    // Create RDS Signer object    const signer = new Signer({        hostname: proxy_host_name,        port: port,        region: aws_region,        username: db_user_name    });    // Request authorization token from RDS, specifying the username    const token = await signer.getAuthToken();    return token;}async function dbOps(): Promise<mysql.QueryResult | undefined> {    try {        // Obtain auth token        const token = await createAuthToken();        const conn = await mysql.createConnection({            host: proxy_host_name,            user: db_user_name,            password: token,            database: db_name,            ssl: 'Amazon RDS' // Ensure you have the CA bundle for SSL connection        });        const [rows, fields] = await conn.execute('SELECT ? + ? AS sum', [3, 2]);        console.log('result:', rows);        return rows;    }    catch (err) {        console.log(err);    }}export const lambdaHandler = async (event: any): Promise<{ statusCode: number; body: string }> => {    // Execute database flow    const result = await dbOps();    // Return error is result is undefined    if (result == undefined)        return {            statusCode: 500,            body: JSON.stringify(`Error with connection to DB host`)        }    // Return result    return {        statusCode: 200,        body: JSON.stringify(`The selected sum is: ${result[0].sum}`)    };};PHPSDK for PHPNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Connecting to an Amazon RDS database in a Lambda function using PHP.<?php# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.# SPDX-License-Identifier: Apache-2.0# using bref/bref and bref/logger for simplicityuse Bref\\Context\\Context;use Bref\\Event\\Handler as StdHandler;use Bref\\Logger\\StderrLogger;use Aws\\Rds\\AuthTokenGenerator;use Aws\\Credentials\\CredentialProvider;require __DIR__ . '/vendor/autoload.php';class Handler implements StdHandler{    private StderrLogger $logger;    public function __construct(StderrLogger $logger)    {        $this->logger = $logger;    }    private function getAuthToken(): string {        // Define connection authentication parameters        $dbConnection = [            'hostname' => getenv('DB_HOSTNAME'),            'port' => getenv('DB_PORT'),            'username' => getenv('DB_USERNAME'),            'region' => getenv('AWS_REGION'),        ];        // Create RDS AuthTokenGenerator object        $generator = new AuthTokenGenerator(CredentialProvider::defaultProvider());        // Request authorization token from RDS, specifying the username        return $generator->createToken(            $dbConnection['hostname'] . ':' . $dbConnection['port'],            $dbConnection['region'],            $dbConnection['username']        );    }    private function getQueryResults() {        // Obtain auth token        $token = $this->getAuthToken();        // Define connection configuration        $connectionConfig = [            'host' => getenv('DB_HOSTNAME'),            'user' => getenv('DB_USERNAME'),            'password' => $token,            'database' => getenv('DB_NAME'),        ];        // Create the connection to the DB        $conn = new PDO(            \"mysql:host={$connectionConfig['host']};dbname={$connectionConfig['database']}\",            $connectionConfig['user'],            $connectionConfig['password'],            [                PDO::MYSQL_ATTR_SSL_CA => '/path/to/rds-ca-2019-root.pem',                PDO::MYSQL_ATTR_SSL_VERIFY_SERVER_CERT => true,            ]        );        // Obtain the result of the query        $stmt = $conn->prepare('SELECT ?+? AS sum');        $stmt->execute([3, 2]);        return $stmt->fetch(PDO::FETCH_ASSOC);    }    /**     * @param mixed $event     * @param Context $context     * @return array     */    public function handle(mixed $event, Context $context): array    {        $this->logger->info(\"Processing query\");        // Execute database flow        $result = $this->getQueryResults();        return [            'sum' => $result['sum']        ];    }}$logger = new StderrLogger();return new Handler($logger);PythonSDK for Python (Boto3)Note        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Connecting to an Amazon RDS database in a Lambda function using Python.import jsonimport osimport boto3import pymysql# RDS settingsproxy_host_name = os.environ['PROXY_HOST_NAME']port = int(os.environ['PORT'])db_name = os.environ['DB_NAME']db_user_name = os.environ['DB_USER_NAME']aws_region = os.environ['AWS_REGION']# Fetch RDS Auth Tokendef get_auth_token():    client = boto3.client('rds')    token = client.generate_db_auth_token(        DBHostname=proxy_host_name,        Port=port        DBUsername=db_user_name        Region=aws_region    )    return tokendef lambda_handler(event, context):    token = get_auth_token()    try:        connection = pymysql.connect(            host=proxy_host_name,            user=db_user_name,            password=token,            db=db_name,            port=port,            ssl={'ca': 'Amazon RDS'}  # Ensure you have the CA bundle for SSL connection        )                with connection.cursor() as cursor:            cursor.execute('SELECT %s + %s AS sum', (3, 2))            result = cursor.fetchone()        return result            except Exception as e:        return (f\"Error: {str(e)}\")  # Return an error message if an exception occurs     RubySDK for RubyNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Connecting to an Amazon RDS database in a Lambda function using Ruby.# Ruby code here.require 'aws-sdk-rds'require 'json'require 'mysql2'def lambda_handler(event:, context:)  endpoint = ENV['DBEndpoint'] # Add the endpoint without https\"  port = ENV['Port']           # 3306  user = ENV['DBUser']  region = ENV['DBRegion']     # 'us-east-1'  db_name = ENV['DBName']  credentials = Aws::Credentials.new(    ENV['AWS_ACCESS_KEY_ID'],    ENV['AWS_SECRET_ACCESS_KEY'],    ENV['AWS_SESSION_TOKEN']  )  rds_client = Aws::RDS::AuthTokenGenerator.new(    region: region,     credentials: credentials  )  token = rds_client.auth_token(    endpoint: endpoint+ ':' + port,    user_name: user,    region: region  )  begin    conn = Mysql2::Client.new(      host: endpoint,      username: user,      password: token,      port: port,      database: db_name,      sslca: '/var/task/global-bundle.pem',       sslverify: true,      enable_cleartext_plugin: true    )    a = 3    b = 2    result = conn.query(\"SELECT #{a} + #{b} AS sum\").first['sum']    puts result    conn.close    {      statusCode: 200,      body: result.to_json    }  rescue => e    puts \"Database connection failed due to #{e}\"  endendRustSDK for RustNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Connecting to an Amazon RDS database in a Lambda function using Rust.use aws_config::BehaviorVersion;use aws_credential_types::provider::ProvideCredentials;use aws_sigv4::{    http_request::{sign, SignableBody, SignableRequest, SigningSettings},    sign::v4,};use lambda_runtime::{run, service_fn, Error, LambdaEvent};use serde_json::{json, Value};use sqlx::postgres::PgConnectOptions;use std::env;use std::time::{Duration, SystemTime};const RDS_CERTS: &[u8] = include_bytes!(\"global-bundle.pem\");async fn generate_rds_iam_token(    db_hostname: &str,    port: u16,    db_username: &str,) -> Result<String, Error> {    let config = aws_config::load_defaults(BehaviorVersion::v2024_03_28()).await;    let credentials = config        .credentials_provider()        .expect(\"no credentials provider found\")        .provide_credentials()        .await        .expect(\"unable to load credentials\");    let identity = credentials.into();    let region = config.region().unwrap().to_string();    let mut signing_settings = SigningSettings::default();    signing_settings.expires_in = Some(Duration::from_secs(900));    signing_settings.signature_location = aws_sigv4::http_request::SignatureLocation::QueryParams;    let signing_params = v4::SigningParams::builder()        .identity(&identity)        .region(&region)        .name(\"rds-db\")        .time(SystemTime::now())        .settings(signing_settings)        .build()?;    let url = format!(        \"https://{db_hostname}:{port}/?Action=connect&DBUser={db_user}\",        db_hostname = db_hostname,        port = port,        db_user = db_username    );    let signable_request =        SignableRequest::new(\"GET\", &url, std::iter::empty(), SignableBody::Bytes(&[]))            .expect(\"signable request\");    let (signing_instructions, _signature) =        sign(signable_request, &signing_params.into())?.into_parts();    let mut url = url::Url::parse(&url).unwrap();    for (name, value) in signing_instructions.params() {        url.query_pairs_mut().append_pair(name, &value);    }    let response = url.to_string().split_off(\"https://\".len());    Ok(response)}#[tokio::main]async fn main() -> Result<(), Error> {    run(service_fn(handler)).await}async fn handler(_event: LambdaEvent<Value>) -> Result<Value, Error> {    let db_host = env::var(\"DB_HOSTNAME\").expect(\"DB_HOSTNAME must be set\");    let db_port = env::var(\"DB_PORT\")        .expect(\"DB_PORT must be set\")        .parse::<u16>()        .expect(\"PORT must be a valid number\");    let db_name = env::var(\"DB_NAME\").expect(\"DB_NAME must be set\");    let db_user_name = env::var(\"DB_USERNAME\").expect(\"DB_USERNAME must be set\");    let token = generate_rds_iam_token(&db_host, db_port, &db_user_name).await?;    let opts = PgConnectOptions::new()        .host(&db_host)        .port(db_port)        .username(&db_user_name)        .password(&token)        .database(&db_name)        .ssl_root_cert_from_pem(RDS_CERTS.to_vec())        .ssl_mode(sqlx::postgres::PgSslMode::Require);    let pool = sqlx::postgres::PgPoolOptions::new()        .connect_with(opts)        .await?;    let result: i32 = sqlx::query_scalar(\"SELECT $1 + $2\")        .bind(3)        .bind(2)        .fetch_one(&pool)        .await?;    println!(\"Result: {:?}\", result);    Ok(json!({        \"statusCode\": 200,        \"content-type\": \"text/plain\",        \"body\": format!(\"The selected sum is: {result}\")    }))}anchoranchoranchoranchoranchoranchoranchorGoJavaJavaScriptPHPPythonRubyRustSDK for Go V2Note        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Connecting to an Amazon RDS database in a Lambda function using Go./*Golang v2 code here.*/package mainimport (\t\"context\"\t\"database/sql\"\t\"encoding/json\"\t\"fmt\"\t\"os\"\t\"github.com/aws/aws-lambda-go/lambda\"\t\"github.com/aws/aws-sdk-go-v2/config\"\t\"github.com/aws/aws-sdk-go-v2/feature/rds/auth\"\t_ \"github.com/go-sql-driver/mysql\")type MyEvent struct {\tName string `json:\"name\"`}func HandleRequest(event *MyEvent) (map[string]interface{}, error) {\tvar dbName string = os.Getenv(\"DatabaseName\")\tvar dbUser string = os.Getenv(\"DatabaseUser\")\tvar dbHost string = os.Getenv(\"DBHost\") // Add hostname without https\tvar dbPort int = os.Getenv(\"Port\")      // Add port number\tvar dbEndpoint string = fmt.Sprintf(\"%s:%d\", dbHost, dbPort)\tvar region string = os.Getenv(\"AWS_REGION\")\tcfg, err := config.LoadDefaultConfig(context.TODO())\tif err != nil {\t\tpanic(\"configuration error: \" + err.Error())\t}\tauthenticationToken, err := auth.BuildAuthToken(\t\tcontext.TODO(), dbEndpoint, region, dbUser, cfg.Credentials)\tif err != nil {\t\tpanic(\"failed to create authentication token: \" + err.Error())\t}\tdsn := fmt.Sprintf(\"%s:%s@tcp(%s)/%s?tls=true&allowCleartextPasswords=true\",\t\tdbUser, authenticationToken, dbEndpoint, dbName,\t)\tdb, err := sql.Open(\"mysql\", dsn)\tif err != nil {\t\tpanic(err)\t}\tdefer db.Close()\tvar sum int\terr = db.QueryRow(\"SELECT ?+? AS sum\", 3, 2).Scan(&sum)\tif err != nil {\t\tpanic(err)\t}\ts := fmt.Sprint(sum)\tmessage := fmt.Sprintf(\"The selected sum is: %s\", s)\tmessageBytes, err := json.Marshal(message)\tif err != nil {\t\treturn nil, err\t}\tmessageString := string(messageBytes)\treturn map[string]interface{}{\t\t\"statusCode\": 200,\t\t\"headers\":    map[string]string{\"Content-Type\": \"application/json\"},\t\t\"body\":       messageString,\t}, nil}func main() {\tlambda.Start(HandleRequest)}",
                            "For a complete list of AWS SDK developer guides and code examples, see    Using Lambda with an AWS SDK.    This topic also includes information about getting started and details about previous SDK versions."
                        ]
                    },
                    {
                        "title": "Invoke a Lambda function from a Kinesis trigger",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_serverless_Kinesis_Lambda_section.html",
                        "sections": [
                            "",
                            "The following code examples show how to implement a Lambda function that receives an event triggered by receiving records from a Kinesis stream. The function retrieves the Kinesis payload, decodes from Base64, and logs the record contents.",
                            ".NETAWS SDK for .NETNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Consuming a Kinesis event with Lambda using .NET.// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.// SPDX-License-Identifier: Apache-2.0﻿using System.Text;using Amazon.Lambda.Core;using Amazon.Lambda.KinesisEvents;using AWS.Lambda.Powertools.Logging;// Assembly attribute to enable the Lambda function's JSON input to be converted into a .NET class.[assembly: LambdaSerializer(typeof(Amazon.Lambda.Serialization.SystemTextJson.DefaultLambdaJsonSerializer))]namespace KinesisIntegrationSampleCode;public class Function{    // Powertools Logger requires an environment variables against your function    // POWERTOOLS_SERVICE_NAME    [Logging(LogEvent = true)]    public async Task FunctionHandler(KinesisEvent evnt, ILambdaContext context)    {        if (evnt.Records.Count == 0)        {            Logger.LogInformation(\"Empty Kinesis Event received\");            return;        }        foreach (var record in evnt.Records)        {            try            {                Logger.LogInformation($\"Processed Event with EventId: {record.EventId}\");                string data = await GetRecordDataAsync(record.Kinesis, context);                Logger.LogInformation($\"Data: {data}\");                // TODO: Do interesting work based on the new data            }            catch (Exception ex)            {                Logger.LogError($\"An error occurred {ex.Message}\");                throw;            }        }        Logger.LogInformation($\"Successfully processed {evnt.Records.Count} records.\");    }    private async Task<string> GetRecordDataAsync(KinesisEvent.Record record, ILambdaContext context)    {        byte[] bytes = record.Data.ToArray();        string data = Encoding.UTF8.GetString(bytes);        await Task.CompletedTask; //Placeholder for actual async work        return data;    }}GoSDK for Go V2Note        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Consuming a Kinesis event with Lambda using Go.// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.// SPDX-License-Identifier: Apache-2.0package mainimport (\t\"context\"\t\"log\"\t\"github.com/aws/aws-lambda-go/events\"\t\"github.com/aws/aws-lambda-go/lambda\")func handler(ctx context.Context, kinesisEvent events.KinesisEvent) error {\tif len(kinesisEvent.Records) == 0 {\t\tlog.Printf(\"empty Kinesis event received\")\t\treturn nil\t}\tfor _, record := range kinesisEvent.Records {\t\tlog.Printf(\"processed Kinesis event with EventId: %v\", record.EventID)\t\trecordDataBytes := record.Kinesis.Data\t\trecordDataText := string(recordDataBytes)\t\tlog.Printf(\"record data: %v\", recordDataText)\t\t// TODO: Do interesting work based on the new data\t}\tlog.Printf(\"successfully processed %v records\", len(kinesisEvent.Records))\treturn nil}func main() {\tlambda.Start(handler)}JavaSDK for Java 2.xNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Consuming a Kinesis event with Lambda using Java.// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.// SPDX-License-Identifier: Apache-2.0package example;import com.amazonaws.services.lambda.runtime.Context;import com.amazonaws.services.lambda.runtime.LambdaLogger;import com.amazonaws.services.lambda.runtime.RequestHandler;import com.amazonaws.services.lambda.runtime.events.KinesisEvent;public class Handler implements RequestHandler<KinesisEvent, Void> {    @Override    public Void handleRequest(final KinesisEvent event, final Context context) {        LambdaLogger logger = context.getLogger();        if (event.getRecords().isEmpty()) {            logger.log(\"Empty Kinesis Event received\");            return null;        }        for (KinesisEvent.KinesisEventRecord record : event.getRecords()) {            try {                logger.log(\"Processed Event with EventId: \"+record.getEventID());                String data = new String(record.getKinesis().getData().array());                logger.log(\"Data:\"+ data);                // TODO: Do interesting work based on the new data            }            catch (Exception ex) {                logger.log(\"An error occurred:\"+ex.getMessage());                throw ex;            }        }        logger.log(\"Successfully processed:\"+event.getRecords().size()+\" records\");        return null;    }}JavaScriptSDK for JavaScript (v3)Note        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Consuming a Kinesis event with Lambda using JavaScript.// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.// SPDX-License-Identifier: Apache-2.0exports.handler = async (event, context) => {  for (const record of event.Records) {    try {      console.log(`Processed Kinesis Event - EventID: ${record.eventID}`);      const recordData = await getRecordDataAsync(record.kinesis);      console.log(`Record Data: ${recordData}`);      // TODO: Do interesting work based on the new data    } catch (err) {      console.error(`An error occurred ${err}`);      throw err;    }  }  console.log(`Successfully processed ${event.Records.length} records.`);};async function getRecordDataAsync(payload) {  var data = Buffer.from(payload.data, \"base64\").toString(\"utf-8\");  await Promise.resolve(1); //Placeholder for actual async work  return data;}Consuming a Kinesis event with Lambda using TypeScript.// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.// SPDX-License-Identifier: Apache-2.0import {  KinesisStreamEvent,  Context,  KinesisStreamHandler,  KinesisStreamRecordPayload,} from \"aws-lambda\";import { Buffer } from \"buffer\";import { Logger } from \"@aws-lambda-powertools/logger\";const logger = new Logger({  logLevel: \"INFO\",  serviceName: \"kinesis-stream-handler-sample\",});export const functionHandler: KinesisStreamHandler = async (  event: KinesisStreamEvent,  context: Context): Promise<void> => {  for (const record of event.Records) {    try {      logger.info(`Processed Kinesis Event - EventID: ${record.eventID}`);      const recordData = await getRecordDataAsync(record.kinesis);      logger.info(`Record Data: ${recordData}`);      // TODO: Do interesting work based on the new data    } catch (err) {      logger.error(`An error occurred ${err}`);      throw err;    }    logger.info(`Successfully processed ${event.Records.length} records.`);  }};async function getRecordDataAsync(  payload: KinesisStreamRecordPayload): Promise<string> {  var data = Buffer.from(payload.data, \"base64\").toString(\"utf-8\");  await Promise.resolve(1); //Placeholder for actual async work  return data;}PHPSDK for PHPNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Consuming an Kinesis event with Lambda using PHP.// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.// SPDX-License-Identifier: Apache-2.0<?php# using bref/bref and bref/logger for simplicityuse Bref\\Context\\Context;use Bref\\Event\\Kinesis\\KinesisEvent;use Bref\\Event\\Kinesis\\KinesisHandler;use Bref\\Logger\\StderrLogger;require __DIR__ . '/vendor/autoload.php';class Handler extends KinesisHandler{    private StderrLogger $logger;    public function __construct(StderrLogger $logger)    {        $this->logger = $logger;    }    /**     * @throws JsonException     * @throws \\Bref\\Event\\InvalidLambdaEvent     */    public function handleKinesis(KinesisEvent $event, Context $context): void    {        $this->logger->info(\"Processing records\");        $records = $event->getRecords();        foreach ($records as $record) {            $data = $record->getData();            $this->logger->info(json_encode($data));            // TODO: Do interesting work based on the new data            // Any exception thrown will be logged and the invocation will be marked as failed        }        $totalRecords = count($records);        $this->logger->info(\"Successfully processed $totalRecords records\");    }}$logger = new StderrLogger();return new Handler($logger);PythonSDK for Python (Boto3)Note        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Consuming a Kinesis event with Lambda using Python.# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.# SPDX-License-Identifier: Apache-2.0import base64def lambda_handler(event, context):    for record in event['Records']:        try:            print(f\"Processed Kinesis Event - EventID: {record['eventID']}\")            record_data = base64.b64decode(record['kinesis']['data']).decode('utf-8')            print(f\"Record Data: {record_data}\")            # TODO: Do interesting work based on the new data        except Exception as e:            print(f\"An error occurred {e}\")            raise e    print(f\"Successfully processed {len(event['Records'])} records.\")RubySDK for RubyNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Consuming an Kinesis event with Lambda using Ruby.# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.# SPDX-License-Identifier: Apache-2.0require 'aws-sdk'def lambda_handler(event:, context:)  event['Records'].each do |record|    begin      puts \"Processed Kinesis Event - EventID: #{record['eventID']}\"      record_data = get_record_data_async(record['kinesis'])      puts \"Record Data: #{record_data}\"      # TODO: Do interesting work based on the new data    rescue => err      $stderr.puts \"An error occurred #{err}\"      raise err    end  end  puts \"Successfully processed #{event['Records'].length} records.\"enddef get_record_data_async(payload)  data = Base64.decode64(payload['data']).force_encoding('UTF-8')  # Placeholder for actual async work  # You can use Ruby's asynchronous programming tools like async/await or fibers here.  return dataendRustSDK for RustNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Consuming an Kinesis event with Lambda using Rust.// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.// SPDX-License-Identifier: Apache-2.0use aws_lambda_events::event::kinesis::KinesisEvent;use lambda_runtime::{run, service_fn, Error, LambdaEvent};async fn function_handler(event: LambdaEvent<KinesisEvent>) -> Result<(), Error> {    if event.payload.records.is_empty() {        tracing::info!(\"No records found. Exiting.\");        return Ok(());    }    event.payload.records.iter().for_each(|record| {        tracing::info!(\"EventId: {}\",record.event_id.as_deref().unwrap_or_default());        let record_data = std::str::from_utf8(&record.kinesis.data);        match record_data {            Ok(data) => {                // log the record data                tracing::info!(\"Data: {}\", data);            }            Err(e) => {                tracing::error!(\"Error: {}\", e);            }        }    });    tracing::info!(        \"Successfully processed {} records\",        event.payload.records.len()    );    Ok(())}#[tokio::main]async fn main() -> Result<(), Error> {    tracing_subscriber::fmt()        .with_max_level(tracing::Level::INFO)        // disable printing the name of the module in every log line.        .with_target(false)        // disabling time is handy because CloudWatch will add the ingestion time.        .without_time()        .init();    run(service_fn(function_handler)).await}anchoranchoranchoranchoranchoranchoranchoranchor.NETGoJavaJavaScriptPHPPythonRubyRustAWS SDK for .NETNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Consuming a Kinesis event with Lambda using .NET.// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.// SPDX-License-Identifier: Apache-2.0﻿using System.Text;using Amazon.Lambda.Core;using Amazon.Lambda.KinesisEvents;using AWS.Lambda.Powertools.Logging;// Assembly attribute to enable the Lambda function's JSON input to be converted into a .NET class.[assembly: LambdaSerializer(typeof(Amazon.Lambda.Serialization.SystemTextJson.DefaultLambdaJsonSerializer))]namespace KinesisIntegrationSampleCode;public class Function{    // Powertools Logger requires an environment variables against your function    // POWERTOOLS_SERVICE_NAME    [Logging(LogEvent = true)]    public async Task FunctionHandler(KinesisEvent evnt, ILambdaContext context)    {        if (evnt.Records.Count == 0)        {            Logger.LogInformation(\"Empty Kinesis Event received\");            return;        }        foreach (var record in evnt.Records)        {            try            {                Logger.LogInformation($\"Processed Event with EventId: {record.EventId}\");                string data = await GetRecordDataAsync(record.Kinesis, context);                Logger.LogInformation($\"Data: {data}\");                // TODO: Do interesting work based on the new data            }            catch (Exception ex)            {                Logger.LogError($\"An error occurred {ex.Message}\");                throw;            }        }        Logger.LogInformation($\"Successfully processed {evnt.Records.Count} records.\");    }    private async Task<string> GetRecordDataAsync(KinesisEvent.Record record, ILambdaContext context)    {        byte[] bytes = record.Data.ToArray();        string data = Encoding.UTF8.GetString(bytes);        await Task.CompletedTask; //Placeholder for actual async work        return data;    }}",
                            "For a complete list of AWS SDK developer guides and code examples, see    Using Lambda with an AWS SDK.    This topic also includes information about getting started and details about previous SDK versions."
                        ]
                    },
                    {
                        "title": "Invoke a Lambda function from a DynamoDB trigger",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_serverless_DynamoDB_Lambda_section.html",
                        "sections": [
                            "",
                            "The following code examples show how to implement a Lambda function that receives an event triggered by receiving records from a DynamoDB stream. The function retrieves the DynamoDB payload and logs the record contents.",
                            ".NETAWS SDK for .NETNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Consuming a DynamoDB event with Lambda using .NET.// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.// SPDX-License-Identifier: Apache-2.0using System.Text.Json;using System.Text;using Amazon.Lambda.Core;using Amazon.Lambda.DynamoDBEvents;// Assembly attribute to enable the Lambda function's JSON input to be converted into a .NET class.[assembly: LambdaSerializer(typeof(Amazon.Lambda.Serialization.SystemTextJson.DefaultLambdaJsonSerializer))]namespace AWSLambda_DDB;public class Function{    public void FunctionHandler(DynamoDBEvent dynamoEvent, ILambdaContext context)    {        context.Logger.LogInformation($\"Beginning to process {dynamoEvent.Records.Count} records...\");        foreach (var record in dynamoEvent.Records)        {            context.Logger.LogInformation($\"Event ID: {record.EventID}\");            context.Logger.LogInformation($\"Event Name: {record.EventName}\");            context.Logger.LogInformation(JsonSerializer.Serialize(record));        }        context.Logger.LogInformation(\"Stream processing complete.\");    }}GoSDK for Go V2Note        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Consuming a DynamoDB event with Lambda using Go.// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.// SPDX-License-Identifier: Apache-2.0package mainimport (\t\"context\"\t\"github.com/aws/aws-lambda-go/lambda\"\t\"github.com/aws/aws-lambda-go/events\"\t\"fmt\")func HandleRequest(ctx context.Context, event events.DynamoDBEvent) (*string, error) {\tif len(event.Records) == 0 {\t\treturn nil, fmt.Errorf(\"received empty event\")\t}\tfor _, record := range event.Records {\t \tLogDynamoDBRecord(record)\t}\tmessage := fmt.Sprintf(\"Records processed: %d\", len(event.Records))\treturn &message, nil}func main() {\tlambda.Start(HandleRequest)}func LogDynamoDBRecord(record events.DynamoDBEventRecord){\tfmt.Println(record.EventID)\tfmt.Println(record.EventName)\tfmt.Printf(\"%+v\\n\", record.Change)}JavaSDK for Java 2.xNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Consuming a DynamoDB event with Lambda using Java.import com.amazonaws.services.lambda.runtime.Context;import com.amazonaws.services.lambda.runtime.RequestHandler;import com.amazonaws.services.lambda.runtime.events.DynamodbEvent;import com.amazonaws.services.lambda.runtime.events.DynamodbEvent.DynamodbStreamRecord;import com.google.gson.Gson;import com.google.gson.GsonBuilder;public class example implements RequestHandler<DynamodbEvent, Void> {    private static final Gson GSON = new GsonBuilder().setPrettyPrinting().create();    @Override    public Void handleRequest(DynamodbEvent event, Context context) {        System.out.println(GSON.toJson(event));        event.getRecords().forEach(this::logDynamoDBRecord);        return null;    }    private void logDynamoDBRecord(DynamodbStreamRecord record) {        System.out.println(record.getEventID());        System.out.println(record.getEventName());        System.out.println(\"DynamoDB Record: \" + GSON.toJson(record.getDynamodb()));    }}JavaScriptSDK for JavaScript (v3)Note        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Consuming a DynamoDB event with Lambda using JavaScript.// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.// SPDX-License-Identifier: Apache-2.0exports.handler = async (event, context) => {    console.log(JSON.stringify(event, null, 2));    event.Records.forEach(record => {        logDynamoDBRecord(record);    });};const logDynamoDBRecord = (record) => {    console.log(record.eventID);    console.log(record.eventName);    console.log(`DynamoDB Record: ${JSON.stringify(record.dynamodb)}`);};Consuming a DynamoDB event with Lambda using TypeScript.export const handler = async (event, context) => {    console.log(JSON.stringify(event, null, 2));    event.Records.forEach(record => {        logDynamoDBRecord(record);    });}const logDynamoDBRecord = (record) => {    console.log(record.eventID);    console.log(record.eventName);    console.log(`DynamoDB Record: ${JSON.stringify(record.dynamodb)}`);};PHPSDK for PHPNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Consuming a DynamoDB event with Lambda using PHP.<?php# using bref/bref and bref/logger for simplicityuse Bref\\Context\\Context;use Bref\\Event\\DynamoDb\\DynamoDbEvent;use Bref\\Event\\DynamoDb\\DynamoDbHandler;use Bref\\Logger\\StderrLogger;require __DIR__ . '/vendor/autoload.php';class Handler extends DynamoDbHandler{    private StderrLogger $logger;    public function __construct(StderrLogger $logger)    {        $this->logger = $logger;    }    /**     * @throws JsonException     * @throws \\Bref\\Event\\InvalidLambdaEvent     */    public function handleDynamoDb(DynamoDbEvent $event, Context $context): void    {        $this->logger->info(\"Processing DynamoDb table items\");        $records = $event->getRecords();        foreach ($records as $record) {            $eventName = $record->getEventName();            $keys = $record->getKeys();            $old = $record->getOldImage();            $new = $record->getNewImage();                        $this->logger->info(\"Event Name:\".$eventName.\"\\n\");            $this->logger->info(\"Keys:\". json_encode($keys).\"\\n\");            $this->logger->info(\"Old Image:\". json_encode($old).\"\\n\");            $this->logger->info(\"New Image:\". json_encode($new));                        // TODO: Do interesting work based on the new data            // Any exception thrown will be logged and the invocation will be marked as failed        }        $totalRecords = count($records);        $this->logger->info(\"Successfully processed $totalRecords items\");    }}$logger = new StderrLogger();return new Handler($logger);PythonSDK for Python (Boto3)Note        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Consuming a DynamoDB event with Lambda using Python.import jsondef lambda_handler(event, context):    print(json.dumps(event, indent=2))    for record in event['Records']:        log_dynamodb_record(record)def log_dynamodb_record(record):    print(record['eventID'])    print(record['eventName'])    print(f\"DynamoDB Record: {json.dumps(record['dynamodb'])}\")RubySDK for RubyNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Consuming a DynamoDB event with Lambda using Ruby.def lambda_handler(event:, context:)    return 'received empty event' if event['Records'].empty?      event['Records'].each do |record|      log_dynamodb_record(record)    end      \"Records processed: #{event['Records'].length}\"  end    def log_dynamodb_record(record)    puts record['eventID']    puts record['eventName']    puts \"DynamoDB Record: #{JSON.generate(record['dynamodb'])}\"  end  RustSDK for RustNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Consuming a DynamoDB event with Lambda using Rust.use lambda_runtime::{service_fn, tracing, Error, LambdaEvent};use aws_lambda_events::{    event::dynamodb::{Event, EventRecord},   };// Built with the following dependencies://lambda_runtime = \"0.11.1\"//serde_json = \"1.0\"//tokio = { version = \"1\", features = [\"macros\"] }//tracing = { version = \"0.1\", features = [\"log\"] }//tracing-subscriber = { version = \"0.3\", default-features = false, features = [\"fmt\"] }//aws_lambda_events = \"0.15.0\"async fn function_handler(event: LambdaEvent<Event>) ->Result<(), Error> {        let records = &event.payload.records;    tracing::info!(\"event payload: {:?}\",records);    if records.is_empty() {        tracing::info!(\"No records found. Exiting.\");        return Ok(());    }    for record in records{        log_dynamo_dbrecord(record);    }    tracing::info!(\"Dynamo db records processed\");    // Prepare the response    Ok(())}fn log_dynamo_dbrecord(record: &EventRecord)-> Result<(), Error>{    tracing::info!(\"EventId: {}\", record.event_id);    tracing::info!(\"EventName: {}\", record.event_name);    tracing::info!(\"DynamoDB Record: {:?}\", record.change );    Ok(())}#[tokio::main]async fn main() -> Result<(), Error> {    tracing_subscriber::fmt()    .with_max_level(tracing::Level::INFO)    .with_target(false)    .without_time()    .init();    let func = service_fn(function_handler);    lambda_runtime::run(func).await?;    Ok(())    }anchoranchoranchoranchoranchoranchoranchoranchor.NETGoJavaJavaScriptPHPPythonRubyRustAWS SDK for .NETNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Consuming a DynamoDB event with Lambda using .NET.// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.// SPDX-License-Identifier: Apache-2.0using System.Text.Json;using System.Text;using Amazon.Lambda.Core;using Amazon.Lambda.DynamoDBEvents;// Assembly attribute to enable the Lambda function's JSON input to be converted into a .NET class.[assembly: LambdaSerializer(typeof(Amazon.Lambda.Serialization.SystemTextJson.DefaultLambdaJsonSerializer))]namespace AWSLambda_DDB;public class Function{    public void FunctionHandler(DynamoDBEvent dynamoEvent, ILambdaContext context)    {        context.Logger.LogInformation($\"Beginning to process {dynamoEvent.Records.Count} records...\");        foreach (var record in dynamoEvent.Records)        {            context.Logger.LogInformation($\"Event ID: {record.EventID}\");            context.Logger.LogInformation($\"Event Name: {record.EventName}\");            context.Logger.LogInformation(JsonSerializer.Serialize(record));        }        context.Logger.LogInformation(\"Stream processing complete.\");    }}",
                            "For a complete list of AWS SDK developer guides and code examples, see    Using Lambda with an AWS SDK.    This topic also includes information about getting started and details about previous SDK versions."
                        ]
                    },
                    {
                        "title": "Invoke a Lambda function from a Amazon DocumentDB trigger",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_serverless_DocumentDB_Lambda_section.html",
                        "sections": [
                            "",
                            "The following code examples show how to implement a Lambda function that receives an event triggered by receiving records from a DocumentDB change stream. The function retrieves the DocumentDB payload and logs the record contents.",
                            ".NETAWS SDK for .NETNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Consuming a Amazon DocumentDB event with Lambda using .NET.using Amazon.Lambda.Core;using System.Text.Json;using System;using System.Collections.Generic;using System.Text.Json.Serialization;//Assembly attribute to enable the Lambda function's JSON input to be converted into a .NET class.[assembly: LambdaSerializer(typeof(Amazon.Lambda.Serialization.SystemTextJson.DefaultLambdaJsonSerializer))]namespace LambdaDocDb;public class Function{         /// <summary>    /// Lambda function entry point to process Amazon DocumentDB events.    /// </summary>    /// <param name=\"event\">The Amazon DocumentDB event.</param>    /// <param name=\"context\">The Lambda context object.</param>    /// <returns>A string to indicate successful processing.</returns>    public string FunctionHandler(Event evnt, ILambdaContext context)    {                foreach (var record in evnt.Events)        {            ProcessDocumentDBEvent(record, context);        }        return \"OK\";    }     private void ProcessDocumentDBEvent(DocumentDBEventRecord record, ILambdaContext context)    {                var eventData = record.Event;        var operationType = eventData.OperationType;        var databaseName = eventData.Ns.Db;        var collectionName = eventData.Ns.Coll;        var fullDocument = JsonSerializer.Serialize(eventData.FullDocument, new JsonSerializerOptions { WriteIndented = true });        context.Logger.LogLine($\"Operation type: {operationType}\");        context.Logger.LogLine($\"Database: {databaseName}\");        context.Logger.LogLine($\"Collection: {collectionName}\");        context.Logger.LogLine($\"Full document:\\n{fullDocument}\");    }    public class Event    {        [JsonPropertyName(\"eventSourceArn\")]        public string EventSourceArn { get; set; }        [JsonPropertyName(\"events\")]        public List<DocumentDBEventRecord> Events { get; set; }        [JsonPropertyName(\"eventSource\")]        public string EventSource { get; set; }    }    public class DocumentDBEventRecord    {        [JsonPropertyName(\"event\")]        public EventData Event { get; set; }    }    public class EventData    {        [JsonPropertyName(\"_id\")]        public IdData Id { get; set; }        [JsonPropertyName(\"clusterTime\")]        public ClusterTime ClusterTime { get; set; }        [JsonPropertyName(\"documentKey\")]        public DocumentKey DocumentKey { get; set; }        [JsonPropertyName(\"fullDocument\")]        public Dictionary<string, object> FullDocument { get; set; }        [JsonPropertyName(\"ns\")]        public Namespace Ns { get; set; }        [JsonPropertyName(\"operationType\")]        public string OperationType { get; set; }    }    public class IdData    {        [JsonPropertyName(\"_data\")]        public string Data { get; set; }    }    public class ClusterTime    {        [JsonPropertyName(\"$timestamp\")]        public Timestamp Timestamp { get; set; }    }    public class Timestamp    {        [JsonPropertyName(\"t\")]        public long T { get; set; }        [JsonPropertyName(\"i\")]        public int I { get; set; }    }    public class DocumentKey    {        [JsonPropertyName(\"_id\")]        public Id Id { get; set; }    }    public class Id    {        [JsonPropertyName(\"$oid\")]        public string Oid { get; set; }    }    public class Namespace    {        [JsonPropertyName(\"db\")]        public string Db { get; set; }        [JsonPropertyName(\"coll\")]        public string Coll { get; set; }    }}GoSDK for Go V2Note        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Consuming a Amazon DocumentDB event with Lambda using Go.package mainimport (\t\"context\"\t\"encoding/json\"\t\"fmt\"\t\"github.com/aws/aws-lambda-go/lambda\")type Event struct {\tEvents []Record `json:\"events\"`}type Record struct {\tEvent struct {\t\tOperationType string `json:\"operationType\"`\t\tNS            struct {\t\t\tDB   string `json:\"db\"`\t\t\tColl string `json:\"coll\"`\t\t} `json:\"ns\"`\t\tFullDocument interface{} `json:\"fullDocument\"`\t} `json:\"event\"`}func main() {\tlambda.Start(handler)}func handler(ctx context.Context, event Event) (string, error) {\tfmt.Println(\"Loading function\")\tfor _, record := range event.Events {\t\tlogDocumentDBEvent(record)\t}\treturn \"OK\", nil}func logDocumentDBEvent(record Record) {\tfmt.Printf(\"Operation type: %s\\n\", record.Event.OperationType)\tfmt.Printf(\"db: %s\\n\", record.Event.NS.DB)\tfmt.Printf(\"collection: %s\\n\", record.Event.NS.Coll)\tdocBytes, _ := json.MarshalIndent(record.Event.FullDocument, \"\", \"  \")\tfmt.Printf(\"Full document: %s\\n\", string(docBytes))}JavaScriptSDK for JavaScript (v3)Note        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Consuming a Amazon DocumentDB event with Lambda using JavaScript.console.log('Loading function');exports.handler = async (event, context) => {    event.events.forEach(record => {        logDocumentDBEvent(record);    });    return 'OK';};const logDocumentDBEvent = (record) => {    console.log('Operation type: ' + record.event.operationType);    console.log('db: ' + record.event.ns.db);    console.log('collection: ' + record.event.ns.coll);    console.log('Full document:', JSON.stringify(record.event.fullDocument, null, 2));};Consuming a Amazon DocumentDB event with Lambda using TypeScriptimport { DocumentDBEventRecord, DocumentDBEventSubscriptionContext } from 'aws-lambda';console.log('Loading function');export const handler = async (  event: DocumentDBEventSubscriptionContext,  context: any): Promise<string> => {  event.events.forEach((record: DocumentDBEventRecord) => {    logDocumentDBEvent(record);  });  return 'OK';};const logDocumentDBEvent = (record: DocumentDBEventRecord): void => {  console.log('Operation type: ' + record.event.operationType);  console.log('db: ' + record.event.ns.db);  console.log('collection: ' + record.event.ns.coll);  console.log('Full document:', JSON.stringify(record.event.fullDocument, null, 2));};PHPSDK for PHPNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Consuming a Amazon DocumentDB event with Lambda using PHP.<?phprequire __DIR__.'/vendor/autoload.php';use Bref\\Context\\Context;use Bref\\Event\\Handler;class DocumentDBEventHandler implements Handler{    public function handle($event, Context $context): string    {        $events = $event['events'] ?? [];        foreach ($events as $record) {            $this->logDocumentDBEvent($record['event']);        }        return 'OK';    }    private function logDocumentDBEvent($event): void    {        // Extract information from the event record        $operationType = $event['operationType'] ?? 'Unknown';        $db = $event['ns']['db'] ?? 'Unknown';        $collection = $event['ns']['coll'] ?? 'Unknown';        $fullDocument = $event['fullDocument'] ?? [];        // Log the event details        echo \"Operation type: $operationType\\n\";        echo \"Database: $db\\n\";        echo \"Collection: $collection\\n\";        echo \"Full document: \" . json_encode($fullDocument, JSON_PRETTY_PRINT) . \"\\n\";    }}return new DocumentDBEventHandler();PythonSDK for Python (Boto3)Note        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Consuming a Amazon DocumentDB event with Lambda using Python.import jsondef lambda_handler(event, context):    for record in event.get('events', []):        log_document_db_event(record)    return 'OK'def log_document_db_event(record):    event_data = record.get('event', {})    operation_type = event_data.get('operationType', 'Unknown')    db = event_data.get('ns', {}).get('db', 'Unknown')    collection = event_data.get('ns', {}).get('coll', 'Unknown')    full_document = event_data.get('fullDocument', {})    print(f\"Operation type: {operation_type}\")    print(f\"db: {db}\")    print(f\"collection: {collection}\")    print(\"Full document:\", json.dumps(full_document, indent=2))RubySDK for RubyNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Consuming a Amazon DocumentDB event with Lambda using Ruby.require 'json'def lambda_handler(event:, context:)  event['events'].each do |record|    log_document_db_event(record)  end  'OK'enddef log_document_db_event(record)  event_data = record['event'] || {}  operation_type = event_data['operationType'] || 'Unknown'  db = event_data.dig('ns', 'db') || 'Unknown'  collection = event_data.dig('ns', 'coll') || 'Unknown'  full_document = event_data['fullDocument'] || {}  puts \"Operation type: #{operation_type}\"  puts \"db: #{db}\"  puts \"collection: #{collection}\"  puts \"Full document: #{JSON.pretty_generate(full_document)}\"endRustSDK for RustNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Consuming a Amazon DocumentDB event with Lambda using Rust.use lambda_runtime::{service_fn, tracing, Error, LambdaEvent};use aws_lambda_events::{    event::documentdb::{DocumentDbEvent, DocumentDbInnerEvent},   };// Built with the following dependencies://lambda_runtime = \"0.11.1\"//serde_json = \"1.0\"//tokio = { version = \"1\", features = [\"macros\"] }//tracing = { version = \"0.1\", features = [\"log\"] }//tracing-subscriber = { version = \"0.3\", default-features = false, features = [\"fmt\"] }//aws_lambda_events = \"0.15.0\"async fn function_handler(event: LambdaEvent<DocumentDbEvent>) ->Result<(), Error> {        tracing::info!(\"Event Source ARN: {:?}\", event.payload.event_source_arn);    tracing::info!(\"Event Source: {:?}\", event.payload.event_source);      let records = &event.payload.events;       if records.is_empty() {        tracing::info!(\"No records found. Exiting.\");        return Ok(());    }    for record in records{        log_document_db_event(record);    }    tracing::info!(\"Document db records processed\");    // Prepare the response    Ok(())}fn log_document_db_event(record: &DocumentDbInnerEvent)-> Result<(), Error>{    tracing::info!(\"Change Event: {:?}\", record.event);        Ok(())}#[tokio::main]async fn main() -> Result<(), Error> {    tracing_subscriber::fmt()    .with_max_level(tracing::Level::INFO)    .with_target(false)    .without_time()    .init();    let func = service_fn(function_handler);    lambda_runtime::run(func).await?;    Ok(())    }anchoranchoranchoranchoranchoranchoranchor.NETGoJavaScriptPHPPythonRubyRustAWS SDK for .NETNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Consuming a Amazon DocumentDB event with Lambda using .NET.using Amazon.Lambda.Core;using System.Text.Json;using System;using System.Collections.Generic;using System.Text.Json.Serialization;//Assembly attribute to enable the Lambda function's JSON input to be converted into a .NET class.[assembly: LambdaSerializer(typeof(Amazon.Lambda.Serialization.SystemTextJson.DefaultLambdaJsonSerializer))]namespace LambdaDocDb;public class Function{         /// <summary>    /// Lambda function entry point to process Amazon DocumentDB events.    /// </summary>    /// <param name=\"event\">The Amazon DocumentDB event.</param>    /// <param name=\"context\">The Lambda context object.</param>    /// <returns>A string to indicate successful processing.</returns>    public string FunctionHandler(Event evnt, ILambdaContext context)    {                foreach (var record in evnt.Events)        {            ProcessDocumentDBEvent(record, context);        }        return \"OK\";    }     private void ProcessDocumentDBEvent(DocumentDBEventRecord record, ILambdaContext context)    {                var eventData = record.Event;        var operationType = eventData.OperationType;        var databaseName = eventData.Ns.Db;        var collectionName = eventData.Ns.Coll;        var fullDocument = JsonSerializer.Serialize(eventData.FullDocument, new JsonSerializerOptions { WriteIndented = true });        context.Logger.LogLine($\"Operation type: {operationType}\");        context.Logger.LogLine($\"Database: {databaseName}\");        context.Logger.LogLine($\"Collection: {collectionName}\");        context.Logger.LogLine($\"Full document:\\n{fullDocument}\");    }    public class Event    {        [JsonPropertyName(\"eventSourceArn\")]        public string EventSourceArn { get; set; }        [JsonPropertyName(\"events\")]        public List<DocumentDBEventRecord> Events { get; set; }        [JsonPropertyName(\"eventSource\")]        public string EventSource { get; set; }    }    public class DocumentDBEventRecord    {        [JsonPropertyName(\"event\")]        public EventData Event { get; set; }    }    public class EventData    {        [JsonPropertyName(\"_id\")]        public IdData Id { get; set; }        [JsonPropertyName(\"clusterTime\")]        public ClusterTime ClusterTime { get; set; }        [JsonPropertyName(\"documentKey\")]        public DocumentKey DocumentKey { get; set; }        [JsonPropertyName(\"fullDocument\")]        public Dictionary<string, object> FullDocument { get; set; }        [JsonPropertyName(\"ns\")]        public Namespace Ns { get; set; }        [JsonPropertyName(\"operationType\")]        public string OperationType { get; set; }    }    public class IdData    {        [JsonPropertyName(\"_data\")]        public string Data { get; set; }    }    public class ClusterTime    {        [JsonPropertyName(\"$timestamp\")]        public Timestamp Timestamp { get; set; }    }    public class Timestamp    {        [JsonPropertyName(\"t\")]        public long T { get; set; }        [JsonPropertyName(\"i\")]        public int I { get; set; }    }    public class DocumentKey    {        [JsonPropertyName(\"_id\")]        public Id Id { get; set; }    }    public class Id    {        [JsonPropertyName(\"$oid\")]        public string Oid { get; set; }    }    public class Namespace    {        [JsonPropertyName(\"db\")]        public string Db { get; set; }        [JsonPropertyName(\"coll\")]        public string Coll { get; set; }    }}",
                            "For a complete list of AWS SDK developer guides and code examples, see    Using Lambda with an AWS SDK.    This topic also includes information about getting started and details about previous SDK versions."
                        ]
                    },
                    {
                        "title": "Invoke a Lambda function from an Amazon MSK trigger",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_serverless_MSK_Lambda_section.html",
                        "sections": [
                            "",
                            "The following code examples show how to implement a Lambda function that receives an event triggered by receiving records from an Amazon MSK cluster. The function retrieves the MSK payload and logs the record contents.",
                            ".NETAWS SDK for .NETNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Consuming an Amazon MSK event with Lambda using .NET.using System.Text;using Amazon.Lambda.Core;using Amazon.Lambda.KafkaEvents;// Assembly attribute to enable the Lambda function's JSON input to be converted into a .NET class.[assembly: LambdaSerializer(typeof(Amazon.Lambda.Serialization.SystemTextJson.DefaultLambdaJsonSerializer))]namespace MSKLambda;public class Function{            /// <param name=\"input\">The event for the Lambda function handler to process.</param>    /// <param name=\"context\">The ILambdaContext that provides methods for logging and describing the Lambda environment.</param>    /// <returns></returns>    public void FunctionHandler(KafkaEvent evnt, ILambdaContext context)    {        foreach (var record in evnt.Records)        {            Console.WriteLine(\"Key:\" + record.Key);             foreach (var eventRecord in record.Value)            {                var valueBytes = eventRecord.Value.ToArray();                    var valueText = Encoding.UTF8.GetString(valueBytes);                                Console.WriteLine(\"Message:\" + valueText);            }        }    }    }GoSDK for Go V2Note        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Consuming an Amazon MSK event with Lambda using Go.package mainimport (\t\"encoding/base64\"\t\"fmt\"\t\"github.com/aws/aws-lambda-go/events\"\t\"github.com/aws/aws-lambda-go/lambda\")func handler(event events.KafkaEvent) {\tfor key, records := range event.Records {\t\tfmt.Println(\"Key:\", key)\t\tfor _, record := range records {\t\t\tfmt.Println(\"Record:\", record)\t\t\tdecodedValue, _ := base64.StdEncoding.DecodeString(record.Value)\t\t\tmessage := string(decodedValue)\t\t\tfmt.Println(\"Message:\", message)\t\t}\t}}func main() {\tlambda.Start(handler)}JavaSDK for Java 2.xNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Consuming an Amazon MSK event with Lambda using Java.import com.amazonaws.services.lambda.runtime.Context;import com.amazonaws.services.lambda.runtime.RequestHandler;import com.amazonaws.services.lambda.runtime.events.KafkaEvent;import com.amazonaws.services.lambda.runtime.events.KafkaEvent.KafkaEventRecord;import java.util.Base64;import java.util.Map;public class Example implements RequestHandler<KafkaEvent, Void> {    @Override    public Void handleRequest(KafkaEvent event, Context context) {        for (Map.Entry<String, java.util.List<KafkaEventRecord>> entry : event.getRecords().entrySet()) {            String key = entry.getKey();            System.out.println(\"Key: \" + key);            for (KafkaEventRecord record : entry.getValue()) {                System.out.println(\"Record: \" + record);                byte[] value = Base64.getDecoder().decode(record.getValue());                String message = new String(value);                System.out.println(\"Message: \" + message);            }        }        return null;    }}JavaScriptSDK for JavaScript (v3)Note        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Consuming an Amazon MSK event with Lambda using JavaScript.exports.handler = async (event) => {    // Iterate through keys    for (let key in event.records) {      console.log('Key: ', key)      // Iterate through records      event.records[key].map((record) => {        console.log('Record: ', record)        // Decode base64        const msg = Buffer.from(record.value, 'base64').toString()        console.log('Message:', msg)      })     }}PHPSDK for PHPNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Consuming an Amazon MSK event with Lambda using PHP.<?php// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.// SPDX-License-Identifier: Apache-2.0// using bref/bref and bref/logger for simplicityuse Bref\\Context\\Context;use Bref\\Event\\Kafka\\KafkaEvent;use Bref\\Event\\Handler as StdHandler;use Bref\\Logger\\StderrLogger;require __DIR__ . '/vendor/autoload.php';class Handler implements StdHandler{    private StderrLogger $logger;    public function __construct(StderrLogger $logger)    {        $this->logger = $logger;    }    /**     * @throws JsonException     * @throws \\Bref\\Event\\InvalidLambdaEvent     */    public function handle(mixed $event, Context $context): void    {        $kafkaEvent = new KafkaEvent($event);        $this->logger->info(\"Processing records\");        $records = $kafkaEvent->getRecords();        foreach ($records as $record) {            try {                $key = $record->getKey();                $this->logger->info(\"Key: $key\");                $values = $record->getValue();                $this->logger->info(json_encode($values));                foreach ($values as $value) {                    $this->logger->info(\"Value: $value\");                }                            } catch (Exception $e) {                $this->logger->error($e->getMessage());            }        }        $totalRecords = count($records);        $this->logger->info(\"Successfully processed $totalRecords records\");    }}$logger = new StderrLogger();return new Handler($logger);PythonSDK for Python (Boto3)Note        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Consuming an Amazon MSK event with Lambda using Python.import base64def lambda_handler(event, context):    # Iterate through keys    for key in event['records']:        print('Key:', key)        # Iterate through records        for record in event['records'][key]:            print('Record:', record)            # Decode base64            msg = base64.b64decode(record['value']).decode('utf-8')            print('Message:', msg)RubySDK for RubyNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Consuming an Amazon MSK event with Lambda using Ruby.require 'base64'def lambda_handler(event:, context:)  # Iterate through keys  event['records'].each do |key, records|    puts \"Key: #{key}\"    # Iterate through records    records.each do |record|      puts \"Record: #{record}\"      # Decode base64      msg = Base64.decode64(record['value'])      puts \"Message: #{msg}\"    end  endendanchoranchoranchoranchoranchoranchoranchor.NETGoJavaJavaScriptPHPPythonRubyAWS SDK for .NETNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Consuming an Amazon MSK event with Lambda using .NET.using System.Text;using Amazon.Lambda.Core;using Amazon.Lambda.KafkaEvents;// Assembly attribute to enable the Lambda function's JSON input to be converted into a .NET class.[assembly: LambdaSerializer(typeof(Amazon.Lambda.Serialization.SystemTextJson.DefaultLambdaJsonSerializer))]namespace MSKLambda;public class Function{            /// <param name=\"input\">The event for the Lambda function handler to process.</param>    /// <param name=\"context\">The ILambdaContext that provides methods for logging and describing the Lambda environment.</param>    /// <returns></returns>    public void FunctionHandler(KafkaEvent evnt, ILambdaContext context)    {        foreach (var record in evnt.Records)        {            Console.WriteLine(\"Key:\" + record.Key);             foreach (var eventRecord in record.Value)            {                var valueBytes = eventRecord.Value.ToArray();                    var valueText = Encoding.UTF8.GetString(valueBytes);                                Console.WriteLine(\"Message:\" + valueText);            }        }    }    }",
                            "For a complete list of AWS SDK developer guides and code examples, see    Using Lambda with an AWS SDK.    This topic also includes information about getting started and details about previous SDK versions."
                        ]
                    },
                    {
                        "title": "Invoke a Lambda function from an Amazon S3 trigger",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_serverless_S3_Lambda_section.html",
                        "sections": [
                            "",
                            "The following code examples show how to implement a Lambda function that receives an event triggered by uploading an object to an S3 bucket. The function retrieves the S3 bucket name and object key from the event parameter and calls the Amazon S3 API to retrieve and log the content type of the object.",
                            ".NETAWS SDK for .NETNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Consuming an S3 event with Lambda using .NET.// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.// SPDX-License-Identifier: Apache-2.0﻿using System.Threading.Tasks;using Amazon.Lambda.Core;using Amazon.S3;using System;using Amazon.Lambda.S3Events;using System.Web;// Assembly attribute to enable the Lambda function's JSON input to be converted into a .NET class.[assembly: LambdaSerializer(typeof(Amazon.Lambda.Serialization.SystemTextJson.DefaultLambdaJsonSerializer))]namespace S3Integration{    public class Function    {        private static AmazonS3Client _s3Client;        public Function() : this(null)        {        }        internal Function(AmazonS3Client s3Client)        {            _s3Client = s3Client ?? new AmazonS3Client();        }        public async Task<string> Handler(S3Event evt, ILambdaContext context)        {            try            {                if (evt.Records.Count <= 0)                {                    context.Logger.LogLine(\"Empty S3 Event received\");                    return string.Empty;                }                var bucket = evt.Records[0].S3.Bucket.Name;                var key = HttpUtility.UrlDecode(evt.Records[0].S3.Object.Key);                context.Logger.LogLine($\"Request is for {bucket} and {key}\");                var objectResult = await _s3Client.GetObjectAsync(bucket, key);                context.Logger.LogLine($\"Returning {objectResult.Key}\");                return objectResult.Key;            }            catch (Exception e)            {                context.Logger.LogLine($\"Error processing request - {e.Message}\");                return string.Empty;            }        }    }}GoSDK for Go V2Note        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Consuming an S3 event with Lambda using Go.// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.// SPDX-License-Identifier: Apache-2.0package mainimport (\t\"context\"\t\"log\"\t\"github.com/aws/aws-lambda-go/events\"\t\"github.com/aws/aws-lambda-go/lambda\"\t\"github.com/aws/aws-sdk-go-v2/config\"\t\"github.com/aws/aws-sdk-go-v2/service/s3\")func handler(ctx context.Context, s3Event events.S3Event) error {\tsdkConfig, err := config.LoadDefaultConfig(ctx)\tif err != nil {\t\tlog.Printf(\"failed to load default config: %s\", err)\t\treturn err\t}\ts3Client := s3.NewFromConfig(sdkConfig)\tfor _, record := range s3Event.Records {\t\tbucket := record.S3.Bucket.Name\t\tkey := record.S3.Object.URLDecodedKey\t\theadOutput, err := s3Client.HeadObject(ctx, &s3.HeadObjectInput{\t\t\tBucket: &bucket,\t\t\tKey:    &key,\t\t})\t\tif err != nil {\t\t\tlog.Printf(\"error getting head of object %s/%s: %s\", bucket, key, err)\t\t\treturn err\t\t}\t\tlog.Printf(\"successfully retrieved %s/%s of type %s\", bucket, key, *headOutput.ContentType)\t}\treturn nil}func main() {\tlambda.Start(handler)}JavaSDK for Java 2.xNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Consuming an S3 event with Lambda using Java.// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.// SPDX-License-Identifier: Apache-2.0package example;import software.amazon.awssdk.services.s3.model.HeadObjectRequest;import software.amazon.awssdk.services.s3.model.HeadObjectResponse;import software.amazon.awssdk.services.s3.S3Client;import com.amazonaws.services.lambda.runtime.Context;import com.amazonaws.services.lambda.runtime.RequestHandler;import com.amazonaws.services.lambda.runtime.events.S3Event;import com.amazonaws.services.lambda.runtime.events.models.s3.S3EventNotification.S3EventNotificationRecord;import org.slf4j.Logger;import org.slf4j.LoggerFactory;public class Handler implements RequestHandler<S3Event, String> {    private static final Logger logger = LoggerFactory.getLogger(Handler.class);    @Override    public String handleRequest(S3Event s3event, Context context) {        try {          S3EventNotificationRecord record = s3event.getRecords().get(0);          String srcBucket = record.getS3().getBucket().getName();          String srcKey = record.getS3().getObject().getUrlDecodedKey();          S3Client s3Client = S3Client.builder().build();          HeadObjectResponse headObject = getHeadObject(s3Client, srcBucket, srcKey);          logger.info(\"Successfully retrieved \" + srcBucket + \"/\" + srcKey + \" of type \" + headObject.contentType());          return \"Ok\";        } catch (Exception e) {          throw new RuntimeException(e);        }    }    private HeadObjectResponse getHeadObject(S3Client s3Client, String bucket, String key) {        HeadObjectRequest headObjectRequest = HeadObjectRequest.builder()                .bucket(bucket)                .key(key)                .build();        return s3Client.headObject(headObjectRequest);    }}JavaScriptSDK for JavaScript (v3)Note        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Consuming an S3 event with Lambda using JavaScript.import { S3Client, HeadObjectCommand } from \"@aws-sdk/client-s3\";const client = new S3Client();export const handler = async (event, context) => {    // Get the object from the event and show its content type    const bucket = event.Records[0].s3.bucket.name;    const key = decodeURIComponent(event.Records[0].s3.object.key.replace(/\\+/g, ' '));    try {        const { ContentType } = await client.send(new HeadObjectCommand({            Bucket: bucket,            Key: key,        }));        console.log('CONTENT TYPE:', ContentType);        return ContentType;    } catch (err) {        console.log(err);        const message = `Error getting object ${key} from bucket ${bucket}. Make sure they exist and your bucket is in the same region as this function.`;        console.log(message);        throw new Error(message);    }};Consuming an S3 event with Lambda using TypeScript.// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.// SPDX-License-Identifier: Apache-2.0import { S3Event } from 'aws-lambda';import { S3Client, HeadObjectCommand } from '@aws-sdk/client-s3';const s3 = new S3Client({ region: process.env.AWS_REGION });export const handler = async (event: S3Event): Promise<string | undefined> => {  // Get the object from the event and show its content type  const bucket = event.Records[0].s3.bucket.name;  const key = decodeURIComponent(event.Records[0].s3.object.key.replace(/\\+/g, ' '));  const params = {    Bucket: bucket,    Key: key,  };  try {    const { ContentType } = await s3.send(new HeadObjectCommand(params));    console.log('CONTENT TYPE:', ContentType);    return ContentType;  } catch (err) {    console.log(err);    const message = `Error getting object ${key} from bucket ${bucket}. Make sure they exist and your bucket is in the same region as this function.`;    console.log(message);    throw new Error(message);  }};PHPSDK for PHPNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Consuming an S3 event with Lambda using PHP.<?phpuse Bref\\Context\\Context;use Bref\\Event\\S3\\S3Event;use Bref\\Event\\S3\\S3Handler;use Bref\\Logger\\StderrLogger;require __DIR__ . '/vendor/autoload.php';class Handler extends S3Handler {    private StderrLogger $logger;    public function __construct(StderrLogger $logger)    {        $this->logger = $logger;    }        public function handleS3(S3Event $event, Context $context) : void    {        $this->logger->info(\"Processing S3 records\");        // Get the object from the event and show its content type        $records = $event->getRecords();                foreach ($records as $record)         {            $bucket = $record->getBucket()->getName();            $key = urldecode($record->getObject()->getKey());            try {                $fileSize = urldecode($record->getObject()->getSize());                echo \"File Size: \" . $fileSize . \"\\n\";                // TODO: Implement your custom processing logic here            } catch (Exception $e) {                echo $e->getMessage() . \"\\n\";                echo 'Error getting object ' . $key . ' from bucket ' . $bucket . '. Make sure they exist and your bucket is in the same region as this function.' . \"\\n\";                throw $e;            }        }    }}$logger = new StderrLogger();return new Handler($logger);PythonSDK for Python (Boto3)Note        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Consuming an S3 event with Lambda using Python.# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.# SPDX-License-Identifier: Apache-2.0import jsonimport urllib.parseimport boto3print('Loading function')s3 = boto3.client('s3')def lambda_handler(event, context):    #print(\"Received event: \" + json.dumps(event, indent=2))    # Get the object from the event and show its content type    bucket = event['Records'][0]['s3']['bucket']['name']    key = urllib.parse.unquote_plus(event['Records'][0]['s3']['object']['key'], encoding='utf-8')    try:        response = s3.get_object(Bucket=bucket, Key=key)        print(\"CONTENT TYPE: \" + response['ContentType'])        return response['ContentType']    except Exception as e:        print(e)        print('Error getting object {} from bucket {}. Make sure they exist and your bucket is in the same region as this function.'.format(key, bucket))        raise e              RubySDK for RubyNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Consuming an S3 event with Lambda using Ruby.require 'json'require 'uri'require 'aws-sdk'puts 'Loading function'def lambda_handler(event:, context:)  s3 = Aws::S3::Client.new(region: 'region') # Your AWS region  # puts \"Received event: #{JSON.dump(event)}\"  # Get the object from the event and show its content type  bucket = event['Records'][0]['s3']['bucket']['name']  key = URI.decode_www_form_component(event['Records'][0]['s3']['object']['key'], Encoding::UTF_8)  begin    response = s3.get_object(bucket: bucket, key: key)    puts \"CONTENT TYPE: #{response.content_type}\"    return response.content_type  rescue StandardError => e    puts e.message    puts \"Error getting object #{key} from bucket #{bucket}. Make sure they exist and your bucket is in the same region as this function.\"    raise e  endendRustSDK for RustNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Consuming an S3 event with Lambda using Rust.// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.// SPDX-License-Identifier: Apache-2.0use aws_lambda_events::event::s3::S3Event;use aws_sdk_s3::{Client};use lambda_runtime::{run, service_fn, Error, LambdaEvent};/// Main function#[tokio::main]async fn main() -> Result<(), Error> {    tracing_subscriber::fmt()        .with_max_level(tracing::Level::INFO)        .with_target(false)        .without_time()        .init();    // Initialize the AWS SDK for Rust    let config = aws_config::load_from_env().await;    let s3_client = Client::new(&config);    let res = run(service_fn(|request: LambdaEvent<S3Event>| {        function_handler(&s3_client, request)    })).await;    res}async fn function_handler(    s3_client: &Client,    evt: LambdaEvent<S3Event>) -> Result<(), Error> {    tracing::info!(records = ?evt.payload.records.len(), \"Received request from SQS\");    if evt.payload.records.len() == 0 {        tracing::info!(\"Empty S3 event received\");    }    let bucket = evt.payload.records[0].s3.bucket.name.as_ref().expect(\"Bucket name to exist\");    let key = evt.payload.records[0].s3.object.key.as_ref().expect(\"Object key to exist\");    tracing::info!(\"Request is for {} and object {}\", bucket, key);    let s3_get_object_result = s3_client        .get_object()        .bucket(bucket)        .key(key)        .send()        .await;    match s3_get_object_result {        Ok(_) => tracing::info!(\"S3 Get Object success, the s3GetObjectResult contains a 'body' property of type ByteStream\"),        Err(_) => tracing::info!(\"Failure with S3 Get Object request\")    }    Ok(())}anchoranchoranchoranchoranchoranchoranchoranchor.NETGoJavaJavaScriptPHPPythonRubyRustAWS SDK for .NETNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Consuming an S3 event with Lambda using .NET.// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.// SPDX-License-Identifier: Apache-2.0﻿using System.Threading.Tasks;using Amazon.Lambda.Core;using Amazon.S3;using System;using Amazon.Lambda.S3Events;using System.Web;// Assembly attribute to enable the Lambda function's JSON input to be converted into a .NET class.[assembly: LambdaSerializer(typeof(Amazon.Lambda.Serialization.SystemTextJson.DefaultLambdaJsonSerializer))]namespace S3Integration{    public class Function    {        private static AmazonS3Client _s3Client;        public Function() : this(null)        {        }        internal Function(AmazonS3Client s3Client)        {            _s3Client = s3Client ?? new AmazonS3Client();        }        public async Task<string> Handler(S3Event evt, ILambdaContext context)        {            try            {                if (evt.Records.Count <= 0)                {                    context.Logger.LogLine(\"Empty S3 Event received\");                    return string.Empty;                }                var bucket = evt.Records[0].S3.Bucket.Name;                var key = HttpUtility.UrlDecode(evt.Records[0].S3.Object.Key);                context.Logger.LogLine($\"Request is for {bucket} and {key}\");                var objectResult = await _s3Client.GetObjectAsync(bucket, key);                context.Logger.LogLine($\"Returning {objectResult.Key}\");                return objectResult.Key;            }            catch (Exception e)            {                context.Logger.LogLine($\"Error processing request - {e.Message}\");                return string.Empty;            }        }    }}",
                            "For a complete list of AWS SDK developer guides and code examples, see    Using Lambda with an AWS SDK.    This topic also includes information about getting started and details about previous SDK versions."
                        ]
                    },
                    {
                        "title": "Invoke a Lambda function from an Amazon SNS trigger",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_serverless_SNS_Lambda_section.html",
                        "sections": [
                            "",
                            "The following code examples show how to implement a Lambda function that receives an event triggered by receiving messages from an SNS topic. The function retrieves the messages from the event parameter and logs the content of each message.",
                            ".NETAWS SDK for .NETNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Consuming an SNS event with Lambda using .NET.// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.// SPDX-License-Identifier: Apache-2.0using Amazon.Lambda.Core;using Amazon.Lambda.SNSEvents;// Assembly attribute to enable the Lambda function's JSON input to be converted into a .NET class.[assembly: LambdaSerializer(typeof(Amazon.Lambda.Serialization.SystemTextJson.DefaultLambdaJsonSerializer))]namespace SnsIntegration;public class Function{    public async Task FunctionHandler(SNSEvent evnt, ILambdaContext context)    {        foreach (var record in evnt.Records)        {            await ProcessRecordAsync(record, context);        }        context.Logger.LogInformation(\"done\");    }    private async Task ProcessRecordAsync(SNSEvent.SNSRecord record, ILambdaContext context)    {        try        {            context.Logger.LogInformation($\"Processed record {record.Sns.Message}\");            // TODO: Do interesting work based on the new message            await Task.CompletedTask;        }        catch (Exception e)        {            //You can use Dead Letter Queue to handle failures. By configuring a Lambda DLQ.            context.Logger.LogError($\"An error occurred\");            throw;        }    }}GoSDK for Go V2Note        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Consuming an SNS event with Lambda using Go.// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.// SPDX-License-Identifier: Apache-2.0package mainimport (\t\"context\"\t\"fmt\"\t\"github.com/aws/aws-lambda-go/events\"\t\"github.com/aws/aws-lambda-go/lambda\")func handler(ctx context.Context, snsEvent events.SNSEvent) {\tfor _, record := range snsEvent.Records {\t\tprocessMessage(record)\t}\tfmt.Println(\"done\")}func processMessage(record events.SNSEventRecord) {\tmessage := record.SNS.Message\tfmt.Printf(\"Processed message: %s\\n\", message)\t// TODO: Process your record here}func main() {\tlambda.Start(handler)}JavaSDK for Java 2.xNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Consuming an SNS event with Lambda using Java.// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.// SPDX-License-Identifier: Apache-2.0package example;import com.amazonaws.services.lambda.runtime.Context;import com.amazonaws.services.lambda.runtime.LambdaLogger;import com.amazonaws.services.lambda.runtime.RequestHandler;import com.amazonaws.services.lambda.runtime.events.SNSEvent;import com.amazonaws.services.lambda.runtime.events.SNSEvent.SNSRecord;import java.util.Iterator;import java.util.List;public class SNSEventHandler implements RequestHandler<SNSEvent, Boolean> {    LambdaLogger logger;    @Override    public Boolean handleRequest(SNSEvent event, Context context) {        logger = context.getLogger();        List<SNSRecord> records = event.getRecords();        if (!records.isEmpty()) {            Iterator<SNSRecord> recordsIter = records.iterator();            while (recordsIter.hasNext()) {                processRecord(recordsIter.next());            }        }        return Boolean.TRUE;    }    public void processRecord(SNSRecord record) {        try {            String message = record.getSNS().getMessage();            logger.log(\"message: \" + message);        } catch (Exception e) {            throw new RuntimeException(e);        }    }}JavaScriptSDK for JavaScript (v3)Note        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Consuming an SNS event with Lambda using JavaScript.// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.// SPDX-License-Identifier: Apache-2.0exports.handler = async (event, context) => {  for (const record of event.Records) {    await processMessageAsync(record);  }  console.info(\"done\");};async function processMessageAsync(record) {  try {    const message = JSON.stringify(record.Sns.Message);    console.log(`Processed message ${message}`);    await Promise.resolve(1); //Placeholder for actual async work  } catch (err) {    console.error(\"An error occurred\");    throw err;  }}Consuming an SNS event with Lambda using TypeScript.// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.// SPDX-License-Identifier: Apache-2.0import { SNSEvent, Context, SNSHandler, SNSEventRecord } from \"aws-lambda\";export const functionHandler: SNSHandler = async (  event: SNSEvent,  context: Context): Promise<void> => {  for (const record of event.Records) {    await processMessageAsync(record);  }  console.info(\"done\");};async function processMessageAsync(record: SNSEventRecord): Promise<any> {  try {    const message: string = JSON.stringify(record.Sns.Message);    console.log(`Processed message ${message}`);    await Promise.resolve(1); //Placeholder for actual async work  } catch (err) {    console.error(\"An error occurred\");    throw err;  }}PHPSDK for PHPNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Consuming an SNS event with Lambda using PHP.// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.// SPDX-License-Identifier: Apache-2.0<?php/* Since native PHP support for AWS Lambda is not available, we are utilizing Bref's PHP functions runtime for AWS Lambda.For more information on Bref's PHP runtime for Lambda, refer to: https://bref.sh/docs/runtimes/functionAnother approach would be to create a custom runtime. A practical example can be found here: https://aws.amazon.com/blogs/apn/aws-lambda-custom-runtime-for-php-a-practical-example/*/// Additional composer packages may be required when using Bref or any other PHP functions runtime.// require __DIR__ . '/vendor/autoload.php';use Bref\\Context\\Context;use Bref\\Event\\Sns\\SnsEvent;use Bref\\Event\\Sns\\SnsHandler;class Handler extends SnsHandler{    public function handleSns(SnsEvent $event, Context $context): void    {        foreach ($event->getRecords() as $record) {            $message = $record->getMessage();            // TODO: Implement your custom processing logic here            // Any exception thrown will be logged and the invocation will be marked as failed            echo \"Processed Message: $message\" . PHP_EOL;        }    }}return new Handler();PythonSDK for Python (Boto3)Note        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Consuming an SNS event with Lambda using Python.# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.# SPDX-License-Identifier: Apache-2.0def lambda_handler(event, context):    for record in event['Records']:        process_message(record)    print(\"done\")def process_message(record):    try:        message = record['Sns']['Message']        print(f\"Processed message {message}\")        # TODO; Process your record here            except Exception as e:        print(\"An error occurred\")        raise eRubySDK for RubyNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Consuming an SNS event with Lambda using Ruby.# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.# SPDX-License-Identifier: Apache-2.0def lambda_handler(event:, context:)  event['Records'].map { |record| process_message(record) }enddef process_message(record)  message = record['Sns']['Message']  puts(\"Processing message: #{message}\")rescue StandardError => e  puts(\"Error processing message: #{e}\")  raiseendRustSDK for RustNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Consuming an SNS event with Lambda using Rust.// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.// SPDX-License-Identifier: Apache-2.0use aws_lambda_events::event::sns::SnsEvent;use aws_lambda_events::sns::SnsRecord;use lambda_runtime::{run, service_fn, Error, LambdaEvent};use tracing::info;// Built with the following dependencies://  aws_lambda_events = { version = \"0.10.0\", default-features = false, features = [\"sns\"] }//  lambda_runtime = \"0.8.1\"//  tokio = { version = \"1\", features = [\"macros\"] }//  tracing = { version = \"0.1\", features = [\"log\"] }//  tracing-subscriber = { version = \"0.3\", default-features = false, features = [\"fmt\"] }async fn function_handler(event: LambdaEvent<SnsEvent>) -> Result<(), Error> {    for event in event.payload.records {        process_record(&event)?;    }        Ok(())}fn process_record(record: &SnsRecord) -> Result<(), Error> {    info!(\"Processing SNS Message: {}\", record.sns.message);    // Implement your record handling code here.    Ok(())}#[tokio::main]async fn main() -> Result<(), Error> {    tracing_subscriber::fmt()        .with_max_level(tracing::Level::INFO)        .with_target(false)        .without_time()        .init();    run(service_fn(function_handler)).await}anchoranchoranchoranchoranchoranchoranchoranchor.NETGoJavaJavaScriptPHPPythonRubyRustAWS SDK for .NETNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Consuming an SNS event with Lambda using .NET.// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.// SPDX-License-Identifier: Apache-2.0using Amazon.Lambda.Core;using Amazon.Lambda.SNSEvents;// Assembly attribute to enable the Lambda function's JSON input to be converted into a .NET class.[assembly: LambdaSerializer(typeof(Amazon.Lambda.Serialization.SystemTextJson.DefaultLambdaJsonSerializer))]namespace SnsIntegration;public class Function{    public async Task FunctionHandler(SNSEvent evnt, ILambdaContext context)    {        foreach (var record in evnt.Records)        {            await ProcessRecordAsync(record, context);        }        context.Logger.LogInformation(\"done\");    }    private async Task ProcessRecordAsync(SNSEvent.SNSRecord record, ILambdaContext context)    {        try        {            context.Logger.LogInformation($\"Processed record {record.Sns.Message}\");            // TODO: Do interesting work based on the new message            await Task.CompletedTask;        }        catch (Exception e)        {            //You can use Dead Letter Queue to handle failures. By configuring a Lambda DLQ.            context.Logger.LogError($\"An error occurred\");            throw;        }    }}",
                            "For a complete list of AWS SDK developer guides and code examples, see    Using Lambda with an AWS SDK.    This topic also includes information about getting started and details about previous SDK versions."
                        ]
                    },
                    {
                        "title": "Invoke a Lambda function from an Amazon SQS trigger",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_serverless_SQS_Lambda_section.html",
                        "sections": [
                            "",
                            "The following code examples show how to implement a Lambda function that receives an event triggered by receiving messages from an SQS queue. The function retrieves the messages from the event parameter and logs the content of each message.",
                            ".NETAWS SDK for .NETNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Consuming an SQS event with Lambda using .NET.// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.// SPDX-License-Identifier: Apache-2.0﻿using Amazon.Lambda.Core;using Amazon.Lambda.SQSEvents;// Assembly attribute to enable the Lambda function's JSON input to be converted into a .NET class.[assembly: LambdaSerializer(typeof(Amazon.Lambda.Serialization.SystemTextJson.DefaultLambdaJsonSerializer))]namespace SqsIntegrationSampleCode{    public async Task FunctionHandler(SQSEvent evnt, ILambdaContext context)    {        foreach (var message in evnt.Records)        {            await ProcessMessageAsync(message, context);        }        context.Logger.LogInformation(\"done\");    }    private async Task ProcessMessageAsync(SQSEvent.SQSMessage message, ILambdaContext context)    {        try        {            context.Logger.LogInformation($\"Processed message {message.Body}\");            // TODO: Do interesting work based on the new message            await Task.CompletedTask;        }        catch (Exception e)        {            //You can use Dead Letter Queue to handle failures. By configuring a Lambda DLQ.            context.Logger.LogError($\"An error occurred\");            throw;        }    }}GoSDK for Go V2Note        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Consuming an SQS event with Lambda using Go.// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.// SPDX-License-Identifier: Apache-2.0package integration_sqs_to_lambdaimport (\t\"fmt\"\t\"github.com/aws/aws-lambda-go/events\"\t\"github.com/aws/aws-lambda-go/lambda\")func handler(event events.SQSEvent) error {\tfor _, record := range event.Records {\t\terr := processMessage(record)\t\tif err != nil {\t\t\treturn err\t\t}\t}\tfmt.Println(\"done\")\treturn nil}func processMessage(record events.SQSMessage) error {\tfmt.Printf(\"Processed message %s\\n\", record.Body)\t// TODO: Do interesting work based on the new message\treturn nil}func main() {\tlambda.Start(handler)}JavaSDK for Java 2.xNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Consuming an SQS event with Lambda using Java.// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.// SPDX-License-Identifier: Apache-2.0import com.amazonaws.services.lambda.runtime.Context;import com.amazonaws.services.lambda.runtime.RequestHandler;import com.amazonaws.services.lambda.runtime.events.SQSEvent;import com.amazonaws.services.lambda.runtime.events.SQSEvent.SQSMessage;public class Function implements RequestHandler<SQSEvent, Void> {    @Override    public Void handleRequest(SQSEvent sqsEvent, Context context) {        for (SQSMessage msg : sqsEvent.getRecords()) {            processMessage(msg, context);        }        context.getLogger().log(\"done\");        return null;    }    private void processMessage(SQSMessage msg, Context context) {        try {            context.getLogger().log(\"Processed message \" + msg.getBody());            // TODO: Do interesting work based on the new message        } catch (Exception e) {            context.getLogger().log(\"An error occurred\");            throw e;        }    }}JavaScriptSDK for JavaScript (v3)Note        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Consuming an SQS event with Lambda using JavaScript.// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.// SPDX-License-Identifier: Apache-2.0exports.handler = async (event, context) => {  for (const message of event.Records) {    await processMessageAsync(message);  }  console.info(\"done\");};async function processMessageAsync(message) {  try {    console.log(`Processed message ${message.body}`);    // TODO: Do interesting work based on the new message    await Promise.resolve(1); //Placeholder for actual async work  } catch (err) {    console.error(\"An error occurred\");    throw err;  }}Consuming an SQS event with Lambda using TypeScript.// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.// SPDX-License-Identifier: Apache-2.0import { SQSEvent, Context, SQSHandler, SQSRecord } from \"aws-lambda\";export const functionHandler: SQSHandler = async (  event: SQSEvent,  context: Context): Promise<void> => {  for (const message of event.Records) {    await processMessageAsync(message);  }  console.info(\"done\");};async function processMessageAsync(message: SQSRecord): Promise<any> {  try {    console.log(`Processed message ${message.body}`);    // TODO: Do interesting work based on the new message    await Promise.resolve(1); //Placeholder for actual async work  } catch (err) {    console.error(\"An error occurred\");    throw err;  }}PHPSDK for PHPNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Consuming an SQS event with Lambda using PHP.// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.// SPDX-License-Identifier: Apache-2.0<?php# using bref/bref and bref/logger for simplicityuse Bref\\Context\\Context;use Bref\\Event\\InvalidLambdaEvent;use Bref\\Event\\Sqs\\SqsEvent;use Bref\\Event\\Sqs\\SqsHandler;use Bref\\Logger\\StderrLogger;require __DIR__ . '/vendor/autoload.php';class Handler extends SqsHandler{    private StderrLogger $logger;    public function __construct(StderrLogger $logger)    {        $this->logger = $logger;    }    /**     * @throws InvalidLambdaEvent     */    public function handleSqs(SqsEvent $event, Context $context): void    {        foreach ($event->getRecords() as $record) {            $body = $record->getBody();            // TODO: Do interesting work based on the new message        }    }}$logger = new StderrLogger();return new Handler($logger);PythonSDK for Python (Boto3)Note        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Consuming an SQS event with Lambda using Python.# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.# SPDX-License-Identifier: Apache-2.0def lambda_handler(event, context):    for message in event['Records']:        process_message(message)    print(\"done\")def process_message(message):    try:        print(f\"Processed message {message['body']}\")        # TODO: Do interesting work based on the new message    except Exception as err:        print(\"An error occurred\")        raise errRubySDK for RubyNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Consuming an SQS event with Lambda using Ruby.# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.# SPDX-License-Identifier: Apache-2.0def lambda_handler(event:, context:)  event['Records'].each do |message|    process_message(message)  end  puts \"done\"enddef process_message(message)  begin    puts \"Processed message #{message['body']}\"    # TODO: Do interesting work based on the new message  rescue StandardError => err    puts \"An error occurred\"    raise err  endendRustSDK for RustNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Consuming an SQS event with Lambda using Rust.// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.// SPDX-License-Identifier: Apache-2.0use aws_lambda_events::event::sqs::SqsEvent;use lambda_runtime::{run, service_fn, Error, LambdaEvent};async fn function_handler(event: LambdaEvent<SqsEvent>) -> Result<(), Error> {    event.payload.records.iter().for_each(|record| {        // process the record        tracing::info!(\"Message body: {}\", record.body.as_deref().unwrap_or_default())    });    Ok(())}#[tokio::main]async fn main() -> Result<(), Error> {    tracing_subscriber::fmt()        .with_max_level(tracing::Level::INFO)        // disable printing the name of the module in every log line.        .with_target(false)        // disabling time is handy because CloudWatch will add the ingestion time.        .without_time()        .init();    run(service_fn(function_handler)).await}anchoranchoranchoranchoranchoranchoranchoranchor.NETGoJavaJavaScriptPHPPythonRubyRustAWS SDK for .NETNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Consuming an SQS event with Lambda using .NET.// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.// SPDX-License-Identifier: Apache-2.0﻿using Amazon.Lambda.Core;using Amazon.Lambda.SQSEvents;// Assembly attribute to enable the Lambda function's JSON input to be converted into a .NET class.[assembly: LambdaSerializer(typeof(Amazon.Lambda.Serialization.SystemTextJson.DefaultLambdaJsonSerializer))]namespace SqsIntegrationSampleCode{    public async Task FunctionHandler(SQSEvent evnt, ILambdaContext context)    {        foreach (var message in evnt.Records)        {            await ProcessMessageAsync(message, context);        }        context.Logger.LogInformation(\"done\");    }    private async Task ProcessMessageAsync(SQSEvent.SQSMessage message, ILambdaContext context)    {        try        {            context.Logger.LogInformation($\"Processed message {message.Body}\");            // TODO: Do interesting work based on the new message            await Task.CompletedTask;        }        catch (Exception e)        {            //You can use Dead Letter Queue to handle failures. By configuring a Lambda DLQ.            context.Logger.LogError($\"An error occurred\");            throw;        }    }}",
                            "For a complete list of AWS SDK developer guides and code examples, see    Using Lambda with an AWS SDK.    This topic also includes information about getting started and details about previous SDK versions."
                        ]
                    },
                    {
                        "title": "Reporting batch item failures for Lambda functions with a Kinesis trigger",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_serverless_Kinesis_Lambda_batch_item_failures_section.html",
                        "sections": [
                            "",
                            "The following code examples show how to implement partial batch response for Lambda functions that receive events from a Kinesis stream. The function reports the batch item failures in the response, signaling to Lambda to retry those messages later.",
                            ".NETAWS SDK for .NETNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Reporting Kinesis batch item failures with Lambda using .NET.// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.// SPDX-License-Identifier: Apache-2.0﻿using System.Text;using System.Text.Json.Serialization;using Amazon.Lambda.Core;using Amazon.Lambda.KinesisEvents;using AWS.Lambda.Powertools.Logging;// Assembly attribute to enable the Lambda function's JSON input to be converted into a .NET class.[assembly: LambdaSerializer(typeof(Amazon.Lambda.Serialization.SystemTextJson.DefaultLambdaJsonSerializer))]namespace KinesisIntegration;public class Function{    // Powertools Logger requires an environment variables against your function    // POWERTOOLS_SERVICE_NAME    [Logging(LogEvent = true)]    public async Task<StreamsEventResponse> FunctionHandler(KinesisEvent evnt, ILambdaContext context)    {        if (evnt.Records.Count == 0)        {            Logger.LogInformation(\"Empty Kinesis Event received\");            return new StreamsEventResponse();        }        foreach (var record in evnt.Records)        {            try            {                Logger.LogInformation($\"Processed Event with EventId: {record.EventId}\");                string data = await GetRecordDataAsync(record.Kinesis, context);                Logger.LogInformation($\"Data: {data}\");                // TODO: Do interesting work based on the new data            }            catch (Exception ex)            {                Logger.LogError($\"An error occurred {ex.Message}\");                /* Since we are working with streams, we can return the failed item immediately.                   Lambda will immediately begin to retry processing from this failed item onwards. */                return new StreamsEventResponse                {                    BatchItemFailures = new List<StreamsEventResponse.BatchItemFailure>                    {                        new StreamsEventResponse.BatchItemFailure { ItemIdentifier = record.Kinesis.SequenceNumber }                    }                };            }        }        Logger.LogInformation($\"Successfully processed {evnt.Records.Count} records.\");        return new StreamsEventResponse();    }    private async Task<string> GetRecordDataAsync(KinesisEvent.Record record, ILambdaContext context)    {        byte[] bytes = record.Data.ToArray();        string data = Encoding.UTF8.GetString(bytes);        await Task.CompletedTask; //Placeholder for actual async work        return data;    }}public class StreamsEventResponse{    [JsonPropertyName(\"batchItemFailures\")]    public IList<BatchItemFailure> BatchItemFailures { get; set; }    public class BatchItemFailure    {        [JsonPropertyName(\"itemIdentifier\")]        public string ItemIdentifier { get; set; }    }}GoSDK for Go V2Note        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Reporting Kinesis batch item failures with Lambda using Go.// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.// SPDX-License-Identifier: Apache-2.0package mainimport (\t\"context\"\t\"fmt\"\t\"github.com/aws/aws-lambda-go/events\"\t\"github.com/aws/aws-lambda-go/lambda\")func handler(ctx context.Context, kinesisEvent events.KinesisEvent) (map[string]interface{}, error) {\tbatchItemFailures := []map[string]interface{}{}\tfor _, record := range kinesisEvent.Records {\t\tcurRecordSequenceNumber := \"\"\t\t// Process your record\t\tif /* Your record processing condition here */ {\t\t\tcurRecordSequenceNumber = record.Kinesis.SequenceNumber\t\t}\t\t// Add a condition to check if the record processing failed\t\tif curRecordSequenceNumber != \"\" {\t\t\tbatchItemFailures = append(batchItemFailures, map[string]interface{}{\"itemIdentifier\": curRecordSequenceNumber})\t\t}\t}\tkinesisBatchResponse := map[string]interface{}{\t\t\"batchItemFailures\": batchItemFailures,\t}\treturn kinesisBatchResponse, nil}func main() {\tlambda.Start(handler)}JavaSDK for Java 2.xNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Reporting Kinesis batch item failures with Lambda using Java.// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.// SPDX-License-Identifier: Apache-2.0import com.amazonaws.services.lambda.runtime.Context;import com.amazonaws.services.lambda.runtime.RequestHandler;import com.amazonaws.services.lambda.runtime.events.KinesisEvent;import com.amazonaws.services.lambda.runtime.events.StreamsEventResponse;import java.io.Serializable;import java.util.ArrayList;import java.util.List;public class ProcessKinesisRecords implements RequestHandler<KinesisEvent, StreamsEventResponse> {    @Override    public StreamsEventResponse handleRequest(KinesisEvent input, Context context) {        List<StreamsEventResponse.BatchItemFailure> batchItemFailures = new ArrayList<>();        String curRecordSequenceNumber = \"\";        for (KinesisEvent.KinesisEventRecord kinesisEventRecord : input.getRecords()) {            try {                //Process your record                KinesisEvent.Record kinesisRecord = kinesisEventRecord.getKinesis();                curRecordSequenceNumber = kinesisRecord.getSequenceNumber();            } catch (Exception e) {                /* Since we are working with streams, we can return the failed item immediately.                   Lambda will immediately begin to retry processing from this failed item onwards. */                batchItemFailures.add(new StreamsEventResponse.BatchItemFailure(curRecordSequenceNumber));                return new StreamsEventResponse(batchItemFailures);            }        }              return new StreamsEventResponse(batchItemFailures);       }}JavaScriptSDK for JavaScript (v3)Note        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Reporting Kinesis batch item failures with Lambda using Javascript.// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.// SPDX-License-Identifier: Apache-2.0exports.handler = async (event, context) => {  for (const record of event.Records) {    try {      console.log(`Processed Kinesis Event - EventID: ${record.eventID}`);      const recordData = await getRecordDataAsync(record.kinesis);      console.log(`Record Data: ${recordData}`);      // TODO: Do interesting work based on the new data    } catch (err) {      console.error(`An error occurred ${err}`);      /* Since we are working with streams, we can return the failed item immediately.            Lambda will immediately begin to retry processing from this failed item onwards. */      return {        batchItemFailures: [{ itemIdentifier: record.kinesis.sequenceNumber }],      };    }  }  console.log(`Successfully processed ${event.Records.length} records.`);  return { batchItemFailures: [] };};async function getRecordDataAsync(payload) {  var data = Buffer.from(payload.data, \"base64\").toString(\"utf-8\");  await Promise.resolve(1); //Placeholder for actual async work  return data;}Reporting Kinesis batch item failures with Lambda using TypeScript.// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.// SPDX-License-Identifier: Apache-2.0import {  KinesisStreamEvent,  Context,  KinesisStreamHandler,  KinesisStreamRecordPayload,  KinesisStreamBatchResponse,} from \"aws-lambda\";import { Buffer } from \"buffer\";import { Logger } from \"@aws-lambda-powertools/logger\";const logger = new Logger({  logLevel: \"INFO\",  serviceName: \"kinesis-stream-handler-sample\",});export const functionHandler: KinesisStreamHandler = async (  event: KinesisStreamEvent,  context: Context): Promise<KinesisStreamBatchResponse> => {  for (const record of event.Records) {    try {      logger.info(`Processed Kinesis Event - EventID: ${record.eventID}`);      const recordData = await getRecordDataAsync(record.kinesis);      logger.info(`Record Data: ${recordData}`);      // TODO: Do interesting work based on the new data    } catch (err) {      logger.error(`An error occurred ${err}`);      /* Since we are working with streams, we can return the failed item immediately.            Lambda will immediately begin to retry processing from this failed item onwards. */      return {        batchItemFailures: [{ itemIdentifier: record.kinesis.sequenceNumber }],      };    }  }  logger.info(`Successfully processed ${event.Records.length} records.`);  return { batchItemFailures: [] };};async function getRecordDataAsync(  payload: KinesisStreamRecordPayload): Promise<string> {  var data = Buffer.from(payload.data, \"base64\").toString(\"utf-8\");  await Promise.resolve(1); //Placeholder for actual async work  return data;}PHPSDK for PHPNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Reporting Kinesis batch item failures with Lambda using PHP.// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.// SPDX-License-Identifier: Apache-2.0<?php# using bref/bref and bref/logger for simplicityuse Bref\\Context\\Context;use Bref\\Event\\Kinesis\\KinesisEvent;use Bref\\Event\\Handler as StdHandler;use Bref\\Logger\\StderrLogger;require __DIR__ . '/vendor/autoload.php';class Handler implements StdHandler{    private StderrLogger $logger;    public function __construct(StderrLogger $logger)    {        $this->logger = $logger;    }    /**     * @throws JsonException     * @throws \\Bref\\Event\\InvalidLambdaEvent     */    public function handle(mixed $event, Context $context): array    {        $kinesisEvent = new KinesisEvent($event);        $this->logger->info(\"Processing records\");        $records = $kinesisEvent->getRecords();        $failedRecords = [];        foreach ($records as $record) {            try {                $data = $record->getData();                $this->logger->info(json_encode($data));                // TODO: Do interesting work based on the new data            } catch (Exception $e) {                $this->logger->error($e->getMessage());                // failed processing the record                $failedRecords[] = $record->getSequenceNumber();            }        }        $totalRecords = count($records);        $this->logger->info(\"Successfully processed $totalRecords records\");        // change format for the response        $failures = array_map(            fn(string $sequenceNumber) => ['itemIdentifier' => $sequenceNumber],            $failedRecords        );        return [            'batchItemFailures' => $failures        ];    }}$logger = new StderrLogger();return new Handler($logger);PythonSDK for Python (Boto3)Note        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Reporting Kinesis batch item failures with Lambda using Python.# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.# SPDX-License-Identifier: Apache-2.0def handler(event, context):    records = event.get(\"Records\")    curRecordSequenceNumber = \"\"        for record in records:        try:            # Process your record            curRecordSequenceNumber = record[\"kinesis\"][\"sequenceNumber\"]        except Exception as e:            # Return failed record's sequence number            return {\"batchItemFailures\":[{\"itemIdentifier\": curRecordSequenceNumber}]}    return {\"batchItemFailures\":[]}RubySDK for RubyNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Reporting Kinesis batch item failures with Lambda using Ruby.# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.# SPDX-License-Identifier: Apache-2.0require 'aws-sdk'def lambda_handler(event:, context:)  batch_item_failures = []  event['Records'].each do |record|    begin      puts \"Processed Kinesis Event - EventID: #{record['eventID']}\"      record_data = get_record_data_async(record['kinesis'])      puts \"Record Data: #{record_data}\"      # TODO: Do interesting work based on the new data    rescue StandardError => err      puts \"An error occurred #{err}\"      # Since we are working with streams, we can return the failed item immediately.      # Lambda will immediately begin to retry processing from this failed item onwards.      return { batchItemFailures: [{ itemIdentifier: record['kinesis']['sequenceNumber'] }] }    end  end  puts \"Successfully processed #{event['Records'].length} records.\"  { batchItemFailures: batch_item_failures }enddef get_record_data_async(payload)  data = Base64.decode64(payload['data']).force_encoding('utf-8')  # Placeholder for actual async work  sleep(1)  dataendRustSDK for RustNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Reporting Kinesis batch item failures with Lambda using Rust.// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.// SPDX-License-Identifier: Apache-2.0use aws_lambda_events::{    event::kinesis::KinesisEvent,    kinesis::KinesisEventRecord,    streams::{KinesisBatchItemFailure, KinesisEventResponse},};use lambda_runtime::{run, service_fn, Error, LambdaEvent};async fn function_handler(event: LambdaEvent<KinesisEvent>) -> Result<KinesisEventResponse, Error> {    let mut response = KinesisEventResponse {        batch_item_failures: vec![],    };    if event.payload.records.is_empty() {        tracing::info!(\"No records found. Exiting.\");        return Ok(response);    }    for record in &event.payload.records {        tracing::info!(            \"EventId: {}\",            record.event_id.as_deref().unwrap_or_default()        );        let record_processing_result = process_record(record);        if record_processing_result.is_err() {            response.batch_item_failures.push(KinesisBatchItemFailure {                item_identifier: record.kinesis.sequence_number.clone(),            });            /* Since we are working with streams, we can return the failed item immediately.            Lambda will immediately begin to retry processing from this failed item onwards. */            return Ok(response);        }    }    tracing::info!(        \"Successfully processed {} records\",        event.payload.records.len()    );    Ok(response)}fn process_record(record: &KinesisEventRecord) -> Result<(), Error> {    let record_data = std::str::from_utf8(record.kinesis.data.as_slice());    if let Some(err) = record_data.err() {        tracing::error!(\"Error: {}\", err);        return Err(Error::from(err));    }    let record_data = record_data.unwrap_or_default();    // do something interesting with the data    tracing::info!(\"Data: {}\", record_data);    Ok(())}#[tokio::main]async fn main() -> Result<(), Error> {    tracing_subscriber::fmt()        .with_max_level(tracing::Level::INFO)        // disable printing the name of the module in every log line.        .with_target(false)        // disabling time is handy because CloudWatch will add the ingestion time.        .without_time()        .init();    run(service_fn(function_handler)).await}anchoranchoranchoranchoranchoranchoranchoranchor.NETGoJavaJavaScriptPHPPythonRubyRustAWS SDK for .NETNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Reporting Kinesis batch item failures with Lambda using .NET.// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.// SPDX-License-Identifier: Apache-2.0﻿using System.Text;using System.Text.Json.Serialization;using Amazon.Lambda.Core;using Amazon.Lambda.KinesisEvents;using AWS.Lambda.Powertools.Logging;// Assembly attribute to enable the Lambda function's JSON input to be converted into a .NET class.[assembly: LambdaSerializer(typeof(Amazon.Lambda.Serialization.SystemTextJson.DefaultLambdaJsonSerializer))]namespace KinesisIntegration;public class Function{    // Powertools Logger requires an environment variables against your function    // POWERTOOLS_SERVICE_NAME    [Logging(LogEvent = true)]    public async Task<StreamsEventResponse> FunctionHandler(KinesisEvent evnt, ILambdaContext context)    {        if (evnt.Records.Count == 0)        {            Logger.LogInformation(\"Empty Kinesis Event received\");            return new StreamsEventResponse();        }        foreach (var record in evnt.Records)        {            try            {                Logger.LogInformation($\"Processed Event with EventId: {record.EventId}\");                string data = await GetRecordDataAsync(record.Kinesis, context);                Logger.LogInformation($\"Data: {data}\");                // TODO: Do interesting work based on the new data            }            catch (Exception ex)            {                Logger.LogError($\"An error occurred {ex.Message}\");                /* Since we are working with streams, we can return the failed item immediately.                   Lambda will immediately begin to retry processing from this failed item onwards. */                return new StreamsEventResponse                {                    BatchItemFailures = new List<StreamsEventResponse.BatchItemFailure>                    {                        new StreamsEventResponse.BatchItemFailure { ItemIdentifier = record.Kinesis.SequenceNumber }                    }                };            }        }        Logger.LogInformation($\"Successfully processed {evnt.Records.Count} records.\");        return new StreamsEventResponse();    }    private async Task<string> GetRecordDataAsync(KinesisEvent.Record record, ILambdaContext context)    {        byte[] bytes = record.Data.ToArray();        string data = Encoding.UTF8.GetString(bytes);        await Task.CompletedTask; //Placeholder for actual async work        return data;    }}public class StreamsEventResponse{    [JsonPropertyName(\"batchItemFailures\")]    public IList<BatchItemFailure> BatchItemFailures { get; set; }    public class BatchItemFailure    {        [JsonPropertyName(\"itemIdentifier\")]        public string ItemIdentifier { get; set; }    }}",
                            "For a complete list of AWS SDK developer guides and code examples, see    Using Lambda with an AWS SDK.    This topic also includes information about getting started and details about previous SDK versions."
                        ]
                    },
                    {
                        "title": "Reporting batch item failures for Lambda functions with a DynamoDB trigger",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_serverless_DynamoDB_Lambda_batch_item_failures_section.html",
                        "sections": [
                            "",
                            "The following code examples show how to implement partial batch response for Lambda functions that receive events from a DynamoDB stream. The function reports the batch item failures in the response, signaling to Lambda to retry those messages later.",
                            ".NETAWS SDK for .NETNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Reporting DynamoDB batch item failures with Lambda using .NET.// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.// SPDX-License-Identifier: Apache-2.0using System.Text.Json;using System.Text;using Amazon.Lambda.Core;using Amazon.Lambda.DynamoDBEvents;// Assembly attribute to enable the Lambda function's JSON input to be converted into a .NET class.[assembly: LambdaSerializer(typeof(Amazon.Lambda.Serialization.SystemTextJson.DefaultLambdaJsonSerializer))]namespace AWSLambda_DDB;public class Function{    public StreamsEventResponse FunctionHandler(DynamoDBEvent dynamoEvent, ILambdaContext context)    {        context.Logger.LogInformation($\"Beginning to process {dynamoEvent.Records.Count} records...\");        List<StreamsEventResponse.BatchItemFailure> batchItemFailures = new List<StreamsEventResponse.BatchItemFailure>();        StreamsEventResponse streamsEventResponse = new StreamsEventResponse();        foreach (var record in dynamoEvent.Records)        {            try            {                var sequenceNumber = record.Dynamodb.SequenceNumber;                context.Logger.LogInformation(sequenceNumber);            }            catch (Exception ex)            {                context.Logger.LogError(ex.Message);                batchItemFailures.Add(new StreamsEventResponse.BatchItemFailure() { ItemIdentifier = record.Dynamodb.SequenceNumber });            }        }        if (batchItemFailures.Count > 0)        {            streamsEventResponse.BatchItemFailures = batchItemFailures;        }        context.Logger.LogInformation(\"Stream processing complete.\");        return streamsEventResponse;    }}GoSDK for Go V2Note        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Reporting DynamoDB batch item failures with Lambda using Go.// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.// SPDX-License-Identifier: Apache-2.0package mainimport (\t\"context\"\t\"github.com/aws/aws-lambda-go/events\"\t\"github.com/aws/aws-lambda-go/lambda\")type BatchItemFailure struct {\tItemIdentifier string `json:\"ItemIdentifier\"`}type BatchResult struct {\tBatchItemFailures []BatchItemFailure `json:\"BatchItemFailures\"`}func HandleRequest(ctx context.Context, event events.DynamoDBEvent) (*BatchResult, error) {\tvar batchItemFailures []BatchItemFailure\tcurRecordSequenceNumber := \"\"\tfor _, record := range event.Records {\t\t// Process your record\t\tcurRecordSequenceNumber = record.Change.SequenceNumber\t}\tif curRecordSequenceNumber != \"\" {\t\tbatchItemFailures = append(batchItemFailures, BatchItemFailure{ItemIdentifier: curRecordSequenceNumber})\t}\t\tbatchResult := BatchResult{\t\tBatchItemFailures: batchItemFailures,\t}\treturn &batchResult, nil}func main() {\tlambda.Start(HandleRequest)}JavaSDK for Java 2.xNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Reporting DynamoDB batch item failures with Lambda using Java.// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.// SPDX-License-Identifier: Apache-2.0import com.amazonaws.services.lambda.runtime.Context;import com.amazonaws.services.lambda.runtime.RequestHandler;import com.amazonaws.services.lambda.runtime.events.DynamodbEvent;import com.amazonaws.services.lambda.runtime.events.StreamsEventResponse;import com.amazonaws.services.lambda.runtime.events.models.dynamodb.StreamRecord;import java.io.Serializable;import java.util.ArrayList;import java.util.List;public class ProcessDynamodbRecords implements RequestHandler<DynamodbEvent, Serializable> {    @Override    public StreamsEventResponse handleRequest(DynamodbEvent input, Context context) {        List<StreamsEventResponse.BatchItemFailure> batchItemFailures = new ArrayList<>();        String curRecordSequenceNumber = \"\";        for (DynamodbEvent.DynamodbStreamRecord dynamodbStreamRecord : input.getRecords()) {          try {                //Process your record                StreamRecord dynamodbRecord = dynamodbStreamRecord.getDynamodb();                curRecordSequenceNumber = dynamodbRecord.getSequenceNumber();                            } catch (Exception e) {                /* Since we are working with streams, we can return the failed item immediately.                   Lambda will immediately begin to retry processing from this failed item onwards. */                batchItemFailures.add(new StreamsEventResponse.BatchItemFailure(curRecordSequenceNumber));                return new StreamsEventResponse(batchItemFailures);            }        }              return new StreamsEventResponse();       }}JavaScriptSDK for JavaScript (v3)Note        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Reporting DynamoDB batch item failures with Lambda using JavaScript.export const handler = async (event) => {  const records = event.Records;  let curRecordSequenceNumber = \"\";  for (const record of records) {    try {      // Process your record      curRecordSequenceNumber = record.dynamodb.SequenceNumber;    } catch (e) {      // Return failed record's sequence number      return { batchItemFailures: [{ itemIdentifier: curRecordSequenceNumber }] };    }  }  return { batchItemFailures: [] };};Reporting DynamoDB batch item failures with Lambda using TypeScript.import {  DynamoDBBatchResponse,  DynamoDBBatchItemFailure,  DynamoDBStreamEvent,} from \"aws-lambda\";export const handler = async (  event: DynamoDBStreamEvent): Promise<DynamoDBBatchResponse> => {  const batchItemFailures: DynamoDBBatchItemFailure[] = [];  let curRecordSequenceNumber;  for (const record of event.Records) {    curRecordSequenceNumber = record.dynamodb?.SequenceNumber;    if (curRecordSequenceNumber) {      batchItemFailures.push({        itemIdentifier: curRecordSequenceNumber,      });    }  }  return { batchItemFailures: batchItemFailures };};PHPSDK for PHPNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Reporting DynamoDB batch item failures with Lambda using PHP.<?php# using bref/bref and bref/logger for simplicityuse Bref\\Context\\Context;use Bref\\Event\\DynamoDb\\DynamoDbEvent;use Bref\\Event\\Handler as StdHandler;use Bref\\Logger\\StderrLogger;require __DIR__ . '/vendor/autoload.php';class Handler implements StdHandler{    private StderrLogger $logger;    public function __construct(StderrLogger $logger)    {        $this->logger = $logger;    }    /**     * @throws JsonException     * @throws \\Bref\\Event\\InvalidLambdaEvent     */    public function handle(mixed $event, Context $context): array    {        $dynamoDbEvent = new DynamoDbEvent($event);        $this->logger->info(\"Processing records\");        $records = $dynamoDbEvent->getRecords();        $failedRecords = [];        foreach ($records as $record) {            try {                $data = $record->getData();                $this->logger->info(json_encode($data));                // TODO: Do interesting work based on the new data            } catch (Exception $e) {                $this->logger->error($e->getMessage());                // failed processing the record                $failedRecords[] = $record->getSequenceNumber();            }        }        $totalRecords = count($records);        $this->logger->info(\"Successfully processed $totalRecords records\");        // change format for the response        $failures = array_map(            fn(string $sequenceNumber) => ['itemIdentifier' => $sequenceNumber],            $failedRecords        );        return [            'batchItemFailures' => $failures        ];    }}$logger = new StderrLogger();return new Handler($logger);PythonSDK for Python (Boto3)Note        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Reporting DynamoDB batch item failures with Lambda using Python.# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.# SPDX-License-Identifier: Apache-2.0def handler(event, context):    records = event.get(\"Records\")    curRecordSequenceNumber = \"\"        for record in records:        try:            # Process your record            curRecordSequenceNumber = record[\"dynamodb\"][\"SequenceNumber\"]        except Exception as e:            # Return failed record's sequence number            return {\"batchItemFailures\":[{\"itemIdentifier\": curRecordSequenceNumber}]}    return {\"batchItemFailures\":[]}RubySDK for RubyNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Reporting DynamoDB batch item failures with Lambda using Ruby.def lambda_handler(event:, context:)    records = event[\"Records\"]    cur_record_sequence_number = \"\"      records.each do |record|      begin        # Process your record        cur_record_sequence_number = record[\"dynamodb\"][\"SequenceNumber\"]      rescue StandardError => e        # Return failed record's sequence number        return {\"batchItemFailures\" => [{\"itemIdentifier\" => cur_record_sequence_number}]}      end    end      {\"batchItemFailures\" => []}  endRustSDK for RustNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Reporting DynamoDB batch item failures with Lambda using Rust.use aws_lambda_events::{    event::dynamodb::{Event, EventRecord, StreamRecord},    streams::{DynamoDbBatchItemFailure, DynamoDbEventResponse},};use lambda_runtime::{run, service_fn, Error, LambdaEvent};/// Process the stream recordfn process_record(record: &EventRecord) -> Result<(), Error> {    let stream_record: &StreamRecord = &record.change;    // process your stream record here...    tracing::info!(\"Data: {:?}\", stream_record);    Ok(())}/// Main Lambda handler here...async fn function_handler(event: LambdaEvent<Event>) -> Result<DynamoDbEventResponse, Error> {    let mut response = DynamoDbEventResponse {        batch_item_failures: vec![],    };    let records = &event.payload.records;    if records.is_empty() {        tracing::info!(\"No records found. Exiting.\");        return Ok(response);    }    for record in records {        tracing::info!(\"EventId: {}\", record.event_id);        // Couldn't find a sequence number        if record.change.sequence_number.is_none() {            response.batch_item_failures.push(DynamoDbBatchItemFailure {                item_identifier: Some(\"\".to_string()),            });            return Ok(response);        }        // Process your record here...        if process_record(record).is_err() {            response.batch_item_failures.push(DynamoDbBatchItemFailure {                item_identifier: record.change.sequence_number.clone(),            });            /* Since we are working with streams, we can return the failed item immediately.            Lambda will immediately begin to retry processing from this failed item onwards. */            return Ok(response);        }    }    tracing::info!(\"Successfully processed {} record(s)\", records.len());    Ok(response)}#[tokio::main]async fn main() -> Result<(), Error> {    tracing_subscriber::fmt()        .with_max_level(tracing::Level::INFO)        // disable printing the name of the module in every log line.        .with_target(false)        // disabling time is handy because CloudWatch will add the ingestion time.        .without_time()        .init();    run(service_fn(function_handler)).await}anchoranchoranchoranchoranchoranchoranchoranchor.NETGoJavaJavaScriptPHPPythonRubyRustAWS SDK for .NETNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Reporting DynamoDB batch item failures with Lambda using .NET.// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.// SPDX-License-Identifier: Apache-2.0using System.Text.Json;using System.Text;using Amazon.Lambda.Core;using Amazon.Lambda.DynamoDBEvents;// Assembly attribute to enable the Lambda function's JSON input to be converted into a .NET class.[assembly: LambdaSerializer(typeof(Amazon.Lambda.Serialization.SystemTextJson.DefaultLambdaJsonSerializer))]namespace AWSLambda_DDB;public class Function{    public StreamsEventResponse FunctionHandler(DynamoDBEvent dynamoEvent, ILambdaContext context)    {        context.Logger.LogInformation($\"Beginning to process {dynamoEvent.Records.Count} records...\");        List<StreamsEventResponse.BatchItemFailure> batchItemFailures = new List<StreamsEventResponse.BatchItemFailure>();        StreamsEventResponse streamsEventResponse = new StreamsEventResponse();        foreach (var record in dynamoEvent.Records)        {            try            {                var sequenceNumber = record.Dynamodb.SequenceNumber;                context.Logger.LogInformation(sequenceNumber);            }            catch (Exception ex)            {                context.Logger.LogError(ex.Message);                batchItemFailures.Add(new StreamsEventResponse.BatchItemFailure() { ItemIdentifier = record.Dynamodb.SequenceNumber });            }        }        if (batchItemFailures.Count > 0)        {            streamsEventResponse.BatchItemFailures = batchItemFailures;        }        context.Logger.LogInformation(\"Stream processing complete.\");        return streamsEventResponse;    }}",
                            "For a complete list of AWS SDK developer guides and code examples, see    Using Lambda with an AWS SDK.    This topic also includes information about getting started and details about previous SDK versions."
                        ]
                    },
                    {
                        "title": "Reporting batch item failures for Lambda functions with an Amazon SQS trigger",
                        "href": "https://docs.aws.amazon.com/lambda/latest/dg/example_serverless_SQS_Lambda_batch_item_failures_section.html",
                        "sections": [
                            "",
                            "The following code examples show how to implement partial batch response for Lambda functions that receive events from an SQS queue. The function reports the batch item failures in the response, signaling to Lambda to retry those messages later.",
                            ".NETAWS SDK for .NETNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Reporting SQS batch item failures with Lambda using .NET.// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.// SPDX-License-Identifier: Apache-2.0using Amazon.Lambda.Core;using Amazon.Lambda.SQSEvents;// Assembly attribute to enable the Lambda function's JSON input to be converted into a .NET class.[assembly: LambdaSerializer(typeof(Amazon.Lambda.Serialization.SystemTextJson.DefaultLambdaJsonSerializer))]namespace sqsSample;public class Function{    public async Task<SQSBatchResponse> FunctionHandler(SQSEvent evnt, ILambdaContext context)    {        List<SQSBatchResponse.BatchItemFailure> batchItemFailures = new List<SQSBatchResponse.BatchItemFailure>();        foreach(var message in evnt.Records)        {            try            {                //process your message                await ProcessMessageAsync(message, context);            }            catch (System.Exception)            {                //Add failed message identifier to the batchItemFailures list                batchItemFailures.Add(new SQSBatchResponse.BatchItemFailure{ItemIdentifier=message.MessageId});             }        }        return new SQSBatchResponse(batchItemFailures);    }    private async Task ProcessMessageAsync(SQSEvent.SQSMessage message, ILambdaContext context)    {        if (String.IsNullOrEmpty(message.Body))        {            throw new Exception(\"No Body in SQS Message.\");        }        context.Logger.LogInformation($\"Processed message {message.Body}\");        // TODO: Do interesting work based on the new message        await Task.CompletedTask;    }}GoSDK for Go V2Note        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Reporting SQS batch item failures with Lambda using Go.// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.// SPDX-License-Identifier: Apache-2.0package mainimport (\t\"context\"\t\"encoding/json\"\t\"fmt\"\t\"github.com/aws/aws-lambda-go/events\"\t\"github.com/aws/aws-lambda-go/lambda\")func handler(ctx context.Context, sqsEvent events.SQSEvent) (map[string]interface{}, error) {\tbatchItemFailures := []map[string]interface{}{}\tfor _, message := range sqsEvent.Records {\t\t\t\tif /* Your message processing condition here */ {\t\t\t\t\t\tbatchItemFailures = append(batchItemFailures, map[string]interface{}{\"itemIdentifier\": message.MessageId})\t\t}\t}\tsqsBatchResponse := map[string]interface{}{\t\t\"batchItemFailures\": batchItemFailures,\t}\treturn sqsBatchResponse, nil}func main() {\tlambda.Start(handler)}JavaSDK for Java 2.xNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Reporting SQS batch item failures with Lambda using Java.// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.// SPDX-License-Identifier: Apache-2.0import com.amazonaws.services.lambda.runtime.Context;import com.amazonaws.services.lambda.runtime.RequestHandler;import com.amazonaws.services.lambda.runtime.events.SQSEvent;import com.amazonaws.services.lambda.runtime.events.SQSBatchResponse; import java.util.ArrayList;import java.util.List; public class ProcessSQSMessageBatch implements RequestHandler<SQSEvent, SQSBatchResponse> {    @Override    public SQSBatchResponse handleRequest(SQSEvent sqsEvent, Context context) {          List<SQSBatchResponse.BatchItemFailure> batchItemFailures = new ArrayList<SQSBatchResponse.BatchItemFailure>();         String messageId = \"\";         for (SQSEvent.SQSMessage message : sqsEvent.getRecords()) {             try {                 //process your message                 messageId = message.getMessageId();             } catch (Exception e) {                 //Add failed message identifier to the batchItemFailures list                 batchItemFailures.add(new SQSBatchResponse.BatchItemFailure(messageId));             }         }         return new SQSBatchResponse(batchItemFailures);     }}JavaScriptSDK for JavaScript (v3)Note        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Reporting SQS batch item failures with Lambda using JavaScript.// Node.js 20.x Lambda runtime, AWS SDK for Javascript V3export const handler = async (event, context) => {    const batchItemFailures = [];    for (const record of event.Records) {        try {            await processMessageAsync(record, context);        } catch (error) {            batchItemFailures.push({ itemIdentifier: record.messageId });        }    }    return { batchItemFailures };};async function processMessageAsync(record, context) {    if (record.body && record.body.includes(\"error\")) {        throw new Error(\"There is an error in the SQS Message.\");    }    console.log(`Processed message: ${record.body}`);}Reporting SQS batch item failures with Lambda using TypeScript.// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.// SPDX-License-Identifier: Apache-2.0import { SQSEvent, SQSBatchResponse, Context, SQSBatchItemFailure, SQSRecord } from 'aws-lambda';export const handler = async (event: SQSEvent, context: Context): Promise<SQSBatchResponse> => {    const batchItemFailures: SQSBatchItemFailure[] = [];    for (const record of event.Records) {        try {            await processMessageAsync(record);        } catch (error) {            batchItemFailures.push({ itemIdentifier: record.messageId });        }    }    return {batchItemFailures: batchItemFailures};};async function processMessageAsync(record: SQSRecord): Promise<void> {    if (record.body && record.body.includes(\"error\")) {        throw new Error('There is an error in the SQS Message.');    }    console.log(`Processed message ${record.body}`);}PHPSDK for PHPNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Reporting SQS batch item failures with Lambda using PHP.// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.// SPDX-License-Identifier: Apache-2.0<?phpuse Bref\\Context\\Context;use Bref\\Event\\Sqs\\SqsEvent;use Bref\\Event\\Sqs\\SqsHandler;use Bref\\Logger\\StderrLogger;require __DIR__ . '/vendor/autoload.php';class Handler extends SqsHandler{    private StderrLogger $logger;    public function __construct(StderrLogger $logger)    {        $this->logger = $logger;    }    /**     * @throws JsonException     * @throws \\Bref\\Event\\InvalidLambdaEvent     */    public function handleSqs(SqsEvent $event, Context $context): void    {        $this->logger->info(\"Processing SQS records\");        $records = $event->getRecords();        foreach ($records as $record) {            try {                // Assuming the SQS message is in JSON format                $message = json_decode($record->getBody(), true);                $this->logger->info(json_encode($message));                // TODO: Implement your custom processing logic here            } catch (Exception $e) {                $this->logger->error($e->getMessage());                // failed processing the record                $this->markAsFailed($record);            }        }        $totalRecords = count($records);        $this->logger->info(\"Successfully processed $totalRecords SQS records\");    }}$logger = new StderrLogger();return new Handler($logger);PythonSDK for Python (Boto3)Note        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Reporting SQS batch item failures with Lambda using Python.# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.# SPDX-License-Identifier: Apache-2.0def lambda_handler(event, context):    if event:        batch_item_failures = []        sqs_batch_response = {}             for record in event[\"Records\"]:            try:                # process message            except Exception as e:                batch_item_failures.append({\"itemIdentifier\": record['messageId']})                sqs_batch_response[\"batchItemFailures\"] = batch_item_failures        return sqs_batch_responseRubySDK for RubyNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Reporting SQS batch item failures with Lambda using Ruby.# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.# SPDX-License-Identifier: Apache-2.0require 'json'def lambda_handler(event:, context:)  if event    batch_item_failures = []    sqs_batch_response = {}    event[\"Records\"].each do |record|      begin        # process message      rescue StandardError => e        batch_item_failures << {\"itemIdentifier\" => record['messageId']}      end    end    sqs_batch_response[\"batchItemFailures\"] = batch_item_failures    return sqs_batch_response  endendRustSDK for RustNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Reporting SQS batch item failures with Lambda using Rust.// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.// SPDX-License-Identifier: Apache-2.0use aws_lambda_events::{    event::sqs::{SqsBatchResponse, SqsEvent},    sqs::{BatchItemFailure, SqsMessage},};use lambda_runtime::{run, service_fn, Error, LambdaEvent};async fn process_record(_: &SqsMessage) -> Result<(), Error> {    Err(Error::from(\"Error processing message\"))}async fn function_handler(event: LambdaEvent<SqsEvent>) -> Result<SqsBatchResponse, Error> {    let mut batch_item_failures = Vec::new();    for record in event.payload.records {        match process_record(&record).await {            Ok(_) => (),            Err(_) => batch_item_failures.push(BatchItemFailure {                item_identifier: record.message_id.unwrap(),            }),        }    }    Ok(SqsBatchResponse {        batch_item_failures,    })}#[tokio::main]async fn main() -> Result<(), Error> {    run(service_fn(function_handler)).await}anchoranchoranchoranchoranchoranchoranchoranchor.NETGoJavaJavaScriptPHPPythonRubyRustAWS SDK for .NETNote        There's more on GitHub. Find the complete example and learn how to set up and run in the        Serverless examples        repository.    Reporting SQS batch item failures with Lambda using .NET.// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.// SPDX-License-Identifier: Apache-2.0using Amazon.Lambda.Core;using Amazon.Lambda.SQSEvents;// Assembly attribute to enable the Lambda function's JSON input to be converted into a .NET class.[assembly: LambdaSerializer(typeof(Amazon.Lambda.Serialization.SystemTextJson.DefaultLambdaJsonSerializer))]namespace sqsSample;public class Function{    public async Task<SQSBatchResponse> FunctionHandler(SQSEvent evnt, ILambdaContext context)    {        List<SQSBatchResponse.BatchItemFailure> batchItemFailures = new List<SQSBatchResponse.BatchItemFailure>();        foreach(var message in evnt.Records)        {            try            {                //process your message                await ProcessMessageAsync(message, context);            }            catch (System.Exception)            {                //Add failed message identifier to the batchItemFailures list                batchItemFailures.Add(new SQSBatchResponse.BatchItemFailure{ItemIdentifier=message.MessageId});             }        }        return new SQSBatchResponse(batchItemFailures);    }    private async Task ProcessMessageAsync(SQSEvent.SQSMessage message, ILambdaContext context)    {        if (String.IsNullOrEmpty(message.Body))        {            throw new Exception(\"No Body in SQS Message.\");        }        context.Logger.LogInformation($\"Processed message {message.Body}\");        // TODO: Do interesting work based on the new message        await Task.CompletedTask;    }}",
                            "For a complete list of AWS SDK developer guides and code examples, see    Using Lambda with an AWS SDK.    This topic also includes information about getting started and details about previous SDK versions."
                        ]
                    }
                ],
                "sections": [
                    "",
                    "The following code examples show how to use Lambda with AWS SDKs.",
                    "  1.Connecting to an Amazon RDS database in a Lambda function",
                    "  2.Invoke a Lambda function from a Kinesis trigger",
                    "  3.Invoke a Lambda function from a DynamoDB trigger",
                    "  4.Invoke a Lambda function from a Amazon DocumentDB trigger",
                    "  5.Invoke a Lambda function from an Amazon MSK trigger",
                    "  6.Invoke a Lambda function from an Amazon S3 trigger",
                    "  7.Invoke a Lambda function from an Amazon SNS trigger",
                    "  8.Invoke a Lambda function from an Amazon SQS trigger",
                    "  9.Reporting batch item failures for Lambda functions with a Kinesis trigger",
                    "  10.Reporting batch item failures for Lambda functions with a DynamoDB trigger",
                    "  11.Reporting batch item failures for Lambda functions with an Amazon SQS trigger"
                ]
            }
        ],
        "sections": [
            "",
            "The following code examples show how to use Lambda with an AWS software development kit (SDK).        ",
            "Basics are code examples that show you how to perform the essential operations within a service.",
            "Actions are code excerpts from larger programs and must be run in context. While actions    show you how to call individual service functions, you can see actions in context in their related scenarios.",
            "Scenarios are code examples that show you how to accomplish specific tasks by    calling multiple functions within a service or combined with other AWS services.",
            "For a complete list of AWS SDK developer guides and code examples, see    Using Lambda with an AWS SDK.    This topic also includes information about getting started and details about previous SDK versions.",
            "Get started",
            {
                "code_example": "namespace LambdaActions;\n\nusing Amazon.Lambda;\n\npublic class HelloLambda\n{\n    static async Task Main(string[] args)\n    {\n        var lambdaClient = new AmazonLambdaClient();\n\n        Console.WriteLine(\"Hello AWS Lambda\");\n        Console.WriteLine(\"Let's get started with AWS Lambda by listing your existing Lambda functions:\");\n\n        var response = await lambdaClient.ListFunctionsAsync();\n        response.Functions.ForEach(function =>\n        {\n            Console.WriteLine($\"{function.FunctionName}\\t{function.Description}\");\n        });\n    }\n}\n\n\n"
            },
            "  1.BasicsHello LambdaLearn the basicsActionsCreateAliasCreateFunctionDeleteAliasDeleteFunctionDeleteFunctionConcurrencyDeleteProvisionedConcurrencyConfigGetAccountSettingsGetAliasGetFunctionGetFunctionConcurrencyGetFunctionConfigurationGetPolicyGetProvisionedConcurrencyConfigInvokeListFunctionsListProvisionedConcurrencyConfigsListTagsListVersionsByFunctionPublishVersionPutFunctionConcurrencyPutProvisionedConcurrencyConfigRemovePermissionTagResourceUntagResourceUpdateAliasUpdateFunctionCodeUpdateFunctionConfiguration",
            "  2.Hello Lambda",
            "  3.Learn the basics",
            "  4.ActionsCreateAliasCreateFunctionDeleteAliasDeleteFunctionDeleteFunctionConcurrencyDeleteProvisionedConcurrencyConfigGetAccountSettingsGetAliasGetFunctionGetFunctionConcurrencyGetFunctionConfigurationGetPolicyGetProvisionedConcurrencyConfigInvokeListFunctionsListProvisionedConcurrencyConfigsListTagsListVersionsByFunctionPublishVersionPutFunctionConcurrencyPutProvisionedConcurrencyConfigRemovePermissionTagResourceUntagResourceUpdateAliasUpdateFunctionCodeUpdateFunctionConfiguration",
            "  5.CreateAlias",
            "  6.CreateFunction",
            "  7.DeleteAlias",
            "  8.DeleteFunction",
            "  9.DeleteFunctionConcurrency",
            "  10.DeleteProvisionedConcurrencyConfig",
            "  11.GetAccountSettings",
            "  12.GetAlias",
            "  13.GetFunction",
            "  14.GetFunctionConcurrency",
            "  15.GetFunctionConfiguration",
            "  16.GetPolicy",
            "  17.GetProvisionedConcurrencyConfig",
            "  18.Invoke",
            "  19.ListFunctions",
            "  20.ListProvisionedConcurrencyConfigs",
            "  21.ListTags",
            "  22.ListVersionsByFunction",
            "  23.PublishVersion",
            "  24.PutFunctionConcurrency",
            "  25.PutProvisionedConcurrencyConfig",
            "  26.RemovePermission",
            "  27.TagResource",
            "  28.UntagResource",
            "  29.UpdateAlias",
            "  30.UpdateFunctionCode",
            "  31.UpdateFunctionConfiguration",
            "  32.ScenariosAutomatically confirm known users with a Lambda functionAutomatically migrate known users with a Lambda functionCreate a REST API to track COVID-19 dataCreate a lending library REST APICreate a messenger applicationCreate a serverless application to manage photosCreate a websocket chat applicationCreate an application to analyze customer feedbackInvoke a Lambda function from a browserTransform data with S3 Object LambdaUse API Gateway to invoke a Lambda functionUse Step Functions to invoke Lambda functionsUse scheduled events to invoke a Lambda functionWrite custom activity data with a Lambda function after Amazon Cognito user authentication",
            "  33.Automatically confirm known users with a Lambda function",
            "  34.Automatically migrate known users with a Lambda function",
            "  35.Create a REST API to track COVID-19 data",
            "  36.Create a lending library REST API",
            "  37.Create a messenger application",
            "  38.Create a serverless application to manage photos",
            "  39.Create a websocket chat application",
            "  40.Create an application to analyze customer feedback",
            "  41.Invoke a Lambda function from a browser",
            "  42.Transform data with S3 Object Lambda",
            "  43.Use API Gateway to invoke a Lambda function",
            "  44.Use Step Functions to invoke Lambda functions",
            "  45.Use scheduled events to invoke a Lambda function",
            "  46.Write custom activity data with a Lambda function after Amazon Cognito user authentication",
            "  47.Serverless examples        Connecting to an Amazon RDS database in a Lambda functionInvoke a Lambda function from a Kinesis triggerInvoke a Lambda function from a DynamoDB triggerInvoke a Lambda function from a Amazon DocumentDB triggerInvoke a Lambda function from an Amazon MSK triggerInvoke a Lambda function from an Amazon S3 triggerInvoke a Lambda function from an Amazon SNS triggerInvoke a Lambda function from an Amazon SQS triggerReporting batch item failures for Lambda functions with a Kinesis triggerReporting batch item failures for Lambda functions with a DynamoDB triggerReporting batch item failures for Lambda functions with an Amazon SQS trigger",
            "  48.Connecting to an Amazon RDS database in a Lambda function",
            "  49.Invoke a Lambda function from a Kinesis trigger",
            "  50.Invoke a Lambda function from a DynamoDB trigger",
            "  51.Invoke a Lambda function from a Amazon DocumentDB trigger",
            "  52.Invoke a Lambda function from an Amazon MSK trigger",
            "  53.Invoke a Lambda function from an Amazon S3 trigger",
            "  54.Invoke a Lambda function from an Amazon SNS trigger",
            "  55.Invoke a Lambda function from an Amazon SQS trigger",
            "  56.Reporting batch item failures for Lambda functions with a Kinesis trigger",
            "  57.Reporting batch item failures for Lambda functions with a DynamoDB trigger",
            "  58.Reporting batch item failures for Lambda functions with an Amazon SQS trigger"
        ]
    }
]